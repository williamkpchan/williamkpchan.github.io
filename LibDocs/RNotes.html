<base target="_blank"><html><head><title>R Notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="../lazyload.min.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script src="D:/Dropbox/Public/commonfunctions.js"></script>
<script src="https://d3js.org/d3.v4.min.js"></script>

<script>
  var showTopicNumber = true;
  var bookid = "R Notes"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:24px;}
h1, h2, h3 {color: gold; display:block;}
strong {color: orange;}
pre{width:80%;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px; background-color:#044;}
</style></head><body onkeypress="chkKey()"><center>
<h1>R Notes</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<a href="../Component Analysis PCA.html">&#x25B2;<b class="redword bordred1 borRad15">Component Analysis PCA</b></a>
<a href="http://uc-r.github.io/ann_classification" class="whitebut ">Classification Artificial Neural Network</a>
<br>
<a href="common R snippets.html" class="whitebut gold limebs">common R snippets</a>
<br>
<a href="https://www.youtube.com/watch?v=EOjObl_GSi4" class="goldbut red whitets bluebs borRad20">larger-than-RAM data manipulation with {disk.frame}</a>

<a href="https://stackoverflow.com/questions/12636764/r-built-in-web-server" class="whitebut ">R built in Web server</a>
<a href="https://github.com/yihui/servr" class="whitebut ">servr</a>
<a href="http://brooksandrew.github.io/simpleblog/articles/advanced-data-table/">Advanced tips and tricks with data.table package</a>


<a href="https://www.youtube.com/watch?v=tfN10IUX9Lo" class="whitebut ">Building Web Application in R Shiny</a>
<a href="https://www.youtube.com/watch?v=aReuLtY0YMI" class="whitebut ">Hadoop In 5 Minutes</a>

<a href="RProgrammingVideos.html" class="redbut gold">R Programming Videos</a>
<a href="RProgrammingMethods.html" class="redbut gold">R Programming Methods</a>

<a href="https://cran.r-project.org/web/packages/" class="whitebut ">Available Packages</a>

<a href="https://www.listendata.com/2016/08/dplyr-tutorial.html" class="whitebut ">DPLYR</a>
<a href="https://www.listendata.com/" class="whitebut ">listendata</a>

data visualization (ggplot2)
data manipulation (dplyr, lubridate, tidyr, stringr, readr, & forcats)
data analysis (combine ggplot2, dplyr to explore data and find insights)
<a href="https://beckmw.wordpress.com/" class="whitebut ">R is my friend</a>
<a href="https://www.youtube.com/user/westlandindia/playlists" class="whitebut ">Dr. Bharatendra Rai</a>
<pre>
<a href="R4DataScience.html" class="whitebut ">R4DataScience</a>

<a href="https://statisticsglobe.com/r-functions-list/" class="whitebut ">R Basic Commands of the</a>

<a href="Non-standard evaluation.html" class="whitebut ">Non-standard evaluation</a>
<a href="https://statisticsglobe.com" class="whitebut ">Statistics Globe</a>
<a href="http://www.datasciencemadesimple.com" class="whitebut ">DataScience Made Simple</a>

<a href="Libraries for Python & R.html">Libraries for Python & R</a>
<a href="sparklyr.html" class="redbut red blueblackgrad">sparklyr</a>

functions on non-tabular data
rlist is a set of tools for working with list objects.
<a href="RList Turorial.html" class="yellowbut gold purpleblackgrad">RList Turorial</a>
<a href="http://joshuamccrain.com/tutorials/web_scraping_R_selenium.html" class="whitebut gold redts">RSelenium Tutorial</a>
<a href="https://github.com/ropensci/RSelenium" class="whitebut gold redts">RSelenium</a>
</pre>

</div>
<pre>
<br>
<br>
<a href="Libraries for Python & R.html">Libraries for Python & R</a>

<h2>Introduction to R</h2>
<a href="Introduction to R.html" class="whitebut ">Introduction to R</a>

<h2>free books for R</h2>
<a href="http://www.cookbook-r.com/" target="_blank">Cookbook for R</a>
<a href="RCookbook.html" target="_blank" class="orangesha">&diams;RCookbook</a>
<a href="https://bookdown.org" class="whitebut " target="_blank">bookdown R books</a>
<a href="https://bookdown.org/home/archive/" class="whitebut " target="_blank">bookdown all books</a>
<a href="https://bookdown.org/home/tags/r-programming/" class="whitebut " target="_blank">bookdown r-programming books</a>
<a href="https://bookdown.org/rdpeng/rprogdatascience/" class="whitebut " target="_blank">R Programming for Data Science</a>

<h2>Data Frame</h2>

Data Frame is a list of vectors of equal length

to create a dataframe:
n = c(2,3,5)
s = c('a','b','c')
b = c(TRUE, FALSE, FALSE)
df = data.frame(n,s,b)

Components of dataframe:
header, column names, data row, name of the row cell
single square bracket "[]", comma

Functions:
nrow(), ncol(), head()

Inport Data:
read.table("mydata.txt")
read.csv("mydata.csv")

retrieve the column vector by the double square bracket or the "$" operator
mtcars[[9]]
mtcars[["am"]]
mtcars$am
mtcars[,"am"]


retrieve a column slice with the single square bracket "[]"
mtcars[1]
mtcars["mpg"]
mtcars[c("mpg", "hp")]

Data frame Row Slice
mtcars[24,]
mtcars[c(3,24),]
mtcars["camaro z28",]
mtcars[c("datsun 710","camaro z28"),]

<h2># MLFundStat and Hangseng Fund Stat</h2>
#=================
MLFundStat.html
the computation is long, it is possible to cut time by adjusting the cutdate variable.
this should be modified to new version using r chart.

<h2># Start Of R</h2>
#=================
Sys.setlocale(category = 'LC_ALL', 'Chinese')

use the .Rprofile.site file to run R commands for all users when their R session starts.
D:\R-3.5.1\etc\Rprofile.site
See: Initialization at startup.

This command could be an environment set:
Sys.setenv(FAME="/opt/fame")

<a href="https://www.rdocumentation.org/packages/base/versions/3.5.0/topics/Startup">Start Of R Initialization</a>
<h2>Startup : Initialization at Start of an R Session</h2>
<h3>Description</h3>
In R, the startup mechanism is as follows.

Unless --no-environ was given on the command line, R searches for site and user files to process for setting environment variables.  
The name of the site file is the one pointed to by the environment variable <code>R_ENVIRON</code>; if this is unset, <code><a href="https://www.rdocumentation.org/link/R_HOME?package=base&version=3.5.0" data-mini-rdoc="base::R_HOME">R_HOME</a>/etc/Renviron.site</code> is used (if it exists, which it does not in a "factory-fresh" installation).  
The name of the user file can be specified by the <code>R_ENVIRON_USER</code> environment variable; if this is unset, the files searched for are <code>.Renviron</code> in the current or in the user's home directory (in that order).  
See "Details" for how the files are read.

Then R searches for the site-wide startup profile file of R code unless the command line option --no-site-file was given.  
The path of this file is taken from the value of the <code>R_PROFILE</code> environment variable (after <a href="https://www.rdocumentation.org/link/tilde%20expansion?package=base&version=3.5.0" data-mini-rdoc="base::tilde expansion">tilde expansion</a>).  
If this variable is unset, the default is <code><a href="https://www.rdocumentation.org/link/R_HOME?package=base&version=3.5.0" data-mini-rdoc="base::R_HOME">R_HOME</a>/etc/Rprofile.site</code>, which is used if it exists (which it does not in a "factory-fresh" installation). (it contains settings from the installer in a "factory-fresh" installation). This code is sourced into the base package.  
Users need to be careful not to unintentionally overwrite objects in base, and it is normally advisable to use <code><a href="https://www.rdocumentation.org/link/local?package=base&version=3.5.0" data-mini-rdoc="base::local">local</a></code> if code needs to be executed: see the examples.

Then, unless --no-init-file was given, R searches for a user profile, a file of R code.  
The path of this file can be specified by the <code>R_PROFILE_USER</code> environment variable (and <a href="https://www.rdocumentation.org/link/tilde%20expansion?package=base&version=3.5.0" data-mini-rdoc="base::tilde expansion">tilde expansion</a> will be performed).  
If this is unset, a file called <code>.Rprofile</code> is searched for in the current directory or in the user's home directory (in that order).  
The user profile file is sourced into the workspace.

Note that when the site and user profile files are sourced only the base package is loaded, so objects in other packages need to be referred to by e.g.<code>utils::dump.frames</code> or after explicitly loading the package concerned.

R then loads a saved image of the user workspace from <code>.RData</code> in the current directory if there is one (unless --no-restore-data or --no-restore was specified on the command line).

Next, if a function <code>.First</code> is found on the search path, it is executed as <code>.First()</code>.  
Finally, function <code>.First.sys()</code> in the base package is run.  
This calls <code><a href="https://www.rdocumentation.org/link/require?package=base&version=3.5.0" data-mini-rdoc="base::require">require</a></code> to attach the default packages specified by <code><a href="https://www.rdocumentation.org/link/options?package=base&version=3.5.0" data-mini-rdoc="base::options">options</a>("defaultPackages")</code>.  
If the methods package is included, this will have been attached earlier (by function <code>.OptRequireMethods()</code>) so that namespace initializations such as those from the user workspace will proceed correctly.

A function <code>.First</code> (and <code><a href="https://www.rdocumentation.org/link/.Last?package=base&version=3.5.0" data-mini-rdoc="base::.Last">.Last</a></code>) can be defined in appropriate <code>.Rprofile</code> or <code>Rprofile.site</code> files or have been saved in <code>.RData</code>.  
If you want a different set of packages than the default ones when you start, insert a call to <code><a href="https://www.rdocumentation.org/link/options?package=base&version=3.5.0" data-mini-rdoc="base::options">options</a></code> in the <code>.Rprofile</code> or <code>Rprofile.site</code> file.  
For example, <code>options(defaultPackages = character())</code> will attach no extra packages on startup (only the base package) (or set <code>R_DEFAULT_PACKAGES=NULL</code> as an environment variable before running R).  
Using <code>options(defaultPackages = "")</code> or <code>R_DEFAULT_PACKAGES=""</code> enforces the R <em>system</em> default.

On front-ends which support it, the commands history is read from the file specified by the environment variable <code>R_HISTFILE</code> (default <code>.Rhistory</code> in the current directory) unless --no-restore-history or --no-restore was specified.

The command-line option --vanilla implies --no-site-file, --no-init-file, --no-environ and (except for <code>R CMD</code>) --no-restore Under Windows, it also implies --no-Rconsole, which prevents loading the <code><a href="https://www.rdocumentation.org/link/Rconsole?package=base&version=3.5.0" data-mini-rdoc="base::Rconsole">Rconsole</a></code> file.
<h3>Arguments</h3><h3>Details</h3>Note that there are two sorts of files used in startup: <em>environment files</em> which contain lists of environment variables to be set, and <em>profile files</em> which contain R code.

Lines in a site or user environment file should be either comment lines starting with <code>#</code>, or lines of the form <code>name=value</code>. The latter sets the environmental variable <code>name</code> to <code>value</code>, overriding an existing value.  
If <code>value</code> contains an expression of the form <code>${foo-bar}</code>, the value is that of the environmental variable <code>foo</code> if that exists and is set to a non-empty value, otherwise <code>bar</code>.  
(If it is of the form <code>${foo}</code>, the default is <code>""</code>.)  This construction can be nested, so <code>bar</code> can be of the same form (as in <code>${foo-${bar-blah}}</code>).  
Note that the braces are essential: for example <code>$HOME</code> will not be interpreted.

Leading and trailing white space in <code>value</code> are stripped. <code>value</code> is then processed in a similar way to a Unix shell: in particular the outermost level of (single or double) quotes is stripped, and backslashes are removed except inside quotes.

On systems with sub-architectures (mainly Windows), the files <code>Renviron.site</code> and <code>Rprofile.site</code> are looked for first in architecture-specific directories, e.g.<code><a href="https://www.rdocumentation.org/link/R_HOME?package=base&version=3.5.0" data-mini-rdoc="base::R_HOME">R_HOME</a>/etc/i386/Renviron.site</code>. And e.g.<code>.Renviron.i386</code> will be used in preference to <code>.Renviron</code>.
<h3>See Also</h3>For the definition of the "home" directory on Windows see the <code>rw-FAQ</code> Q2.14.  
It can be found from a running R by <code>Sys.getenv("R_USER")</code>.

<code><a href="https://www.rdocumentation.org/link/.Last?package=base&version=3.5.0" data-mini-rdoc="base::.Last">.Last</a></code> for final actions at the close of an R session. <code><a href="https://www.rdocumentation.org/link/commandArgs?package=base&version=3.5.0" data-mini-rdoc="base::commandArgs">commandArgs</a></code> for accessing the command line arguments.

There are examples of using startup files to set defaults for graphics devices in the help for <code><a href="https://www.rdocumentation.org/link/windows.options?package=base&version=3.5.0" data-mini-rdoc="base::windows.options">windows.options</a></code>. <code><a href="https://www.rdocumentation.org/link/X11?package=base&version=3.5.0" data-mini-rdoc="base::X11">X11</a></code> and <code><a href="https://www.rdocumentation.org/link/quartz?package=base&version=3.5.0" data-mini-rdoc="base::quartz">quartz</a></code>.

<em>An Introduction to R</em> for more command-line options: those affecting memory management are covered in the help file for <a href="https://www.rdocumentation.org/link/Memory?package=base&version=3.5.0" data-mini-rdoc="base::Memory">Memory</a>.

<code><a href="https://www.rdocumentation.org/link/readRenviron?package=base&version=3.5.0" data-mini-rdoc="base::readRenviron">readRenviron</a></code> to read <code>.Renviron</code> files.

For profiling code, see <code><a href="https://www.rdocumentation.org/link/Rprof?package=base&version=3.5.0" data-mini-rdoc="base::Rprof">Rprof</a></code>.
<h3>Examples</h3>
# NOT RUN {
## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", "onLoad"),       function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First &lt;- function() cat("\n   Welcome to R!\n\n")
.Last &lt;- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({ # add MASS to the default packages, set a CRAN mirror old &lt;- getOption("defaultPackages"); r &lt;- getOption("repos") r["CRAN"] &lt;- "http://my.local.cran" options(defaultPackages = c(old, "MASS"), repos = r) ## (for Unix terminal users) set the width from COLUMNS if set cols &lt;- Sys.getenv("COLUMNS") if(nzchar(cols)) options(width = as.integer(cols)) # interactive sessions get a fortune cookie (needs fortunes package) if (interactive())   fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), "\n")
# coo\bardoh\exabc"def'
# }

<h2># Encoding Problems</h2>
<code>To write text UTF8 encoding on Windows</code>
Firstly, set encoding
options(encoding = "UTF-8")

To write text UTF8 encoding on Windows one has to use the <b class="gold embossts redbs borRad10">useBytes=T</b> options in functions like writeLines or readLines:

txt &lt;- "在"
<code>writeLines(txt, "test.txt", useBytes=T)

readLines("test.txt", encoding="UTF-8")</code>
[1] "在"

<code>writeLines(wholePage, theFilename, useBytes=T)</code>
The UTF-8 BOM is a sequence of bytes at the start of a text stream
(0xEF, 0xBB, 0xBF) that allows the reader to more reliably guess a file as being encoded in UTF-8.

Normally, the BOM is used to signal the endianness of an encoding, but since endianness is irrelevant to UTF-8, the BOM is unnecessary.

According to the Unicode standard, the BOM for UTF-8 files is not recommended

#=================
# Encoding Problems
Sys.getlocale()
getOption("encoding")
options(encoding = "UTF-8")
Encoding(txtstring) &lt;- "UTF-8"
Encoding(txtstring)
txtstring
Sys.setlocale
Sys.setlocale(category = 'LC_ALL', 'Chinese')
Sys.setlocale(category = "LC_ALL", locale = "chs") 
Sys.setlocale(category = "LC_ALL", locale = "cht") # fanti

Note: 
default: options("encoding" = "native.enc")
statTxtFile = "test.txt"
write("建设银行", statTxtFile, append=TRUE)
result file is ansi

add:
options("encoding" = "UTF-8")
write("建设银行", statTxtFile, append=TRUE)
result file is utf-8

mytext &lt;- "this is my text"
Encoding(mytext)

options(encoding = "UTF-8")
getOption("encoding")

options(encoding='native.enc')
getOption("encoding")


iconvlist()
theHeader = "http://qt.gtimg.cn/r=2&q=r_hk"
onecode = "02009"
con = url(paste0(theHeader,onecode), encoding = "GB2312")
thepage=readLines(con)
close(con)
Info=unlist(strsplit(thepage,"~"))
codename=Info[2]
codename
Encoding(codename)

==================
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
readLines(filename, encoding="UTF-8")
readLines(con = stdin(), n = -1L, ok = TRUE, warn = TRUE, encoding = "unknown", skipNul = FALSE)

<span class="redword"># note! the chiname encoding is ok inside R, but will be wrong when write to file by local pc locale, to solve the problem, set Sys.setlocale(category = 'LC_ALL', 'Chinese') </span>

readLines(con &lt;- file("Unicode.txt", encoding = "UCS-2LE"))
close(con)
unique(Encoding(A)) # will most likely be UTF-8
==================
guess_encoding(pageHeader)
pageHeader = repair_encoding(pageHeader, from="utf-8")
pageHeader = repair_encoding(pageHeader, "UTF-8")

iconv(pageHeader, to="UTF-8")
Encoding(pageHeader) &lt;- "UTF-8"

Sys.getlocale("LC_ALL")
https://rpubs.com/mauriciocramos/encoding
==================

Read text as UTF-8 encoding

the following reads in encoding twice and works but reasons unknown
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
[1] "Zürich"

==================
the page source claim to be using UTF-8 encoding:
meta http-equiv="Content-Type" content="text/html; charset=utf-8"

So, the question is, are they really using a different enough encoding, 
or can we just convert to utf-8, guessing that any errors will be negligible?

A quick and dirty approach just force utf-8 using iconv:

TV_Audio_Video &lt;- read_html(iconv(page_source[[1]], to = "UTF-8"), encoding = "utf8")

In general, this is a bad idea - better to specify the encoding it's from.
In this case, maybe the error is theirs, so this quick and dirty approach might be ok.


<h2>to remove leading zeros</h2>
substr(t,regexpr("[^0]",t),nchar(t))

<h2>Pop up message in windows 8.1</h2>
use the tcl/tk package in R to create a messageBox. 
Here is a very simple example:

require(tcltk)
tkmessageBox(title = "Title of message box",
                       message = "Hello, world!", icon = "info", type = "ok")

library(tcltk)
tk_messageBox(type='ok',message='I am a tkMessageBox!')

different types of messagebox (yesno, okcancel, etc).
See ?tk_messageBox.


or
use cmd
system('CMD /C "ECHO The R process has finished running && PAUSE"', 

or
use hta

in one line:
mshta "about:&lt;script>alert('Hello, world!');close()&lt;/script>"
or
mshta "javascript:alert('message');close()"
or
mshta.exe vbscript:Execute("msgbox ""message"",0,""title"":close")


mshta "about:&lt;script src='file://%~f0'>&lt;/script>&lt;script>close()&lt;/script>" %*

msg = paste0(
'mshta ',
"\"about:&lt;script>alert('Hello, world!');close()&lt;/script>\""
)

to show web page, use script to create

#=================
Pop up message in windows 8.1
c.bat:  start MessageBox.vbs "This will be shown in a popup."

MessageBox.vbs :
Set objArgs = WScript.Arguments
messageText = objArgs(0)
MsgBox messageText

in fact, save a file named test.vbs with content:
MsgBox "some message"

double click the file will run directly


# options("scipen"=999)
# format(xx, scientific=F)
# options("scipen"=100, "digits"=4)
# getOption("scipen")
# or as.integer(functionResult);

df &lt;- data.frame(matrix(ncol = 10000, nrow = 0))
colnames(df) &lt;- c("a", "b," "c")
rm(list=ls())
Extracting a Single, Simple Table
The first step is to load the ¡§XML¡¨ package, 
then use the htmlParse() function to read the html document into an R object, 
and readHTMLTable() to read the table(s) in the document. 
The length() function indicates there is a single table in the document, simplifying our work.

The plot3d() function in the rgl package
library(rgl)
open3d()
attach(mtcars)
plot3d(disp,wt,mpg, col = rainbow(10))






<h2>library(stringr)</h2>
#============
library(stringr)
library(htmltools)
library(threejs)
data(mtcars)
data &lt;- mtcars[order(mtcars$cyl),]
uv &lt;- tabulate(mtcars$cyl)
col &lt;- c(rep("red",uv[4]),rep("yellow",uv[6]),rep("blue",uv[8]))
row.names(mtcars)
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")

tabulate(bin, nbins = max(1, bin, na.rm = TRUE))
tabulate takes the integer-valued vector bin and counts the number of times each integer occurs in it.

tabulate(c(2,3,3,5), nbins = 10)
[1] 0 1 2 0 1 0 0 0 0 0

table(c(2,3,3,5))
2 3 5 
1 2 1 

tabulate(c(-2,0,2,3,3,5))  # -2 and 0 are ignored
[1] 0 1 2 0 1

tabulate(c(-2,0,2,3,3,5), nbins = 3)
[1] 0 1 2

tabulate(factor(letters[1:10])
[1] 1 1 1 1 1 1 1 1 1 1




<h2>scatterplot3js(data[,c(3,6,1)],</h2>
#============
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")
# Gumball machine
N &lt;- 100
i &lt;- sample(3, N, replace=TRUE)
x &lt;- matrix(rnorm(N*3),ncol=3)
lab &lt;- c("small", "bigger", "biggest")
scatterplot3js(x, color=rainbow(N), labels=lab[i],
               size=i, renderer="canvas")
# Example 1 from the scatterplot3d package (cf.)
z &lt;- seq(-10, 10, 0.1)
x &lt;- cos(z)
y &lt;- sin(z)
scatterplot3js(x,y,z, color=rainbow(length(z)),
   labels=sprintf("x=%.2f, y=%.2f, z=%.2f", x, y, z))
# Interesting 100,000 point cloud example, should run this with WebGL!
N1 &lt;- 10000
N2 &lt;- 90000
x &lt;- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
y &lt;- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
z &lt;- c(rnorm(N1, sd=0.5), rpois(N2, lambda=20)-20)
col &lt;- c(rep("#ffff00",N1),rep("#0000ff",N2))
scatterplot3js(x,y,z, color=col, size=0.25)
cat("\014")	CLS Screen
#
match returns a vector of the positions
v1 &lt;- c("a","b","c","d")
v2 &lt;- c("g","x","d","e","f","a","c")
x &lt;- match(v1,v2)
6 NA  7  3
v1 %in% v2
TRUE FALSE  TRUE  TRUE
x &lt;- match(v1,v2,nomatch=-1)
6 -1  7  3
%in% returns a logical vector indicating if there is a match or not






<h2>this check whether an element is inside a group</h2>
#=============
this check whether an element is inside a group
v &lt;- c('a','b','c','e')
'b' %in% v





<h2>check vector includes in 31:37 %in% 0:36</h2>
#=============
31:37 %in% 0:36
#
dmInfo=data.matrix(Info)	# convert dataframe to matrix, but the row and column is exchanged
#
bob &lt;- data.frame(lapply(bob, as.character), stringsAsFactors=FALSE)	#Change numeric to characters
#
write.csv(Info,quote=FALSE, row.names = FALSE)	# write csv is the proper way to write the datafile
#

attach an excel file in R:
1: Install packages XLConnect and foreign and run both libraries
2: abcd &lt;- readWorksheet(loadWorkbook('file extension'),sheet=1)
#
allocate vector of size 1.7 Gb
Try memory.limit() for the current memory limit Use memory.limit (size=50000) to increase memory limit. Try using a cloud based environment, 
try using package slam
use factors 

Concatenate and Split Strings in R
==================================
use the paste() function to concatenate
strsplit() function to split
pangram &lt;- "The quick brown fox jumps over the lazy dog"
strsplit(pangram, " ")
"The"  "quick" "brown" "fox"  "jumps" "over" "the"  "lazy" "dog"

the unique elements
unique() function
unique(tolower(words))
"the"  "quick" "brown" "fox"  "jumps" "over" "lazy" "dog"

# <span class="gold">find duplicates</span>
# the intersect function is used for different set, not in inside a vector
# instead, use the duplicated function will be OK.

words = unlist(strsplit(pangram, " "))
words = tolower(words)
duplicated(words)
words[duplicated(words)]

arr = sample(1:36,6,replace=TRUE)
cat(arr, "\n")
arr[duplicated(arr)]


R split Function
================
split() function divides the data in a vector. 
unsplit() funtion do the reverse.
split(x, f, drop = FALSE, ...)
split(x, f, drop = FALSE, ...) &lt;- value
unsplit(value, f, drop = FALSE)
x: vector, data frame
f: indices
drop: discard non existing levels or not


<h2>list objects in the working environment</h2>
ls()
data() will give you a list of the datasets of all loaded packages
help(package = "datasets")

show structure of datasets
dataStr = function(package="datasets", ...)
  {
  d = data(package=package, envir=new.env(), ...)$results[,"Item"]
  d = sapply(strsplit(d, split=" ", fixed=TRUE), "[", 1)
  d = d[order(tolower(d))]
  for(x in d){ message(x, ":  ", class(get(x))); message(str(get(x)))}
  }
dataStr()




<h2>x &lt;- read.csv("anova.csv",header=T,sep=",")</h2>
#=============
x &lt;- read.csv("anova.csv",header=T,sep=",")
Subtype,Gender,Expression
A,m,-0.54
A,m,-0.8
Split the "Expression" values into two groups based on "Gender" variable, 
"f" for female group, and 
"m" for male group:
>g &lt;- split(x$Expression, x$Gender)
>g
$f
  [1] -0.66 -1.15 -0.30 -0.40 -0.24 -0.92  0.48 -1.68 -0.80 -0.55 -0.11 -1.26
$m
  [1] -0.54 -0.80 -1.03 -0.41 -1.31 -0.43  1.01  0.14  1.42 -0.16  0.15 -0.62

Calculate the length, mean value of each group:
sapply(g,length)
  f   m 
135 146 
sapply(g,mean)
         f          m 
-0.3946667 -0.2227397

You may use lapply, return is a list:
lapply(g,mean)
unsplit() function combines the groups:
unsplit(g,x$Gender)

<h2><span class="blink red">Apply</span></h2>
=====
m &lt;- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=30, ncol=3)
apply(m, 1, mean)
a 1 in the second argument, giving the mean of each row. 
apply(m, 2, mean)giving the mean of each column. 
apply(m, 2, function(x) length(x[x&lt;0]))	# count -ve values
apply(m, 2, function(x) is.matrix(x))
apply(m, 2, is.vector)
apply(m, 2, function(x) mean(x[x>0]))

#=========
ma &lt;- matrix(c(1:4, 1, 6:8), nrow = 2)

apply(ma, 1, table)

apply(ma, 1, stats::quantile)
apply(ma, 2, mean)

apply(m, 2, function(x) length(x[x&lt;0]))

sapply lapply rollapply
sapply(1:3, function(x) x^2)

lapply return a list:
lapply(1:3, function(x) x^2)
use unlist with lapply to get a vector

sapply(1:3, function(x, y) mean(y[,x]), y=m)

A&lt;-matrix(1:9, 3,3)
B&lt;-matrix(4:15, 4,3)
C&lt;-matrix(8:10, 3,2)
MyList&lt;-list(A,B,C)
Z=sapply(MyList,"[", 1,1 )

#==========
te=matrix(1:20,nrow=2)
sapply(te,mean)	# this is a vector, order arrange in matrix direction
matrix(sapply(te,mean),nrow=2)	# this is changed to matrix

subset()
apply()
sapply()
lapply()
tapply()
aggregate()
apply 	apply a function to the rows or columns of a matrix
M &lt;- matrix(seq(1,16), 4, 4)
apply(M, 1, min)
lapply 	apply a function to each element of a list in turn and get a list back
x &lt;- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
sapply 	apply a function to each element of a list in turn, but you want a vector back
x &lt;- list(a = 1, b = 1:3, c = 10:100)
sapply(x, FUN = length)  
vapply 	squeeze some more speed out of sapply
x &lt;- list(a = 1, b = 1:3, c = 10:100)
vapply(x, FUN = length, FUN.VALUE = 0L) 

mapply 	apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in sapply

Note: 
mApply(X, INDEX, FUN, …, simplify=TRUE, keepmatrix=FALSE)
from Hmisc package

is different from 
mapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)


Examples

#Sums the 1st elements, the 2nd elements, etc. 
mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15

mapply(rep, 1:4, 4:1)
mapply(rep, times = 1:4, x = 4:1)
mapply(rep, times = 1:4, MoreArgs = list(x = 42))
mapply(function(x, y) seq_len(x) + y,
       c(a =  1, b = 2, c = 3),  # names from first
       c(A = 10, B = 0, C = -10))
word &lt;- function(C, k) paste(rep.int(C, k), collapse = "")
utils::str(mapply(word, LETTERS[1:6], 6:1, SIMPLIFY = FALSE))

mapply(function(x,y){x^y},x=c(2,3),y=c(3,4))
8 81

values1 &lt;- list(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))
values2 &lt;- list(a = c(10, 11, 12), b = c(13, 14, 15), c = c(16, 17, 18)) 
mapply(function(num1, num2) max(c(num1, num2)), values1, values2)
 a  b  c 
12 15 18 



Map 	A wrapper to mapply with SIMPLIFY = FALSE, so it is guaranteed to return a list
rapply	For when you want to apply a function to each element of a nested list structure, recursively
tapply	For when you want to apply a function to subsets of a vector and the subsets are defined by some other vector, usually a factor
lapply is a list apply which acts on a list or vector and returns a list.
sapply is a simple lapply (function defaults to returning a vector or matrix when possible)
vapply is a verified apply (allows the return object type to be prespecified)
rapply is a recursive apply for nested lists, i.e. lists within lists
tapply is a tagged apply where the tags identify the subsets
apply is generic: applies a function to a matrix's rows or columns
by	a "wrapper" for tapply. The power of by arises when we want to compute a task that tapply can't handle
aggregate can be seen as another a different way of use tapply if we use it in such a way


xx = c(1,3,5,7,9,8,6,4,2,1,5)
duplicated(xx)
xx[duplicated(xx)]

Accessing dataframe by names:
mtcars["mpg"]
QueueNo = 12
mtcars[QueueNo,"mpg"]

some functions to remember
charToRaw(key)
as.raw(key)

A motion chart is a dynamic chart to explore several indicators over time. 
subset(airquality, Temp > 80, select = c(Ozone, Temp))
subset(airquality, Day == 1, select = -Temp)
subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp > 80))
 ## sometimes requiring a logical 'subset' argument is a nuisance nm &lt;- rownames(state.x77) start_with_M &lt;- nm %in% grep("^M", nm, value = TRUE)
subset(state.x77, start_with_M, Illiteracy:Murder) # but in recent versions of R this can simply be
subset(state.x77, grepl("^M", nm), Illiteracy:Murder)

join 3 dataframes
library("plyr")
join() function
names(gdp)[3] &lt;- "GDP"
names(life_expectancy)[3] = "LifeExpectancy"
names(population)[3] = "Population"
gdp_life_exp &lt;- join(gdp, life_expectancy)
development &lt;- join(gdp_life_exp, population)

subset() function
dev_2005 &lt;- subset(development, Year == 2005)
dev_2005_big &lt;- subset(dev_2005, GDP >= 30000)

development_motion &lt;- subset(development_complete, Country %in% selection)
library(googleVis)
gvisMotionChart() function
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year")
plot(motion_graph)
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "GDP", yvar = "LifeExpectancy", sizevar = "Population")
development_motion$logGDP &lt;- log(development_motion$GDP)
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "logGDP", yvar = "LifeExpectancy", sizevar = "Population")

my_list[[1]] extracts the first element of the list my_list, and my_list[["name"]] extracts the element in my_list that is called name. 
If the list is nested you can travel down the heirarchy by recursive subsetting. 
mylist[[1]][["name"]] is the element called name inside the first element of my_list.
A data frame is just a special kind of list, so you can use double bracket subsetting on data frames too. 
my_df[[1]] will extract the first column of a data frame and my_df[["name"]] will extract the column named name from the data frame.
names() and str() is a great way to explore the structure of a list.

i in 1:ncol(df)
This is a pretty common model for a sequence: a sequence of consecutive integers designed to index over one dimension of our data.
What might surprise you is that this isn't the best way to generate such a sequence, especially when you are using for loops inside your own functions. Let's look at an example where df is an empty data frame:
df &lt;- data.frame()
1:ncol(df)
for (i in 1:ncol(df)) {
  print(median(df[[i]]))
}
Our sequence is now the somewhat non-sensical: 1, 0. You might think you wouldn't be silly enough to use a for loop with an empty data frame, but once you start writing your own functions, there's no telling what the input will be.
A better method is to use the seq_along() function.
if you grow the for loop at each iteration (e.g. using c()), your for loop will be very slow.
A general way of creating an empty vector of given length is the vector() function. 
It has two arguments: the type of the vector ("logical", "integer", "double", "character", etc.) and the length of the vector.
Then, at each iteration of the loop you must store the output in the corresponding entry of the output vector, i.e. assign the result to output[[i]]. (You might ask why we are using double brackets here when output is a vector. It's primarily for generalizability: this subsetting will work whether output is a vector or a list.)

A time series can be thought of as a vector or matrix of numbers, 
along with some information about what times those numbers were recorded. This information is stored in a ts object in R.
read in some time series data from an xlsx file using read_excel(), 
a function from the readxl package, 
and store the data as a ts object.
Use the read_excel() function to read the data from "exercise1.xlsx" into mydata.
mydata &lt;- read_excel("exercise1.xlsx")
Create a ts object called myts using the ts() function. 
myts &lt;- ts(mydata[,2:4], start = c(1981, 1), frequency = 4)

The first step in any data analysis task is to plot the data. 
Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. 
The features that you see in the plots must then be incorporated into the forecasting methods that you use. 
Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.
You will use the autoplot() function to produce time plots of the data. 
In each plot, look out for outliers, seasonal patterns, and other interesting features.
Use which.max() to spot the outlier in the gold series. 

library("fpp2")
autoplot(a10)
ggseasonplot(a10)
An interesting variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal. 
ggseasonplot(a10, polar = TRUE)
beer &lt;- window(a10, start=1992)
autoplot(beer)
ggseasonplot(beer)
Use the window() function to consider only the ausbeer data from 1992 and save this to beer. 
Set a keyword start to the appropriate year.

x &lt;- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});





<h2>x &lt;- c(sort(sample(1:20, 9)), NA)</h2>
#===================
x &lt;- c(sort(sample(1:20, 9)), NA)
y &lt;- c(sort(sample(3:23, 7)), NA)
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)

alist = readLines("alist.txt")
blist = readLines("blist.txt")
out = setdiff(blist, alist)

writeClipboard(out)





<h2># To skip 3rd iteration and go to next iteration</h2>
#===================
# To skip 3rd iteration and go to next iteration
for(n in 1:5) {
  if(n==3) next
  cat(n)
}





<h2>googleVis chart</h2>
#===================
googleVis chart
===============
library(googleVis)

Line chart
==========
df=data.frame(country=c("US", "GB", "BR"), 
              val1=c(10,13,14), val2=c(23,12,32))
Line &lt;- gvisLineChart(df)
plot(Line)

Scatter chart
=======================
# example 1
dat &lt;- data.frame(x=c(1,2,3,4,5), y1=c(0,3,7,5,2), y2=c(1,NA,0,3,2))
plot(gvisScatterChart(dat, options=list(lineWidth=2, pointSize=2, width=900, height=600)))

# example 2, women
Scatter &lt;- gvisScatterChart(women, 
               options=list(
                 legend="none", lineWidth=1, pointSize=2,
                 title="Women", vAxis="{title:'weight (lbs)'}",
                 hAxis="{title:'height (in)'}", width=900, height=600)
           )
plot(Scatter)

# example 3
ex3dat &lt;- data.frame(x=c(1,2,3,4,5,6,7,8), y1=c(0,3,7,5,2,0,8,6), y2=c(1,NA,0,3,2,6,4,2))
ex3 &lt;- gvisScatterChart(ex3dat, 
           options=list(
             legend="none", lineWidth=1, pointSize=2,
             title="ex3", vAxis="{title:'weight (lbs)'}",
             hAxis="{title:'height (in)'}", width=900, height=600)
       )
plot(ex3)
# Note: to plot timeline chart, arrange the time in x axis, beginning with -ve and the last is 1 to show the sequence


<h2>cat to a file using file(filename, open = "a")</h2>
cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "data.txt", sep = "\n")

<h3>cat append to a file, open file in "a" mode</h3>
#===================
textVector = c("First thing","Second thing","c")

catObj &lt;- file("theappend.txt", open = "a")
cat(textVector, file = catObj, sep="\n")
close(catObj)






<h2>install.packages("readr")</h2>
#===================
install.packages("readr")
library(readr)

to read rectangular data (like csv, tsv, and fwf)
readr is part of the core tidyverse
library(tidyverse)

readr supports seven file formats with seven read_ functions:

read_csv(): comma separated (CSV) files
read_tsv(): tab separated files
read_delim(): general delimited files
read_fwf(): fixed width files
read_table(): tabular files where columns are separated by white-space.
read_log(): web log files





<h2>iconv(keyword, "unknown", "GB2312")</h2>
#===================
iconv(keyword, "unknown", "GB2312")






<h2>Grabbing HTML Tags</h2>
#==========
Grabbing HTML Tags

\b[^>]*>(.*?) matches the opening and closing pair of a specific HTML tag. 

Anything between the tags is captured into the first backreference. 
The question mark in the regex makes the star lazy, to make sure it stops before the first closing tag rather than before the last, like a greedy star would do. 
This regex will not properly match tags nested inside themselves, like in one two one.

<([A-Z][A-Z0-9]*)\b[^>]*>(.*?)</\1> will match the opening and closing pair of any HTML tag. 
Be sure to turn off case sensitivity. 
The key in this solution is the use of the backreference \1 in the regex. 
Anything between the tags is captured into the second backreference. 
This solution will also not match tags nested in themselves






<h2>find the new item</h2>
#==========
find the new item

theList = c("00700","02318","02007")
newList=c("03333","01398","02007")

newList[!(newList %in% theList)]





<h2>formating numbers</h2>
#==========
formating numbers
a &lt;- seq(1,101,25)
sprintf("%03d", a)

format(round(a, 2), nsmall = 2)





<h2>the match function:</h2>
#==========
the match function:
match(x, table, nomatch = NA_integer_, incomparables = NULL)
%in%
match returns a vector of the positions of (first) matches of its first argument in its second.

Corpus&lt;- c('animalada', 'fe', 'fernandez', 'ladrillo')
Lexicon&lt;- c('animal', 'animalada', 'fe', 'fernandez', 'ladr', 'ladrillo')
Lexicon %in% Corpus

Lexicon[Lexicon %in% Corpus]





<h2>Machine Learning:</h2>
<a href="https://www.youtube.com/watch?v=atiYXm7JZv0" class="redbut white bluebs">Machine Learning with R and TensorFlow</a>

<a href="https://machinelearningmastery.com/machine-learning-in-r-step-by-step/">machine-learning-in-r-step-by-step</a>
<a href="https://lgatto.github.io/IntroMachineLearningWithR/index.html">An Introduction to Machine Learning with R</a>
<a href="https://www.r-bloggers.com/image-recognition-tutorial-in-r-using-deep-convolutional-neural-networks-mxnet-package/">mxnet</a>
<a href="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/">image classification</a>
<a href="https://www.youtube.com/watch?v=iExh0qj2Ouo" class="whitebut ">Image Recognition & Classification with Keras</a>

#==========
Machine Learning:

The caret package

Caret contains wrapper functions that allow you to use the exact same functions for training and predicting with dozens of different algorithms. On top of that, it includes sophisticated built-in methods for evaluating the effectiveness of the predictions you get from the model. 

Use The Titanic dataset

Training a model
training a bunch of different decision trees and having them vote 
Random forests work pretty well in *lots* of different situations, so I often try them first.

Evaluating the model

Cross-validation is a way to evaluate the performance of a model without needing any other data than the training data. 

Making predictions on the test set

Improving the model







<h2>to handle error 404 when scraping: use tryCatch()</h2>
#==========
to handle error 404 when scraping: use tryCatch()

for (i in urls) {
    tmp &lt;- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}
#==========
try(readLines(url), silent = TRUE)

tryCatch(readLines(url), error = function (e) conditionMessage(e))






<h2>write.table</h2>
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
#==========
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
            eol = "\n", na = "NA", dec = ".", row.names = FALSE,
            col.names = FALSE, qmethod = c("escape", "double"),
            fileEncoding = "")

write.table(finalTableList, theOutputname, row.names=FALSE, col.names=FALSE, quote = FALSE, sep = "\t" )






<h2>Four normal distribution functions:</h2>
#==========
Four normal distribution functions:

<a href="https://www.r-bloggers.com/normal-distribution-functions/">Four normal distribution functions:</a>

RNORM	Generates random numbers from normal distribution	
rnorm(n, mean, sd)
rnorm(1000, 3, .25)	Generates 1000 numbers from a normal with mean 3 and sd=.25

DNORM	Probability Density Function(PDF)
dnorm(x, mean, sd)
dnorm(0, 0, .5)	Gives the density (height of the PDF) of the normal with mean=0 and sd=.5. 

	
dnorm returns the value of the normal distribution given parameters for x, μ, and σ.
# x = 0, mu = 0 and sigma = 0

dnorm(0, mean = 0, sd = 1)
dnorm(1, mean = 1.2, sd = 0.5)  # result: 0.7365403

change x to dataset
dataset = seq(-3, 3, by = .1)
dvalues = dnorm(dataset)
plot(dvalues,      # y = values and x = index
     xaxt = "n",   # Don't label the x-axis
     type = "l",   # Make it a line plot
     main = "pdf of the Standard Normal",
     xlab= "Data Set") 

compare the data with dnorm:
dataset = c( 5, 1,2,5,3,5,6,4,7,4,5,4,8,6,3,3,6,5,4,3,4,3,4,3)
plot(dvalues,      # y = values and x = index
     xaxt = "n",   # Don't label the x-axis
     type = "l",   # Make it a line plot
     main = "pdf of the Standard Normal",
     xlab= "Data Set") 

to create a dnorm of a dataset to compare with current dataset
make a cut index
cutindex = seq(min(dataset),max(dataset),length = 10)
yfit = dnorm(cutindex, mean=mean(dataset), sd=sd(dataset))
lines(cutindex, yfit)


# Kernel Density Plot
d = density(mtcars$mpg) # returns the density data
plot(d) # plots the results

# Filled Density Plot
d = density(mtcars$mpg)
plot(d, main="Kernel Density of Miles Per Gallon")
polygon(d, col="red", border="blue")

Kernel density estimation is a technique that let's you create a smooth curve given a set of data.

PNORM	Cumulative Distribution Function
(CDF)	pnorm(q, mean, sd)
pnorm(1.96, 0, 1)	Gives the area under the standard normal curve to the left of 1.96, i.e. ~0.975

QNORM	Quantile Function – inverse of
pnorm	qnorm(p, mean, sd)
qnorm(0.975, 0, 1)	Gives the value at which the CDF of the standard normal is .975, i.e. ~1.96

Note that for all functions, leaving out the mean and standard deviation would result in default values of mean=0 and sd=1, a standard normal distribution.






<h2>pnorm students scoring higher than 84</h2>
#==========
pnorm students scoring higher than 84
> pnorm(84, mean=72, sd=15.2, lower.tail=FALSE) 
[1] 0.21492
Answer
The percentage of students scoring 84 or more in the college entrance exam is 21.5%.






<h2>plot a histogram of 1000</h2>
draws from a normal distribution with mean 10, standard deviation 2.
#==========
plot a histogram of 1000 draws from a normal distribution with mean 10, standard deviation 2.
set.seed(seed)
x = rnorm(1000, 10, 2)
plot(x)
hist(x)

Using a QQ plot. Assess the normality:
qqnorm(x)
qqline(x)

In statistics, a Q–Q (quantile-quantile) plot is a probability plot, 
which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.
First, the set of intervals for the quantiles is chosen. 
A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). 
Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.






<h2>format leading zeros</h2>
#==========
format leading zeros

formatC(1, width = 2, format = "d", flag = "0")
"01"
formatC(125, width = 5, format = "d", flag = "0")
"00125"






<h2>library(pdftools)</h2>
#==========
setwd("C:/Users/User/Desktop")
library(pdftools)
txt &lt;- pdf_text("a.pdf")
str(txt)	# 361 pages
writeClipboard(txt[1])

Sys.setlocale(category = 'LC_ALL', 'Chinese')
options("encoding" = "UTF-8")
sink("war.txt")
  for(i in txt){ cat(i, sep="\n")}
sink()


txt1 = gsub(".*ORIGINATOR", "", txt)
txt1 = gsub("          ", "", txt1)

list = c(13:16, 19:22, 25:28, 31:34, 37:42, 45:48, 52:58, 62:68, 71:75, 78:85, 88:95, 98:105, 108:115, 118:124, 127:133, 136:142, 145:156, 159:169, 173:202, 206:221, 225:240, 244:258, 261:274, 277:290, 294:298, 302:308, 312:318, 323:331, 334:345, 348:359)

txt1 = txt1[list]

writeClipboard(txt1)

pdf_info("a.pdf")
pdf_text("a.pdf")
pdf_fonts("a.pdf")
pdf_attachments("a.pdf")
pdf_toc("a.pdf")

toc = pdf_toc("a.pdf")
sink("test.txt")
print(toc)
sink()


#==========
library(pdftools)
txt &lt;- pdf_text("a.pdf")
str(txt)
txtList = unlist(strsplit(txt, "\\s{2,}"))

writeClipboard(txtList)
<a href="https://cran.r-project.org/web/packages/pdftools/pdftools.pdf" class="whitebut ">pdftools.pdf</a>

pdftools Usage
pdf_text(pdf)

<h2>pdfimages</h2>
https://stackoverflow.com/questions/47133072/how-to-extract-images-from-a-scanned-pdf

http://www.xpdfreader.com/pdfimages-man.html
http://www.xpdfreader.com/download.html

https://rdrr.io/cran/metagear/src/R/PDF_extractImages.R

pdfimages a.pdf -j

Quote a string to be passed to an operating system shell.
Usage:
shQuote(string, type = c("sh", "csh", "cmd", "cmd2"))

  #("PDF to PPM")      
      files <- list.files(path = dest, pattern = 
 "pdf", full.names = TRUE)
    lapply(files, function(i){
      shell(shQuote(paste0("pdftoppm -f 1 -l 10 -r 300 ", i,".pdf", " ",i)))
      })

You could also just use the CMD prompt and type
pdftoppm -f 1 -l 10 -r 300 stuff.pdf stuff.ppm


<h2>OCR Extract Text from Images</h2>

Using the Tesseract OCR engine in R
library(tesseract)
i = "https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Chineselanguage.svg/1200px-Chineselanguage.svg.png"
chi <- tesseract("chi_sim")
text <- ocr(i, engine = chi)
cat(text) # In love

# text <- ocr(i) # for english, default engine

library(tesseract)
eng <- tesseract("eng")
text <- tesseract::ocr("http://jeroen.github.io/images/testocr.png", engine = eng)
cat(text)
results <- tesseract::ocr_data("http://jeroen.github.io/images/testocr.png", engine = eng)

# list the languages have installed.
tesseract_info()
$datapath
[1] "/Users/jeroen/Library/Application Support/tesseract4/tessdata/"

$available
[1] "chi_sim" "eng"     "osd"    

chinese character recognition using Tesseract OCR
download chinese trained data (it will be a file like chi_sim.traineddata) and add it to your tessdata folder.

C:/Users/User/AppData/Local/tesseract4/tesseract4/tessdata/

To download the file https://github.com/tesseract-ocr/tessdata/raw/master/chi_sim.traineddata


library(tesseract)
chi <- tesseract("chi_sim")

datapath = "C:/Users/User/Desktop/testReact/"
setwd(datapath)
shell(shQuote("D:/XpdfReader-win64/xpdf-tools-win-4.03/bin64/pdfimages a.pdf -j"))

allFiles <- list.files(path = datapath, pattern = 
 "jpg", full.names = TRUE)

allText = character()
# for(i in allFiles){
for(file in 1:5){
    i = allFiles[file]
    cat(i, "\n")
    text <- tesseract::ocr(i, engine = chi)
    allText = c(allText, text)
}

setwd(datapath)

Sys.setlocale(category = 'LC_ALL', 'Chinese')

options("encoding" = "UTF-8")

sink("result.txt")
  cat(allText, sep="\n")
sink()

options("encoding" = "native.enc")
thepage = readLines("result.txt", encoding="UTF-8")
thepage = gsub(" ","", thepage)

sink("resultNew.txt")
  cat(thepage, sep="\n")
sink()

thepage = readLines("resultNew.txt", encoding="UTF-8")
thepage = gsub("。","。\n", thepage)
sink("resultNew.txt")
  cat(thepage, sep="\n")
sink()

<a href="https://ropensci.org/blog/2016/11/16/tesseract/" class="whitebut ">High Quality OCR in R</a>
<a href="https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html" class="whitebut ">Using the Tesseract OCR engine in R</a>
<a href="https://b98606021.medium.com/%E5%AF%A6%E7%94%A8%E5%BF%83%E5%BE%97-tesseract-ocr-eef4fcd425f0" class="whitebut ">實用心得 Tesseract-OCR</a>

<h2>train tessdata library</h2>
<a href="https://tesseract-ocr.github.io/tessdoc/TrainingTesseract-4.00.html#creating-training-data" class="whitebut ">creating training data</a>
<a href="https://pretius.com/how-to-prepare-training-files-for-tesseract-ocr-and-improve-characters-recognition/" class="whitebut ">improve characters recognition</a>

Tesseract ocr train tessdata library on batch with lots of single character image

If they are of same font, put them in a multi-page TIFF and conduct training on it.
jTessBoxEditor can help you with the TIFF merging and box editing.
<a href="http://vietocr.sourceforge.net/training.html" class="whitebut ">jTessBoxEditor</a>

Here is a summary:
1. Obtain a good, clean, uncompressed 300 DPI TIFF scan of a page of your document
2. Obtain the text by retyping the page for correction and testing purpose
3. The more data, the better the OCR result, so repeat (1) and (2) until you have at least 4 pages. Limit is 32
4. Execute tesseract command to obtain the box files
5. Edit the box file using the bbTesseract editing tool
6. Execute tesseract command to generate the data files (clustering)
7. Rename files with "vie." prefix and copy the files to tessdata directory, overriding the existing data
8. Run OCR on the original images to validate your work. The accuracy rate should be in the high 90%
So that the community can benefit from your work, please submit your data files. They will be posted in the VietOCR's Download page. Be sure to indicate the names of the fonts that you have trained for, so users can know which data set they should load into tessdata directory when OCRing their document.

<a href="https://www.endpoint.com/blog/2018/07/09/training-tesseract-models-from-scratch" class="whitebut ">training tesseract models from scratch</a>

<h2>The name of the site environment variable R_ENVIRON</h2>
#==========
The name of the site environment variable R_ENVIRON
"R_HOME/etc/Renviron.site"

the default is "R_HOME/etc/Rprofile.site"

Sys.getenv("R_USER").

Examples

## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, 
so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, 
digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", 
"onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First &lt;- function() cat("\n   Welcome to R!\n\n")
.Last &lt;- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, 
set a CRAN mirror
  old &lt;- getOption("defaultPackages"); r &lt;- getOption("repos")
  r["CRAN"] &lt;- "http://my.local.cran"
  options(defaultPackages = c(old, 
"MASS"), 
repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols &lt;- Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), 
"\n")
# coo\bardoh\exabc"def'







<h2>How to Convert Factor into Numerical?</h2>
#==========
How to Convert Factor into Numerical?

When you convert factors to numeric, 
first you should convert it into characters and then convert into numeric. 
as.numeric(as.character(X))

Df$column&lt;-as.numeric(as.factor(df$column)

as.integer(as.factor(region))





<h2>options(error=recover)</h2>
#==========
options(error=recover)

recover {utils}
Browsing after an Error

This function allows the user to browse directly on any of the currently active function calls, and is suitable as an error option.
The expression options(error = recover) will make this the error option.

Usage
recover()

When called, recover prints the list of current calls, and prompts the user to select one of them.
The standard R browser is then invoked from the corresponding environment;
the user can type ordinary R language expressions to be evaluated in that environment.

Turning off the options() debugging mode in R
options(error=NULL)





<h2>Extract hyperlink from Excel file in R</h2>

#==========

library(XML)

# rename file to .zip
my.zip.file &lt;- sub("xlsx", "zip", my.excel.file)
file.copy(from = my.excel.file, to = my.zip.file)

# unzip the file
unzip(my.zip.file)

# unzipping produces a bunch of files which we can read using the XML package
# assume sheet1 has our data
xml &lt;- xmlParse("xl/worksheets/sheet1.xml")

# finally grab the hyperlinks
hyperlinks &lt;- xpathApply(xml, "//x:hyperlink/@display", namespaces="x")


<span class="redword">To repair Hyperlink address corrupted:</span>
copy file to desk top and rename to zip file
open zip file and locate: <span class="redword">\xl\worksheets\_rels</span>
open the sheet1.xml.rels with editor
remove all text: D:\Users\Lawht\AppData\Roaming\Microsoft\Excel\




<h2>Extract part of a string</h2>

#==========
x &lt;- c("75 to 79", "80 to 84", "85 to 89")
substr(x, start = 1, stop = 2)

substr(x, start, stop)
x &lt;- "1234567890"
substr(x, 5, 7)
"567"

<h2>alter grades</h2>

#==========
alter grades

locate the word
get the line location
alter the score table
#==========

locate the word
v &lt;- c('a','b','c','e')
'b' %in% v ## returns TRUE
match('b',v) ## returns the first location of 'b', in this case: 2

subv &lt;- c('a', 'f')
subv %in% v ## returns a vector TRUE FALSE
is.element(subv, v) ## returns a vector TRUE FALSE

which()
which('a' == v) #[1] 2 4 For finding all occurances as vector of indices

grep() returns a vector of integers, which indicate where matches are.
yo &lt;- c("a", "a", "b", "b", "c", "c")
grep("b", yo) # [1] 3 4

ROC&lt;-"中華民國 – 維基百科，自由的百科全書"
grep("中華民國",ROC)

Partial String Matching
pmatch("med", c("mean", "median", "mode")) # returns 2

<h2>table, cut and barplot</h2>

atab=c(1,2,3,2,1,2,3,4,5,4)
table(atab)
cut(atab, 2)
table( cut(atab, 2))
counts = table( cut(atab, 4))
barplot(counts, main="Qty", xlab="grade")

<h2>non-paste answer to concatenate two strings</h2>

capture.output(cat(counts, sep = ","))


<h2>V8 is an R interface JavaScript engine. </h2>

This package helps us execute javascript code in R

#Loading both the required libraries
library(rvest)
library(V8)

#URL with js-rendered content to be scraped
link &lt;- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'

#Read the html page content and extract all javascript codes that are inside a list

emailjs &lt;- read_html(link) %>% html_nodes('li') %>% html_nodes('script') %>% html_text()

# Create a new v8 context
ct &lt;- v8()

#parse the html content from the js output and print it as text
read_html(ct$eval(gsub('document.write','',emailjs))) %>%  html_text()

info@brewhemia.co.uk

Thus we have used rvest to extract the javascript code snippet from the desired location (that is coded in place of email ID) and used V8 to execute the javascript snippet (with slight code formatting) and output the actual email (that is hidden behind the javascript code). 

####################
Getting email address through rvest
You need a javascript engine here to process the js code.
R has got V8.

Modify your code after installing V8 package:
library(rvest)
library(V8)

link &lt;- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'
page &lt;- read_html(link)
name_html &lt;- html_nodes(page,'.placeHeading')
business_adr &lt;- html_text(adr_html)
tel_html &lt;- html_nodes(page,'.value')
business_tel &lt;- html_text(tel_html)
emailjs &lt;- page %>% html_nodes('li') %>% html_nodes('script') %>% html_text()
ct &lt;- v8()
read_html(ct$eval(gsub('document.write','',emailjs))) %>% html_text()

<h2>extract protected pdf document</h2>

library(pdftools)
setwd("C:/Users/User/Desktop")
txt &lt;- pdf_text("a.pdf")
str(txt)	# 361 pages
# copy page 1
writeClipboard(txt[1])
# copy page 2
writeClipboard(txt[2])
# copy page 3
writeClipboard(txt[3])

Convert unicode character to string format: remove "\u"

theStr = "\u9999\u6e2f\u98df\u54c1\u6295\u8d44"	#  "香港食品投资"

=============================

Sys.setlocale(category = 'LC_ALL', 'Chinese')

library(pdftools)
setwd("C:/Users/User/Desktop")
txt &lt;- pdf_text("45.pdf")
str(txt)

chi1 = gsub('\\u' , '&#x', txt[1])
chi2 = gsub('\\u' , '&#x', txt[2])
chi3 = gsub('\\u' , '&#x', txt[3])
sink("aaa.txt")
cat(chi1)
cat(chi2)
cat(chi3)
sink()

<h2>Writing an R package</h2>

  <a href="https://support.rstudio.com/hc/en-us/articles/200486488-Developing-Packages-with-RStudio">Develop Packages with RStudio</a>

<a href="http://web.mit.edu/insong/www/pdf/rpackage_instructions.pdf">rpackage_instructions.pdf</a>

<a href="https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/">Writing an R package from scratch</a>
<a href="Writing an R package.html"><span class="goldb">Writing an R package</span></a> 

<h2>table, cut and breaks</h2>

table(cut(as.numeric(resultTable[,3]), 10))
cut(as.numeric(resultTable[,3]),10)
breaks = c(seq(lower, 0, by = 5), 0, seq(0, upper, by = 5))

tableA = c(1,3,5,7,9)
tableB = c(1,3,5,7,2,4,6,8)
tableA = c(tableA, tableB)
tableA = sort(tableA)

table(tableA)

table(cut(tableA, 3))

breaks = c(seq(1, 3, by = 1), 4, seq(5, 9, by = 2))
table(cut(tableA, breaks))

<h2>List the Files in a Directory</h2>

List the Files in a Directory/Folder
list.files()

list.dirs(R.home("doc"))
list.dirs()

<h2>test url exist</h2>

library(httr)
http_error(theUrl)

<a href="https://stackoverflow.com/questions/18407177/load-image-from-website">Load image from website</a>
download.file("url", destfile="tmp.png", mode="wb")

<a href="https://jangorecki.gitlab.io/data.table/library/RCurl/html/url.exists.html">url.exists {RCurl}	</a> return true of false
<a href="https://stackoverflow.com/questions/31420210/r-check-existence-of-url-problems-with-httrget-and-url-exists">With httr use url_success()</a>

<h2>download.file</h2>
This function can be used to download a file from the Internet.
download.file(url, destfile, method, quiet = FALSE, mode = "w",
              cacheOK = TRUE,
              extra = getOption("download.file.extra"),
              headers = NULL, ...)
method	
Method to be used for downloading files. Current download methods are "internal", "wininet" (Windows only) "libcurl", "wget" and "curl", and there is a value "auto": see ‘Details’ and ‘Note’.

The method can also be set through the option "download.file.method": see options().

quiet	
If TRUE, suppress status messages (if any), and the progress bar.

mode	
character. The mode with which to write the file. Useful values are "w", "wb" (binary), "a" (append) and "ab". Not used for methods "wget" and "curl". See also ‘Details’, notably about using "wb" for Windows.

cacheOK	
logical. Is a server-side cached value acceptable?

extra	
character vector of additional command-line arguments for the "wget" and "curl" methods.

headers	
named character vector of HTTP headers to use in HTTP requests. It is ignored for non-HTTP URLs. The User-Agent header, coming from the HTTPUserAgent option (see options) is used as the first header, automatically.

...	
allow additional arguments to be passed, unused.



<h2>Passing arguments to R script</h2>

<a href="Passing arguments to R script.html"><span class="goldb">Passing arguments to R script</span></a> 

Rscript --vanilla testargument.R iris.txt newname

To avoid Rscript.exe loop forever for keyboard input:
use this:
cat("a string please: ");
a &lt;- readLines("stdin",n=1);

<h2>School Revision Papers</h2>

http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2018/
http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2017/
https://curriculum.gov.mt/en/Examination-Papers/Pages/list_secondary_papers.aspx
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F1-2ndTest-Eng.pdf
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F2-2ndTest-Eng.pdf
http://www.sttss.edu.hk/parents_corner/pastpaper.php
<h2>difference between 1L and 1</h2>

L specifies an integer type, rather than a double, it uses only 4 bytes per element
the function as.integer is simplified yb  "L " suffix

> str(1)
 num 1

> str(1L)
 int 1
<h2><span class="white goldbs">Datatable</span></h2>
<a href="data.table FAQ.html" class="redbut blue goldbs">data.table FAQ</a>
<a href="Data.Table Tutorial.html"><span class="silverredb">&diams;Data.Table Tutorial</span></a>
<a href="R Data.Table Tutorial.html" class="bluebut gold whitebs">R Data.Table Tutorial</a>

<a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf">Datatable Cheat Sheet</a>
<h3>setkey does two things:</h3>
reorders the rows of the data.table DT by the column(s) provided (a, b) by reference, always in increasing order.

marks those columns as key columns by setting an attribute called sorted to DT.

The reordering is both fast (due to data.table's internal radix sorting) and memory efficient (only one extra column of type double is allocated).

When is setkey() required?
For grouping operations, setkey() was never an absolute requirement. 
That is, we can perform a cold-by or adhoc-by.

A key is basically an index into a dataset, which allows for very fast and efficient sort, filter, and join operations. 
These are probably the best reasons to use data tables instead of data frames (the syntax for using data tables is also much more user friendly, but that has nothing to do with keys).

library(data.table)

dt=data.table(read.table("wAveTable.txt", header=TRUE, colClasses=c('character', 'numeric', 'numeric')))
colnames(dt)
"Code"   "WAve5"  "WAve10"
dt[WAve5 > 5, ]
summary(dt[WAve5 = 5, ])
summary(dt[WAve5 %between% c(7,9), ])

data.table dt subset rows using i, and manipulate columns with j, grouped according to by	dt[i, j, by]
Create a data.table	data.table(a = c(1, 2), b = c("a", "b"))
convert a data frame or a list to a data.table	setDT(df) or as.data.table(df)
Subset data.table rows using i	dt[1:2, ]
subset data.table rows based on values in one or more columns	dt[a > 5, ]
data.table Logical Operators To Use In i	>,<,<=,>=, |, !,&, is.na(),!is.na(), %in%, %like%,  %between%
data.table extract column(s) by number. Prefix column numbers with “-” to drop	dt[, c(2)]
data.table extract column(s) by name	dt[, .(b, c)]
create a data.table with new columns based on the summarized values of rows	dt[, .(x = sum(a))]
compute a data.table column based on an expression	dt[, c := 1 + 2]
compute a data.table column based on an expression but only for a subset of rows	dt[a == 1, c := 1 + 2]
compute a data.table multiple columns based on separate expressions	dt[, `:=`(c = 1 , d = 2)]
delete a data.table column	dt[, c := NULL]
convert the type of a data.table column using as.integer(), as.numeric(), as.character(), as.Date(), etc..	dt[, b := as.integer(b)]
group data.table rows by values in specified column(s)	dt[, j, by = .(a)]
group data.table and simultaneously sort rows according to values in specified column(s)	dt[, j, keyby = .(a)]
summarize data.table rows within groups	dt[, .(c = sum(b)), by = a]
create a new data.table column and compute rows within groups	dt[, c := sum(b), by = a]
extract first data.table row of groups	dt[, .SD[1], by = a]
extract last data.table row of groups	dt[, .SD[.N], by = a]
perform a sequence of data.table operations by chaining multiple “[]”	dt[…][…]
reorder a data.table according to specified columns	setorder(dt, a, -b), “-” for descending
data.table’s functions prefixed with “set” and the operator “:=”	work without “&lt;-” to alter data without making copies in memory
df &lt;- as.data.table(df)	setDT(df)
extract unique data.table rows based on columns specified in “by”. Leave out “by” to use all columns	unique(dt, by = c("a", "b"))
return the number of unique data.table rows based on columns specified in “by”	uniqueN(dt, by = c("a", "b"))
rename data.table column(s)	setnames(dt, c("a", "b"), c("x", "y"))
data.table Syntax	DT[ i , j , by], i refers to rows. j refers to columns. by refers to adding a group
data.table Syntax arguments	DT[ i , j , by], with, which, allow.cartesian, roll, rollends, .SD, .SDcols, on, mult, nomatch
data.table fread() function	to read data, mydata = fread("https://github.com/flights_2014.csv")
data.table select only 'origin' column returns a vector	dat1 = mydata[ , origin]
data.table select only 'origin' column returns a data.table	dat1 = mydata[ , .(origin)] or dat1 = mydata[, c("origin"), with=FALSE]
data.table select column	dat2 =mydata[, 2, with=FALSE]
data.table select column Multiple Columns	dat3 = mydata[, .(origin, year, month, hour)], dat4 = mydata[, c(2:4), with=FALSE]
data.table Dropping Column	adding ! sign, dat5 = mydata[, !c("origin"), with=FALSE]
data.table Dropping Multiple Columns	dat6 = mydata[, !c("origin", "year", "month"), with=FALSE]
data.table select variables that contain 'dep'	use %like% operator, dat7 = mydata[,names(mydata) %like% "dep", with=FALSE]
data.table Rename Variables	setnames(mydata, c("dest"), c("Destination"))
data.table  rename multiple variables	setnames(mydata, c("dest","origin"), c("Destination", "origin.of.flight"))
data.table find all the flights whose origin is 'JFK'	dat8 = mydata[origin == "JFK"]
data.table Filter Multiple Values	dat9 = mydata[origin %in% c("JFK", "LGA")]
data.table selects not equal to 'JFK' and 'LGA'	dat10 = mydata[!origin %in% c("JFK", "LGA")]
data.table Filter Multiple variables	dat11 = mydata[origin == "JFK" & carrier == "AA"]
data.table Indexing Set Key	tells system that data is sorted by the key column
data.table setting 'origin' as a key	setkey(mydata, origin), 'origin' key is turned on. data12 = mydata[c("JFK", "LGA")]
data.table Indexing Multiple Columns	setkey(mydata, origin, dest), key is turned on. mydata[.("JFK", "MIA")] # First key 'origin' matches “JFK” second key 'dest' matches “MIA”
data.table Indexing Multiple Columns equivalent	mydata[origin == "JFK" & dest == "MIA"]
data.table  identify the column(s) indexed by	key(mydata)
data.table sort data using setorder()	mydata01 = setorder(mydata, origin)
data.table sorting on descending order	mydata02 = setorder(mydata, -origin)
data.table Sorting Data based on multiple variables	mydata03 = setorder(mydata, origin, -carrier)
data.table Adding Columns (Calculation on rows)	use := operator, mydata[, dep_sch:=dep_time - dep_delay]
data.table Adding Multiple Columns	mydata002 = mydata[, c("dep_sch","arr_sch"):=list(dep_time - dep_delay, arr_time - arr_delay)]
data.table IF THEN ELSE Method I	mydata[, flag:= 1*(min < 50)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table IF THEN ELSE Method II	mydata[, flag:= ifelse(min < 50, 1,0)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table build a chain	DT[ ] [ ] [ ], mydata[, dep_sch:=dep_time - dep_delay][,.(dep_time,dep_delay,dep_sch)]
data.table Aggregate Columns mean	mydata[, .(mean = mean(arr_delay, na.rm = TRUE),
data.table Aggregate Columns median	median = median(arr_delay, na.rm = TRUE),
data.table Aggregate Columns min	min = min(arr_delay, na.rm = TRUE),
data.table Aggregate Columns max	max = max(arr_delay, na.rm = TRUE))]
data.table Summarize Multiple Columns	all the summary function in a bracket, mydata[, .(mean(arr_delay), mean(dep_delay))]
data.table .SD operator	implies 'Subset of Data'
data.table .SD and .SDcols operators	calculate summary statistics for a larger list of variables
data.table calculates mean of two variables	mydata[, lapply(.SD, mean), .SDcols = c("arr_delay", "dep_delay")]
data.table Summarize all numeric Columns	mydata[, lapply(.SD, mean)]
data.table Summarize with multiple statistics	mydata[, sapply(.SD, function(x) c(mean=mean(x), median=median(x)))]
data.table Summarize by group 'origin	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = origin]
data.table Summary by group useing keyby= operator	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), keyby = origin]
data.table Summarize multiple variables by group 'origin'	mydata[, .(mean(arr_delay, na.rm = TRUE), mean(dep_delay, na.rm = TRUE)), by = origin], or mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin]
data.table remove non-unique / duplicate cases with unique()	setkey(mydata, "carrier"), unique(mydata)
data.table remove duplicated	setkey(mydata, NULL), unique(mydata), Note : Setting key to NULL is not required if no key is already set.
data.table Extract values within a group	mydata[, .SD[1:2], by=carrier], selects first and second values from a categorical variable carrier.
data.table Select LAST value from a group	mydata[, .SD[.N], by=carrier]
data.table window function frank()	dt = mydata[, rank:=frank(-distance,ties.method = "min"), by=carrier], calculating rank of variable 'distance' by 'carrier'. 
data.table cumulative sum cumsum()	dat = mydata[, cum:=cumsum(distance), by=carrier]
data.table lag and lead with shift()	shift(variable_name, number_of_lags, type=c("lag", "lead")), DT &lt;- data.table(A=1:5), DT[ , X := shift(A, 1, type="lag")], DT[ , Y := shift(A, 1, type="lead")]
data.table  %between% operator to define a range	DT = data.table(x=6:10), DT[x %between% c(7,9)]
data.table %like% to find all the values that matches a pattern	DT = data.table(Name=c("dep_time","dep_delay","arrival"), ID=c(2,3,4)), DT[Name %like% "dep"] 
data.table Inner Join	Sample Data: (dt1 &lt;- data.table(A = letters[rep(1:3, 2)], X = 1:6, key = "A")), (dt2 &lt;- data.table(A = letters[rep(2:4, 2)], Y = 6:1, key = "A")), merge(dt1, dt2, by="A")
data.table Left Join	merge(dt1, dt2, by="A", all.x = TRUE)
data.table Right Join	merge(dt1, dt2, by="A", all.y = TRUE)
data.table Full Join	merge(dt1, dt2, all=TRUE)
Convert a data.table to data.frame	setDF(mydata)
convert data frame to data table	setDT(), setDT(X, key = "A")
data.table Reshape Data	dcast.data.table() and melt.data.table()
data.table Calculate total number of rows by month and then sort on descending order	mydata[, .N, by = month] [order(-N)], The .N operator is used to find count.
data.table Find top 3 months with high mean arrival delay	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = month][order(-mean_arr_delay)][1:3]
data.table Find origin of flights having average total delay is greater than 20 minutes	mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin][(arr_delay + dep_delay) > 20]
data.table Extract average of arrival and departure delays for carrier == 'DL' by 'origin' and 'dest' variables	mydata[carrier == "DL", lapply(.SD, mean, na.rm = TRUE), by = .(origin, dest), .SDcols = c("arr_delay", "dep_delay")]
data.table Pull first value of 'air_time' by 'origin' and then sum the returned values when it is greater than 300	mydata[, .SD[1], .SDcols="air_time", by=origin][air_time > 300, sum(air_time)]

<h2>extract flickr image</h2>
seek .context-thumb
get background-image
convert _m.jpg -> _b.jpg

https://live.staticflickr.com/2941/15170815109_f81b1994d2_m.jpg
https://live.staticflickr.com/2941/15170815109_f81b1994d2_b.jpg

<h2><span class="gold bordred1 blink">R Web Scraping</span></h2>

<a href="https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples" class="whitebut red bluebs blueblackgrad whitets blinkNmove">get started xpath selectors</a>

<a href="https://www.datacamp.com/community/tutorials/r-web-scraping-rvest">R Web Scraping Rvest</a>
<a href="http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl/">webscraping-using-readlines-and-rcurl</a>
<a href="https://www.rdocumentation.org/packages/XML/versions/3.98-1.16/topics/xmlTreeParse">xmlTreeParse, htmlTreeParse</a>
<a href="http://www.cse.chalmers.se/~chrdimi/downloads/web/getting_web_data_r4_parsing_xml_html.pdf">getting web data parsing xml html</a>
<a href="https://stackoverflow.com/questions/35479549/error-in-r-no-applicable-method-for-xpathapply">error in r no applicable method for xpathapply</a>
<a href="https://blog.rstudio.com/2015/04/21/xml2/">Parse and process XML (and HTML) with xml2</a>
==================
web_page &lt;- readLines("http://www.interestingwebsite.com")
web_page &lt;- read.csv("http://www.programmingr.com/jan09rlist.html")

    # General-purpose data wrangling
    library(tidyverse)  

    # Parsing of HTML/XML files  
    library(rvest)    

    # String manipulation
    library(stringr)   

    # Verbose regular expressions
    library(rebus)     

    # Eases DateTime manipulation
    library(lubridate)

==================
install.packages("RCurl", dependencies = TRUE)
library("RCurl")
library("XML")

past &lt;- getURL("http://www.iciba.com/past", ssl.verifypeer = FALSE)	# getURL cannot work
webpage &lt;- read_html("http://www.iciba.com/past")	# getURL cannot work

jan09_parsed &lt;- htmlTreeParse(jan09)

==================
http://www.iciba.com/past
ul class="base-list switch_part" class

library('rvest')
library(tidyverse)
url &lt;- 'http://www.iciba.com/past'
webpage &lt;- readLines(url, warn=FALSE)
webpage &lt;- read_html(webpage)
grappedData &lt;- html_nodes(webpage,'.base-list switch_part')

parseData = htmlTreeParse(webpage)

rank_data &lt;- html_text(grappedData)

html_node("#mw-content-text > div > table:nth-child(18)")
html_table()

the function htmlParse() which is equivalent to xmlParse(file, isHTML = TRUE)
output = htmlParse(webpage)
class(output)

To parse content into an R structure :
htmlTreeParse() which is equivalent to htmlParse(file, useInternalNodes = FALSE)
output = htmlTreeParse(webpage)
class(output)

htmlTreeParse(file) especially suited for parsing HTML content
returns class "XMLDocumentContent" (R data structure)
equivalent to
xmlParse(file, isHTML = TRUE, useInternalNodes = FALSE)
htmlParse(file, useInternalNodes = FALSE)

root =xmlRoot(output)
xmlChildren(output)
xmlChildren(xmlRoot(output))
XMLNodeList

Functions for a given node
Function Description
xmlName() name of the node
xmlSize() number of subnodes
xmlAttrs() named character vector of all attributes
xmlGetAttr() value of a single attribute
xmlValue() contents of a leaf node
xmlParent() name of parent node
xmlAncestors() name of ancestor nodes
getSibling() siblings to the right or to the left
xmlNamespace() the namespace (if there’s one)

to parse HTML tables using R
sched &lt;- readHTMLTable(html, stringsAsFactors = FALSE)

The html.raw object is not immediately useful because it literally contains all of the raw HTML for the entire webpage. We can parse the raw code using the xpathApply function which parses HTML based on the path argument, which in this case specifies parsing of HTML using the paragraph tag.

html.raw&lt;-htmlTreeParse('http://www.dnr.state.mn.us/lakefind/showreport.html?downum=27013300',
    useInternalNodes=T    )
html.parse&lt;-xpathApply(html.raw, "//p", xmlValue)

# evaluate input and convert to text
txt &lt;- htmlToText(url)

==================
url &lt;- 'http://www.iciba.com/past'
webpage &lt;- readLines(url, warn=FALSE)
scraping_wiki &lt;- read_html(webpage)
scraping_wiki %>% html_nodes("h1") %>% html_text()

url &lt;- 'testvibrate.html'
webpage &lt;- readLines(url, warn=FALSE)
x &lt;- read_xml(webpage)
xml_name(x)
===========

This cannot work in office
library(rvest)
Sys.setlocale(category = 'LC_ALL', 'Chinese')
webpage &lt;- read_html("http://www.iciba.com/haunt")
ullist = webpage %>% html_nodes("ul")
content = ullist[2] %>% html_text()
content = gsub("n.| |\n|adj.|adv.|prep.|vt.|vi.|&","",content)
content = gsub("，|；"," ",content) %>%  strsplit(split = " ") %>% unlist() %>% sort() %>% unique()
paste0("past","\t",capture.output(cat(content)))

<h2>R scraping html text example</h2>

<a href="http://bradleyboehmke.github.io/2015/12/scraping-html-text.html">scraping-html-text</a>
library(rvest)

scraping_wiki &lt;- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scraping_wiki %>% html_nodes("h1")
scraping_wiki %>% html_nodes("h2")
scraping_wiki %>% html_nodes("h1") %>% html_text()
scraping_wiki %>% html_nodes("h2") %>% html_text()
p_nodes &lt;- scraping_wiki %>% html_nodes("p")
length(p_nodes)
p_text &lt;- scraping_wiki %>% html_nodes("p") %>% html_text()
p_text[1]
p_text[5]

ul_text &lt;- scraping_wiki %>% html_nodes("ul") %>% html_text()
length(ul_text)
ul_text[1]
substr(ul_text[2], start = 1, stop = 200)

li_text &lt;- scraping_wiki %>% html_nodes("li") %>% html_text()
length(li_text)
li_text[1:8]
li_text[104:136]

all_text &lt;- scraping_wiki %>% html_nodes("div") %>%  html_text()

body_text &lt;- scraping_wiki %>% html_nodes("#mw-content-text") %>%  html_text()

# read the first 207 characters
substr(body_text, start = 1, stop = 207)

# read the last 73 characters
substr(body_text, start = nchar(body_text)-73, stop = nchar(body_text))

# Scraping a specific heading
scraping_wiki %>% html_nodes("#Techniques") %>%  html_text()
## [1] "Techniques"

# Scraping a specific paragraph
scraping_wiki %>% html_nodes("#mw-content-text > p:nth-child(20)") %>%  html_text()

# Scraping a specific list
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

# Scraping a specific reference list item
scraping_wiki %>% html_nodes("#cite_note-22") %>%  html_text()

# Cleaning up
library(magrittr)
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text() %>%  strsplit(split = "\n") %>% unlist() %>% .[. != ""]


library(stringr)

# read the last 700 characters
substr(body_text, start = nchar(body_text)-700, stop = nchar(body_text))

# clean up text
body_text %>% 
str_replace_all(pattern = "\n", replacement = " ") %>% 
str_replace_all(pattern = "[\\^]", replacement = " ") %>% 
str_replace_all(pattern = "\"", replacement = " ") %>% 
str_replace_all(pattern = "\\s+", replacement = " ") %>% 
str_trim(side = "both") %>% 
substr(start = nchar(body_text)-700, stop = nchar(body_text))

################
# rvest tutorials
https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/
https://blog.gtwang.org/r/rvest-web-scraping-with-r/
https://www.rdocumentation.org/packages/rvest/versions/0.3.4
https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/
https://lmyint.github.io/post/dnd-scraping-rvest-rselenium/

################
# parse guancha
library(rvest)
pageHeader="https://user.guancha.cn/main/content?id=181885"
pagesource &lt;- read_html(pageHeader)

################
# parse RTHK and metroradio
library(rvest)
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource &lt;- read_html(pageHeader)
className = ".ns2-title"
keywordList &lt;- html_nodes(pagesource, className)
html_text(keywordList)

pageHeader = "http://www.metroradio.com.hk/MetroFinance/News/NewsLive.aspx"
pagesource &lt;- read_html(pageHeader)
className = ".n13newslist"
keywordList &lt;- html_nodes(pagesource, className)
className = "a"
keywordList &lt;- html_nodes(keywordList, className)
html_text(keywordList)

################
# parse xhamster
library(rvest)
pageHeader = "https://xhamster.com/users/fredlake/photos"
pagesource &lt;- read_html(pageHeader)
className = ".xh-paginator-button"
keywordList &lt;- html_nodes(pagesource, className)
html_text(keywordList)
html_name(keywordList)
html_attrs(keywordList)

thelist = unlist(html_attrs(keywordList))

length(keywordList)
as.numeric(html_text(keywordList[length(keywordList)]))

pagesource %>% html_nodes(className) %>% html_text() %>% as.numeric()

for ( i in keywordList ) { 
 qlink &lt;- html_nodes(s, ".gallery-thumb")
 cat("Title:", html_text(qlink), "\n")
 qviews &lt;- html_nodes(s, "name")
 cat("Views:", html_text(qviews), "\n")
}
################
# parse text and href
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource &lt;- read_html(pageHeader)

className = ".ns2-title"
keywordList &lt;- html_nodes(pagesource, className)

className = "a"
a &lt;- html_nodes(keywordList, className)

html_text(a)
html_attr(a, "href")

################
# extract huanqiu.com gallery

pageHeader = "https://china.huanqiu.com/gallery/9CaKrnQhXac"
pagesource &lt;- read_html(pageHeader)
className = "article"
keywordList &lt;- html_nodes(pagesource, className)

className = "img"
img &lt;- html_nodes(keywordList, className)
html_attr(img, "src")
html_attr(img, "data-alt")

################
# html_nodes samples
html_nodes(".a1.b1")
html_nodes(".b1:not(.a1)")  # <i>Select class contains b1 not a1:</i>
html_nodes(".content__info__item__value")
html_nodes("[class='b1']")
html_nodes("center")
html_nodes("font")
html_nodes(ateam, "center")
html_nodes(ateam, "center font")
html_nodes(ateam, "center font b")
html_nodes("table") %>% .[[3]] %>% html_table()
html_nodes("td")
html_nodes() returns all nodes
html_nodes(pagesource, className)
html_nodes(pg, "div > input:first-of-type"), "value")
html_nodes(s, ".gallery-thumb")
html_nodes(s, "name")
html_nodes(xpath = '//*[@id="a"]')

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td &lt;- ateam %>% html_nodes("center") %>% html_nodes("td")
td %>% html_nodes("font")

if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_node("font")
}

# To pick out an element at specified position, use magrittr::extract2
# which is an alias for [[
library(magrittr)
ateam %>% html_nodes("table") %>% extract2(1) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% `[[`(1) %>% html_nodes("img")

# Find all images contained in the first two tables
ateam %>% html_nodes("table") %>% `[`(1:2) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% extract(1:2) %>% html_nodes("img")

# XPath selectors ---------------------------------------------
# If you prefer, you can use xpath selectors instead of css: 
html_nodes(doc, xpath = "//table//td")).

# chaining with XPath is a little trickier - you may need to vary
# the prefix you're using - // always selects from the root node
# regardless of where you currently are in the doc
ateam %>% html_nodes(xpath = "//center//font//b") %>% html_nodes(xpath = "//b")


read_html()
html_node()	# to find the first node
html_nodes(doc, "table td")	# to find the all node
html_nodes(doc, xpath = "//table//td"))

html_name()	# the name of the tag
html_tag()	# Extract the tag names
html_text()	# Extract all text inside the tag 

html_attr()	Extract the a single attribute
html_attrs()	Extract all the attributes

# html_attrs(keywordList) this cannot use id, just list all details
# html_attr(keywordList, "id") this select the ids
# html_attr(keywordList, "href") this select the hrefs

html_nodes("#titleCast .itemprop span")
html_nodes("#img_primary img")
html_nodes("div.name > strong > a")
html_attr("href")

html_text(keywordList, trim = FALSE)
html_name(keywordList)
html_children(keywordList)
html_attrs(keywordList)
html_attr(keywordList, "[href]", default = NA_character_)

parse with xml()
then extract components using 
xml_node()
xml_attr()
xml_attrs()
xml_text() and xml_name()

Parse tables into data frames with 
html_table().

Extract, modify and submit forms with 
html_form()
set_values()
submit_form().

Detect and repair encoding problems with 
guess_encoding()	Detect text encoding
repair_encoding()	repair text encoding

Navigate around a website as if you’re in a browser with 
html_session()
jump_to()
follow_link()
back()
forward()

Extract, modify and submit forms with 
html_form(), 
set_values() 
and submit_form()


The toString() function collapse the list of strings into one.

html_node(":not(#commentblock)")	# exclude tags

######### demos #########
# Inspired by https://github.com/notesofdabbler
library(rvest)
library(tidyr)

page &lt;- read_html("http://www.zillow.com/homes/for_sale/....")

houses &lt;- page %>% html_nodes(".photo-cards li article")
z_id &lt;- houses %>% html_attr("id")

address &lt;- houses %>% html_node(".zsg-photo-card-address") %>% html_text()

price &lt;- houses %>% html_node(".zsg-photo-card-price") %>% html_text() %>% readr::parse_number()

params &lt;- houses %>% html_node(".zsg-photo-card-info") %>% html_text() %>% strsplit("\u00b7")

beds &lt;- params %>% purrr::map_chr(1) %>% readr::parse_number()
baths &lt;- params %>% purrr::map_chr(2) %>% readr::parse_number()
house_area &lt;- params %>% purrr::map_chr(3) %>% readr::parse_number()

################
pagesource %>% html_nodes("table") %>% .[[3]] %>% html_table()

read_html(doc) %>% html_nodes(".b1:not(.a1)") # <i>Select class contains b1 not a1:</i>
# [1] <span class="b1"> text2 </span>

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")
# [1] <span class="b1"> text2 </span>

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
# [1] <span class="a1 b1"> text1 </span>

combine class and ID in CSS selector
div#content.sectionA  # <i>this is 'and' operation</i>

=====================
<i>select 2 classes in 1 tag</i>
Select class contains b1 not a1:
read_html(doc) %>% html_nodes(".b1:not(.a1)")

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
=====================
standard CSS selector specify either or both

html_nodes(".content__info__item__value, skill")  # <i>the comma is 'or' operation</i>
{xml_nodeset (4)}
[1] <span class="content__info__item__value duration">5h 59m 42s</span>
[2] <span class="content__info__item__value skill">Beginner + Intermediate</span>
[3] <span class="content__info__item__value released">September 26, 2013</span>
[4] <span class="content__info__item__value viewers">82,552</span>

# has both classes in_learning_page
html_nodes(".content__info__item__value.skill")   # <i>this is 'and' operation</i>
{xml_nodeset (1)}
[1] <span class="content__info__item__value skill">Beginner + Intermediate</span>


in_learning_page %>%
  html_nodes(".content__info__item__value") %>% 
  str_subset(., "viewers")

h &lt;- read_html(text)

h %>% html_nodes(xpath = '//*[@id="a"]') %>% xml_attr("value")

html_attr(html_nodes(pg, "div > input:first-of-type"), "value")

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td &lt;- ateam %>% html_nodes("center") %>% html_nodes("td")

# When applied to a list of nodes, html_nodes() returns all nodes,
# collapsing results into a new nodelist.
td %>% html_nodes("font")
# nodes, it returns a "missing" node
if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_nodes("font")

<h2>sort() rank() order()</h2>

<strong>Rank</strong> references the position of the value in the sorted vector and is in the same order as the <strong>original </strong>sequence
<strong>Order</strong> returns the position of the original value and is in the order of <strong>sorted </strong>sequence
The graphic below helps tie together the values reported by rank and order with the positions from which they come.

<img class="lazy" data-src="https://cdn-images-1.medium.com/max/800/1*3KeaXU6luJDyoatWkEwdug.jpeg">
x = c(1, 8,9, 4)
sort(x)
1 4 8 9

# the original position in the sorted order
rank(x)
1 3 4 2

# the sorted position in the original position
order(x)
1 4 2 3
<h2>Bioinformatics</h2>

<a href="https://cran.r-project.org/doc/contrib/Krijnen-IntroBioInfStatistics.pdf">Bioinformatics using R</a>
<a href="https://www.bioconductor.org/">bioconductor</a>
<a href="https://www.r-exercises.com/product/introduction-to-bioconductor-annotation-and-analysis-of-genomes-and-genomic-assays/">Introduction to Bioconductor:Annotation and Analysis of Genomes and Genomics Assays</a>

<h2>a list of dataframes, 3D data arrangement</h2>

d1 &lt;- data.frame(y1=c(1,2,3),y2=c(4,5,6))
d2 &lt;- data.frame(y1=c(3,2,1),y2=c(6,5,4))
d3 &lt;- data.frame(y1=c(7,8,9),y2=c(5,2,6))
mylist &lt;- list(d1, d2, d3)
names(mylist) &lt;- c("List1","List2","List3")

mylist[1]	# same as mylist$List1

mylist[[2]][1,2]	# access an element inside a dataframe
mylist[[2]][2,2]	# same as mylist$List2[2,2]

to concate another dataframe:

d4 &lt;- data.frame(y1=c(2,5,8),y2=c(1,4,7))
mylist[[4]] &lt;- d4

to create an empty list:
data &lt;- list()
<h2>format time string</h2>

Sys.time()

sub(".* | .*", "", Sys.time())

format(Sys.time(), '%H:%M')

gsub(":", "", format(Sys.time(), '%H:%M'))


<h2>extract 5 digit from string</h2>

activityListCode = str_replace(activityListCode, ".*\\b(\\d{5})\\b.*", "\\1")


<h2>access Components of a Data Frame</h2>
<a href="https://www.datamentor.io/r-programming/data-frame/">access Components of a Data Frame</a>

Components of data frame can be accessed like a list or like a matrix.

<h3>Accessing like a list</h3>
We can use either <code>[</code>, <code>[[</code> or <code>$</code> operator to access columns of data frame.

<code>&gt; x["Name"]
Name
1 John
2 Dora
&gt; x$Name
[1] "John" "Dora"
&gt; x[["Name"]]
[1] "John" "Dora"
&gt; x[[3]]
[1] "John" "Dora"
</code>
Accessing with <code>[[</code> or <code>$</code> is similar. However, it differs for <code>[</code> in that, indexing with <code>[</code> <span class="redword">will return us a data frame</span> but the other two will <span class="redword">reduce it into a vector</span>.


<h3>Accessing like a matrix</h3>
Data frames can be accessed like a matrix by providing index for row and column.

To illustrate this, we use datasets already available in R. Datasets that are available can be listed with the command <code>library(help = "datasets")</code>.

We will use the <code>trees</code> dataset which contains <code>Girth</code>, <code>Height</code> and <code>Volume</code> for Black Cherry Trees.

A data frame can be examined using functions like <code>str()</code> and <code>head()</code>.

<code>&gt; str(trees)
'data.frame':   31 obs. of 3 variables:
$ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...
$ Height: num  70 65 63 72 81 83 66 75 80 75 ...
$ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...
&gt; head(trees,n=3)
Girth Height Volume
1   8.3     70   10.3
2   8.6     65   10.3
3   8.8     63   10.2
</code>
We can see that <code>trees</code> is a data frame with 31 rows and 3 columns. We also display the first 3 rows of the data frame.

Now we proceed to access the data frame like a matrix.

<code>&gt; trees[2:3,]    # select 2nd and 3rd row
Girth Height Volume
2   8.6     65   10.3
3   8.8     63   10.2
&gt; trees[trees$Height &gt; 82,]    # selects rows with Height greater than 82
Girth Height Volume
6   10.8     83   19.7
17  12.9     85   33.8
18  13.3     86   27.4
31  20.6     87   77.0
&gt; trees[10:12,2]
[1] 75 79 76
</code>
We can see in the last case that the returned type is a vector since we extracted data from a single column.

This behavior can be avoided by passing the argument <code>drop=FALSE</code> as follows.

<code>&gt; trees[10:12,2, drop = FALSE]
Height
10     75
11     79
12     76
</code>


# access first row by index, returns a data.frame
x[1,]

# access first row by "name", returns a data.frame
> x["1",]

# access first row returns a vector
use as.numeric
str(as.numeric(wAveTable["1",]))

unlist which keeps the names.
str(unlist(wAveTable["1",]))

use transpose and as.vector
str(as.vector(t(wAveTable["1",])[,1]))

use only as.vector cannot convert to vector
str(as.vector(wAveTable["1",]))

# convert dataframe to matrix
data.matrix(wAveTable)

<h2>read.csv as character</h2>

wAveTable = read.csv("wAveTable.txt", sep="\t", colClasses=c('character', 'character', 'character'))
<h2>frequency manipulation</h2>

grade = c("low", "high", "medium", "high", "low", "medium", "high")

# using factor to count the frequency
foodfac &lt;- factor(grade)
summary(foodfac)
max(summary(foodfac))
min(summary(foodfac))
levels(foodfac)
nlevels(foodfac)
summary(levels(foodfac))

# use of table to count frequency:
table(grade)
sort(table(grade))

table(grade)[1]
max(table(grade))
summary(table(grade))

# this locate the max item:
table(grade)[which(table(grade) == max(table(grade)))]

# change to dataframe and find the max item:
theTable = as.data.frame(table(grade))
theTable[which(theTable$Freq == max(theTable$Freq)),]

# use of the count function in plyr:
library(plyr)
count(grade)
count(mtcars, 'gear')

# use of the which function:
which(letters == "g")
x &lt;- c(1,5,8,4,6)
which(x == 5)
which(x != 5)

<h2>5 must have R programming tools</h2>

<h4>1) RStudio</h4>
<h4>2) lintr</h4>
If you come from the world of Python, you’ve probably heard of 
<a href="https://stackoverflow.com/questions/8503559/what-is-linting" data-href="https://stackoverflow.com/questions/8503559/what-is-linting" rel="nofollow noopener" target="_blank">linting</a>. 
Essentially, linting 
<a href="https://en.wikipedia.org/wiki/Lint_%28software%29" data-href="https://en.wikipedia.org/wiki/Lint_%28software%29" rel="nofollow noopener" target="_blank">analyzes</a> your code for readability. 
It makes sure you don’t produce code that looks like this:
# This is some bad R code
<br>if ( mean(x,na.rm=T)==1) { print(“This code is bad”); } # Still bad code because this line is SO long
There are 
<em>many</em> things wrong with this code. 
For starters, the code is too long. 
Nobody likes to read code with seemingly endless lines. 
There are also no spaces after the comma in the 
<code>mean()</code> function, or any spaces between the 
<code>==</code> operator. 
Oftentimes data science is done hastily, but linting your code is a good reminder for creating portable and understandable code. 
After all, if you can’t explain what you are doing or how you are doing it, your data science job is incomplete. 

<a href="https://cran.r-project.org/web/packages/lintr/index.html" data-href="https://cran.r-project.org/web/packages/lintr/index.html" rel="nofollow noopener" target="_blank">lintr</a> is an R package, growing in popularity, that allows you to lint your code. 
Once you install lintr, linting a file is as easy as 
<code>lint("filename.R")</code> .
<h4>3) Caret</h4>
<a href="http://topepo.github.io/caret/index.html" data-href="http://topepo.github.io/caret/index.html" rel="nofollow noopener" target="_blank">Caret</a>, which you can find on 
<a href="https://cran.r-project.org/web/packages/caret/caret.pdf" data-href="https://cran.r-project.org/web/packages/caret/caret.pdf" rel="nofollow noopener" target="_blank">CRAN</a>, is central to a data scientist’s toolbox in R. 
Caret allows one to quickly develop models, set cross-validation methods and analyze model performance all in one. 
Right out of the box, Caret abstracts the various interfaces to user-made algorithms and allows you to swiftly create models from averaged neural networks to boosted trees. 
It can even handle parallel processing. 
Some of the models caret includes are: AdaBoost, Decision Trees &amp; Random Forests, Neural Networks, Stochastic Gradient Boosting, nearest neighbors, support vector machines — among the most commonly used machine learning algorithms.
<h4>4) Tidyverse</h4>
You may not have heard of 
<code>tidyverse</code> as a whole, but chances are, you’ve used one of the packages in it. 
Tidyverse is a set of unified packages meant to make data science… 
<em>easyr</em> (classic R pun). 
These packages alleviate many of the problems a data scientist may run into when dealing with data, such as loading data into your workspace, manipulating data, tidying data or visualizing data. 
Undoubtedly, these packages make dealing with data in R more efficient.
It’s incredibly easy to get Tidyverse, you just run 
<code>install.packages("tidyverse")</code> and you get:
<a href="http://ggplot2.tidyverse.org/" data-href="http://ggplot2.tidyverse.org/" rel="nofollow noopener" target="_blank">ggplot2</a>: A popular R package for creating graphics
<a href="http://dplyr.tidyverse.org/" data-href="http://dplyr.tidyverse.org/" rel="nofollow noopener" target="_blank">dplyr</a>: A popular R package for efficiently manipulating data
tidyr: An R package for tidying up data sets
<a href="http://readr.tidyverse.org/" data-href="http://readr.tidyverse.org/" rel="nofollow noopener" target="_blank">readr</a>: An R package for reading in data
<a href="http://purrr.tidyverse.org/" data-href="http://purrr.tidyverse.org/" rel="nofollow noopener" target="_blank">purrr</a>: An R package which extends R’s functional programming toolkit
<a href="http://tibble.tidyverse.org/" data-href="http://tibble.tidyverse.org/" rel="nofollow noopener" target="_blank">tibble</a>: An R package which introduces the 
<em>tibble (tbl_df)</em>, an enhancement of the data frame
By and large, ggplot2 and dplyr are some of the most common packages in the R sphere today, and you’ll see countless posts on StackOverflow on how to use either package.

<em>(Fine Print: Keep in mind, you can’t just load everything with </em>
<code>
<em>library(tidyverse)</em></code>
<em> you must load each individually!)</em>
<h4>5) Jupyter Notebooks or R Notebooks</h4>
Data science 
<em>MUST</em> be transparent and reproducible. 
For this to happen, we have to see your code! The two most common ways to do this are through 
<a href="http://jupyter.org/" data-href="http://jupyter.org/" rel="nofollow noopener" target="_blank">Jupyter Notebooks</a> or 
<a href="http://rmarkdown.rstudio.com/r_notebooks.html" data-href="http://rmarkdown.rstudio.com/r_notebooks.html" rel="nofollow noopener" target="_blank">R Notebooks</a>.
Essentially, a notebook (of either kind) allows you to run R code block by block, and show output block my block. 
We can see on the left that we are summarizing the data, then checking the output. 
After, we plot the data, then view the plot. 
All of these actions take place within the notebook, and it makes analyzing both output and code a simultaneous process. 
This can help data scientists collaborate and ease the friction of having to open up someone’s code and understand what it does. 
Additionally, notebooks also make data science 
<em>reproducible</em>, which gives validity to whatever data science work you do!
<h4>Honorable Mention: Git</h4>
Last but not least, I want to mention Git. 
Git is a 
<em>version control</em> system. 
So why use it? Well, it’s in the name. 
Git allows you to keep versions of the code you are working on. 
It also allows multiple people to work on the same project and allows those changes to be attributed to certain contributors. 
You’ve probably heard of 
<a href="http://www.github.com" data-href="http://www.github.com" rel="nofollow noopener" target="_blank">Github</a>, undoubtedly one of the most popular git servers.
You can visit my website at 
<a href="http://www.peterxeno.com" data-href="http://www.peterxeno.com" rel="nofollow noopener" target="_blank">www.peterxeno.com</a> and my Github at 
<a href="https://github.com/peterxeno" data-href="https://github.com/peterxeno" rel="nofollow noopener" target="_blank">www.github.com/peterxeno</a>


<h2>R with Javascript</h2>
<a href="R and D3.html">R and D3</a>
<a href="https://www.opencpu.org/posts/js-release-0-1/">tools for working with JavaScript in R</a>
<a href="http://www.di.fc.ul.pt/~jpn/r/langs/javascript.html">R Connecting with Javascript</a>

<h2><span class="white bordgreen1">tryCatch</span></h2>
<a href="https://codeday.me/bug/20170502/13495.html">如何在R中写trycatch</a>
<a href="https://stackoverflow.com/questions/12193779/how-to-write-trycatch-in-r">write trycatch in R</a>
<a href="http://mazamascience.com/WorkingWithData/?p=912">error Handing with tryCatch()</a>

e.g.
readUrl = function(url) {
    out = tryCatch(
        {
            message("This is the 'try' part")
            readLines(con=url, warn=FALSE) 
        },
        error=function(cond) {
            message(paste("URL does not seem to exist:", url))
            message("Here's the original error message:")
            message(cond)
            # Choose a return value in case of error
            return(NA)
        },
        warning=function(cond) {
            message(paste("URL caused a warning:", url))
            message("Here's the original warning message:")
            message(cond)
            # Choose a return value in case of warning
            return(NULL)
        },
        finally={
        # Here goes everything that should be executed at the end,
        # regardless of success or error.
        # If you want more than one expression to be executed, then you 
        # need to wrap them in curly brackets ({...}); otherwise you could
        # just have written 'finally={expression}' 
            message(paste("Processed URL:", url))
            message("Some other message at the end")
        }
    )    
    return(out)
}

e.g.
x &lt;- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});

e.g.
for (i in urls) {
    tmp &lt;- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}

a retry function:
retry &lt;- function(dothis, max = 10, init = 0){
	suppressWarnings( tryCatch({
		if(init&lt;max) dothis}, 
			error = function(e){retry(dothis, max, init = init+1)}
		)
	)
}
dothis &lt;- function(){do somthing}

<h2>Download Image</h2>
<a href="https://stackoverflow.com/questions/29110903/how-to-download-and-display-an-image-from-an-url-in-r">Download Image</a>
If I try your code it looks like the image is downloaded. However, when opened with windows image viewer it also says it is corrupt. The reason for this is that you don't have specified the mode in the download.file statement.

Try this:

download.file(y,'y.jpg', mode = 'wb')

download.file('http://78.media.tumblr.com/83a81c41926c1da585916a5c092b4789/tumblr_or0y0vdjOP1rttk8po1_1280.jpg','y.jpg', mode = 'wb')

To view the image in R, have a look at

library(jpeg)
jj &lt;- readJPEG("y.jpg",native=TRUE)
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(jj,0,0,1,1)

<h2>Download Something</h2>
Download Something
Download Something
Download Something

<h2>testShiny</h2>
setwd("D:/KPC/testShiny")
runApp("D:/KPC/testShiny")


<h2>Error in file(filename, "r", encoding = encoding)</h2>
The error indicate that either the file doesn't exist or the source() command an incorrect path. 

<h2>call a R program from another R program</h2>
source("program_B.R")

<h2>to view all the functions present in a package</h2>

To list all objects in the package use ls
ls("package:Hmisc")
Note that the package must be attached.

To list all strings
lsf.str("package:dplyr")
lsf.str("package:Hmisc")

To see the list of currently loaded packages use
search()

Alternatively calling the help would also do, even if the package is not attached:
help(package = dplyr)
help(package = Hmisc)

Finally, use RStudio which provides an autocomplete function.
So, for instance, typing Hmisc:: in the console or while editing a file will result in a popup list of all dplyr functions/objects.



<h2>cut2</h2>

Function like cut but left endpoints are inclusive.

install.packages("Hmisc")
library(Hmisc)

alist = c(-15,18,2,5,4,-7,-5,-3,-1,0,2,1,5,4,6)
breaks = c(-5,-3,-1,0,1,3,5)
table(cut2(alist, breaks))

<h2>Reference A Data Frame Column</h2>

with the double square bracket "[[]]" operator.
LastDayTable[["Vol"]] 
or
LastDayTable$Vol
or
<span class="cyanword">LastDayTable[,"Vol"] </span>

<h2>Writing data to a file</h2>
<h3 id="problem">Problem</h3>

You want to write data to a file.


<h3 id="solution">Solution</h3>

<h3 id="writing-to-a-delimited-text-file">Writing to a delimited text file</h3>

The easiest way to do this is to use <code>write.csv()</code>. By default, <code>write.csv()</code> includes row names, but these are usually unnecessary and may cause confusion.


<code># A sample data frame
data &lt;- read.table(header=TRUE, text='
 subject sex size
       1   M    7
       2   F    NA
       3   F    9
       4   M   11
 ')


# Write to a file, suppress row names
write.csv(data, "data.csv", row.names=FALSE)

# Same, except that instead of "NA", output blank cells
write.csv(data, "data.csv", row.names=FALSE, na="")

# Use tabs, suppress row names and column names
write.table(data, "data.csv", sep="\t", row.names=FALSE, col.names=FALSE) 
</code>

<h3>Saving in R data format</h3>

<code>write.csv()</code> and <code>write.table()</code> are best for interoperability with other data analysis programs. They will not, however, preserve special attributes of the data structures, such as whether a column is a character type or factor, or the order of levels in factors. In order to do that, it should be written out in a special format for R.


Below are are three primary ways of doing this:


The first method is to output R source code which, when run, will re-create the object. This should work for most data objects, but it may not be able to faithfully re-create some more complicated data objects.


<code># Save in a text format that can be easily loaded in R
dump("data", "data.Rdmpd")
# Can save multiple objects:
dump(c("data", "data1"), "data.Rdmpd")

# To load the data again: 
source("data.Rdmpd")
# When loaded, the original data names will automatically be used.
</code>

The next method is to write out individual data objects in RDS format. This format can be binary or ASCII. Binary is more compact, while ASCII will be more efficient with version control systems like Git.

<code># Save a single object in binary RDS format
saveRDS(data, "data.rds")
# Or, using ASCII format
saveRDS(data, "data.rds", ascii=TRUE)

# To load the data again:
data &lt;- readRDS("data.rds")
</code>

It’s also possible to save multiple objects into an single file, using the RData format.


<code># Saving multiple objects in binary RData format
save(data, file="data.RData")
# Or, using ASCII format
save(data, file="data.RData", ascii=TRUE)
# Can save multiple objects
save(data, data1, file="data.RData")

# To load the data again:
load("data.RData")
</code>
An important difference between <code>saveRDS()</code> and <code>save()</code> is that, with the former, when you <code>readRDS()</code> the data, you specify the name of the object, and with the latter, when you <code>load()</code> the data, the original object names are automatically used. Automatically using the original object names can sometimes simplify a workflow, but it can also be a drawback if the data object is meant to be distributed to others for use in a different environment.


<h2>Debugging a script or function</h2>
    <h3>Problem</h3>

You want to debug a script or function.


<h3>Solution</h3>

Insert this into your code at the place where you want to start debugging:


browser()

When the R interpreter reaches that line, it will pause your code and you will be able to look at and change variables.


In the browser, typing these letters will do things:


<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>c</td>
      <td>Continue</td>
    </tr>
    <tr>
      <td>n (or Return)</td>
      <td>Next step</td>
    </tr>
    <tr>
      <td>Q</td>
      <td>quit</td>
    </tr>
    <tr>
      <td>Ctrl-C</td>
      <td>go to top level</td>
    </tr>
  </tbody>
</table>

When in the browser, you can see what variables are in the current scope.


ls()

To pause and start a browser for every line in your function:


debug(myfunction)
myfunction(x)

<h3>Useful options</h3>

By default, every time you press Enter at the browser prompt, it runs the next step. This is equivalent to pressing <code class="highlighter-rouge">n</code> and then Enter. This can be annoying. To disable it use:

options(browserNLdisabled=TRUE)

To start debugging whenever an error is thrown, run this before your function which throws an error:


options(error=recover)

If you want these options to be set every time you start R, you can put them in your ~/.Rprofile file.

<h2>Options Settings</h2>
options(digits = 3)

Usage
options(...)
getOption(x)
.Options

Arguments
...	any options can be defined, using name = value. However, only the ones below are used in ``base R''.
Further, options('name') == options()['name'], see the example.

prompt	a string, used for R's prompt; should usually end in a blank (" ").
continue	a string setting the prompt used for lines which continue over one line.
width	controls the number of characters on a line. You may want to change this if you re-size the window that R is running in.
digits	controls the number of digits to print when printing numeric values. It is a suggestion only.
editor	sets the default text editor, e.g., for edit. Set from the environment variable VISUAL on UNIX.
pager	the (stand-alone) program used for displaying ASCII files on R's console. Defaults to `$R_HOME/bin/pager' on UNIX.
browser	default HTML browser used by help.start() on UNIX.
mailer	default mailer used by bug.report(). can be "none".
contrasts	the default contrasts used in model fitting such as with aov or lm. A character vector of length two, the first giving the function to be used with unordered factors and the second the function to be used with ordered factors.
expressions	sets a limit on the number of nested expressions that will be evaluated. This is especially important on the Macintosh since stack overflow is likely if this is set too high.
keep.source	When TRUE, the default, the source code for functions loaded by is stored in their "source" attribute, allowing comments to be kept in the right places. This does not apply to functions loaded by library.
na.action	the name of a function for treating missing values (NA's) for certain situations.
papersize	the paper format used for graphics printing; currently read-only, set by environment variable R_PAPERSIZE, or in `config.site'.
printcmd	the command used for graphics printing; currently read-only, set by environment variable R_PRINTCMD, or in `config.site'.
show.signif.stars, show.coef.Pvalues	logical, affecting P value printing, see print.coefmat.
ts.eps	the relative tolerance for certain time series (ts) computations.
error	an expression governing the handling of non-catastrophic errors such as those generated by stop as well as by signals and internally detected errors. The default expression is NULL: see stop for the behaviour in that case. The function dump.frames provides one alternative that allows post-mortem debugging.
show.error.messages	a logical. Should error messages be printed? Intended for use with try or a user-installed error handler.
warn	sets the handling of warning messages. If warn is negative all warnings are ignored. If warn is zero (the default) warnings are stored until the top–level function returns. If fewer than 10 warnings were signalled they will be printed otherwise a message saying how many (max 50) were signalled. A top–level variable called last.warning is created and can be viewed through the function warnings. If warn is one, warnings are printed as they occur. If warn is two or larger all warnings are turned into errors.
echo	logical. Only used in non-interactive mode, when it controls whether input is echoed. Command-line options --quiet and --slave set this initially to FALSE.
verbose	logical. Should R report extra information on progress? Set to TRUE by the command-line option --verbose.
device	a character string giving the default device for that session.
CRAN	The URL of the preferred CRAN node for use by update.packages. Defaults to http://cran.r-project.org.
unzip	the command used unzipping help files. Defaults to "internal" when the internal unzip DLL is used.
x	a character string holding one of the above option names.

Details
Invoking options() with no arguments returns a list with the current values of the options. 
To access the value of a single option, one should use getOption("width"), e.g., rather than options("width") which is a list of length one.

The default settings of some of these options are
prompt	"> "	continue	"+ "
width	80	digits	7
expressions	500	keep.source	TRUE
show.signif.stars	TRUE	show.coef.Pvalues	TRUE
na.action	na.omit	ts.eps	1e-5
error	NULL	warn	0
echo	TRUE	verbose	FALSE
Others are set from environment variables or are platform-dependent.

Value
A list (in any case) with the previous values of the options changed, or all options when no arguments were given.

Examples
options() # printing all current options
op = options(); str(op) # nicer printing

# .Options is the same:
all(sapply(1:length(op), function(i) all(.Options[[i]] == op[[i]])))

options('width')[[1]] == options()$width # the latter needs more memory
options(digits=20)
pi

# set the editor, and save previous value
old.o = options(editor="nedit")
old.o

options(op)     # reset (all) initial options
options('digits')

## set contrast handling to be like S
options(contrasts=c("contr.helmert", "contr.poly"))

## on error, terminate the R session with error status 66
options(error=quote(q("no", status=66, runLast=FALSE)))
stop("test it")

options(papersize="a4")	
options(editor="notepad")	
options(tab.width = 2)	
options(width = 130)	
options(digits=4)	
options(stringsAsFactors=FALSE)	
options(show.signif.stars=FALSE)	
grDevices::windows.options(record=TRUE)
options(prompt="> ")	
options(continue="+ ")	
.libPaths("C:/my_R_library")	
local({r = getOption("repos")	
     r["CRAN"] = "http://cran.case.edu/"	
     options(repos=r)})	

 .First = function(){	
 library(lattice)	
 library(Hmisc)	
 source("C:/mydir/myfunctions.R")	
 cat("\nWelcome at", date(), "\n")	
}	
.Last = function(){	
 cat("\nGoodbye at ", date(), "\n")	
}	

<h2>data.table vs data.frame</h2>
<a href="data.table vs data.frame.html">data.table vs data.frame</a>
<a href="Introduction to data.table.html">Introduction to data.table</a>
<a href="http://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html">JOINing data in R using data.table</a>
<a href="Advanced tips and tricks with data.table.html"><span class="goldwhiteb">&diams;Advanced tips and tricks with data.table</span></a>

X = data.table(a=1:5, b=6:10, c=c(5:1))

length(X[b %between% c(7,9)])
length(X[b %inrange% c(7,9)])

# inrange()
Y = data.table(a=c(8,3,10,7,-10), val=runif(5))
range = data.table(start = 1:5, end = 6:10)
Y[a %inrange% range]

https://stackoverflow.com/questions/16652533/insert-a-row-in-a-data-table
insert-a-row-in-a-data-table
dt1 &lt;- list(1,4,7)
rbind(dt1, X)

dt1 &lt;- data.table(1,4,7)
rbindlist(list(dt1, X))

===================
use data.frame
df &lt;- data.frame( name=c("John", "Adam"), date=c(3, 5) )

Extract exact matches:

subset(df, date==3)
nrow(subset(df, date==3))

Extract matches in range:

subset(df, date>4 & date&lt;6)

  name date
2 Adam    5



<h2>DiagrammeR</h2>
<a href="http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html">DiagrammeR</a><br>
<a href="DiagrammeR Docs.html">DiagrammeR Docs</a>

<h2>ROC Curves</h2>
<a href="ROC Curves.html">ROC Curves</a>
<h2>capture.output to file</h2>
capture.output(options(), file="temp.txt")

capture.output to string vector
outvec= capture.output(options())

<h2>writing functions</h2>

a simple function:

square = function(x){x*x}

to square a vector:

x = c(1,3,5)
square(x)

to square a matrix:
x = cbind(c(1,3),c(5,7))
square(x)

to return a list of objects, use list():
square = function(x){return(list(x*x,x*x*x))}
square(x)

using debug() to debug:
debug(square)
square(x-a)

using print to debug in function:
square = function(x){
print(x)
print(x*x)
x*x
}

using stop() and stopifnot() to write your own error msg:
squareRoot = function(x){
	if(x&lt;0){
		stop("cannot use negative number!")
	}
	sqrt(x)
}
squareRoot(-1)

good function practices:
keep short function
write comments
try with examples
use debug and error msg

<h2>For Loop in R with Examples</h2>
<a href="https://www.guru99.com/r-for-loop.html">For Loop in R with Examples</a>

<h2>case_when and switch</h2>

switch("shape", "color" = "red", "shape" = "square", "length" = 5)

library(dplyr)
Length=3.5
mode &lt;- case_when(
                (Length < 1) ~ "Walk",
                (1 <= Length & Length < 5) ~ "bike",
                (5 <= Length & Length < 10) ~ "drive",
                (Length >= 10) ~ "fly"
          )

<h2>Calling multiple external program from R</h2>

<a href="https://stackoverflow.com/questions/21966209/calling-external-program-from-r-with-multiple-commands-in-system">Calling multiple external program from R</a>
for(i in 1:10){
cmd=paste("export FOO=",i," ; echo \"$FOO\" ",sep='')
system(cmd)
}

<h2>rmItems</h2>
# rmItems(fmList, itemList) remove itemList from fmList
 rmItems &lt;- function(fmList, itemList){return(fmList [! fmList %in% itemList])}

fmList = 1:10
itemList = c(2,4,5)
rmItems(fmList, itemList)

# remove fraudSTK
 CodeTable = rmItems(CodeTable, fraudSTK)

milList8 = c("a","b","c","d")
milList20 = c("a","b","c","f")
setdiff(fmList,itemList)


<h2>R porjects</h2>
<a href="R porjects.html">R porjects</a>

<h2>call C from R</h2>
<a href="call C from R.html">call C from R</a>

<h2>Make R Beep</h2>

<a href="https://www.geoffchappell.com/studies/windows/win32/kernel32/api/index.htm">KERNEL32 Functions</a>
<a href="https://stackoverflow.com/questions/3365657/is-there-a-way-to-make-r-beep-play-a-sound-at-the-end-of-a-script">Make R Beep</a>

rundll32.exe Kernel32.dll,Beep 550,1000
rundll32.exe cmdext.dll,MessageBeepStub
rundll32 user32.dll,MessageBeep
BOOL Beep(
  DWORD dwFreq,
  DWORD dwDuration
);

C:\Windows\Media\Delta

install.packages("audio") 
library(audio)

# play(x, rate, ...)
# x = audioSample(sin(1:8000/10), 8000)
# play(x)
# 10000 is the set of numbers, 10 is the freq code

play(sin(c(2000:1000,1500:2000) / 3))

play(sin(1:10000/3))
Sys.sleep(1)
play(sin(1:10000/4))
Sys.sleep(1)
play(sin(1:10000/5))
Sys.sleep(1)
play(sin(1:10000/6))
Sys.sleep(1)
play(sin(1:10000/7))
Sys.sleep(1)
play(sin(1:10000/8))
Sys.sleep(1)
play(sin(1:10000/9))
Sys.sleep(1)
play(sin(1:10000/10))
Sys.sleep(1)
play(sin(1:10000/20))
Sys.sleep(1)
play(sin(1:10000/30))
Sys.sleep(1)

The alarm function. It works by sending \a to the console
alarm()
cat('Hello world!\a')

alarm doesn't work so created a function that does actually make noise.

beep = function(n = 3){
    for(i in seq(n)){
        system("rundll32 user32.dll,MessageBeep -1")
        Sys.sleep(.5)
    }
}

On MacOSX you can let the computer speak:
system("say Just finished!")
and you can also change the artificial voice that will speak:
system("say -v Kathy Just finished!")

Use shell.exec("url") to open some YouTube clip on Windows

playing some music
shell.exec("foo/Born.to.be.wild.mp3")

Use notify-send command:
system("notify-send \"R script finished running\"")

Plays a typical Windows sound, which is usually on any Windows

#Function with loop, press Esc to stop

    alarm3 = function(){
        system("cmd.exe",input="C:/Windows/WinSxS/amd64_microsoft-windows-shell-sounds_31bf3856ad364e35_10.0.17134.1_none_fc93088a1eb3fd11/tada.wav")
        Sys.sleep(1)
    }

shell("C:/Users/User/Music/freesound/cmdmp3.exe C:/Users/User/Music/freesound/12.mp3")

<h2>Play a random sound</h2>
# Update all packages and "ping" when it's ready
# danger! will take a long time and may get wrong result

library(beepr)
update.packages(ask=FALSE); beep()

#Play a fanfare instead of a "ping".
beep("fanfare")
#or
beep(3)

# Play a random sound
beep(0)

beep(sound = 1, expr = NULL)
Arguments
sound character string or number specifying what sound to be played by either specifying one of the built in sounds or specifying the path to a wav file. The default is 1.
Possible sounds are:
"ping" "coin" "fanfare" "complete" "treasure" "ready" "shotgun" "mario" "wilhelm" "facebook" "sword"
beep("shotgun")

library(beepr)

alist = c("ping", "coin", "fanfare", "complete", "treasure", "ready", "shotgun", "mario", "wilhelm", "facebook", "sword")
for(item in alist){
   cat(item, "\n")
   beep(item)
   Sys.sleep(5)
}


<h2>read clipboard</h2>

simply use: readClipboard()

this gives too many columns:
read.table(file = "clipboard", sep = ",")

<h2>dplyr Data Manipulation</h2>
<a href="dplyr Data Manipulation.html">dplyr Data Manipulation</a>

<h2>Language Server Protocol</h2>
<a href="D:\R-3.4.3\bin\x64\R.exe">install.packages("languageserver") Language Server Protocol:</a>
Adding features like auto complete, go to definition, or documentation on hover for a programming language takes significant effort.

<h2>to run R by batch script</h2>
Rscript.exe  alert.r
Rscript.exe  something.r

Note: Rscript.exe cannot run with Chinese

calling chrome by batch script in sequence
can also call by R

<a href="https://kknews.cc/tech/92k2828.html">R与中文那些事 R script with Chinese</a>

<a href="https://stackoverflow.com/questions/31190468/integrating-r-and-its-graphics-with-existing-javascript-html-application"><strong>R and Javascript : Execution, Libraries, Integration</strong></a>

<h2>Create Apps with Rt</h2>
<a href="https://www.r-bloggers.com/deploying-desktop-apps-with-r/">Create Apps with R</a>


<h2>Integrate R into JavaScript</h2>
There can be various ways through which you can integrate R with JavaScript. 
Here I am discussing the following methods that I prefer for Rand Javascript integration.
<strong>1. Deploy R open</strong>
<a href="https://mran.microsoft.com/documents/getting-started" class="whitebut ">R open</a>
Through Deploy R opens you can easily embed results of various R functions like- data and charts into any application. 
This specific structure is an open source server-based system planned especially for R, which makes it simple to call the R code at a real time.
The workflow for this is simple: first, the programmer develops R script which is then published on the Deploy R server. 
The published R script that can be executed from any standard application using DeployR API. 
Using client libraries JavaScript now can make calls to the server. 
The results returned by the call can be embedded into the displayed or processed according to the application.
<strong>2. Open CPU JavaScript API</strong>
This offers straightforward RPC and information input/Output through Ajax strategies that can be fused in JavaScript of your HTML page.

<h2>Visualization with R and JavaScript</h2>
You can make use of numerous JavaScript libraries that help in creating web functionality for dynamic data visualizations for R.
Here I will be elaborating some of those tools like D3, Highchart, and leaflet. 
You can quickly implement these tools in your R and program knowledge of JavaScript is not mandatory for this.
As I have already mentioned that R is an open source analytical software, it can create high dimensional data visualizations. 
Ggplot2 is a standout among the most downloaded bundle that has helped R to accomplish best quality level as a data visualization tool.
Javascript then again is a scripting dialect in which R can be consolidated to make data visualisation. 
Numerous javascript libraries can help in creating great intuitive plots, some of them are d3.Js, c3.js, vis.js, plotly.js, sigma.js, dygraphs.js.
HTM widgets act as a bridge between R and JavaScript. 
It is the principal support for building connectors between two languages. 
The flow of a program for HTM widgets r can be visualized as under:
• Information is perused into R
• Data is handled (and conceivably controlled) by R
• Data is changed over to JavaScript Object Notation (JSON) arrange
• Information is bound to JavaScript
• Information is prepared (and conceivably controlled) by JavaScript
• Information is mapped to plotting highlights and rendered
Now let us discuss some of the data visualization packages:
<strong>• r d3 package</strong>
Data-driven documents or d3 is one of the popular JavaScript visualization libraries. 
D3 can produce visualization for almost everything including choropleths, scatter plots, graphs, network visualizations and many more. 
Multiple R packages are using only D3 plotting methods. 
You can refer r d3 package tutorials to learn about this.

<a href="https://datatricks.co.uk/creating-a-d3-js-bar-chart-in-r" class="whitebut ">Creating a D3.js bar chart in R</a>

<a href="https://blog.rstudio.com/2018/10/05/r2d3-r-interface-to-d3-visualizations/" class="whitebut ">r2d3 - R Interface to D3 Visualizations</a>
<a href="https://rstudio.github.io/r2d3/" class="whitebut ">r2d3: R Interface to D3 Visualizations</a>
<a href="https://rstudio.github.io/r2d3/articles/learning_d3.html" class="whitebut ">Learning D3</a>
<a href="https://rstudio.github.io/r2d3/articles/gallery.html" class="whitebut ">r2d3 examples</a>
• <strong>ggplot2</strong> <br> <br> It is really very easy to create plots in R, but you may ask me whether it is same for creating custom plots, the answer is “yes”, and that is the primary motivation behind why ggplot came into existence. 
With ggplot, you can make complex multi-layered designs effectively.
Here you can start plotting with axes then add points and lines. 
But the only drawback that it has it is relatively slower than base R, and new developers might find it difficult to learn.
• <strong>Leaflet</strong>
The leaflet has found its profound use in GIS (mapping), this is an open source library. 
The R packages that backings this is composed and kept up by RStudio and ports. 
Using this developer can create pop up text, custom zoom levels, tiles, polygon, planning and many more.
The ggmap bundle of javaScript can be utilised for the estimation of the latitude and longitude.
• <strong>Lattice</strong>
Lattice helps in plotting visualized multivariate data. 
Here you can have tilled plots that help in comparing values or subgroups of a given variable. 
Here you will discover numerous lattice highlights has been acquired as utilizes grid package for its usage. 
The underlying logic used by lattice is very much similar to base R.
<strong>• visNetwork</strong>
For the graphical representation of nodes and edges, the visual network is referred. 
Vis.js is a standout amongst the most famous library among numerous that can do this sort of plotting. 
visNetwork is the related with R package for this.
Network plots ought to be finished remembering nodes and edges. 
For visNetwork, these two should be separated into two different data frames one for the nodes and the other
<strong>• Highcarter</strong>
This is another visualization tool which is very similar to D3. 
You can use this tool for a variety of plots like line, spline, arealinerange, column range, polar chart and many more. 
For the commercial use of Highcarter, you need to get a license while for the non-commercial you don’t need one.
Highcarter library can be accessed very easily using various chart () functions. 
Using this function, you can create a plot in a single task. 
This function is very much similar to qplot() of ggplot2 of D3. 
chart () can produce different types of scenarios depending on the data inputs and specifications.
<strong>• RColor Brewer</strong>
With this package, you can use color for your plots, graphs, and maps. 
This package works nicely with schemes.
<strong>• Plotly</strong>
It is a well distinguish podium for data visualization that works inordinately with R and Python notebook. 
It has similarity with the high career as both are known for interactive plotting. 
But here you get some extra as it offers something that most of the package don’t like contour plots, candlestick chart, and 3d charts.
• <strong>SunTrust</strong>
It is the way for representing data visualization as it nicely describes the sequence of events. 
The diagram that it produces speaks about itself. 
You don’t need an explanation for the chart as it is self-explanatory.
• <strong>RGL</strong>
For creating three-dimensional plots in R you should check out RGL. 
It has comparability with lattice, and on the off chance that you are an accomplished R developer you will think that its simple.
<strong>• Threejs</strong>
This is an R package and an HTML widget that helps in incorporating several data visualization from the JavaScript library.
Some of the visualization function three are as follows:
• Graphjs: this is used for implementing 3D interactive data visualization. 
This function accepts igraph as the first argument. 
This manages definition for nodes and edges.
• Scatterplot3js: this function is used for creating three dimensional scatter plot.
• Globejs: this function of JavaScript is used for plotting surface maps and data points on earth.
• <strong>Shiny</strong>
The most significant benefit of JavaScript visualization is it can be implanted voluntarily into the web application. 
They can be injected into several frameworks, one of such context of R development is shiny.
Shiny is created and maintained by R Studio. 
It is a <a href="https://www.cuelogic.com/custom-software-development" data-href="https://www.cuelogic.com/custom-software-development" target="_blank">software application development</a> instrument, to a great extent employed for making wise interfaces with R. 
R shiny tutorial will take in more about shiny.
Shiny is a podium for facilitating R web development.
Connecting R with javascript using libraries
Web scuffling has formed into an original piece of examination as through this movement you can pucker your required information. 
But the data should be extracted before any web developer start to insert javascript render content into the web page. 
To help in such situation R has an excellent package called V8 which acts as an interface to JavaScript. 
R v8 is the most generally utilized capacity utilized for interfacing r in javascript. 
You can undoubtedly implement JS code in R without parting the current session. 
The library function used for this is rvest().
To run the JavaScript in R, we need a context handler, within that context handler you can start programming. 
Then you can export the R data into JavaScript.
Some other JavaScript libraries that help in analytical programming such as Linear Regression, SVMs etc. 
are as follows:
• Brain.js()
• Mljs
• Webdnn
• Convnetjs

<a href="https://hackernoon.com/r-and-javascript-execution-libraries-integration-40a30726f295">Integrating R and Javascript/HTML Application</a>
<h2>Rserve package</h2>
There is javascript implementation of Rserve client available rserve-js.
You can call R from javascript efficiently using Rserve package. 

<h2>FastRWeb</h2>
FastRWeb is an infrastructure that allows any webserver to use R scripts for generating content on the fly, such as web pages or graphics. 
URLs are mapped to scripts and can have optional arguments that are passed to the R function run from the script. 
For example http://my.server/cgi-bin/R/foo.png?n=100 would cause FastRWeb to look up a script foo.png.R, source it and call run(n="100"). 
So for example the script could be as simple as

run &lt;- function(n=10, ...) {
   p &lt;- WebPlot(800, 600)
   n &lt;- as.integer(n)
   plot(rnorm(n), rnorm(n), col=2, pch=19)
   p
}
This can potentially then be called using JavaScript to dynamically load images and display them.

<a href="https://stackoverflow.com/questions/22179512/suggestions-needed-for-building-r-server-rest-apis-that-i-can-call-from-externa/29537593#29537593">building R server REST API's that I can call from external app</a>
<h2>httpuv</h2>
You can use httpuv to fire up a basic server then handle the GET/POST requests. The following isn't "REST" per se, but it should provide the basic framework:

library(httpuv)
library(RCurl)
library(httr)

app &lt;- list(call=function(req) {

  query &lt;- req$QUERY_STRING
  qs &lt;- httr:::parse_query(gsub("^\\?", "", query))

  status &lt;- 200L
  headers &lt;- list('Content-Type' = 'text/html')

  if (!is.character(query) || identical(query, "")) {
    body &lt;- "\r\n<html><body></body></html>"
  } else {
    body &lt;- sprintf("\r\n<html><body>a=%s</body></html>", qs$a)
  }

  ret &lt;- list(status=status,
              headers=headers,
              body=body)

  return(ret)

})

message("Starting server...")

server &lt;- startServer("127.0.0.1", 8000, app=app)
on.exit(stopServer(server))

while(TRUE) {
  service()
  Sys.sleep(0.001)
}

stopServer(server)

<h2>Cucumber Selenium</h2>
<a href="https://www.guru99.com/using-cucumber-selenium.html">Cucumber Selenium</a>
<a href="https://www.youtube.com/watch?v=ZSfOEBh9BRM">Cucumber Selenium Tutorial</a>
<a href="https://blog.gtwang.org/r/rselenium-r-selenium-browser-web-scraping-tutorial/">RSelenium：R 使用 Selenium 操控瀏覽器下載網頁資料</a>

<h2>SQL databases and R</h2>
<a href="https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html">SQL databases and R</a>

<h2>SQLite</h2>
<a href="https://db.rstudio.com/databases/sqlite/">R SQLite</a>

install.packages("RSQLite")

Or install the latest development version from GitHub with:

devtools: Tools to Make Developing R Packages Easier

# install.packages("devtools")
devtools::install_github("rstats-db/RSQLite")

To install from GitHub, you’ll need a development environment.

Basic usage
library(DBI)
# Create an ephemeral in-memory RSQLite database
con &lt;- dbConnect(RSQLite::SQLite(), ":memory:")

dbListTables(con)
## character(0)
dbWriteTable(con, "mtcars", mtcars)
dbListTables(con)
## [1] "mtcars"
dbListFields(con, "mtcars")
##  [1] "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"   "gear"
## [11] "carb"
dbReadTable(con, "mtcars")
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
...

# You can fetch all results:
res &lt;- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
dbFetch(res)
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
...

dbClearResult(res)

# Or a chunk at a time
res &lt;- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
while(!dbHasCompleted(res)){
  chunk &lt;- dbFetch(res, n = 5)
  print(nrow(chunk))
}
## [1] 5
## [1] 5
## [1] 1
# Clear the result
dbClearResult(res)

# Disconnect from the database
dbDisconnect(con)
Acknowledgements
Many thanks to Doug Bates, Seth Falcon, Detlef Groth, Ronggui Huang, Kurt Hornik, Uwe Ligges, Charles Loboz, Duncan Murdoch, and Brian D. 
Ripley for comments, suggestions, bug reports, and/or patches.



<h2>Invoking the Rstudio Viewer</h2>
viewer &lt;- getOption("viewer")
viewer("<a href="https://www.rt.com/")">viewer("C:/Users/User/Desktop/Debugging with RStudio.html")</a>

<h2>to sum only elements greater than 5</h2>
a&lt;-sample.int(10,20,replace=TRUE)
sum(a[a>5])

<h2>Customizing RStudio themes</h2>
<a href="https://www.r-bloggers.com/make-rstudio-look-the-way-you-want-because-beauty-matters/">Make RStudio Beauty</a>

D:\RStudio\www\rstudio\806BBC582D6B8DF91384AD7E3EFC9A52.cache.css

<a href="https://support.rstudio.com/hc/en-us/articles/200532307-Customizing-Fonts-and-Appearance">Customizing Fonts and Appearance</a>

<h2>table()</h2>
table()的输出可以看成是一个带名字的数字向量。
可以用names()和as.numeric()分别得到名称和频数：> 

x &lt;- sample(c("a", "b", "c"), 100, replace=TRUE)
tablex = table(x)

names(tablex)
[1] "a" "b" "c"

> as.numeric(tablex)
[1] 42 25 33

可以直接把输出结果转化为数据框，as.data.frame()：> 
as.data.frame(tablex)
  x Freq
1 a   42
2 b   25
3 c   33

<h2>with(data, expr, …)</h2>
applys an expression to a dataset.
eg
with(BOD,{BOD$demand &lt;- BOD$demand + 1; print(BOD$demand)})

<h2>R regular expression</h2>
<a href="https://blog.yjtseng.info/post/regexpr/">R regex</a>

<h2>R Operator Syntax</h2>
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Syntax.html">R Operator Syntax and Precedence</a>

:: :::	access variables in a namespace
$ @	component / slot extraction
[ [[	indexing
^	exponentiation (right to left)
- +	unary minus and plus
:	sequence operator
%any%	special operators (including %% and %/%)
* /	multiply, divide
+ -	(binary) add, subtract
< > <= >= == !=	ordering and comparison
!	negation
& &&	and
| ||	or
~	as in formulae
-> ->>	rightwards assignment
&lt;- <&lt;-	assignment (right to left)
=	assignment (right to left)
?	help (unary and binary)

%%  indicates x mod y (“x modulo y”)     # modulo 餘數
%/% indicates integer division           # quotient 商

exampleRPackage
The exampleRPackage can be installed from github:

# install.packages("devtools")
devtools::install_github("mvuorre/exampleRPackage")

The file you are reading now is the package’s README, which describes how to create R packages with functions, data, and appropriate documentation. 


A Simple Example of Using replyr::gapply
It’s a common situation to have data from multiple processes in a “long” data format. 
It’s also natural to split that data apart to analyze or transform it, per-process — and then to bring the results of that data processing together, for comparison. 
Such a work pattern is called “Split-Apply-Combine”. 
A simple example of one such implementation, replyr::gapply, from package, replyr.

K-means clustering
K-means is a clustering techniques that subdivide the data sets into a set of k groups, where k is the number of groups pre-specified by the analyst.

Determining the optimal number of clusters: use factoextra::fviz_nbclust()


<h2>树状图</h2>
<a href="Dendrograms in R.html" target="_blank">Dendrograms in R</a>
<h2>shiny and rpanel - a quick comparison</h2>

Shiny is a package from RStudio that lets you produce interactive web pages. 
You build a page with some control widgets and a handler that does something dependent on the value of those widgets. 
You can build your interface programmatically or create a boilerplate html page that gets filled in by control and output widgets.

A conceptually similar pattern is implemented by the rpanel package, but this uses the tcltk toolkit. 
A panel is created, control widgets added, and callbacks on the controls can run R code to, for example, update a plot.

qq plot example
Here's the rpanel version:

require(rpanel)
# box-cox transform
bc.fn &lt;- function(y, lambda) {
    if (abs(lambda) < 0.001) 
        z &lt;- log(y) else z &lt;- (y^lambda - 1)/lambda
}

# qqplot of transformed data
qq.draw &lt;- function(panel) {
    z &lt;- bc.fn(panel$y, panel$lambda)
    qqnorm(z, main = paste("lambda =", round(panel$lambda, 2)))
    panel
}

# create a new panel with some initial data
panel &lt;- rp.control(y = exp(rnorm(50)), lambda = 1)

# add a slider that calls qq.draw on change
rp.slider(panel, lambda, -2, 2, qq.draw)

Run these functions and you should see a slider and a graphics window. 
Move the slider to modify the plot.
Note that this might not work too well under RStudio because of the way the embedded RStudio graphics device captures output.


And here is the shiny version, which comes in two files living in their own folder.  ui.R and server.R

First qqplot/ui.R:

library(shiny)
# this defines our page layout
shinyUI(pageWithSidebar(
  headerPanel("qqplot example"),
  sidebarPanel(
  # a slider called 'lambda':
    sliderInput("lambda", "Lambda value", min = -2, max = 2, step=0.01, value = 0)
  ),
  mainPanel(
    # the main panel is the plotted output from qqplot:
    plotOutput("qqPlot")
  )
))

and qqplot/server.R:

library(shiny)
shinyServer(function(input, output) {
    # b-c transform
    bc.fn &lt;- function(y, lambda) {
        if (abs(lambda) < 0.001) 
            z &lt;- log(y) else z &lt;- (y^lambda - 1)/lambda
    }
    # initial data
    y = exp(rnorm(50))
    # here's the qqplot method:
    output$qqPlot &lt;- reactivePlot(function() {
        z &lt;- bc.fn(y, input$lambda)
        qqnorm(z, main = paste("lambda =", round(input$lambda, 2)))
    })
})

With that done, launch the app with:

runApp("qqplot")
that should open up the page in your web browser. 
Hit break, stop, or control-C to quit.


Notes
The rpanel plot updates as you drag the slider, whereas shiny updates only when you let go of the slider.

I find that when I hit Control-C and break a running shiny app, then my tcltk windows go all unresponsive until I quit R and start again. 
Threading issues? This is on Linux. 
I've always had problems with tcltk widgets going unresponsive on me, or ending up unkillable.

The shiny UI looks, well, “shiny”, but the rpanel interface looks a bit old and not very exciting (if you can get excited by user interfaces).

Using the tkrplot package, you can build integrated rpanel packages with controls and plots in the same window. 
Without it, you are stuck with separate graphics and control windows.

Which should I use?
How do I know?! Shiny looks better, but I do like the update on drag of rpanel - it gives you much better feedback as you control the plot. 
Maybe this can be done in shiny with some additional work.

I don't really like the two-file method of shiny. 
Looking at the code I see the files just get sourced in, so conceivably it could be possible to run shiny apps just by specifying the shinyServer and shinyUI functions - but shiny monitors the server.R and ui.R file for changes and updates the application, which is quite nice.

So there's the basic existential dilemma. 
Choice. 
I can even throw some more things into the mix if you want - there' RServe, or RApache with gWidgetsWWW and probably many many more. 
I'm sure we can all agree that the days of needing Java and Apache Tomcat to deploy R applications to the web are now over (http://sysbio.mrc-bsu.cam.ac.uk/Rwui/tutorial/quick_tour.html).

I might try and implement some more of the rpanel examples in shiny shortly. 
Or why don't you have a go, and publish your works here?

<h2>R GUI 視窗程式設計</h2>
<a href="http://www.hmwu.idv.tw/web/R/F01-hmwu_R-GUI-Design.pdf">R GUI 視窗程式設計 tcltk/tcltk2, rpanel</a>
<a href="http://adrian.waddell.ch/EssentialSoftware/Rtcltk_geometry.pdf">Rtcltk_geometry</a>

<a href="http://rstudio-pubs-static.s3.amazonaws.com/2666_f0de0980ac9048d0a71d0f507cd83c3f.html">shiny and rpanel - a quick comparison</a>

<h3>rpanel sample</h3>
<a href="https://www.academia.edu/370260/rpanel_Simple_Interactive_Controls_for_R_Functions_Using_the_tcltk_Package_9999_">rpanel: Simple Interactive Controls for R Functions Using the tcltk Package </a>
<a href="https://www.academia.edu/attachments/1880059/download_file?st=MTU2NDAxOTg4MywxODIuMjM5LjExNS4zNg%3D%3D&s=swp-splash-header">download rpanel sample</a>
<a href="www.stats.gla.ac.uk/~adrian/rpanel">The `rpanel' package</a>
<a href="http://www.stats.gla.ac.uk/~adrian/rpanel/scripts/rpanel-paper-scripts.r">Simple Interactive Controls for R Functions scripts</a>

library(rpanel)
x11(width=4,height=4)
qq.draw &lt;- function(panel)
 { z &lt;- bc.fn(panel$y, panel$lambda)
   qqnorm(z, main = paste("lambda =",round(panel$lambda, 2)))
   panel
 }
 panel &lt;- rp.control(y = exp(rnorm(50)), lambda = 1)
 rp.slider(panel, lambda, -2, 2, qq.draw,showvalue = TRUE)


<h3>create a  matrix</h3>
A = matrix( 
c(2, 4, 3, 1, 5, 7), # the data elements 
nrow=2,              # number of rows 
ncol=3,              # number of columns 
byrow = TRUE)        # fill matrix by rows 


<h2>cross tabulations</h2>
<a href="Contingency Table.html" class="bordred2 borRad10 green whitebs">Contingency Table</a>  <a href="Xtabs exercises.html" class="bordred2 borRad10 white whitebs">Xtabs exercises</a> 

a chart is different from a table
a chart is a graphic representation
a table is a numeric representation

frequaency table is a single row table

cross tabulations, 列联表, contingency tables, 又称交互分类表 按两个或更多变量分类时所列出的频数表。

R provides many methods for creating frequency and contingency tables. 

generate frequency tables using the table( ) function, table( ) function can also create cross tab, table( ) can also generate multidimensional tables based on 3 or more categorical variables.

generate tables of proportions using the prop.table( ) function
generate marginal frequencies using margin.table( )

# 2-Way Frequency Table using table() function
attach(mtcars)
mytable &lt;- table(mtcars$gear,mtcars$cyl) # A will be rows, B will be columns 
mytable # print table 

# 2-Way Frequency Table using xtabs()
y = xtabs(~ cyl + gear, mtcars)	# xtabs gives row and col labels

margin.table(mytable, 1) # A frequencies (summed over B) 
margin.table(mytable, 2) # B frequencies (summed over A)

prop.table(mytable) # cell percentages
prop.table(mytable, 1) # row percentages 
prop.table(mytable, 2) # column percentages

# 3-Way Frequency Table 
mytable &lt;- table(A, B, C) 
mytable &lt;- table(mtcars$gear,mtcars$cyl,mtcars$mpg)
mytable

# 3-Way Frequency Table
mytable &lt;- xtabs(~A+B+c, data=mydata)
mytable &lt;- xtabs(~gear+cyl+mpg, mtcars)
summary(mytable) # chi-square test of indepedence

<a href="https://www.statmethods.net/stats/frequencies.html">Frequencies and Crosstabs</a>

<h2>parallel 平行計算</h2>
<a href="https://blog.gtwang.org/r/r-parallel-computing-module-tutorial/">R 的 parallel 平行計算套件使用教學與範例</a>
<a href="How-to go parallel in R.html">How-to go parallel in R</a>
<h2>edply</h2>
<a href="https://www.r-bloggers.com/edply-combining-plyr-and-expand-grid/">edply: combining plyr and expand.grid</a>

<h2>column merge two tables</h2>
lista = c(1:5)
listb = c(6:10)
listc = paste0(lista, "  ",listb)
lista
listb
listc
1 2 3 4 5
6 7 8 9 10
"1 6" "2 7" "3 8" "4 9" "5 10"

data from files:
lista = readLines("list1.txt")
listb = readLines("list2.txt")
listc = paste0(lista, "  ",listb)
sink("list3.txt")
cat(listc, sep="\n")
sink()

note: may use cbind in dataframe
lista = c(1:5)
listb = c(6:10)
listc = c(11:15)
MC = matrix()  # this is an empty matrix

MB = matrix( c(lista,listb,listc), nrow=5, ncol=3)  # a 3 column matrix
MC = cbind(MB[,1],MB[,3])   # now MC is a two column matrix

<h2>chop in blocks</h2>
groupPageNum = 7
theList = 1:78
if(length(theList)%%groupPageNum==0){
  pageNo = length(theList)%/%groupPageNum
}else{
  pageNo = length(theList)%/%groupPageNum +1
}
pageNo


for(page in 1:pageNo){
  if(length(theList) > groupPageNum){
	thepage= theList[1:groupPageNum]
	theList= theList[-(1:groupPageNum)]
     arrangePages(thepage)
	page = page + 1
  }else{
     arrangePages(theList)
  }
}


<h2>remove items</h2>

fmList=c('02917','01876','01960','03938','02951','02952','06820','06110','03601','01895')
itemList=c('02718','02696')

commons = fmList[fmList %in% itemList]
cat("\n\nnumber of Items to remove: ", length(commons), "\n")
for(item in commons){fmList = fmList[-(which(fmList == item))]}
fmList

<h2>extract chinanews images</h2>
http://www.chinanews.com/tp/hd2011/2019/10-20/909276.shtml
copy the thumb address and replace ending 320x300.jpg with 1000x2000

<h2>cut(x,breaks)</h2>
x = sort(rnorm(13,5,12))
x
-15.0 -11.3  -3.2   1.0   3.8   6.1   7.6   7.8  10.7 13.7  15.4  15.9  23.4

cut(x,5)
(-15.1,-7.36] (-15.1,-7.36] (-7.36,0.339] (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (8.03,15.7]   (8.03,15.7]   (8.03,15.7]   (15.7,23.5]   (15.7,23.5]  
Levels: (-15.1,-7.36] (-7.36,0.339] (0.339,8.03] (8.03,15.7] (15.7,23.5]

<h2>R GUI: RGtk or Tcl/Tk, gWidgets</h2>
<a href="http://www.ggobi.org/rgtk2/">RGtk2</a>

<a href="https://www.r-bloggers.com/playing-with-guis-in-r-with-rgtk2/">Playing with GUIs in R with RGtk2</a>

<a href="https://www.r-bloggers.com/gui-building-in-r-gwidgets-vs-deducer/">GUI building in R: gWidgets vs Deducer</a>

require("RGtk2")

window &lt;- gtkWindow()
window["title"] &lt;- "Calculator"

frame &lt;- gtkFrameNew("Calculate")
window$add(frame)

box1 &lt;- gtkVBoxNew()
box1$setBorderWidth(30)
frame$add(box1)   #add box1 to the frame

box2 &lt;- gtkHBoxNew(spacing= 10) #distance between elements
box2$setBorderWidth(24)

TextToCalculate&lt;- gtkEntryNew() #text field with expresion to calculate
TextToCalculate$setWidthChars(25)
box1$packStart(TextToCalculate)

label = gtkLabelNewWithMnemonic("Result") #text label
box1$packStart(label)

result&lt;- gtkEntryNew() #text field with result of our calculation
result$setWidthChars(25)
box1$packStart(result)

box2 &lt;- gtkHBoxNew(spacing= 10) # distance between elements
box2$setBorderWidth(24)
box1$packStart(box2)

Calculate &lt;- gtkButton("Calculate")
box2$packStart(Calculate,fill=F) #button which will start calculating

Sin &lt;- gtkButton("Sin") #button to paste sin() to TextToCalculate
box2$packStart(Sin,fill=F)

Cos &lt;- gtkButton("Cos") #button to paste cos() to TextToCalculate
box2$packStart(Cos,fill=F)

model&lt;-rGtkDataFrame(c("double","integer"))
combobox &lt;- gtkComboBox(model)
#combobox allowing to decide whether we want result as integer or double

crt &lt;- gtkCellRendererText()
combobox$packStart(crt)
combobox$addAttribute(crt, "text", 0)

gtkComboBoxSetActive(combobox,0)
box2$packStart(combobox)

DoCalculation&lt;-function(button)
{

  if ((TextToCalculate$getText())=="") return(invisible(NULL)) #if no text do nothing

   #display error if R fails at calculating
   tryCatch(
      if (gtkComboBoxGetActive(combobox)==0)
   result$setText((eval(parse(text=TextToCalculate$getText()))))
   else (result$setText(as.integer(eval(parse(text=TextToCalculate$getText()))))),
   error=function(e)
      {
      ErrorBox &lt;- gtkDialogNewWithButtons("Error",window, "modal","gtk-ok", GtkResponseType["ok"])
      box1 &lt;- gtkVBoxNew()
      box1$setBorderWidth(24)
      ErrorBox$getContentArea()$packStart(box1)

      box2 &lt;- gtkHBoxNew()
      box1$packStart(box2)

      ErrorLabel &lt;- gtkLabelNewWithMnemonic("There is something wrong with your text!")
      box2$packStart(ErrorLabel)
      response &lt;- ErrorBox$run()


      if (response == GtkResponseType["ok"])
         ErrorBox$destroy()

      }
   )

}


  PasteSin&lt;-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"sin()",sep=""))

}

PasteCos&lt;-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"cos()",sep=""))

}

#however button variable was never used inside 
#functions, without it gSignalConnect would not work
gSignalConnect(Calculate, "clicked", DoCalculation)
gSignalConnect(Sin, "clicked", PasteSin)
gSignalConnect(Cos, "clicked", PasteCos)
Now it works like planned.



library(RGtk2)
createWindow &lt;- function()
{
    window &lt;- gtkWindow()
    label &lt;- gtkLabel("Hello World")
    window$add(label)
}
createWindow()
gtk.main() # this will create error

# using this will loop dead
gtkMain()


<h2>To keep the scripts and algorithm secret</h2>
by saving functions using save(). 
For example, here's a function f() you want to keep secret:

f &lt;- function(x, y) { return(x + y)}

Save it :
save(f, file = 'C:\\Users\\Joyce\\Documents\\R\\Secret.rda')

And when you want to use the function:
load("C:\\Users\\Joyce\\Documents\\R\\Secret.rda")

Save all functions in separate files, 
put them in a folder and have one plain old .R script
loading them all in and executing whatever.
Zip the whole thing up and distribute it to whoever.
Maybe even compile it into a package.
Effectively the whole thing would be read-only then.

This solution isn't that great though.
You can still see the function in R by typing the name of the function
so it's not hidden in that sense.
But if you open the .rda files their contents are all garbled.
It all depends really on how experienced the recipients of your code are with R.

One form of having encrypted code is implemented in the petals function in the TeachingDemos package.

it would only take intermediate level programing skills to find the hidden code,
however it does take deliberate effort and the user would not be able to claim having seen the code by accident.
You would then need some type of license agreement in place to enforce any no peeking agreements.

Well you are going to need R installed on the deployment machine.

<h2>Test if characters are in a string</h2>
grepl("abc", "abcde")
note: RE will be applied, take care of the expression

<h2>get password</h2>
install.packages("getPass")
pass = getPass::getPass(msg = "PASSWORD: ", noblank = FALSE, forcemask = FALSE)

<h2>tryCatch loop</h2>
  retrieveData &lt;- function(urlAddr){      
    retryCounter = 0
    while(retryCounter < 20) {
      cat("..",retryCounter," ") 
      retriveFile &lt;- tryCatch(readLines(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter &lt;- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

  retrieveData &lt;- function(urlAddr){      
    retryCounter = 1
    while(retryCounter < 20) {
      cat("..try ",retryCounter," ") 
      retriveFile &lt;- tryCatch(read_html(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter &lt;- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

<h2>變異數分析, 方差分析 ANOVA </h2>
<a href="http://personality-project.org/r/r.guide/r.anova.html">r.anova</a>
<a href="https://alex59638.pixnet.net/blog/post/403137005-用r進行anova%28變方分析%29">ANOVA可分析多組間的差異 變異數分析 (ANOVA)</a>
<a href="http://programmermagazine.github.io/201310/htm/article3.html">主成分分析 Principle Component Analysis</a>
<h2>常用統計檢驗法簡介</h2>

T.test(又稱 T 檢定、T檢驗、t.test，以下簡稱T檢驗)
T檢驗主要用於檢定樣本的平均值，這是一項重點。

如果要看一個樣本的平均是否等於某值，要用 T 檢驗。

如果要看兩個樣本的平均是否相等，要用 T 檢驗。

T 檢驗分成三種類別
1.單樣本T檢驗(One smaple T test)
2.獨立雙樣本T檢驗
3.配對雙樣本T檢驗

要看 30 個男生的身高是否等於 180，用單樣本T檢驗。
[R語法:t.test(樣本,mu=平均)]

要看 A 班與 B 班男生身高是否相等，用獨立雙樣本T檢驗。
[R語法:t.test(A樣本,B樣本)]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A樣本,B樣本,paired=TURE)]

每一種 T 檢驗都還可以再分成雙尾與單尾檢驗。
[R語法:t.test(樣本,mu=平均,alternative= "two.sided")]

two.sided代表等於，就是雙尾的意思，也可以改成單尾的大於"greater"或是單尾的小於"less"。

重點只有"檢驗平均等於某值時"是雙尾，"檢驗平均小於某值時"是單尾，"檢驗平均大於某值時"是單尾。
請看到這裡後不要再講單尾或是雙尾了，一點意義也沒有，講大於等於小於就好了。
但Eecel沒有大於小於的選項，只有單尾雙尾，因此要自己判斷是大於還是小於(從樣本平均看即可)。
[Eecel語法:TTEST(A樣本,B樣本,2,2)]，2代表雙尾，改成1就變成單尾。

要看 30 個男生的身高是否大於 180，用單樣本T檢驗
[R語法:t.test(樣本,mu=180),alternative="greater"]

要看 A 班與 B 班男生身高差異是否小於 30，用獨立雙樣本T檢驗
[R語法:t.test(A,B,mu=30,alternative="less")]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A,B,mu=30,paired=T)]
[Eecel語法:TTEST(A樣本,B樣本,2,1)]

其中獨立雙樣本T檢驗(Welch Two smaple T test)還細分成變異數相等或變異數不相等兩種，這要看你母體與取樣的方法，如果不確定，嚴格一點是認為不相等的。

變異數相等
[R語法:t.test(A,B,mu=0,var.equal=T)]

變異數不相等
[R語法:t.test(A,B,mu=0,var.equal=F)]

<h3>卡方檢定 chi-square test(以下簡稱卡方檢定)</h3>
卡方檢驗用於確認樣本是否符合某種分配
骰子丟一百次，每面的機率是否為1/6</a>)，
或是兩個屬性之間是否有所關聯(男生是否比較容易選擇藍色商品)。

這其實是一樣的概念，假設兩個屬性之間無關，其分佈上應該會呈現隨機;
如果兩個屬性有關，例如男生喜歡藍色商品，在同樣的其況下，男生買藍色商品的次數會比男生買紅色商品的次數多，也就是不符合隨機的分配(理論上無關的話次數會一樣多)。

卡方檢定分成三種
1.適合度檢定（Goodness of fit test）
2.獨立性檢定（Test of independence）
3.同質性檢定 (Test of Homogeneity)

其實獨立性與同質性檢定是同一個東西，只是問法不一樣而已卡方適合度檢定用來檢驗樣本是否服從某種分佈，這種分佈你的心裡要有底，比方隨機(丟骰子各面是1/6)，孟德爾的紅花白花是3:1等等，如果你不知道要選擇哪種分佈，那就不能用卡方適合度檢定。

紅花969株，白花360株，檢驗是否符合孟德爾3:1的分佈，用卡方適合度檢定
chisq.test(c(969,360),p=c(0.75,0.25))
#次數表放第一個變數,p後面接機率，機率合要等於1[R語法:chisq.test(次數表,p=機率)]

骰子1000次，檢驗每面是否為1/6的分佈，用卡方適合度檢定
x=ceiling(runif(1000)*6)#丟1000次骰子, ceiling是無條件進位，讓數值落在1~6的整數
table(x)
#這是卡方檢定的重點，必須輸入統計次數，知道骰出1的有幾次，2的有幾次
chisq.test(table(x),p=c(1/6,1/6,1/6,1/6,1/6,1/6))
#次數表放第一個，p後面接分佈的機率，本次是6個1/6。

[R語法:chisq.test(次數表,p=機率)]

<h3>費雪精確性檢定 Fisher's exact test</h3>
類似卡方檢定的小樣本方式，通常用於樣本小於20的狀況，案例是猜八杯茶是先加奶還是先加茶。
fisher.test(table(real,guess))

<h2>變異數分析 ANOVA</h2>
兩組資料連續看是否有差異，用t.test，兩組以上則用ANOVA，其虛無假說H0:u1=u2=u3=...un。
若p值小於0.05，則認為並非所有的資料來自同一個母體。

若要知道到底是哪組資料不同，可使用 
pairwise.t.test(Y, B, p.adjust.method="none")
其中Y為資料列，B為組別列，並且不調整p值。
雙因子變異數分析
aov(cardspent~factor(region)*factor(gender)
使用*符號而不是+


<h2>Logistic Regression</h2>
Logistic regression, also called a logit model, is used to model dichotomous outcome variables. 

Logistic regression is a method for fitting a regression curve, y = f(x), when y is a categorical variable. 
The typical use of this model is predicting y given a set of predictors x. The predictors can be continuous, categorical or a mix of both.

一般的線性迴歸都是連續數值，例如身高或體重。
但有些情況下的應變數為類別，例如生還與否(1或0)，就可以採用Logistic Regression。

Logistic Regression有幾項要點，
1.他需要應變數為類別變項
2.他會給出一個式子，帶入自變數後(可為連續變項或類別變項)，會得出一個值
3.這個值稱為勝算比


以鐵達尼號乘客名單的資料作為範例分析
model1&lt;-glm data="titanic_passenger," family="binomial(link=" formula="survival~fare," logit="" na.action="na.exclude)&lt;/p"&gt;summary(model1)
其中fare 對 survival 的對數機率為 0.013108
勝算比為exp(0.013108)=1.013085
多一英鎊，多1%生還率。
參考資料
<a href="https://sites.google.com/site/rlearningsite/catagory/logit" target="_blank">Logistic迴歸模型</a>
<a href="http://xn--r-vc8at2mlrkqvkh65cu2ccyjyqb/" target="_blank">R语言逻辑回归分析</a>
<a href="https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/" target="_blank">How to perform a Logistic Regression in R</a><!---glm-->


<a href="http://ccckmit.wikidot.com/r:optimize">一維空間優化方法：optimize()</a>
<a href="http://ccckmit.wikidot.com/r:main">R 統計軟體 作者：陳鍾誠</a>
<a href="http://programmermagazine.github.io/201309/htm/article3.html">R 統計軟體(6) – 迴歸分析</a>

<h2>Advanced Statistics Tree-Based Models</h2>
<a href="https://www.datacamp.com/community/tutorials/decision-trees-R">Decision Trees in R</a>
<a href="https://www.guru99.com/r-decision-trees.html">Decision Tree in R with Example</a>
<a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a>
<a href="http://www.di.fc.ul.pt/~jpn/r/tree/tree.html">Classification & Regression Trees</a>
<a href="https://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html">Classification and Regression Trees with the rpart.plot package</a>

<h2>Grouping functions (tapply, by, aggregate) and the *apply family</h2>
R has many *apply functions.
Much of the functionality of the *apply family is covered by the extremely popular <code>plyr</code> package, the base functions remain useful and worth knowing.

<strong>apply</strong> - <em>When you want to apply a function to the rows or columns of a matrix (and higher-dimensional analogues); not generally advisable for data frames as it will coerce to a matrix first.</em>

<code># Two dimensional matrix
M &lt;- matrix(seq(1,16), 4, 4)

# apply min to rows
apply(M, 1, min)
[1] 1 2 3 4

# apply max to columns
apply(M, 2, max)
[1]  4  8 12 16

# 3 dimensional array
M &lt;- array( seq(32), dim = c(4,4,2))

# Apply sum across each M[*, , ] - i.e Sum across 2nd and 3rd dimension, look from top is an area
apply(M, 1, sum)
# Result is one-dimensional
[1] 120 128 136 144

# Apply sum across each M[*, *, ] - i.e Sum across 3rd dimension
apply(M, c(1,2), sum)
# Result is two-dimensional
     [,1] [,2] [,3] [,4]
[1,]   18   26   34   42
[2,]   20   28   36   44
[3,]   22   30   38   46
[4,]   24   32   40   48
</code>

If you want row/column means or sums for a 2D matrix, be sure to investigate the highly optimized, lightning-quick <code>colMeans</code>, <code>rowMeans</code>, <code>colSums</code>, <code>rowSums</code>.
<strong>lapply</strong> - <em>When you want to apply a function to each element of a list in turn and get a list back.</em>

This is the workhorse of many of the other *apply functions. 
Peel back their code and you will often find <code>lapply</code> underneath.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
$a 
[1] 1
$b 
[1] 3
$c 
[1] 91
lapply(x, FUN = sum) 
$a 
[1] 1
$b 
[1] 6
$c 
[1] 5005</code>
<strong>sapply</strong> - <em>When you want to apply a function to each element of a list in turn, but you want a <strong>vector</strong> back, rather than a list.</em>

If you find yourself typing <code>unlist(lapply(...))</code>, stop and consider <code>sapply</code>.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Compare with above; a named vector, not a list 
sapply(x, FUN = length)  
a  b  c   
1  3 91

sapply(x, FUN = sum)   
a    b    c    
1    6 5005 
</code>

In more advanced uses of <code>sapply</code> it will attempt to coerce the result to a multi-dimensional array, if appropriate. 
For example, if our function returns vectors of the same length, <code>sapply</code> will use them as columns of a matrix:

<code>sapply(1:5,function(x) rnorm(3,x))
</code>

If our function returns a 2 dimensional matrix, <code>sapply</code> will do essentially the same thing, treating each returned matrix as a single long vector:

<code>sapply(1:5,function(x) matrix(x,2,2))</code>

Unless we specify <code>simplify = "array"</code>, in which case it will use the individual matrices to build a multi-dimensional array:

<code>sapply(1:5,function(x) matrix(x,2,2), simplify = "array")</code>

Each of these behaviors is of course contingent on our function returning vectors or matrices of the same length or dimension.
<strong>vapply</strong> - <em>When you want to use <code>sapply</code> but perhaps need to squeeze some more speed out of your code.</em>

For <code>vapply</code>, you basically give R an example of what sort of thing your function will return, which can save some time coercing returned values to fit in a single atomic vector.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Note that since the advantage here is mainly speed, this
# example is only for illustration. 
We're telling R that
# everything returned by length() should be an integer of length 1. 

vapply(x, FUN = length, FUN.VALUE = 0L) 
a  b  c  
1  3 91
</code>
<strong>mapply</strong> - <em>For when you have several data structures (e.g. 
vectors, lists) and you want to apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in <code>sapply</code>.</em>

This is multivariate in the sense that your function must accept multiple arguments.

<code>#Sums the 1st elements, the 2nd elements, etc. 

mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15
#To do rep(1,4), rep(2,3), etc.
mapply(rep, 1:4, 4:1)   
[[1]]
[1] 1 1 1 1

[[2]]
[1] 2 2 2

[[3]]
[1] 3 3

[[4]]
[1] 4
</code>
<strong>Map</strong> - <em>A wrapper to <code>mapply</code> with <code>SIMPLIFY = FALSE</code>, so it is guaranteed to return a list.</em>

<code>Map(sum, 1:5, 1:5, 1:5)
[[1]]
[1] 3

[[2]]
[1] 6

[[3]]
[1] 9

[[4]]
[1] 12

[[5]]
[1] 15
</code>
<strong>rapply</strong> - <em>For when you want to apply a function to each element of a <strong>nested list</strong> structure, recursively.</em>

To give you some idea of how uncommon <code>rapply</code> is, I forgot about it when first posting this answer! Obviously, I'm sure many people use it, but YMMV. 
<code>rapply</code> is best illustrated with a user-defined function to apply:

<code># Append ! to string, otherwise increment
myFun &lt;- function(x){
    if(is.character(x)){
      return(paste(x,"!",sep=""))
    }
    else{
      return(x + 1)
    }
}

#A nested list structure
l &lt;- list(a = list(a1 = "Boo", b1 = 2, c1 = "Eeek"), 
          b = 3, c = "Yikes", 
          d = list(a2 = 1, b2 = list(a3 = "Hey", b3 = 5)))


# Result is named vector, coerced to character          
rapply(l, myFun)

# Result is a nested list like l, with values altered
rapply(l, myFun, how="replace")
</code>
<strong>tapply</strong> - <em>For when you want to apply a function to <strong>subsets</strong> of a vector and the subsets are defined by some other vector, usually a factor.</em>

The black sheep of the *apply family, of sorts. 
The help file's use of the phrase "ragged array" can be a bit <a href="https://stackoverflow.com/questions/6297201/explain-r-tapply-description/6297396#6297396">confusing</a>, but it is actually quite simple.

A vector:

<code>x &lt;- 1:20</code>

A factor (of the same length!) defining groups:

<code>y &lt;- factor(rep(letters[1:5], each = 4))</code>

Add up the values in <code>x</code> within each subgroup defined by <code>y</code>:

<code>tapply(x, y, sum)  
 a  b  c  d  e  
10 26 42 58 74 
</code>

More complex examples can be handled where the subgroups are defined by the unique combinations of a list of several factors. 
<code>tapply</code> is similar in spirit to the split-apply-combine functions that are common in R (<code>aggregate</code>, <code>by</code>, <code>ave</code>, <code>ddply</code>, etc.) Hence its black sheep status.</li>

<b>Slice vector</b>
We can use lapply() or sapply() interchangeable to slice a data frame. 
We create a function, below_average(), that takes a vector of numerical values and returns a vector that only contains the values that are strictly above the average. 

below_ave &lt;- function(x) {  
    ave &lt;- mean(x) 
    return(x[x > ave])
}

Compare both results with the identical() function.
dataf_s&lt;- sapply(dataf, below_ave)
dataf_l&lt;- lapply(dataf, below_ave)
identical(dataf_s, dataf_l)


<h2>Principal Component Methods</h2>
<a href="Principal Component Methods.html">Principal Component Methods</a>


<h2>NLP techniques</h2>
<a href="NLP techniques.html">NLP techniques</a>


<h2>RMySQL R connect to MySQL</h2>
root
asdf1234
SHOW DATABASES

# 1. Library
library(RMySQL)

# 2. Settings
db_user &lt;- 'root'
db_password &lt;- 'asdf1234'
db_name &lt;- 'sampledb'
# db_table &lt;- 'example'
db_table &lt;- 'world'

db_host &lt;- '127.0.0.1' # for local access
db_port &lt;- 3306

# 3. Read data from db
mydb &lt;-  dbConnect(MySQL(), user = db_user, password = db_password,
         dbname = db_name, host = db_host, port = db_port)
s &lt;- paste0("select * from ", db_table)
rs &lt;- dbSendQuery(mydb, s)
df &lt;-  fetch(rs, n = -1)
on.exit(dbDisconnect(mydb))

<h2>convert R {xml_node} to plain text while preserving the tags</h2>
className = "#icnt"
keywordList &lt;- html_nodes(pagesource, className)
as.character(keywordList)

<h2>convert R objects into a binary format</h2>
x &lt;- list(1, 2, 3)
serialize(x, NULL)
The serialize() function is used to convert individual R objects into a binary format that can be communicated across an arbitrary connection. This may get sent to a file, but it could get sent over a network or other connection.

<h2>Convert an R Object to a Character String</h2>
x &lt;- c("a", "b", "aaaaaaaaaaa")
toString(x)
toString(x, width = 8)


<h2>html_node, html_nodes</h2>
html_node retrieves the first element it encounter, 
while html_nodes returns each matching element in the page as a list.

use html_nodes instead of html_node.

The toString() function collapse the list of strings into one.

library(rvest)
pagesource &lt;- read_html("url")

testpost &lt;- pagesource %>% 
  html_nodes("#contentmiddle>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>Excluding Nodes in RVest</h2>
library(rvest)
pagesource &lt;- read_html("url")

testpost &lt;- pagesource %>% 
  html_nodes("#content>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>xml_remove()</h2>
By using xml_remove(), you can literally remove any nodes

text &lt;- '
<table> <tr class="alt">
     <td>1</td>
     <td>2</td>
     <td class="hidden">3</td>
   </tr>
   <tr class="tr0 close notule">
     <td colspan="9">4</td> </tr>
</table>'

html_tree &lt;- read_html(text)

#select nodes you want to remove
hidden_nodes &lt;- html_tree %>% html_nodes(".hidden")
close_nodes &lt;- html_tree %>% html_nodes(".tr0.close.notule")

#remove those nodes
xml_remove(hidden_nodes)
xml_remove(close_nodes)

html_tree %>% html_table()


<h2>view all xml_nodeset class object (output of rvest::html_nodes)</h2>
print.AsIs(keywordList)

<h2>Install package loaclly</h2>
# 安装export包
if(!require(export)){
install.packages('export')
require(export)
}

下载安装包文件
打开git bash，执行命令：
git clone https://github.com/tomwenseleers/export.git

BUILD 安装包文件
R CMD BUILD export

安装包压缩文件
R CMD INSTALL

测试export包是否可以使用
require(export)

<h2>e1071 package Support vector machine</h2>
<a href="e1071 package SVM.html" class="whitebut ">e1071 package SVM</a>

<h2>substitute()</h2>
a &lt;- 1
b &lt;- 2
substitute(a + b + z) ## a + b + z

<h2>parse, deparse & expression Functions</h2>
Basic Syntax:

expression(character)
parse(text = character)
deparse(expression)

expression() function creates object of the expression.
parse() function converts character class to an object.
deparse() function turns unevaluated expressions into character strings.

x1 = expression(2^2)  # create string expresion to object
eval(x1)  # 4

x2 = "3^4"
x2 = parse(text = x2)  # convert string object to expression
eval(x2)  # 81

# writeAlarmHtml
  writeAlarmHtml=function(dataVector){
    objName =deparse(substitute(dataVector)) # return the vector to name
    outputFilename = paste0(codeTableName," ",objName, format(Sys.Date(), format="%y%m%d"), '.html')
  }

<h2>When to use CPUs vs GPUs vs TPUs?</h2>
Behind every machine learning algorithm is hardware crunching away at multiple gigahertz. 
You may have noticed several processor options when setting up Kaggle notebooks, but which one is best for you? In this blog post, we compare the relative advantages and disadvantages of using CPUs (<a href="https://www.intel.com/content/www/us/en/products/processors/xeon.html" target="_blank">Intel Xeon</a>*) vs GPUs (<a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a>) vs TPUs (<a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a>) for training machine learning models that were written using <a href="https://keras.io/" target="_blank">tf.keras</a> (Figure 1**). 
We’re hoping this will help you make sense of the options and select the right choice for your project.


<img class="lazy" data-src="https://miro.medium.com/max/1466/1*suXcuHEe29aKLPrQnXGBrg.png">

How we prepared the test
In order to compare the performance of CPUs vs GPUs vs TPUs for accomplishing common data science tasks, we used the <a href="https://www.tensorflow.org/datasets/catalog/tf_flowers" target="_blank">tf_flowers dataset</a> to train a convolutional neural network, and then the exact same code was run three times using the three different backends (CPUs vs GPUs vs TPUs; GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM). 
The accompanying <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">tutorial notebook</a> demonstrates a few best practices for getting the best performance out of your TPU.
For example:

Using a dataset of sharded files (<a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">e.g., .TFRecord</a>)
Using the <a href="https://www.tensorflow.org/guide/data" target="_blank">tf.data</a> API to pass the training data to the TPU
Using large batch sizes (e.g. 
batch_size=128)

By adding these precursory steps to your workflow, it is possible to avoid a common I/O bottleneck that otherwise prevents the TPU from operating at its full potential. 
You can find additional tips for optimizing your code to run on TPUs by visiting the official <a href="https://www.kaggle.com/docs/tpu" target="_blank">Kaggle TPU documentation</a>.
How the hardware performed
The most notable difference between the three hardware types that we tested was the speed that it took to train a model using <a href="https://keras.io/" target="_blank">tf.keras</a>. 
The tf.keras library is one of the most popular machine learning frameworks because tf.keras makes it easy to quickly experiment with new ideas. 
If you spend less time writing code then you have more time to perform your calculations, and if you spend less time waiting for your code to run, then you have more time to evaluate new ideas (Figure 2). 
tf.keras and TPUs are a powerful combination when participating in <a href="https://kaggle.com/c/flower-classification-with-tpus" target="_blank">machine learning competitions</a>!


<img class="lazy" data-src="https://miro.medium.com/max/1438/1*bqmG-YzgJzVeLbQ5Ym1iFg.png">
For our first experiment, we used the same code (a modified version*** of the <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">official tutorial notebook</a>) for all three hardware types, which required using a very small batch size of 16 in order to avoid out-of-memory errors from the CPU and GPU. 
Under these conditions, we observed that TPUs were responsible for a ~100x speedup as compared to CPUs and a ~3.5x speedup as compared to GPUs when training an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model (Figure 3). 
Because TPUs operate more efficiently with large batch sizes, we also tried increasing the batch size to 128 and this resulted in an additional ~2x speedup for TPUs and out-of-memory errors for GPUs and CPUs. 
Under these conditions, the TPU was able to train an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model more than 7x as fast as the GPU from the previous experiment****.


<img class="lazy" data-src="https://miro.medium.com/max/1438/1*p2X9DQcq9K5Iu76Kk82vrg.png">
The observed speedups for model training varied according to the type of model, with Xception and Vgg16 performing better than ResNet50 (Figure 4). Model training was the only type of task where we observed the TPU to outperform the GPU by such a large margin. 
For example, we observed that in our hands the TPUs were ~3x faster than CPUs and ~3x slower than GPUs for performing a small number of predictions (TPUs perform exceptionally when making predictions in some situations such as when <a href="https://docs.google.com/presentation/d/1O49AkNyYV48n0X4nWr7KE-5aask88pz9gBSQ26ZG-5o/edit#slide=id.g50ce3d3866_0_1590" target="_blank">making predictions</a> on very large batches, which were not present in this experiment).


<img class="lazy" data-src="https://miro.medium.com/max/46/1*p7U2zlYn9O5Yvjluh2P-dg.png">

<img class="lazy" data-src="https://miro.medium.com/max/1466/1*p7U2zlYn9O5Yvjluh2P-dg.png">
To supplement these results, we note that <a href="https://arxiv.org/abs/1907.10701" target="_blank">Wang<em> et. 
al</em></a> have developed a rigorous benchmark called ParaDnn [1] that can be used to compare the performance of different hardware types for training machine learning models. 
By using this method Wang<em> et. 
al</em> were able to conclude that the performance benefit for parameterized models ranged from 1x to 10x, and the performance benefit for real models ranged from 3x to 6.8x when a TPU was used instead of a GPU (Figure 5). 
TPUs perform best when combined with sharded datasets, large batch sizes, and large models.


<img class="lazy" data-src="https://miro.medium.com/max/1466/1*QbP2CPDZH5BQWlnaTtW3oA.png">
Price considerations when training models
While our comparisons treated the hardware equally, there is a sizeable difference in pricing. TPUs are ~5x as expensive as GPUs (<a href="https://cloud.google.com/compute/gpus-pricing" target="_blank">$1.46/hr</a> for a <a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a> GPU vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$8.00/hr</a> for a <a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a> vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$4.50/hr</a> for the TPUv2 with “on-demand” access on <a href="https://cloud.google.com/pricing/" target="_blank">GCP</a> ). 
If you are trying to optimize for cost then it makes sense to use a TPU if it will train your model at least 5 times as fast as if you trained the same model using a GPU.
We consistently observed model training speedups on the order of ~5x when the data was stored in <a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">a sharded format</a> in a <a href="https://www.kaggle.com/paultimothymooney/how-to-move-data-from-kaggle-to-gcs-and-back" target="_blank">GCS bucket</a> then passed to the TPU in large batch sizes, and therefore we recommend TPUs to cost-conscious consumers that are familiar with the <a href="http://tf.data" target="_blank">tf.data</a> API.
Some machine learning practitioners prioritize the reduction of model training time as opposed to prioritizing the reduction of model training costs. 
For someone that just wants to train their model as fast as possible, the TPU is the best choice. 
If you spend less time training your model, then you have more time to iterate upon new ideas. 
But don’t take our word for it — you can evaluate the performance benefits of CPUs, GPUs, and TPUs by running your own code in a <a href="https://www.kaggle.com/docs/kernels#the-kernels-environment" target="_blank">Kaggle Notebook</a>, free-of-charge. 
Kaggle users are already having a lot of fun and success experimenting with TPUs and text data: check out <a href="https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127333" target="_blank">this forum post</a> that describes how TPUs were used to train a BERT transformer model to win $8,000 (2nd prize) in a recent <a href="https://www.kaggle.com/c/tensorflow2-question-answering" target="_blank">Kaggle competition</a>.
Which hardware option should you choose?
In summary, we recommend CPUs for their versatility and for their large memory capacity. 
GPUs are a great alternative to CPUs when you want to speed up a variety of data science workflows, and TPUs are best when you specifically want to train a machine learning model as fast as you possibly can.
You can get better results by optimizing your code for the specific hardware that you are using and we think it would be especially interesting to compare runtimes for code that has been optimized for a GPU to runtimes for code that has been optimized for a TPU. 
For example, it would be interesting to record the time that it takes to train a gradient-boosted model using a GPU-accelerated library such as <a href="https://rapids.ai/" target="_blank">RAPIDS.ai</a> and then to compare that to the time that it takes to train a deep learning model using a TPU-accelerated library such as <a href="https://keras.io/" target="_blank">tf.keras</a>.
What is the least amount of time that one can train an accurate machine learning model? How many different ideas can you evaluate in a single day? When used in combination with tf.keras, TPUs allow machine learning practitioners to spend less time writing code and less time waiting for their code to run — leaving more time to evaluate new ideas and improve one’s performance in <a href="http://kaggle.com/c/flower-classification-with-tpus" target="_blank">Kaggle Competitions</a>.

<h3>Footnotes</h3>* CPU types vary according to variability. 
In addition to the Intel Xeon CPUs, you can also get assigned to either Intel Skylake, Intel Broadwell, or Intel Haswell CPUs. 
GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM).
** Image for Figure 1 from <a href="https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference" target="_blank">https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference,</a> with permission.
*** The tutorial notebook was modified to keep the parameters (e.g. 
batch_size, learning_rate, etc) consistent between the three different backends.
**** CPU and GPU experiments used a batch size of 16 because it allowed the Kaggle notebooks to run from top to bottom without memory errors or 9-hr timeout errors. 
Only TPU-enabled notebooks were able to run successfully when the batch size was increased to 128.


<h2>Diff function – Difference between elements of vector</h2>
Differences between elements of a vector

diff(x, lag = 1, differences = 1)
x – numeric vector
lag-an integer indicating how many lags to use.
Difference- order of difference

# diff in r examples
> x=c(1,2,3,5,8,13,21)
> diff(x)
[1] 1 1 2 3 5 8

The diff function provides the option “lag”.
The default specification of this option is 1.

If we want to increase the size of the lag, we can specify the lag option within the diff command as follows:

x &lt;- c(5, 2, 10, 1, 3)
diff(x, lag = 2)                # Apply diff with lag
# 5 -1 -7

Example of difference function in R with lag 1 and differences 2:

#difference function in R with lag=1 and differences=2

diff(c(2,3,5,18,4,6,4),lag=1,differences=2)
First it is differenced with lag=1 and the result is again differenced with lag=1
So the output will be
[1]   1  11  -27   16   -4

ie. get the lag difference result, and then redo the difference again on the result:
2,3,5,18,4,6,4
  1,2,13,-14,2,-2
    1,11,-27,16,-4

<h2>cut2 function</h2>
cut2(x, cuts, m, g, levels.mean, digits, minmax=TRUE, oneval=TRUE)
Cut a Numeric Variable into Intervals
but left endpoints are inclusive and labels are of the form [lower, upper), except that last interval is [lower,upper].

x &lt;- runif(1000, 0, 100)
z &lt;- cut2(x, c(10,20,30))
table(z)
table(cut2(x, g=10))      # quantile groups
table(cut2(x, m=50))      # group x into intevals with at least 50 obs.

<h2>To clear up the memory</h2>
rm(list = ls())
.rs.restartR() # this will restart

memory.size(max=T) # gives the amount of memory obtained by the OS
memory.size(max=F) # gives the amount of memory being used
m = matrix(runif(10e7), 10000, 1000)
memory.size(max=F)

To clear up the memory
gc()
memory.size(max=F)
# still some memory being used

<h2>remove XML nodes</h2>
<a href="https://cran.r-project.org/web/packages/xml2/vignettes/modification.html" class="whitebut ">Node Modification</a>
<a href="https://cran.r-project.org/web/packages/XML/XML.pdf" class="whitebut ">Package XML</a>

#find parent nodes
parent&lt;- review %>% html_nodes("blockquote")

#find children nodes to exclude
toremove&lt;-parent %>% html_node("div.bbcode_container")

#remove nodes
xml_remove(toremove)

The xml_remove() can be used to remove a node (and it’s children) from a tree. 

library(XML)
r &lt;- xmlRoot(doc)
removeNodes(r[names(r) == "location"])

<h2>Comment out block of code</h2>

if(FALSE) {
  all your code
}


<h2>Reading XML data</h2>
Data in XML format are rarely organized in a way that would allow the xmlToDataFrame function to work. 
You're better off extracting everything in lists and then binding the lists together in a data frame:

require(XML)
data &lt;- xmlParse("http://forecast.weather.gov/MapClick.php?lat=29.803&lon=-82.411&FcstType=digitalDWML")

xml_data &lt;- xmlToList(data)

<code>&gt; install.packages("XML")</code>
<code>&gt; library(XML)
text = paste0("&lt;bookstore>&lt;book>","&lt;title>Everyday Italian&lt;/title>","&lt;author>Giada De Laurentiis&lt;/author>","&lt;year>2005&lt;/year>","&lt;/book>&lt;/bookstore>")
</code>
Parse the XML file
<code>xmldoc &lt;- xmlParse(text)
rootNode &lt;- xmlRoot(xmldoc)
rootNode[1]

xmlToDataFrame(nodes = getNodeSet(xmldoc, "//title"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//author"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//book"))

newdf = xmlToDataFrame(getNodeSet(xmldoc, "//book"))
newdf = xmlToDataFrame(getNodeSet(xmldoc, "//title"))
</code>

Extract XML data:

<code>&gt; data &lt;- xmlSApply(rootNode,function(x) xmlSApply(x, xmlValue))</code>

text = paste0("&lt;CD>","&lt;TITLE>Empire Burlesque&lt;/TITLE>","&lt;ARTIST>Bob Dylan&lt;/ARTIST>","&lt;COUNTRY>USA&lt;/COUNTRY>","&lt;COMPANY>Columbia&lt;/COMPANY>","&lt;PRICE>10.90&lt;/PRICE>","&lt;YEAR>1985&lt;/YEAR>","&lt;/CD>")
<code>xmldoc &lt;- xmlParse(text)
rootNode &lt;- xmlRoot(xmldoc)
rootNode[1]</code>

Convert the extracted data into a data frame:

<code>&gt; cd.catalog &lt;- data.frame(t(data),row.names=NULL)</code>

Verify the results

The <code>xmlParse</code> function returns an object of the <code>XMLInternalDocument</code> class, which is a C-level internal data structure.
The <code>xmlRoot()</code> function gets access to the root node and its elements. 
We check the first element of the root node:

<code>&gt; rootNode[1]

$CD
&lt;CD&gt;
  &lt;TITLE&gt;Empire Burlesque&lt;/TITLE&gt;
  &lt;ARTIST&gt;Bob Dylan&lt;/ARTIST&gt;
  &lt;COUNTRY&gt;USA&lt;/COUNTRY&gt;
  &lt;COMPANY&gt;Columbia&lt;/COMPANY&gt;
  &lt;PRICE&gt;10.90&lt;/PRICE&gt;
  &lt;YEAR&gt;1985&lt;/YEAR&gt;
&lt;/CD&gt;
attr(,"class")
[1] "XMLInternalNodeList" "XMLNodeList"</code>
To extract data from the root node, we use the <code>xmlSApply()</code> function iteratively over all the children of the root node. 
The <code>xmlSApply</code> function returns a matrix.
To convert the preceding matrix into a data frame, we transpose the matrix using the <code>t()</code> function. 
We then extract the first two rows from the <code>cd.catalog</code> data frame:

<code>&gt; cd.catalog[1:2,]
             TITLE       ARTIST COUNTRY     COMPANY PRICE YEAR
1 Empire Burlesque    Bob Dylan     USA    Columbia 10.90 1985
2  Hide your heart Bonnie Tyler      UK CBS Records  9.90 1988</code>

XML data can be deeply nested and hence can become complex to extract. 
Knowledge of <code>XPath</code> will be helpful to access specific XML tags. 
R provides several functions such as <code>xpathSApply</code> and <code>getNodeSet</code> to locate specific elements.
<h4>Extracting HTML table data from a web page</h4>
Though it is possible to treat HTML data as a specialized form of XML, R provides specific functions to extract data from HTML tables as follows:

<code>&gt; url &lt;- "http://en.wikipedia.org/wiki/World_population"

webpage = read_html(url)
output = htmlParse(webpage)
tables = readHTMLTable(output)
world.pop = tables[[5]]

table.list = readHTMLTable(output, header=F)

u = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
webpage = read_html(u)
tables = readHTMLTable(webpage)
names(tables)
</code>
The <code>readHTMLTable()</code> function parses the web page and returns a <code>list</code> of all tables that are found on the page. 
For tables that have an <code>id</code> attribute, the function uses the <code>id</code> attribute as the name of that list element.
We are interested in extracting the "10 most populous countries," which is the fifth table; hence we use <code>tables[[5]]</code>.

<h4>Extracting a single HTML table from a web page</h4>

A single table can be extracted using the following command:

<code>&gt; table &lt;- readHTMLTable(url,which=5)</code>
Specify <code>which</code> to get data from a specific table. 
R returns a data frame.

<h2>use xpathSApply to extract html</h2>
library(RCulr)
library(XML)
 
html &lt;- read_html("http://tonybreyal.wordpress.com/2011/11/17/cool-hand-luke-aldwych-theatre-london-2011-production/", followlocation = TRUE)

doc = htmlParse(html, asText=TRUE)
plain.text &lt;- xpathSApply(doc, "//p", xmlValue)
cat(paste(plain.text, collapse = "\n"))

pageHeader = "http://www.hkej.com/template/dnews/jsp/toc_main.jsp"
html &lt;- read_html(pageHeader, followlocation = TRUE)
doc = htmlParse(html, asText=TRUE)
plain.text &lt;- xpathSApply(doc, "//a", xmlValue)
cat(paste(plain.text, collapse = "\n"))

<h2>reading XML using xml2</h2>
library(xml2)
library(purrr)

txt &lt;- '&lt;Doc name="Doc1">
    &lt;Lists Count="1">
        &lt;List Name="List1">
            &lt;Points Count="3">
                &lt;Point Id="1">
                    &lt;Tags Count ="1">"a"&lt;/Tags>
                    &lt;Point Position="1"  /> 
                &lt;/Point>
                &lt;Point Id="2">
                    &lt;Point Position="2"  /> 
                &lt;/Point>
                &lt;Point Id="3">
                    &lt;Tags Count="1">"c"&lt;/Tags>
                    &lt;Point Position="3"  /> 
                &lt;/Point>
            &lt;/Points>
        &lt;/List>
    &lt;/Lists>
&lt;/Doc>'

doc &lt;- read_xml(txt)
xml_find_all(doc, ".//Points/Point") %>% 
  map_df(function(x) {
    list(
      Point=xml_attr(x, "Id"),
      Tag=xml_find_first(x, ".//Tags") %>%  xml_text() %>%  gsub('^"|"$', "", .),
      Position=xml_find_first(x, ".//Point") %>% xml_attr("Position")
    )
  })



<h2>An Introduction to XPath: How to Get Started</h2>

XPath is a powerful language that is often used for scraping the web. 
It allows you to select nodes or compute values from an XML or HTML document and is actually one of the languages that you can use to extract web data using Scrapy. 
The other is CSS and while CSS selectors are a popular choice, XPath can actually allow you to do more.

With XPath, you can extract data based on text elements' contents, and not only on the page structure. 
So when you are scraping the web and you run into a hard-to-scrape website, XPath may just save the day (and a bunch of your time!).

This is an introductory tutorial that will walk you through the basic concepts of XPath, crucial to a good understanding of it, before diving into more complex use cases.
<h3>The basics</h3>
Consider this HTML document:

<code>&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;My page&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h2&gt;Welcome to my &lt;a href="#"&gt;page&lt;/a&gt;&lt;/h2&gt;
    &lt;p&gt;This is the first paragraph.&lt;/p&gt;
    &lt;!-- this is the end --&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

XPath handles any XML/HTML document as a tree. 
This tree's root node is not part of the document itself. 
It is in fact the parent of the document element node (<code>&lt;html&gt;</code> in case of the HTML above). 
This is how the XPath tree for the HTML document looks like:


<img class="lazy" data-src="https://blog.scrapinghub.com/hs-fs/hubfs/tree-7.png" style="background-color: gray">

As you can see, there are many node types in an XPath tree:

<strong>Element node:</strong> represents an HTML element, a.k.a an HTML tag.
<strong>Attribute node:</strong> represents an attribute from an element node, e.g. 
“href” attribute in <code>&lt;a href=”http://www.example.com”&gt;example&lt;/a&gt;</code>.
<strong>Comment node:</strong> represents comments in the document (<code>&lt;!-- … --&gt;</code>).
<strong>Text node:</strong> represents the text enclosed in an element node (<code>example</code> in <code>&lt;p&gt;example&lt;/p&gt;</code>).

Distinguishing between these different types is useful to understand how XPath expressions work. 
Now let's start digging into XPath.

Here is how we can select the title element from the page above using an XPath expression:

/html/head/title


This is what we call a location path. 
It allows us to specify the path from the <strong>context node</strong> (in this case the root of the tree) to the element we want to select, as we do when addressing files in a file system. 
The location path above has three location steps, separated by slashes. 
It roughly means: <em>start from the ‘html’ element, look for a ‘head’ element underneath, and a ‘title’ element underneath that ‘head’</em>. 
The context node changes in each step. 
For example, the <code>head</code> node is the context node when the last step is being evaluated.

However, we usually don't know or don’t care about the full explicit node-by-node path, we just care about the nodes with a given name. 
We can select them using:

//title


Which means:<em> look in the whole tree, starting from the root of the tree (<code>//</code>) and select only those nodes whose name matches <code>title</code></em>. 
In this example, <code>//</code> is the <strong>axis</strong> and <code>title</code> is the <strong>node test</strong>.

In fact, the expressions we've just seen are using XPath's abbreviated syntax. 
Translating <code>//title</code> to the full syntax we get:

/descendant-or-self::node()/child::title


So, <code>//</code> in the abbreviated syntax is short for <code>descendant-or-self</code>, which means <em>the current node or any node below it in the tree</em>. 
This part of the expression is called the <strong>axis</strong> and it specifies a set of nodes to select from, based on their direction on the tree from the current context (downwards, upwards, on the same tree level). 
Other examples of axes are: parent, child, ancestor, etc -- we’ll dig more into this later on.

The next part of the expression, <code>node()</code>, is called a <strong>node test</strong>, and it contains an expression that is evaluated to decide whether a given node should be selected or not. 
In this case, it selects nodes from all types. 
Then we have another axis,<code>child</code>, which means <em>go to the child nodes from the current context</em>, followed by another node test, which selects the nodes named as <code>title</code>.

<blockquote>
So, the <strong>axis</strong> defines where in the tree the <strong>node test</strong> should be applied and the nodes that match the node test will be returned as a <strong>result</strong>.

</blockquote>
You can test nodes against their name or against their type.

Here are some examples of name tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>/html</td>
<td>Selects the node named <code>html</code>, which is under the root.</td>
</tr>
<tr>
<td>/html/head</td>
<td>Selects the node named <code>head</code>, which is under the <code>html</code> node.</td>
</tr>
<tr>
<td>//title</td>
<td>Selects all the <code>title</code> nodes from the HTML tree.</td>
</tr>
<tr>
<td>//h2/a</td>
<td>Selects all <code>a</code> nodes which are directly under an <code>h2</code> node.</td>
</tr>
</tbody>
</table>
And here are some examples of node type tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//comment()</td>
<td>Selects only comment nodes.</td>
</tr>
<tr>
<td>//node()</td>
<td>Selects any kind of node in the tree.</td>
</tr>
<tr>
<td>//text()</td>
<td>Selects only text nodes, such as "This is the first paragraph".</td>
</tr>
<tr>
<td>//*</td>
<td>Selects all nodes, except comment and text nodes.</td>
</tr>
</tbody>
</table>
We can also combine name and node tests in the same expression. 
For example:

//p/text()


This expression selects the text nodes from inside <code>p</code> elements. 
In the HTML snippet shown above, it would select "This is the first paragraph.".

Now, <strong>let’s see how we can further filter and specify things</strong>. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li&gt;Quote 1&lt;/li&gt;
      &lt;li&gt;Quote 2 with &lt;a href="..."&gt;link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Quote 3 with &lt;a href="..."&gt;another link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;h2&gt;Quote 4 title&lt;/h2&gt; ...&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the first <code>li</code> node from the snippet above. 
We can do this with:

//li[position() = 1]


The expression surrounded by square brackets is called a predicate and it filters the node set returned by <code>//li</code> (that is, all <code>li</code> nodes from the document) using the given condition. 
In this case it checks each node's position using the <code>position()</code> function, which returns the position of the current node in the resulting node set (notice that positions in XPath start at 1, not 0). 
We can abbreviate the expression above to:

//li[1]


Both XPath expressions above would select the following element:

    &lt;li class="quote"&gt;Quote 1&lt;/li&gt;


Check out a few more predicate examples:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//li[position()%2=0]</td>
<td>Selects the <code>li</code> elements at even positions.</td>
</tr>
<tr>
<td>//li[a]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element.</td>
</tr>
<tr>
<td>//li[a or h2]</td>
<td>Selects the <code>li</code> elements which enclose either an <code>a</code> or an <code>h2</code> element.</td>
</tr>
<tr>
<td>//li[ a [ text() = "link" ] ]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element whose text is "link". 
Can also be written as <code>//li[ a/text()="link" ]</code>.</td>
</tr>
<tr>
<td>//li[last()]</td>
<td>Selects the last <code>li</code> element in the document.</td>
</tr>
</tbody>
</table>
So, a location path is basically composed by steps, which are separated by <code>/</code> and each step can have an axis, a node test and a predicate. 
Here we have an expression composed by two steps, each one with axis, node test and predicate:

&lt;span style="font-weight: 400;"&gt;//li[ 4 ]/h2[ text() = "Quote 4 title" ]&lt;/span&gt;


And here is the same expression, written using the non-abbreviated syntax:

/descendant-or-self::node()<br>
    /child::li[ position() = 4 ]<br>
        /child::h2[ text() = "Quote 4 title" ]


We can also <strong>combine</strong> multiple XPath expressions in a single one using the union operator <code>|</code>. 
For example, we can select all <code>a</code> and <code>h2</code> elements in the document above using this expression:

//a | //h2


Now, consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li id="begin"&gt;&lt;a href="https://scrapy.org"&gt;Scrapy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://scrapinghub.com"&gt;Scrapinghub&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://blog.scrapinghub.com"&gt;Scrapinghub Blog&lt;/a&gt;&lt;/li&gt;
      &lt;li id="end"&gt;&lt;a href="http://quotes.toscrape.com"&gt;Quotes To Scrape&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the <code>a</code> elements whose link points to an HTTPS URL. 
We can do it by checking their <code>href</code> <strong>attribute</strong>:

//a[starts-with(@href, "https")]


This expression first selects all the <code>a</code> elements from the document and for each of those elements, it checks whether their <code>href</code> attribute starts with "https". 
We can access any node attribute using the <code>@attributename</code> syntax.

Here we have a few additional examples using attributes:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//a[@href="https://scrapy.org"]</td>
<td>Selects the <code>a</code> elements pointing to https://scrapy.org.</td>
</tr>
<tr>
<td>//a/@href</td>
<td>Selects the value of the <code>href</code> attribute from all the <code>a</code> elements in the document.</td>
</tr>
<tr>
<td>//li[@id]</td>
<td>Selects only the <code>li</code> elements which have an <code>id</code> attribute.</td>
</tr>
</tbody>
</table>
<h3>More on Axes</h3>
We've seen only two types of axes so far:

descendant-or-self
child

But there's plenty more where they came from and we'll see a few examples. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;p&gt;Intro paragraph&lt;/p&gt;
    &lt;h1&gt;Title #1&lt;/h1&gt;
    &lt;p&gt;A random paragraph #1&lt;/p&gt;
    &lt;h1&gt;Title #2&lt;/h1&gt;
    &lt;p&gt;A random paragraph #2&lt;/p&gt;
    &lt;p&gt;Another one #2&lt;/p&gt;
    A single paragraph, with no markup
    &lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Now we want to extract only the first paragraph after each of the titles. 
To do that, we can use the <code>following-sibling</code> axis, which selects all the siblings after the context node. 
Siblings are nodes who are children of the same parent, for example all children nodes of the <code>body</code> tag are siblings. 
This is the expression:

//h1/following-sibling::p[1]


In this example, the context node where the <code>following-sibling</code> axis is applied to is each of the <code>h1</code> nodes from the page.

What if we want to select only the text that is right before the <code>footer</code>? We can use the <code>preceding-sibling</code> axis:

//div[@id='footer']/preceding-sibling::text()[1]


In this case, we are selecting the first text node before the <code>div</code> footer (<em>"A single paragraph, with no markup"</em>).

XPath also allows us to select elements based on their text content. 
We can use such a feature, along with the <code>parent</code> axis, to select the parent of the <code>p</code> element whose text is "Footer text":

//p[ text()="Footer text" ]/..


The expression above selects <code>&lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;</code>. 
As you may have noticed, we used <code>..</code> here as a shortcut to the <code>parent</code> axis.

As an alternative to the expression above, we could use:

//*[p/text()="Footer text"]


It selects, from all elements, the ones that have a <code>p</code> child which text is "Footer text", getting the same result as the previous expression.

You can find additional axes in the XPath specification: https://www.w3.org/TR/xpath/#axes

<h3>Wrap up</h3>
XPath is very powerful and this post is just an introduction to the basic concepts. 
If you want to learn more about it, check out these resources:

http://zvon.org/comp/r/tut-XPath_1.html
http://fr.slideshare.net/scrapinghub/xpath-for-web-scraping
https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/

And <strong>stay tuned</strong>, because we will post a series with more XPath tips from the trenches in the following months.

<h2>to handle UTF</h2>
options("encoding" = "native.enc") # this is the natural environment
Sys.setlocale(category = 'LC_ALL', 'Chinese')	# to show chinese
# Sys.getlocale()
# options("encoding")

theNewsHeader = readLines("newsHeader.txt", encoding="UTF-8") # load UTF-8 file
options("encoding" = "UTF-8") # write UTF-8
sink("temp.html")

<h2><span class="gold embossts">R jsonlite to handle JSON</span></h2>
install.packages("jsonlite")
library(jsonlite)

# convert data frame to JSON array
my.json &lt;- toJSON(mtcars)

# convert JSON array to data frame
my.df &lt;- fromJSON(my.json)

# check data equality
all.equal(mtcars, my.df)
[1] TRUE

- set simplifyVector to FALSE, fromJSON will keep the raw JSON structure
ie, convert to list
fromJSON(json, simplifyVector = FALSE)

- fromJSON will convert multiple JSON structures to data frame
we may convert JSOn to data frame, and after fiddling, toJSON back to JSON.

- fromJSON will convert JSON matrix to R matrix

- higher order dimension JSON will be converted to R matrixs

<h2>Extract Components from Lists</h2>
Using [ ]
to extract a list components

Using [[ ]]
to extract only a single component

<h2>to view a list or dataframe</h2>
names(test), summary(test), head(test), tail(test), str(test)
typeof(test)

<h2>R function: cut</h2>
v &lt;- c( 8, 13, 19, 3, 14, 7, 6, 12, 18, 9, 7, 14, 2, 3, 8, 11, 17)
c &lt;- cut(v, c(0, 5, 10, 15, 20))
str(c)
 Factor w/ 4 levels "(0,5]","(5,10]",..: 2 3 4 1 3 2 2 3 4 2 ...

c # shows every element's category
#
#  [1] (5,10]  (10,15] (15,20] (0,5]   (10,15] (5,10]  (5,10]  (10,15] (15,20]
# [10] (5,10]  (5,10]  (10,15] (0,5]   (0,5]   (5,10]  (10,15] (15,20]

# Levels: (0,5] (5,10] (10,15] (15,20]

<h2>use cumsum() to create cumulative frequency graph</h2>

dataset = sample(1:20,100, replace= TRUE)
breaks = seq(0, 20, by=2) 
datasetCategory = cut(dataset, breaks, right=FALSE) 
dataset.freq = table(datasetCategory)

barplot(dataset.freq) # this show every category but not cumulative

We then compute its cumulative frequency with cumsum, add a starting zero element, and plot the graph.

cumfreq0 = c(0, cumsum(dataset.freq)) 
plot(breaks, cumfreq0,                 # plot the data 
 main="Old Faithful Eruptions",        # main title 
 xlab="dataset minutes",               # x−axis label 
 ylab="cumulative frequency graph")    # y−axis label 
lines(breaks, cumfreq0)                # join the points

<h2>to prevent scientific notation</h2>
Use a large positive value like 999 in options:
options(scipen=999)
to revert it back, the default scipen is 0

<h2>process daily data</h2>
# kline_dayqfq={"code":0,"msg":"","data":{"hk00700":{"qfqday":[["2020-01-14","410.000","400.400","413.000","396.600","26827634.000",{},"0.000","1086386.492"],

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/hkfqkline/get?_var=kline_dayqfq&param=hk00700,day,,,40,qfq"

my.json &lt;- readLines(urlAddr, warn=F)
my.json = gsub("kline_dayqfq=","",my.json) # remove the leading command

my.dataframe = fromJSON(my.json)
my.dataframe = my.dataframe[[3]][[1]][[1]] # 40 obs., list of list
# chr "2020-01-14"   Date   1
# chr "410.000"      open   2
# chr "400.400"      close  3
# chr "413.000"      high   4
# chr "396.600"      low    5
# chr "26827634.000" Qty    6
# Named list()              7
# chr "0.000"               8
# chr "1086386.492"  Amt    9

my.dataframe[[1]][1]  # date
my.dataframe[[1]][3]  # close

for (i in 1:40){      # remove column 7
  my.dataframe[[i]] = my.dataframe[[i]][-(7:8)]
}

dataMatrix = matrix(unlist(my.dataframe), nrow=40, ncol=7)  # convert to matrix

<h2>process minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00981"
my.json &lt;- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only the third item is useful
# List of 3
#  $ code: int 0
#  $ msg : chr ""
#  $ data:List of 1

my.dataframe = my.dataframe[[3]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec"
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, most recent day on top

datalist = unlist(my.list) # this is all strings in one vector


<h2>statistics of minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00388"
my.json &lt;- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only list 3 is useful
# List of 3
#  $ code: int 0
#  $ msg : chr ""
#  $ data:List of 1

my.dataframe = my.dataframe[[3]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec", 5 obs for 5days
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, nearest day on top

datalist = unlist(my.list) # this is all strings in one vector

datalist = gsub("^.* ","",datalist) # this is the amount
datalist = round(as.numeric(datalist)/10000,0) # units in wan
datalist = sort(datalist)
datalist = datalist[-(1:20)]
datalist = datalist[-( (length(datalist)-20):length(datalist))] # remove the extremes

# max(datalist); min(datalist); length(datalist)

sections &lt;- cut(datalist, breaks = 100)
table(sections)
barplot(table(sections))

cumulative sums
plot(cumsum(table(sections)))

<h2>R examples</h2>
https://www.datamentor.io/r-programming/examples/
http://www.rexamples.com
https://www.guru99.com/r-tutorial.html
https://r4stats.com/examples/programming/
https://www.statmethods.net/r-tutorial/index.html
http://rprogramming.net

<h2>output text to the R console in color</h2>
library(crayon)
cat(blue("Hello", "world!\n"))

Genaral styles
reset, bold
blurred (usually called ‘dim’, renamed to avoid name clash)
italic (not widely supported)
underline, inverse, hidden
strikethrough (not widely supported)

Text colors
black, red, green, yellow, blue, magenta, cyan, white
silver (usually called ‘gray’, renamed to avoid name clash)

Background colors
bgBlack, bgRed, bgGreen, bgYellow, bgBlue, bgMagenta, bgCyan, bgWhite

Styling
The styling functions take any number of character vectors as arguments, and they concatenate and style them:

Crayon defines the %+% string concatenation operator, to make it easy to assemble stings with different styles.

cat("... to highlight the " %+%
    red("search term") %+%
    " in a block of text\n")

Styles can be combined using the $ operator:
  cat(yellow$bgMagenta$bold('Hello world!\n'))
See also combine_styles().

Styles can also be nested, and then inner style takes precedence:
  cat(green(
    'I am a green line ' %+%
    blue$underline$bold('with a blue substring') %+%
    ' that becomes green again!\n'
  ))

define your own themes:
  error &lt;- red $ bold
  warn &lt;- magenta $ underline
  note &lt;- cyan
  cat(error("Error: subscript out of bounds!\n"))
  cat(warn("Warning: shorter argument was recycled.\n"))
  cat(note("Note: no such directory.\n"))

See Also make_style() for using the 256 ANSI colors.

Examples
cat(blue("Hello", "world!"))
cat("... to highlight the " %+% red("search term") %+%
    " in a block of text")
cat(yellow$bgMagenta$bold('Hello world!'))
cat(green(
 'I am a green line ' %+%
 blue$underline$bold('with a blue substring') %+%
 ' that becomes green again!'
))
error &lt;- red $ bold
warn &lt;- magenta $ underline
note &lt;- cyan
cat(error("Error: subscript out of bounds!\n"))
cat(warn("Warning: shorter argument was recycled.\n"))
cat(note("Note: no such directory.\n"))

<h3>style - Add Style To A String</h3>
Usage
style(string, as = NULL, bg = NULL)
cat(style("I am pink\n", "pink"))
cat(style("#4682B433\n", "#4682B433"))
cat(style("#002050\n", "#002050"))


<h3>rgb()</h3>
To use the function:
rgb(red, green, blue, alpha) : quantity of red (between 0 and 1), of green and of blue, and finally transparency (alpha).
newcolor = rgb(0.5, 0.2, 0.1, 0.8)
newcolor
"#80331ACC"

cat(style("newcolor\n", newcolor))  # note, without quotation marks


<h3>make_style</h3>
pink &lt;- make_style("pink")
bgMaroon &lt;- make_style(rgb(0.93, 0.19, 0.65), bg = TRUE)
cat(bgMaroon(pink("pink style.\n")))

## Create a new style for pink and maroon background
make_style(pink = "pink")
make_style(bgMaroon = rgb(0.0, 0.3, 0.3), bg = TRUE)
"pink" %in% names(styles())
"bgMaroon" %in% names(styles())

cat(style("I am pink, too!\n", "pink"))
cat(style("I am pink, too!\n", "pink", bg = "blue")) # color will change
cat(style("I am pink, too!\n", "pink", bg = "bgMaroon"))
cat(style("I am pink, too!\n", "pink", bg = "cyan"))

<h2>print strings with wordwraps</h2>

strwrap(astring, width = 110, indent = 5, exdent = 2))
use writeLines to print it
note: control characters inside string will be ignored.

astring = "Substituted with the text matched by the capturing group that can be found by counting as many opening parentheses of named or numbered capturing groups as specified by the number from right to left starting at the backreference."

writeLines(strwrap(astring, width = 110, indent = 5, exdent = 2))

<h2>R.utils withTimeout()</h2>

withTimeout() from package R.utils, in concert with tryCatch(), might provide a cleaner solution.

For example:
require(R.utils)

for(i in 1:5) {
    tryCatch(
        expr = {
            withTimeout({Sys.sleep(i); cat(i, "\n")}, 
                         timeout = 3.1)
            }, 
        TimeoutException = function(ex) cat("Timeout. Skipping.\n")
    )
}

# 1 
# 2 
# 3 
# Timeout. Skipping.
# Timeout. Skipping.

In the artificial example above:

The first argument to withTimeout() contains the code to be evaluated within each loop.

The timeout argument to withTimeout() sets the time limit in seconds.

The TimeoutException argument to tryCatch() takes a function that is to be executed when an iteration of the loop is timed out.

<h2>drawing SVG</h2>
<a href="https://cran.r-project.org/web/packages/RIdeogram/vignettes/RIdeogram.html" class="whitebut ">RIdeogram: drawing SVG graphics</a>

<a href="https://cran.r-project.org/web/packages/magick/vignettes/intro.html" class="whitebut ">Magick: Advanced Image-Processing</a>

<a href="http://ralanbutler.com/blog/2016/03/31/animated-SVG-R" class="whitebut ">Animating an SVG</a>

svglite + ggsave function
Saving a plot as an SVG

sample code:
require("ggplot2")

#some sample data
head(diamonds) 

#to see actually what will be plotted and compare 
qplot(clarity, data=diamonds, fill=cut, geom="bar")

#save the plot in a variable image to be able to export to svg
image=qplot(clarity, data=diamonds, fill=cut, geom="bar")

#This actually save the plot in a image
ggsave(file="test.svg", plot=image, width=10, height=8)

<h2>Package ‘TTR’</h2>
Technical Trading Rules
x=c(1,2,4,3,5,6,5,4,5,6,7,9,10,11,10)

Usage
SMA(x, n = 4)
EMA(x, n = 4)
DEMA(x, n = 4)
WMA(x, n = 4, wts = 1:n)
EVWMA(price, volume, n = 4)
ZLEMA(x, n = 4, ratio = NULL)
VWAP(price, volume, n = 4)
VMA(x, w, ratio = 1)
HMA(x, n = 20)
ALMA(x, n = 9, offset = 0.85, sigma = 6)

<h3>Weighted moving average WMA</h3>

<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Weighted_moving_average_weights_N%3D15.png/220px-Weighted_moving_average_weights_N%3D15.png">

<h3>Exponential moving average EMA</h3>
EMA is more exagerating

<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Exponential_moving_average_weights_N%3D15.png/220px-Exponential_moving_average_weights_N%3D15.png">

<h2>自然语言处理中的Transformer和BERT</h2>
2018年马上就要过去，回顾深度学习在今年的进展，让人印象最深刻的就是谷歌提出的应用于自然语言处理领域的BERT解决方案，BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805">
https://arxiv.org/abs/1810.04805</a>）。
BERT解决方案刷新了各大NLP任务的榜单，在各种NLP任务上都做到state of the art。
这里我把BERT说成是解决方案，而不是一个算法，因为这篇文章并没有提出新的算法模型，还是沿用了之前已有的算法模型。
BERT最大的创新点，在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的NLP任务，因此BERT这篇论文对于算法模型完全不做介绍，以至于在我直接看这篇文章的时候感觉云里雾里。
但是本文中，我会从算法模型到解决方案，进行完整的诠释。
本文中我会分3个部分进行介绍，第一部分我会大概介绍一下NLP的发展，第二部分主要讲BERT用到的算法，最后一部分讲BERT具体是怎么操作的。

<h3>一，NLP的发展</h3>
要处理NLP问题，首先要解决文本的表示问题。
虽然我们人去看文本，能够清楚明白文本中的符号表达什么含义，但是计算机只能做数学计算，需要将文本表示成计算机可以处理的形式。
最开始的方法是采用one hot，比如，我们假设英文中常用的单词有3万个，那么我们就用一个3万维的向量表示这个词，所有位置都置0，当我们想表示apple这个词时，就在对应位置设置1，如图1.1所示。
这种表示方式存在的问题就是，高维稀疏，高维是指有多少个词，就需要多少个维度的向量，稀疏是指，每个向量中大部分值都是0。
另外一个不足是这个向量没有任何含义。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-597b011ddd148eb53b5a90730b6090ae_b.jpg">

<figcaption>图1.1</figcaption>
后来出现了词向量，word embedding，用一个低维稠密的向量去表示一个词，如图1.2所示。
通常这个向量的维度在几百到上千之间，相比one hot几千几万的维度就低了很多。
词与词之间可以通过相似度或者距离来表示关系，相关的词向量相似度比较高，或者距离比较近，不相关的词向量相似度低，或者距离比较远，这样词向量本身就有了含义。
文本的表示问题就得到了解决。
词向量可以通过一些无监督的方法学习得到，比如CBOW或者Skip-Gram等，可以预先在语料库上训练出词向量，以供后续的使用。
顺便提一句，在图像中就不存在表示方法的困扰，因为图像本身就是数值矩阵，计算机可以直接处理。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-840859265e735cce77233bb42a4bee6a_b.png">

<figcaption>图1.2</figcaption>
NLP中有各种各样的任务，比如分类（Classification），问答（QA），实体命名识别（NER）等。
对于这些不同的任务，最早的做法是根据每类任务定制不同的模型，输入预训练好的embedding，然后利用特定任务的数据集对模型进行训练，如图1.3所示。
这里存在的问题就是，不是每个特定任务都有大量的标签数据可供训练，对于那些数据集非常小的任务，恐怕就难以得到一个理想的模型。


<img class="lazy" data-src="https://pic1.zhimg.com/v2-4546b7aa51af50d3ac0c7504f965cc70_b.jpg">

<figcaption>图1.3</figcaption>
我们看一下图像领域是如何解决这个问题的。
图像分类是计算机视觉中最基本的任务，当我要解决一个小数据集的图像分类任务时，该怎么做？CV领域已经有了一套成熟的解决方案。
我会用一个通用的网络模型，比如Vgg，ResNet或者GoogleNet，在ImageNet上做预训练（pre-training）。
ImageNet有1400万张有标注的图片，包含1000个类别，这样的数据规模足以训练出一个规模庞大的模型。
在训练过程中，模型会不断的学习如何提取特征，底层的CNN网络结构会提取边缘，角，点等通用特征，模型越往上走，提取的特征也越抽象，与特定的任务更加相关。
当完成预训练之后，根据我自己的分类任务，调整最上层的网络结构，然后在小数据集里对模型进行训练。
在训练时，可以固定住底层的模型参数只训练顶层的参数，也可以对整个模型进行训练，这个过程叫做微调（fine-tuning），最终得到一个可用的模型。
总结一下，整个过程包括两步，拿一个通用模型在ImageNet上做预训练（pre-training），然后针对特定任务进行微调（fine-tuning），完美解决了特定任务数据不足的问题。
还有一个好处是，对于各种各样的任务都不再需要从头开始训练网络，可以直接拿预训练好的结果进行微调，既减少了训练计算量的负担，也减少了人工标注数据的负担。

NLP领域也引入了这种做法，用一个通用模型，在非常大的语料库上进行预训练，然后在特定任务上进行微调，BERT就是这套方案的集大成者。
BERT不是第一个，但目前为止，是效果最好的方案。
BERT用了一个已有的模型结构，提出了一整套的预训练方法和微调方法，我们在后文中再进行详细的描述。

<h3>二，算法</h3>
BERT所采用的算法来自于2017年12月份的这篇文章，Attenion Is All You Need（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762">
https://arxiv.org/abs/1706.03762</a>），同样来自于谷歌。
这篇文章要解决的是翻译问题，比如从中文翻译成英文。
这篇文章完全放弃了以往经常采用的RNN和CNN，提出了一种新的网络结构，即Transformer，其中包括encoder和decoder，我们只关注encoder。
这篇英文博客（<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">
https://jalammar.github.io/illustrated-transformer/</a>）对Transformer介绍得非常详细，有兴趣的读者可以看一下，如果不想看英文博客也可以看本文，本文中的部分图片也截取自这篇博客。


<img class="lazy" data-src="https://pic4.zhimg.com/v2-393d5284f5132c3150b294cfc5e5218f_b.jpg">

<figcaption>图2.1</figcaption>
图2.1是Transformer encoder的结构，后文中我们都简称为Transformer。
首先是输入word embedding，这里是直接输入一整句话的所有embedding。
如图2.1所示，假设我们的输入是Thinking Machines，每个词对应一个embedding，就有2个embedding。
输入embedding需要加上位置编码（Positional Encoding），为什么要加位置编码，后文会做详细介绍。
然后经过一个Multi-Head Attention结构，这个结构是算法单元中最重要的部分，我们会在后边详细介绍。
之后是做了一个shortcut的处理，就是把输入和输出按照对应位置加起来，如果了解残差网络（ResNet）的同学，会对这个结构比较熟悉，这个操作有利于加速训练。
然后经过一个归一化normalization的操作。
接着经过一个两层的全连接网络，最后同样是shortcut和normalization的操作。
可以看到，除了Multi-Head Attention，都是常规操作，没有什么难理解的。
这里需要注意的是，每个小模块的输入和输出向量，维度都是相等的，比如，Multi-Head Attention的输入和输出向量维度是相等的，否则无法进行shortcut的操作；Feed Forward的输入和输出向量维度也是相等的；最终的输出和输入向量维度也是相等的。
但是Multi-Head Attention和Feed Forward内部，向量维度会发生变化。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-4019f1ffead184e3bc00aabb41e6b6b6_b.jpg">

<figcaption>图2.2</figcaption>
我们来详细看一下Multi-Head Attention的结构。
这个Multi-Head表示多头的意思，先从最简单的看起，看看单头Attention是如何操作的。
从图2.1的橙色方块可以看到，embedding在进入到Attention之前，有3个分叉，那表示说从1个向量，变成了3个向量。
具体是怎么算的呢？我们看图2.3，定义一个WQ矩阵（这个矩阵随机初始化，通过训练得到），将embedding和WQ矩阵做乘法，得到查询向量q，假设输入embedding是512维，在图3中我们用4个小方格表示，输出的查询向量是64维，图3中用3个小方格以示不同。
然后类似地，定义WK和WV矩阵，将embedding和WK做矩阵乘法，得到键向量k；将embeding和WV做矩阵乘法，得到值向量v。
对每一个embedding做同样的操作，那么每个输入就得到了3个向量，查询向量，键向量和值向量。
需要注意的是，查询向量和键向量要有相同的维度，值向量的维度可以相同，也可以不同，但一般也是相同的。


<img class="lazy" data-src="https://pic1.zhimg.com/v2-ac045486e0eff3b8a1eb27d2ae61a634_b.jpg">

<figcaption>图2.3</figcaption>
接下来我们计算每一个embedding的输出，以第一个词Thinking为例，参看图2.4。
用查询向量q1跟键向量k1和k2分别做点积，得到112和96两个数值。
这也是为什么前文提到查询向量和键向量的维度必须要一致，否则无法做点积。
然后除以常数8，得到14和12两个数值。
这个常数8是键向量的维度的开方，键向量和查询向量的维度都是64，开方后是8。
做这个尺度上的调整目的是为了易于训练。
然后把14和12丢到softmax函数中，得到一组加和为1的系数权重，算出来是大约是0.88和0.12。
将0.88和0.12对两个值向量v1和v2做加权求和，就得到了Thinking的输出向量z1。
类似的，可以算出Machines的输出z2。
如果一句话中包含更多的词，也是相同的计算方法。


<img class="lazy" data-src="https://pic2.zhimg.com/v2-b25bb6a8f9b57a4831b485015080b8c1_b.jpg">

<figcaption>图2.4</figcaption>
通过这样一系列的计算，可以看到，现在每个词的输出向量z都包含了其他词的信息，每个词都不再是孤立的了。
而且每个位置中，词与词的相关程度，可以通过softmax输出的权重进行分析。
如图2.5所示，这是某一次计算的权重，其中线条颜色的深浅反映了权重的大小，可以看到it中权重最大的两个词是The和animal，表示it跟这两个词关联最大。
这就是attention的含义，输出跟哪个词关联比较强，就放比较多的注意力在上面。
上面我们把每一步计算都拆开了看，实际计算的时候，可以通过矩阵来计算，如图2.6所示。


<img class="lazy" data-src="https://pic2.zhimg.com/v2-2dbdd8dfb5088d22c7dd9d05a0e1035d_b.jpg">

<figcaption>图2.5</figcaption>


<img class="lazy" data-src="https://pic4.zhimg.com/v2-a02ab6ab4cad1f8fef307ced0a4cf9d3_b.jpg" data-caption="">


<img class="lazy" data-src="https://pic2.zhimg.com/v2-d00785a9cfb835b5a345898e37b31be9_b.jpg">

<figcaption>图2.6</figcaption>
讲完了attention，再来讲Multi-Head。
对于同一组输入embedding，我们可以并行做若干组上面的操作，例如，我们可以进行8组这样的运算，每一组都有WQ，WK，WV矩阵，并且不同组的矩阵也不相同。
这样最终会计算出8组输出，我们把8组的输出连接起来，并且乘以矩阵WO做一次线性变换得到输出，WO也是随机初始化，通过训练得到，计算过程如图2.7所示。
这样的好处，一是多个组可以并行计算，二是不同的组可以捕获不同的子空间的信息。



<img class="lazy" data-src="https://pic3.zhimg.com/v2-2afc28e06f5550d5e20a2dc290f2224e_b.jpg">

<figcaption>图2.7</figcaption>
到这里就把Transformer的结构讲完了，同样都是做NLP任务，我们来和RNN做个对比。
图2.8是个最基本的RNN结构，还有计算公式。
当计算隐向量h4时，用到了输入x4，和上一步算出来的隐向量h3，h3包含了前面所有节点的信息。
h4中包含最多的信息是当前的输入x4，越往前的输入，随着距离的增加，信息衰减得越多。
对于每一个输出隐向量h都是如此，包含信息最多得是当前的输入，随着距离拉远，包含前面输入的信息越来越少。
但是Transformer这个结构就不存在这个问题，不管当前词和其他词的空间距离有多远，包含其他词的信息不取决于距离，而是取决于两者的相关性，这是Transformer的第一个优势。
第二个优势在于，对于Transformer来说，在对当前词进行计算的时候，不仅可以用到前面的词，也可以用到后面的词。
而RNN只能用到前面的词，这并不是个严重的问题，因为这可以通过双向RNN来解决。
第三点，RNN是一个顺序的结构，必须要一步一步地计算，只有计算出h1，才能计算h2，再计算h3，隐向量无法同时并行计算，导致RNN的计算效率不高，这是RNN的固有结构所造成的，之前有一些工作就是在研究如何对RNN的计算并行化。
通过前文的介绍，可以看到Transformer不存在这个问题。
通过这里的比较，可以看到Transformer相对于RNN有巨大的优势，因此我看到有人说RNN以后会被取代。


<img class="lazy" data-src="https://pic1.zhimg.com/v2-5bafe804c0dc77f945ade48561de63a0_b.jpg" data-caption="">


<img class="lazy" data-src="https://pic1.zhimg.com/v2-235854b916c55c54bbcad343443885c0_b.jpg">

<figcaption>图2.8</figcaption>
关于上面的第三点优势，可能有人会不认可，RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了。
为了解决时序的问题，Transformer的作者用了一个绝妙的办法，这就是我在前文提到的位置编码（Positional Encoding）。
位置编码是和word embedding同样维度的向量，将位置embedding和词embedding加在一起，作为输入embedding，如图2.9所示。
位置编码可以通过学习得到，也可以通过设置一个跟位置或者时序相关的函数得到，比如设置一个正弦或者余弦函数，这里不再多说。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-df4b9de6b9feb1971ab7225ebc4454d2_b.jpg">

<figcaption>图2.9</figcaption>
我们把图2.1的结构作为一个基本单元，把N个这样的基本单元顺序连起来，就是BERT的算法模型，如图2.10所示。
从前面的描述中可以看到，当输入有多少个embedding，那么输出也就有相同数量的embedding，可以采用和RNN采用相同的叫法，把输出叫做隐向量。
在做具体NLP任务的时候，只需要从中取对应的隐向量作为输出即可。


<img class="lazy" data-src="https://pic1.zhimg.com/v2-81e63e36210c8e342d193be69c441e7c_b.jpg">

<figcaption>图2.10</figcaption>
<h3>三，BERT</h3>
在介绍BERT之前，我们先看看另外一套方案。
我在第一部分说过，BERT并不是第一个提出预训练加微调的方案，此前还有一套方案叫GPT，这也是BERT重点对比的方案，文章在这，Improving Language Understanding by Generative Pre-Training（<a href="https://link.zhihu.com/?target=https%3A//s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>）。
GPT的模型结构和BERT是相同的，都是图2.10的结构，只是BERT的模型规模更加庞大。
GPT是这么预训练的，在一个8亿单词的语料库上做训练，给出前文，不断地预测下一个单词。
比如这句话，Winter is coming，当给出第一个词Winter之后，预测下一个词is，之后再预测下一个词coming。
不需要标注数据，通过这种无监督训练的方式，得到一个预训练模型。

我们再来看看BERT有什么不同。
BERT来自于Bidirectional Encoder Representations from Transformers首字母缩写，这里提到了一个双向（Bidirectional）的概念。
BERT在一个33亿单词的语料库上做预训练，语料库就要比GPT大了几倍。
预训练包括了两个任务，第一个任务是随机地扣掉15%的单词，用一个掩码MASK代替，让模型去猜测这个单词；第二个任务是，每个训练样本是一个上下句，有50%的样本，下句和上句是真实的，另外50%的样本，下句和上句是无关的，模型需要判断两句的关系。
这两个任务各有一个loss，将这两个loss加起来作为总的loss进行优化。
下面两行是一个小栗子，用括号标注的是扣掉的词，用[MASK]来代替。

<b>正样本：我[MASK]（是）个算法工程师，我服务于WiFi万能钥匙这家[MASK]（公司）。
</b>
<b>负样本：我[MASK]（是）个算法工程师，今天[MASK]（股票）又跌了。
</b>
我们来对比下GPT和BERT两种预训练方式的优劣。
GPT在预测词的时候，只预测下一个词，因此只能用到上文的信息，无法利用到下文的信息。
而BERT是预测文中扣掉的词，可以充分利用到上下文的信息，这使得模型有更强的表达能力，这也是BERT中Bidirectional的含义。
在一些NLP任务中需要判断句子关系，比如判断两句话是否有相同的含义。
BERT有了第二个任务，就能够很好的捕捉句子之间的关系。
图3.1是BERT原文中对另外两种方法的预训练对比，包括GPT和ELMo。
ELMo采用的还是LSTM，这里我们不多讲ELMo。
这里会有读者困惑，这里的结构图怎么跟图2.10不一样？如果熟悉LSTM的同学，看到最右边的ELMo，就会知道那些水平相连的LSTM其实只是一个LSTM单元。
左边的BERT和GPT也是一样，水平方向的Trm表示的是同一个单元，图中那些复杂的连线表示的是词与词之间的依赖关系，BERT中的依赖关系既有前文又有后文，而GPT的依赖关系只有前文。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-287ba1129d213df7e2ed5adb7c4a440e_b.jpg">

<figcaption>图3.1</figcaption>
讲完了这两个任务，我们再来看看，如何表达这么复杂的一个训练样本，让计算机能够明白。
图3.2表示“my dog is cute, he likes playing.”的输入形式。
每个符号的输入由3部分构成，一个是词本身的embedding；第二个是表示上下句的embedding，如果是上句，就用A embedding，如果是下句，就用B embedding；最后，根据Transformer模型的特点，还要加上位置embedding，这里的位置embedding是通过学习的方式得到的，BERT设计一个样本最多支持512个位置；将3个embedding相加，作为输入。
需要注意的是，在每个句子的开头，需要加一个Classification（CLS）符号，后文中会进行介绍，其他的一些小细节就不说了。


<img class="lazy" data-src="https://pic1.zhimg.com/v2-ec06762a57a7d7176747627dc3ee20b4_b.jpg">

<figcaption>图3.2</figcaption>
完成预训练之后，就要针对特定任务就行微调了，这里描述一下论文中的4个例子，看图3.4。
首先说下分类任务，分类任务包括对单句子的分类任务，比如判断电影评论是喜欢还是讨厌；多句子分类，比如判断两句话是否表示相同的含义。
图3.4（a）（b）是对这类任务的一个示例，左边表示两个句子的分类，右边是单句子分类。
在输出的隐向量中，取出CLS对应的向量C，加一层网络W，并丢给softmax进行分类，得到预测结果P，计算过程如图3.3中的计算公式。
在特定任务数据集中对Transformer模型的所有参数和网络W共同训练，直到收敛。
新增加的网络W是HxK维，H表示隐向量的维度，K表示分类数量，W的参数数量相比预训练模型的参数少得可怜。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-61486f520243716de645f904e3a36ac2_b.jpg">

<figcaption>图3.3</figcaption>


<img class="lazy" data-src="https://pic3.zhimg.com/v2-42514100ab16b207d2732729c85fccaa_b.jpg">

<figcaption>图3.4</figcaption>
我们再来看问答任务，如图3.4（c），以SQuAD v1.1为例，给出一个问题Question，并且给出一个段落Paragraph，然后从段落中标出答案的具体位置。
需要学习一个开始向量S，维度和输出隐向量维度相同，然后和所有的隐向量做点积，取值最大的词作为开始位置；另外再学一个结束向量E，做同样的运算，得到结束位置。
附加一个条件，结束位置一定要大于开始位置。
最后再看NER任务，实体命名识别，比如给出一句话，对每个词进行标注，判断属于人名，地名，机构名，还是其他。
如图3.4（d）所示，加一层分类网络，对每个输出隐向量都做一次判断。
可以看到，这些任务，都只需要新增少量的参数，然后在特定数据集上进行训练即可。
从实验结果来看，即便是很小的数据集，也能取得不错的效果。

<h2>Delete Files unlink("data.txt")</h2>
Delete Files and Directories. unlink deletes the file(s) or directories specified by x .

Usage. unlink(x, recursive = FALSE, force = FALSE)

<h2>scan</h2>
Read data into a vector or list from the console or file.

cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "ex.data", sep = "\n")
pp &lt;- scan("ex.data", skip = 1, quiet = TRUE)
scan("ex.data", skip = 1)
scan("ex.data", skip = 1, nlines = 1) # only 1 line after the skipped one
scan("ex.data", what = list("","","")) # flush is F -> read "7"
scan("ex.data", what = list("","",""), flush = TRUE)
unlink("ex.data") # tidy up

## "inline" usage
scan(text = "1 2 3")

<h2>Copy an R data.frame to an Excel spreadsheet</h2>
write.excel &lt;- function(x,row.names=FALSE,col.names=TRUE,...) {
  write.table(x,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}

write.excel(my.df)

and finally Ctr+V in Excel :)

<h2>copy a table x to the clipboard preserving the table structure</h2>
write.table(x, "clipboard", sep="\t")

write.table(x, "clipboard", sep="\t", row.names=FALSE)
write.table(x, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

s = c('aa','gb','rc')
n = c('af','rd','ac')
df = data.frame(n,s)

write.table(df, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

"af"	"aa"
"rd"	"gb"
"ac"	"rc"

<h2>read.table</h2>
reads a file into data frame in table format

x &lt;- read.table("tp.txt",header=T,sep="\t");

<h3>copy a table from the clipboard</h3>
x &lt;- read.table("clipboard",header=F,sep="\t");

<h2>reading text file with multiple space as delimiter</h2>
change delimiter.
" " refers to one whitespace character.
"" refers to any length whitespace as being the delimiter

data = read.table(file, header = F , nrows = 100, sep = "" , na.strings ="", stringsAsFactors= F)

<h2>distributed programming</h2>
reasons for distributed programming:

To speed up a process or piece of code
To scale up an interface or application for multiple users

<a href="http://spark.apache.org/docs/latest/sparkr.html" class="whitebut ">SparkR</a>: R on Apache Spark

SparkR provides an R frontend to Apache Spark and using Spark’s distributed computation engine allows us to run large scale data analysis from the R shell.

To get started you need to set up a Spark cluster. 
<a href="http://paxcel.net/blog/how-to-setup-apache-spark-standalone-cluster-on-multiple-machine/" class="whitebut ">SETUP APACHE SPARK STANDALONE CLUSTER ON MULTIPLE MACHINE</a>

The Spark documentation, without using Mesos or YARN as your cluster manager
<a href="http://spark.apache.org/docs/latest/spark-standalone.html" class="whitebut ">Spark Standalone Mode</a>

Once you have Spark set up, see <a href="https://rpubs.com/wendyu/sparkr" class="whitebut ">Wendy Yu's tutorial on SparkR</a>

She also shows how to integrate H20 with Spark which is referred to as 'Sparkling Water'.

R has been shipping with a base library parallel. 

In a nutshell, you can just do something like

mclapply(1:nCores, someFunction())
and the function someFunction() will be run in parallel over nCores. 
A default value of half your physical cores may be a good start.

<a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html" class="whitebut ">High Performance Computing</a>

<h2>matrix operation</h2>
MatA &lt;- matrix(1:9, nrow = 3)  
MatB &lt;- matrix(9:1, nrow = 3)  
MatA + MatB

> A &lt;- matrix(c(2,3,-2,1,2,2),3,2)
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
Is Something a Matrix
> <span class="orange">is.matrix(A)</span>

[1] TRUE

> is.vector(A)

[1] FALSE
<span class="orange">Multiplication by a Scalar</span>
> c &lt;- 3
> c*A

     [,1] [,2]
[1,]    6    3
[2,]    9    6
[3,]   -6    6
<span class="orange">Matrix Addition & Subtraction</span>
> B &lt;- matrix(c(1,4,-2,1,2,1),3,2)
> B

     [,1] [,2]
[1,]    1    1
[2,]    4    2
[3,]   -2    1

> C &lt;- A + B
> C 

     [,1] [,2]
[1,]    3    2
[2,]    7    4
[3,]   -4    3

> D &lt;- A - B
> D

     [,1] [,2]
[1,]    1    0
[2,]   -1    0
[3,]    0    1

<span class="orange">Matrix Multiplication</span>
> D &lt;- matrix(c(2,-2,1,2,3,1),2,3)
> D

     [,1] [,2] [,3]
[1,]    2    1    3
[2,]   -2    2    1

> C &lt;- D %*% A
> C

     [,1] [,2]
[1,]    1   10
[2,]    0    4

> C &lt;- A %*% D
> C

     [,1] [,2] [,3]
[1,]    2    4    7
[2,]    2    7   11
[3,]   -8    2   -4

> D &lt;- matrix(c(2,1,3),1,3)
> D

     [,1] [,2] [,3]
[1,]    2    1    3

> C &lt;- D %*% A
> C

     [,1] [,2]
[1,]    1   10

> C &lt;- A %*% D

Error in A %*% D : non-conformable arguments
<span class="orange">Transpose of a Matrix</span>
> AT &lt;- t(A)
> AT

     [,1] [,2] [,3]
[1,]    2    3   -2
[2,]    1    2    2

> ATT &lt;- t(AT)
>ATT

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
Common Vectors
<span class="orange">Unit Vector</span>
> U &lt;- matrix(1,3,1)
> U

     [,1]
[1,]    1
[2,]    1
[3,]    1
<span class="orange">Zero Vector</span>
> Z &lt;- matrix(0,3,1)
> Z

     [,1]
[1,]    0
[2,]    0
[3,]    0
Common Matrices
<span class="orange">Unit Matrix</span>
> U &lt;- matrix(1,3,2)
> U

     [,1] [,2]
[1,]    1    1
[2,]    1    1
[3,]    1    1
<span class="orange">Zero Matrix</span>
> Z &lt;- matrix(0,3,2)
> Z

     [,1] [,2]
[1,]    0    0
[2,]    0    0
[3,]    0    0
<span class="orange">Diagonal Matrix</span>
> S &lt;- matrix(c(2,3,-2,1,2,2,4,2,3),3,3)
> S

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    2
[3,]   -2    2    3

> D &lt;- diag(S)
> D

[1] 2 2 3

> D &lt;- diag(diag(S))
> D

     [,1] [,2] [,3]
[1,]    2    0    0
[2,]    0    2    0
[3,]    0    0    3
<span class="orange">Identity Matrix</span>
> I &lt;- diag(c(1,1,1))
> I

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
<span class="orange">Symmetric Matrix</span>
> C &lt;- matrix(c(2,1,5,1,3,4,5,4,-2),3,3)
> C

     [,1] [,2] [,3]
[1,]    2    1    5
[2,]    1    3    4
[3,]    5    4   -2

> CT &lt;- t(C)
> CT

     [,1] [,2] [,3]
[1,]    2    1    5
[2,]    1    3    4
[3,]    5    4   -2
<span class="orange">Inverse of a Matrix</span>
> A &lt;- matrix(c(4,4,-2,2,6,2,2,8,4),3,3)
> A

     [,1] [,2] [,3]
[1,]    4    2    2
[2,]    4    6    8
[3,]   -2    2    4


> AI &lt;- solve(A)
> AI

     [,1] [,2] [,3]
[1,]  1.0 -0.5  0.5
[2,] -4.0  2.5 -3.0
[3,]  2.5 -1.5  2.0

> A %*% AI

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1

> AI %*% A

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
<span class="orange">Inverse & Determinant of a Matrix</span>
> C &lt;- matrix(c(2,1,6,1,3,4,6,4,-2),3,3)
> C

     [,1] [,2] [,3]
[1,]    2    1    6
[2,]    1    3    4
[3,]    6    4   -2

> CI &lt;- solve(C)
CI

           [,1]        [,2]        [,3]
[1,]  0.2156863 -0.25490196  0.13725490
[2,] -0.2549020  0.39215686  0.01960784
[3,]  0.1372549  0.01960784 -0.04901961

> d &lt;- det(C)
> d

[1] -102
<span class="orange">Rank of a Matrix</span>
> A &lt;- matrix(c(2,3,-2,1,2,2,4,7,0),3,3)
> A

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    7
[3,]   -2    2    0

> matA &lt;- qr(A)
> matA$rank

[1] 3

> A &lt;- matrix(c(2,3,-2,1,2,2,4,6,-4),3,3)
> A

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    6
[3,]   -2    2   -4

> matA &lt;- qr(A)
> matA$rank

[1] 2

# note column 3 is 2 times column 1
Number of Rows & Columns
> X &lt;- matrix(c(3,2,4,3,2,-2,6,1),4,2)
> X

     [,1] [,2]
[1,]    3    2
[2,]    2   -2
[3,]    4    6
[4,]    3    1

> <span class="orange">dim(X)</span>

[1] 4 2

> r &lt;- nrow(X)
> r

[1] 4

> c &lt;- ncol(X)
> c

[1] 2
Computing Column & Row Sums
# note the uppercase S

> A &lt;- matrix(c(2,3,-2,1,2,2),3,2)
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2

> c &lt;- colSums(A)
> c

[1] 3 5

> r &lt;- rowSums(A)
> r

[1] 3 5 0

> a &lt;- sum(A)
> a

[1] 8
<span class="orange">Computing Column & Row Means</span>
# note the uppercase M

> cm &lt;- colMeans(A)
> cm

[1] 1.000000 1.666667

> rm &lt;- rowMeans(A)
> rm

[1] 1.5 2.5 0.0

> m &lt;- mean(A)
> m

[1] 1.333333
<span class="orange">Horizontal Concatenation</span>
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2

> B &lt;- matrix(c(1,3,2,1,4,2),3,2)
> B

     [,1] [,2]
[1,]    1    1
[2,]    3    4
[3,]    2    2

> C &lt;- cbind(A,B)
> C

     [,1] [,2] [,3] [,4]
[1,]    2    1    1    1
[2,]    3    2    3    4
[3,]   -2    2    2    2
<span class="orange">Vertical Concatenation (Appending)</span>
> C &lt;- rbind(A,B)
> C

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
[4,]    1    1
[5,]    3    4
[6,]    2    2

<span class="orange">Matrix Operations in R</span>
A * B	Element-wise multiplication
A %*% B	Matrix multiplication
A %o% B	Outer product. AB'
crossprod(A,B)
crossprod(A)	A'B and A'A respectively.
t(A)	Transpose
diag(x)	Creates diagonal matrix with elements of x in the principal diagonal
diag(A)	Returns a vector containing the elements of the principal diagonal
diag(k)	If k is a scalar, this creates a k x k identity matrix. Go figure.
solve(A, b)	Returns vector x in the equation b = Ax (i.e., A-1b)
solve(A)	Inverse of A where A is a square matrix.
ginv(A)	Moore-Penrose Generalized Inverse of A.
ginv(A) requires loading the MASS package.
y&lt;-eigen(A)	y$val are the eigenvalues of A
y$vec are the eigenvectors of A
y&lt;-svd(A)	Single value decomposition of A.
y$d = vector containing the singular values of A
y$u = matrix with columns contain the left singular vectors of A
y$v = matrix with columns contain the right singular vectors of A
R &lt;- chol(A)	Choleski factorization of A. Returns the upper triangular factor, such that R'R = A.
y &lt;- qr(A)	QR decomposition of A.
y$qr has an upper triangle that contains the decomposition and a lower triangle that contains information on the Q decomposition.
y$rank is the rank of A.
y$qraux a vector which contains additional information on Q.
y$pivot contains information on the pivoting strategy used.
cbind(A,B,...)	Combine matrices(vectors) horizontally. Returns a matrix.
rbind(A,B,...)	Combine matrices(vectors) vertically. Returns a matrix.
rowMeans(A)	Returns vector of row means.
rowSums(A)	Returns vector of row sums.
colMeans(A)	Returns vector of column means.
colSums(A)	Returns vector of column sums.

<h2>UCLA stat</h2>
<a href="http://www.philender.com/courses/intro/" class="whitebut ">ucla.edu Introduction to Research Design and Statistics</a>
<a href="http://www.philender.com/courses/linearmodels/" class="whitebut ">Linear Statistical Models: Regression & Anova, Better Living Through Linear Models</a>
<a href="http://www.philender.com/courses/multivariate/" class="whitebut ">Multivariate Statistical Analysis</a>

<h2>R Linear Algebra</h2>
R is especially handy with linear algebra. 
Its built-in data types like vectors and matrices mesh well with built-in functions like eigenvalue and determinant solvers and dynamic indexing capabilities.

<h3>Vector Assignment</h3>
x &lt;- c(1, 2, 3, 4)
In most contexts, <code>&lt;-</code> can be switched with <code>=</code>.
The function <code>assign()</code> can also be used:
assign(&#x27;x&#x27;, c(1, 2, 3, 4))
Assignments can also be made in the other direction:
c(1, 2, 3, 4) -&gt; x

<h3>Vector Operations</h3>Vectors can also be used in a variety of ways.
The operation<code> y &lt;- c(x, 0, x)</code> would assign a vector <code>1, 2, 3, 4, 0, 1, 2, 3, 4 </code>to variable <code>y</code>.
Vectors can be freely multiplied and added by constants:
v &lt;- 2*x + y + 1
Note that this operation is valid even when <code>x</code> and <code>y</code> are different lengths. 
In this case, R will simply recycle x (sometimes fractionally) until it meets the length of y. 
Since y is 9 numbers long and x is 4 units long, x will be repeated 2.25 times to match the length of y.
The arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code> can all be used. 
<code>log</code>, <code>exp</code>, <code>sin</code>, <code>cos</code>, <code>tan</code>, <code>sqrt</code>, and more can also be used. 
<code>max(x)</code> and <code>min(x)</code> represent the largest and smallest elements of a vector <code>x</code>, and <code>length(x)</code> is the number of elements in <code>x</code>. 
<code>sum(x)</code> gives the total of the elements in <code>x</code>, and <code>prod(x)</code> their product.
<code>mean(x)</code> calculates the sample mean, and <code>var(x)</code> returns the sample variance. 
<code>sort(x)</code> returns a vector of the same size as x with elements arranged in increasing order.

<h3>Generating Sequences</h3>R has many methods for generating sequences of numbers. 
<code>1:30</code> is the same as <code>c(1, 2, …, 29, 30)</code>. 
The colon as the highest priority in an expression, so<code>2*1:15</code> will return <code>c(2, 4, …, 28, 30)</code> instead of <code>c(2, 3, …, 14, 15)</code>.
30:1 may be used to generate the sequence backwards.
The <code>seq()</code> function can also be used to generate sequences. 
<code>seq(2,10) </code>returns the same vector as <code>2:10</code>. 
In <code>seq()</code>, one can also specify the length of the step in which to take: <code>seq(1,2,by=0.5)</code> returns <code>c(1, 1.5, 2)</code>.
A similar function is <code>rep()</code>, which replicates an object in various ways. 
For example, <code>rep(x, times=5)</code> will return five copies of <code>x</code> end-to-end.

<h3>Logical Vectors</h3>Logical values in R are TRUE, FALSE, and NA. 
Logical vectors are set by conditions. 
<code>val &lt;- x &gt; 13</code> sets <code>val</code> as a vector of the same length as <code>x</code> with values <code>TRUE</code> where the condition is met and <code>FALSE</code> where the condition is not.
The logical operators in r are <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==</code>, and <code>!=</code>, which mean less than, less than or equal to, greater than, greater than or equal to, equality, and inequality.

<h3>Missing Values</h3>The function <code>is.na(x)</code> returns a logical vector of the same size as <code>x</code> with <code>TRUE</code> if the corresponding element to <code>x</code> is <code>NA</code>.
<code>x == NA</code> is different from <code>is.na(x)</code> since <code>NA</code> is not a value but a marker for an unavailable quantity.
A second type of ‘missing value’ is that which is produced by numerical computation, such as <code>0/0</code>. 
In this case, <code>NaN</code> (Not a Number) values are treated as <code>NA</code> values; that is, <code>is.na(x)</code> will return <code>TRUE</code> for both <code>NA</code> and <code>NaN</code> values. 
<code>is.nan(x)</code> can be used only for identifying <code>NaN</code> values.

<h3>Indexing Vectors</h3>The first kind of indexing is through a logical vector. 
<code>y &lt;- x[!is.na(x)]</code> sets <code>y</code> to the values of <code>x</code> that are not equal to <code>NA</code> or <code>NaN</code>.
<code>(x+1)[(!is.na(x)) &amp; x&gt;0] -&gt; z</code> sets <code>z</code> to the values of <code>x+1</code> that are not <code>Na</code> or <code>NaN</code> and larger than 0.
A second method is with a vector of positive integral quantities. 
In this case, the values must be in the set <code>{1, 2, …, length(x)}</code>. 
The corresponding elements of the vector are selected and concatenated in that order to form a result. 
It is important to remember that unlike in other languages, the first index in R is 1 and not 0.
<code>x[1:10]</code> returns the first 10 elements of <code>x</code>, assuming <code>length(x)</code> is not less than 10. 
<code>c(‘x’, ‘y’)[rep(c(1,2,2,1), times=4)]</code> produces a character vector of length 16, where <code>‘x’, ‘y’, ‘y’, ‘x’</code> is repeated four times.
A vector of negative integral numbers specifies the values to be excluded rather than included. 
<code>y &lt;- x[-(1:5)]</code> sets <code>y</code> to all but the first five values of <code>x</code>.
Lastly, a vector of character strings can be used when an object has a names attribute to identify its components. 
With fruit <code>&lt;- c(1, 2, 3, 4)</code>, one can set the names of each index of the vector fruit with <code>names(fruit) &lt;- c(‘mango’, ‘apple’, ‘banana’, ‘orange’)</code>. 
Then, one can call the elements by name with <code>lunch &lt;- fruit[c(‘apple’, ‘orange’)]</code>.
The advantage of this is that alphanumeric names can sometimes be easier to remember than indices.
Note that an indexed expression can also appear on the receiving end of an assignment, in which the assignment is only performed on those elements of a vector. 
For example, <code>x[is.na(x)] &lt;- 0</code> replaces all <code>NA</code> and <code>NaN</code> values in vector <code>x</code> with the value <code>0</code>.
Another example: <code>y[y&lt;0] &lt;- -y[y&lt;0]</code> has the same effect as <code>y &lt;- abs(y)</code>. 
The code simply replaces all the values that are less than 0 with the negative of that value.
<h3>Arrays &amp; Matrices</h3>
<h3>Arrays</h3>An array is a subscripted collection of data entries, not necessarily numeric.
A dimension vector is a vector of non-negative integers. 
If the length is <em>k</em> then the array is <em>k</em>-dimensional. 
The dimensions are indexed from one up to the values given in the dimension vector.
A vector can be used by R as an array as its <code>dim </code>attribute. 
If <code>z</code> were a vector of 1500 elements, the assignment <code>dim(z) &lt;- c(100, 5, 3)</code> would mean <code>z</code> is now treated as a 100 by 5 by 3 array.

<h3>Array Indexing</h3>Individual elements of an array can be referenced by giving the name of the array followed by the subscripts in square brackets, separated by columns.
A 3 by 4 by 6 vector <code>a</code> could have its first value called via <code>a[1, 1, 1]</code> and its last value called via <code>a[3, 4, 6]</code>.
<code>a[,,]</code> represents the entire array; hence, <code>a[1,1,]</code> takes the first row of the first 2-dimensional cross-section in <code>a</code>.

<h3>Indexing Matrices</h3>The following code generates a 4 by 5 array: <code>x &lt;- array(1:20, dim = c(4,5))</code>.
Arrays are specified by a vector of values and the dimensions of the matrix. 
Values are calculated top-down first, left-right second.
<code>array(1:4, dim = c(2,2))</code> would return
1 3
2 4
and not
1 2
3 4
Negative indices are not allowed in index matrices. 
<code>NA</code> and zero values are allowed.

<h3>Outer Product of 2 Arrays</h3>An important operation on arrays is the outer product. 
If <code>a</code> and <code>b</code> are two numeric arrays, their outer product is an array whose dimension vector is obtained by concatenating the two dimension vectors and whose data vector is achieved by forming all possible products of elements of the data vector of <code>a</code> with those of <code>b</code>. 
The outer product is calculated with the operator <code>%o%</code>:
<code>ab &lt;- a %o% b</code>
Another way to achieve this is
<code>ab &lt;- outer(a, b, ‘*’)</code>
In fact, any function can be applied on two arrays using the outer() function. 
Suppose we define a function <code>f &lt;- function(x, y) cos(y)/(1+x²)</code>. 
The function could be applied to two vectors <code>x</code> and <code>y</code> via <code>z &lt;- outer(x, y, f)</code>.

<h3>Demonstration: All Possible Determinants of 2x2 Single-Digit Matrices</h3>Consider the determinants of 2 by 2 matrices [a, b; c, d] where each entry is a non-negative integer from 0 to 9. 
The problem is to find the determinants of all possible matrices in this form and represent the frequency of which the value occurs with a high density plot.
Rephrased, find the probability distribution of the determinant if each digit is chosen independently and uniformly at random.
One clever way of doing this uses the outer(0 function twice.
d &lt;- outer(0:9,0:9)
fr &lt;- table(outer(d, d, ‘-’))
plot(fr, xlab = ‘Determinant’, ylab = ‘Frequency’)
The first line assigns d to this matrix:
The second line uses the outer() function again to calculate all possible determinants, and the last line plots it.
Generalized Transpose of an Array</h3>The function <code>aperm(a, perm)</code> can be used to permute an array a. 
The argument perm must be the permutation of the integers {1,…, <em>k</em>} where <em>k</em> is the number of subscripts in <em>a</em>. 
The result of the function is an array of the same size as a but with the old dimension given by <code>perm[j]</code> becoming the new <code>j-th</code> dimension.
An easy way to think about it is a generalization of transposition for matrices. 
If <code>A</code> is a matrix, then <code>B</code> is simply the transpose of <code>A</code>:
B &lt;- aperm(A, c(2, 1))
In these special cases the function <code>t()</code> performs a transposition.

<h3>Matrix Multiplication</h3>The operator %*% is used for matrix multiplication. 
If <code>A</code> and <code>B</code> are square matrices of the same size, <code>A*B</code> is the element-wise product of the two matrices. 
<code>A %*% B</code> is the dot product (matrix product).
If x is a vector, then <code>x %*% A %*% x</code> is a quadratic form.
<code>crossprod()</code> performs cross-products; thus, <code>crossprod(X, y)</code> is the same as the operation <code>t(X) %*% y</code>, but more efficient.
<code>diag(v)</code>, where <code>v</code> is a vector, gives a diagonal matrix with elements of the vector as the diagonal entries. 
<code>diag(M)</code>, where <code>m</code> is a matrix, gives the vector of the main diagonal entries of <code>M</code> (the same convention as in Matlab). 
<code>diag(k)</code>, where <code>k</code> is a single numeric value, returns a <code>k</code> by <code>k</code> identity matrix.

<h3>Linear Equations and Inversion</h3>Solving linear equations is the inverse of matrix multiplication. 
When
b &lt;- A %*% x
with only <code>A</code> and <code>b</code> given, vector <code>x</code> is the solution of the linear equation system. 
This can be solved quickly in R with
solve(A, b)

<h3>Eigenvalues and Eigenvectors</h3>The function <code>eigen(Sm)</code> calculates the eigenvalues and eigenvectors of a symmetric matrix Sm. 
The result is a list, with the first element named values and the second named vectors. 
<code>ev &lt;- eigen(Sm)</code> assigns this list to <code>ev</code>.
<code>ev$val</code> is the vector of eigenvalues of <code>Sm</code> and <code>ev$vec</code> the matrix of corresponding eigenvectors.
For large matrices, it is better to avoid computing the eigenvectors if they are not needed by using the expression
evals &lt;- eigen(Sm, only.values = TRUE)$values

<h3>Singular Value Decomposition and Determinants</h3>The function <code>svd(m)</code> takes an arbitrary matrix argument, <code>m</code>, and calculates the singular value decomposition of <code>m</code>. 
This consists of a matrix of orthonormal columns <code>U</code> with the same column space as <code>m</code>, a second matrix of orthonormal columns <code>V</code> whose column space is the row space of <code>m</code> and a diagonal matrix of positive entries <code>D</code> such that
m = U %*% D %*% t(V)
<code>det(m)</code> can be used to calculate the determinant of a square matrix <code>m</code>.

<h3>Least Squares Fitting &amp; QR Decomposition</h3>The function <code>lsfit()</code> returns a list giving results of a least squares fitting procedure. 
An assignment like
ans &lt;- lsfit(X, y)
gives results of a least squares fit where y is the vector of observations and X is the design matrix.
<code>ls.diag()</code> can be used for regression diagnostics.
A closely related function is qr().
b &lt;- qr.coef(Xplus,y)
fit &lt;- qr.fitted(Xplus,y)
res &lt;- qr.resid(Xplus,y)
These compute the orthogonal projection of <code>y</code> onto the range of <code>X</code> in <code>fit</code>, the projection onto the orthogonal complement in <code>res</code> and the coefficient vector for the projection in <code>b</code>.

<h3>Forming Partitioned <code>Matrices</code></h3>Matrices can be built up from other vectors and matrices with the functions <code>cbind()</code> and <code>rbind()</code>.
<code>cbind()</code> forms matrices by binding matrices horizontally (column-wise), and <code>rbind()</code> binds matrices vertically (row-wise).
In the assignment <code>X &lt;- cbind(arg_1, arg_2, arg_3, …)</code> the arguments to <code>cbind()</code> must be either vectors of any length, or columns with the same column size (the same number of rows).
<code>rbind()</code> performs a corresponding operation for rows.

<h2>unable to install rvest package</h2>
Error: package or namespace load failed for ‘xml2’ in loadNamespace

install.packages("tidyverse")  # might need other dependencies installed in Rstudio

<h2>tcl/tk package to create messageBox</h2>
library(tcltk)

tkmessageBox(
 title = "Hello Friends Title",
 message = "Hello, world! message",
 icon = "warning",
 detail="This is the message details",
 type = "ok")

tk_messageBox(
message, 
icon = c("error", "info", "question", "warning")
type = c("ok", "okcancel", "yesno", "yesnocancel", "retrycancel", "abortretryignore"),
default = "", ...)

must be -default, -detail, -icon, -message, -parent, -title, or -type.

Arguments
title
character
string specifying title for dialog window

message
character
string specifying message displayed inside the alert extra arguments

A list of other arguments is shown here:
default character string specifying the default button of the dialog

detail
character string specifying a secondary message, usually displayed in a smaller font under the main message

parent
object of the class tkwin representing the window of the application for which this dialog is being posted.

type
character
string specifying predefined set of buttons to be displayed (askquestion only).

Possible values are:
 abortretryignore
  displays three buttons whose symbolic names are ‘abort’, ‘retry’ and ‘ignore’

 ok
  displays one button whose symbolic name is ‘ok’

 okcancel
  displays two buttons whose symbolic names are ‘ok’ and ‘cancel’

 retrycancel
  displays two buttons whose symbolic names are ‘retry’ and ‘cancel’

 yesno
  displays two buttons whose symbolic names are ‘yes’ and ‘no’

 yesnocancel displays three buttons whose symbolic names are ‘yes’, ‘no’ and ‘cancel’

<h2>Machine Learning in R for beginners</h2>
This small tutorial is meant to introduce you to the basics of machine learning in R: it will show you how to use R to work with KNN.

<h3>Introducing: Machine Learning in R</h3>

Machine learning is a branch in computer science that studies the design of algorithms that can learn. 
Typical machine learning tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. 
These tasks are learned through available data that were observed through experiences or instructions, for example. 
Machine learning hopes that including the experience into its tasks will eventually improve the learning. 
The ultimate goal is to improve the learning in such a way that it becomes automatic, so that humans like ourselves don’t need to interfere any more.

This small tutorial is meant to introduce you to the basics of machine learning in R: more specifically, it will show you how to use R to work with the well-known machine learning algorithm called “KNN” or <em>k</em>-nearest neighbors.


<h3>Using R For <em>k</em>-Nearest Neighbors (KNN)</h3>

The KNN or <em>k</em>-nearest neighbors algorithm is one of the simplest machine learning algorithms and is an example of instance-based learning, where new data are classified based on stored, labeled instances.

More specifically, the distance between the stored data and the new instance is calculated by means of some kind of a similarity measure. 
This similarity measure is typically expressed by a distance measure such as the Euclidean distance, cosine similarity or the Manhattan distance.

In other words, the similarity to the data that was already in the system is calculated for any new data point that you input into the system.

Then, you use this similarity value to perform predictive modeling. 
Predictive modeling is either classification, assigning a label or a class to the new instance, or regression, assigning a value to the new instance. 
Whether you classify or assign a value to the new instance depends of course on your how you compose your model with KNN.

The <em>k</em>-nearest neighbor algorithm adds to this basic algorithm that after the distance of the new point to all stored data points has been calculated, the distance values are sorted and the <em>k</em>-nearest neighbors are determined. 
The labels of these neighbors are gathered and a majority vote or weighted vote is used for classification or regression purposes.

In other words, the higher the score for a certain data point that was already stored, the more likely that the new instance will receive the same classification as that of the neighbor. 
In the case of regression, the value that will be assigned to the new data point is the mean of its <em>k</em> nearest neighbors.

<h3>Step One. Get Your Data</h3>

Machine learning usually starts from observed data. 
You can take your own data set or browse through other sources to find one.

<h3>Built-in Datasets of R</h3>

This tutorial uses the Iris data set, which is very well-known in the area of machine learning. 
This dataset is built into R, so you can take a look at this dataset by typing the following into your console:

iris

# Print first lines
head(iris)

<h3>Step Two. Know Your Data</h3>

Just looking or reading about your data is certainly not enough to get started!

You need to get your hands dirty, explore and visualize your data set and even gather some more domain knowledge if you feel the data is way over your head.

Probably you’ll already have the domain knowledge that you need, but just as a reminder, all flowers contain a sepal and a petal. 
The sepal encloses the petals and is typically green and leaf-like, while the petals are typically colored leaves. 
For the iris flowers, this is just a little bit different, as you can see in the following picture:


<img src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png" />

<h3>Initial Overview Of The Data Set</h3>

First, you can already try to get an idea of your data by making some graphs, such as histograms or boxplots. 
In this case, however, scatter plots can give you a great idea of what you’re dealing with: it can be interesting to see how much one variable is affected by another.

In other words, you want to see if there is any correlation between two variables.

You can make scatterplots with the <a href="http://www.rdocumentation.org/packages/ggvis"><code>ggvis</code> package</a>, for example.

<strong>Note</strong> that you first need to load the <code>ggvis</code> package:

<code># Load in `ggvis`
library(ggvis)

# Iris scatter plot
iris %&gt;% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %&gt;% layer_points()</code>


<img alt="correlation iris" height="499" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/plot_768312428.png" width="817" />

You see that there is a high correlation between the sepal length and the sepal width of the Setosa iris flowers, while the correlation is somewhat less high for the Virginica and Versicolor flowers: the data points are more spread out over the graph and don’t form a cluster like you can see in the case of the Setosa flowers.

The scatter plot that maps the petal length and the petal width tells a similar story:

<code>iris %&gt;% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %&gt;% layer_points()</code>


<img alt="scatterplot iris" height="500" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/plot_675020181.png" width="817" />

You see that this graph indicates a positive correlation between the petal length and the petal width for all different species that are included into the Iris data set. 
Of course, you probably need to test this hypothesis a bit further if you want to be really sure of this:

# Overall correlation `Petal.Length` and `Petal.Width`
cor(iris$Petal.Length, iris$Petal.Width)

# Return values of `iris` levels 
x=levels(iris$Species)

# Print Setosa correlation matrix
print(x[1])
cor(iris[iris$Species==x[1],1:4])

# Print Versicolor correlation matrix
print(x[2])
cor(iris[iris$Species==x[2],1:4])

# Print Virginica correlation matrix
print(x[3])
cor(iris[iris$Species==x[3],1:4])

You see that when you combined all three species, the correlation was a bit stronger than it is when you look at the different species separately: the overall correlation is 0.96, while for Versicolor this is 0.79. 
Setosa and Virginica, on the other hand, have correlations of petal length and width at 0.31 and 0.32 when you round up the numbers.

<b>Tip</b>: are you curious about ggvis, graphs or histograms in particular? Check out our <a href="https://www.datacamp.com/community/tutorials/make-histogram-basic-r/">histogram tutorial</a> and/or <a href="https://www.datacamp.com/courses/ggvis-data-visualization-r-tutorial/">ggvis course</a>.

After a general visualized overview of the data, you can also view the data set by entering

# Return all `iris` data
iris

# Return first 5 lines of `iris`
head(iris)

# Return structure of `iris`
str(iris)

However, as you will see from the result of this command, this really isn’t the best way to inspect your data set thoroughly: the data set takes up a lot of space in the console, which will impede you from forming a clear idea about your data. 
It is therefore a better idea to inspect the data set by executing <code>head(iris)</code> or <code>str(iris)</code>.

Note that the last command will help you to clearly distinguish the data type <code>num</code> and the three levels of the <code>Species</code> attribute, which is a factor. 
This is very convenient, since many R machine learning classifiers require that the target feature is coded as a factor.

Remember that factor variables represent categorical variables in R. 
They can thus take on a limited number of different values.

A quick look at the <code>Species</code> attribute through tells you that the division of the species of flowers is 50-50-50. 
On the other hand, if you want to check the percentual division of the <code>Species</code> attribute, you can ask for a table of proportions:

# Division of `Species`
table(iris$Species) 

# Percentual division of `Species`
round(prop.table(table(iris$Species)) * 100, digits = 1)

<strong>Note</strong> that the <code>round</code> argument rounds the values of the first argument, <code>prop.table(table(iris$Species))*100</code> to the specified number of digits, which is one digit after the decimal point. 
You can easily adjust this by changing the value of the <code>digits</code> argument.

<h3>Profound Understanding Of Your Data</h3>

Let’s not remain on this high-level overview of the data! R gives you the opportunity to go more in-depth with the <code>summary()</code> function. 
This will give you the minimum value, first quantile, median, mean, third quantile and maximum value of the data set Iris for numeric data types. 
For the class variable, the count of factors will be returned:

# Summary overview of `iris`
summary(....) 

# Refined summary overview
summary(....[c("Petal.Width", "Sepal.Width")])

As you can see, the <code>c()</code> function is added to the original command: the columns <code>petal width</code> and <code>sepal width</code> are concatenated and a summary is then asked of just these two columns of the Iris data set.

<h3>Step Three. Where To Go Now?</h3>

After you have acquired a good understanding of your data, you have to decide on the use cases that would be relevant for your data set. 
In other words, you think about what your data set might teach you or what you think you can learn from your data. 
From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.
From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.

<strong>Tip</strong>: keep in mind that the more familiar you are with your data, the easier it will be to assess the use cases for your specific data set. 
The same also holds for finding the appropriate machine algorithm.

For this tutorial, the Iris data set will be used for classification, which is an example of predictive modeling. 
The last attribute of the data set, <code>Species</code>, will be the target variable or the variable that you want to predict in this example.

<strong>Note</strong> that you can also take one of the numerical classes as the target variable if you want to use KNN to do regression.

<h3>Step Four. Prepare Your Workspace</h3>

Many of the algorithms used in machine learning are not incorporated by default into R. 
You will most probably need to download the packages that you want to use when you want to get started with machine learning.

<b>Tip</b>: got an idea of which learning algorithm you may use, but not of which package you want or need? You can find a pretty complete overview of all the packages that are used in R <a href="http://www.rdocumentation.org/domains/MachineLearning">right here</a>.

To illustrate the KNN algorithm, this tutorial works with the package <code>class</code>:

library(.....)

If you don’t have this package yet, you can quickly and easily do so by typing the following line of code:

<code>install.packages("&lt;package name&gt;")</code>

<strong>Remember</strong> the nerd tip: if you’re not sure if you have this package, you can run the following command to find out!

<code>any(grepl("&lt;name of your package&gt;", installed.packages()))</code>

<h3>Step Five. Prepare Your Data</h3>

After exploring your data and preparing your workspace, you can finally focus back on the task ahead: making a machine learning model. 
However, before you can do this, it’s important to also prepare your data. 
The following section will outline two ways in which you can do this: by normalizing your data (if necessary) and by splitting your data in training and testing sets.

<h3>Normalization</h3>

As a part of your data preparation, you might need to normalize your data so that its consistent. 
For this introductory tutorial, just remember that normalization makes it easier for the KNN algorithm to learn. 
There are two types of normalization:

example normalization is the adjustment of each example individually, while
feature normalization indicates that you adjust each feature in the same way across all examples.

So when do you need to normalize your dataset?

In short: when you suspect that the data is not consistent.

You can easily see this when you go through the results of the <code>summary()</code> function. 
Look at the minimum and maximum values of all the (numerical) attributes. 
If you see that one attribute has a wide range of values, you will need to normalize your dataset, because this means that the distance will be dominated by this feature.

For example, if your dataset has just two attributes, X and Y, and X has values that range from 1 to 1000, while Y has values that only go from 1 to 100, then Y’s influence on the distance function will usually be overpowered by X’s influence.

When you normalize, you actually adjust the range of all features, so that distances between variables with larger ranges will not be over-emphasised.

<b>Tip</b>: go back to the result of <code>summary(iris)</code> and try to figure out if normalization is necessary.

The Iris data set doesn’t need to be normalized: the <code>Sepal.Length</code> attribute has values that go from 4.3 to 7.9 and <code>Sepal.Width</code> contains values from 2 to 4.4, while <code>Petal.Length</code>’s values range from 1 to 6.9 and <code>Petal.Width</code> goes from 0.1 to 2.5. 
All values of all attributes are contained within the range of 0.1 and 7.9, which you can consider acceptable.

Nevertheless, it’s still a good idea to study normalization and its effect, especially if you’re new to machine learning. 
You can perform feature normalization, for example, by first making your own <code>normalize()</code> function.

You can then use this argument in another command, where you put the results of the normalization in a data frame through <code>as.data.frame()</code> after the function <code>lapply()</code> returns a list of the same length as the data set that you give in. 
Each element of that list is the result of the application of the <code>normalize</code> argument to the data set that served as input:

<code>YourNormalizedDataSet &lt;- as.data.frame(lapply(YourDataSet, normalize))</code>

Test this in the DataCamp Light chunk below!

# Build your own `normalize()` function
normalize &lt;- function(x) {
num &lt;- x - min(x)
denom &lt;- max(x) - min(x)
return (num/denom)
}

# Normalize the `iris` data
iris_norm &lt;- .............(......(iris[1:4], normalize))

# Summarize `iris_norm`
summary(.........)

For the Iris dataset, you would have applied the <code>normalize</code> argument on the four numerical attributes of the Iris data set (<code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code>, <code>Petal.Width</code>) and put the results in a data frame.

<strong>Tip</strong>: to more thoroughly illustrate the effect of normalization on the data set, compare the following result to the summary of the Iris data set that was given in step two.

<h3>Training And Test Sets</h3>

In order to assess your model’s performance later, you will need to divide the data set into two parts: a training set and a test set.

The first is used to train the system, while the second is used to evaluate the learned or trained system. 
In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.

One last look on the data set teaches you that if you performed the division of both sets on the data set as is, you would get a training class with all species of “Setosa” and “Versicolor”, but none of “Virginica”. 
The model would therefore classify all unknown instances as either “Setosa” or “Versicolor”, as it would not be aware of the presence of a third species of flowers in the data.

In short, you would get incorrect predictions for the test set.

You thus need to make sure that all three classes of species are present in the training model. 
What’s more, the amount of instances of all three species needs to be more or less <em>equal</em> so that you do not favour one or the other class in your predictions.

To make your training and test sets, you first set a seed. 
This is a number of R’s random number generator. 
The major advantage of setting a seed is that you can get the same sequence of random numbers whenever you supply the same seed in the random number generator.

<code>set.seed(1234)</code>

Then, you want to make sure that your Iris data set is shuffled and that you have an equal amount of each species in your training and test sets.

You use the <code>sample()</code> function to take a sample with a size that is set as the number of rows of the Iris data set, or 150. 
You sample with replacement: you choose from a vector of 2 elements and assign either 1 or 2 to the 150 rows of the Iris data set. 
The assignment of the elements is subject to probability weights of 0.67 and 0.33.

<code>ind &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))</code>

<strong>Note</strong> that the <code>replace</code> argument is set to <code>TRUE</code>: this means that you assign a 1 or a 2 to a certain row and then reset the vector of 2 to its original state. 
This means that, for the next rows in your data set, you can either assign a 1 or a 2, each time again. 
The probability of choosing a 1 or a 2 should not be proportional to the weights amongst the remaining items, so you specify probability weights. 
Note also that, even though you don’t see it in the DataCamp Light chunk, the seed has still been set to <code>1234</code>.

<strong>Remember</strong> that you want your training set to be 2/3 of your original data set: that is why you assign “1” with a probability of 0.67 and the “2”s with a probability of 0.33 to the 150 sample rows.

You can then use the sample that is stored in the variable <code>ind</code> to define your training and test sets:

# Compose training set
iris.training &lt;- ....[ind==1, 1:4]

# Inspect training set
head(................)

# Compose test set
iris.test &lt;- ....[ind==2, 1:4]

# Inspect test set
head(...........)

<strong>Note</strong> that, in addition to the 2/3 and 1/3 proportions specified above, you don’t take into account all attributes to form the training and test sets. 
Specifically, you only take <code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code> and <code>Petal.Width</code>. 
This is because you actually want to predict the fifth attribute, <code>Species</code>: it is your target variable. 
However, you do want to include it into the KNN algorithm, otherwise there will never be any prediction for it.

You therefore need to store the class labels in factor vectors and divide them over the training and test sets:
# Compose `iris` training labels
iris.trainLabels &lt;- iris[ind==1,5]

# Inspect result
print(iris.trainLabels)

# Compose `iris` test labels
iris.testLabels &lt;- iris[ind==2, 5]

# Inspect result
print(iris.testLabels)

<h3>Step Six. The Actual KNN Model</h3>

<h3>Building Your Classifier</h3>

After all these preparation steps, you have made sure that all your known (training) data is stored. 
No actual model or learning was performed up until this moment. 
Now, you want to find the <em>k</em> nearest neighbors of your training set.

An easy way to do these two steps is by using the <code>knn()</code> function, which uses the Euclidian distance measure in order to find the <em>k</em>-nearest neighbours to your new, unknown instance. 
Here, the <em>k</em> parameter is one that you set yourself.

As mentioned before, new instances are classified by looking at the majority vote or weighted vote. 
In case of classification, the data point with the highest score wins the battle and the unknown instance receives the label of that winning data point. 
If there is an equal amount of winners, the classification happens randomly.

<strong>Note</strong>: the <em>k</em> parameter is often an odd number to avoid ties in the voting scores.

# Build the model
iris_pred &lt;- ...(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)

# Inspect `iris_pred`
.........

You store into <code>iris_pred</code> the <code>knn()</code> function that takes as arguments the training set, the test set, the train labels and the amount of neighbours you want to find with this algorithm. 
The result of this function is a factor vector with the predicted classes for each row of the test data.

<strong>Note</strong> that you don’t want to insert the test labels: these will be used to see if your model is good at predicting the actual classes of your instances!

You see that when you inspect the the result, <code>iris_pred</code>, you’ll get back the factor vector with the predicted classes for each row of the test data.

<h3>Step Seven. Evaluation of Your Model</h3>

An essential next step in machine learning is the evaluation of your model’s performance. 
In other words, you want to analyze the degree of correctness of the model’s predictions.

For a more abstract view, you can just compare the results of <code>iris_pred</code> to the test labels that you had defined earlier:

# Put `iris.testLabels` in a data frame
irisTestLabels &lt;- data.frame(................)

# Merge `iris_pred` and `iris.testLabels` 
merge &lt;- data.frame(........., ...............)

# Specify column names for `merge`
names(.....) &lt;- c("Predicted Species", "Observed Species")

# Inspect `merge` 
merge


You see that the model makes reasonably accurate predictions, with the exception of one wrong classification in row 29, where “Versicolor” was predicted while the test label is “Virginica”.

This is already some indication of your model’s performance, but you might want to go even deeper into your analysis. 
For this purpose, you can import the package <code>gmodels</code>:

<code>install.packages("package name")</code>

However, if you have already installed this package, you can simply enter

<code>library(gmodels)</code>

Then you can make a cross tabulation or a contingency table. 
This type of table is often used to understand the relationship between two variables. 
In this case, you want to understand how the classes of your test data, stored in <code>iris.testLabels</code> relate to your model that is stored in <code>iris_pred</code>:

<code>CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)</code>


<img alt="Crosstable iris knn" height="698" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/Screenshot-2015-03-24-20.05.32.png" width="926" />

<strong>Note</strong> that the last argument <code>prop.chisq</code> indicates whether or not the chi-square contribution of each cell is included. 
The chi-square statistic is the sum of the contributions from each of the individual cells and is used to decide whether the difference between the observed and the expected values is significant.

From this table, you can derive the number of correct and incorrect predictions: one instance from the testing set was labeled <code>Versicolor</code> by the model, while it was actually a flower of species <code>Virginica</code>. 
You can see this in the first row of the “Virginica” species in the <code>iris.testLabels</code> column. 
In all other cases, correct predictions were made. 
You can conclude that the model’s performance is good enough and that you don’t need to improve the model!

<a href="https://www.datacamp.com/courses/" target="_blank">
<img alt="Learn Python for Data Science With DataCamp" src="http://community.datacamp.com.s3.amazonaws.com/community/production/ckeditor_assets/pictures/293/content_blog_banner.png" /></a>

<h3>Machine Learning in R with <code>caret</code></h3>

In the previous sections, you have gotten started with supervised learning in R via the KNN algorithm. 
As you might not have seen above, machine learning in R can get really complex, as there are various algorithms with various syntax, different parameters, etc. 
Maybe you’ll agree with me when I say that remembering the different package names for each algorithm can get quite difficult or that applying the syntax for each specific algorithm is just too much.

That’s where the <code>caret</code> package can come in handy: it’s short for “Classification and Regression Training” and offers everything you need to know to solve supervised machine learning problems: it provides a uniform interface to a ton of machine learning algorithms. 
If you’re a bit familiar with Python machine learning, you might see similarities with <code>scikit-learn</code>!

In the following, you’ll go through the steps as they have been outlined above, but this time, you’ll make use of <code>caret</code> to classify your data. 
Note that you have already done a lot of work if you’ve followed the steps as they were outlined above: you already have a hold on your data, you have explored it, prepared your workspace, etc. 
Now it’s time to preprocess your data with <code>caret</code>!

As you have done before, you can study the effect of the normalization, but you’ll see this later on in the tutorial.

You already know what’s next! Let’s split up the data in a training and test set. 
In this case, though, you handle things a little bit differently: you split up the data based on the labels that you find in <code>iris$Species</code>. 
Also, the ratio is in this case set at 75-25 for the training and test sets.

# Create index to split based on labels  
index &lt;- createDataPartition(iris$Species, p=0.75, list=FALSE)

# Subset training set with index
iris.training &lt;- iris[.......,]

# Subset test set with index
iris.test &lt;- iris[-.........,]

You’re all set to go and train models now! But, as you might remember, <code>caret</code> is an extremely large project that includes a lot of algorithms. 
If you’re in doubt on what algorithms are included in the project, you can get a list of all of them. 
Pull up the list by running <code>names(getModelInfo())</code>, just like the code chunk below demonstrates. 
Next, pick an algorithm and train a model with the <code>train()</code> function:

# Overview of algos supported by caret
names(getModelInfo())

# Train a model
model_knn &lt;- train(iris.training[, 1:4], iris.training[, 5], method='knn')

Note that making other models is extremely simple when you have gotten this far; You just have to change the <code>method</code> argument, just like in this example:

<code>model_cart &lt;- train(iris.training[, 1:4], iris.training[, 5], method='rpart2')</code>

Now that you have trained your model, it’s time to predict the labels of the test set that you have just made and evaluate how the model has done on your data:

# Predict the labels of the test set
predictions&lt;-predict(object=model_knn,iris.test[,1:4])

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions,iris.test[,5])

Additionally, you can try to perform the same test as before, to examine the effect of preprocessing, such as scaling and centering, on your model. 
Run the following code chunk:


# Train the model with preprocessing
model_knn &lt;- train(iris.training[, 1:4], iris.training[, 5], method='knn', preProcess=c("center", "scale"))

# Predict values
predictions&lt;-predict.train(object=model_knn,iris.test[,1:4], type="raw")

# Confusion matrix
confusionMatrix(predictions,iris.test[,5])

<h3>Move On To Big Data</h3>

Congratulations! You’ve made it through this tutorial!

This tutorial was primarily concerned with performing basic machine learning algorithm KNN with the help of R. 
The Iris data set that was used was small and overviewable; Not only did you see how you can perform all of the steps by yourself, but you’ve also seen how you can easily make use of a uniform interface, such as the one that <code>caret</code> offers, to spark your machine learning.

But you can do so much more!

If you have experimented enough with the basics presented in this tutorial and other machine learning algorithms, you might want to find it interesting to go further into R and data analysis.

<h2>Machine Learning in R with Example</h2>
As a kid, you might have come across a picture of a fish and you would have been told by your kindergarten teachers or parents that this is a fish and it has some specific features associated with it like it has fins, gills, a pair of eyes, a tail and so on. 

Now, whenever your brain comes across an image with those set of features, it automatically registers it as a fish because your brain has <em>learned </em>that it is a fish.

That's how our brain functions but what about a machine? If the same image is fed to a machine, how will the machine identify it to be a fish?


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture6.png">

This is where M<em>achine Learning</em> comes in. 

We'll keep on feeding images of a fish to a computer with the tag "fish" until the <em>machine learns all the features associated</em> with a<em> fish. 
</em>


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture7.png">

Once the machine learns all the features associated with a fish, we will feed it new data to determine how much has it learned.


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture8.png">

In other words,<em> Raw Data/Training Data </em>is given to the machine, so that it <em>learns </em>all the features associated with the <em>Training Data. 
</em>Once, the learning is done, it is given <em>New Data/Test Data </em>to determine how well the machine has learned.

Let us move ahead in this Machine Learning with R blog and understand about types of Machine Learning.
<h3><strong>Types of Machine Learning</strong></h3><h3><strong>Supervised Learning: </strong></h3>

Supervised Learning algorithm learns from a known data-set(Training Data) which has labels to make predictions.


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture9.png">

Regression and Classification are some examples of Supervised Learning.
<h4><strong>#Classification:</strong></h4>
Classification determines to which set of categories does a new observation belongs i.e. 

a classification algorithm learns all the features and labels of the training data and when new data is given to it, it has to assign labels to the new observations depending on what it has learned from the training data.


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture10.png">

For this example, if the first observation is given the label "Man" then it is rightly classified but if it is given the label "Woman", the classification is wrong. 

Similarly for the second observation, if the label given is "Woman", it is rightly classified, else the classification is wrong.
<h4><strong>#Regression: </strong></h4>
Regression is a supervised learning algorithm which helps in determining how does one variable influence another variable.


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture11.png">

Over here, "living_area" is the independent variable and "price" is the dependent variable i.e. 

we are determining how does "price" vary with respect to "living_area".
<h3><strong>Unsupervised Learning:</strong></h3>

Unsupervised learning algorithm draws inferences from data which does not have labels.


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture12.png">

<em>Clustering</em> is an example of unsupervised learning. 

"K-means", "Hierarchical", "Fuzzy C-Means" are some examples of clustering algorithms.


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture13.png">

In this example, the set of observations is divided into two clusters. 

Clustering is done on the basis of similarity between the observations. 

There is a high intra-cluster similarity and low inter-cluster similarity i.e. 

there is a very high similarity between all the buses but low similarity between the buses and cars.

<h3><strong>Reinforcement Learning:</strong></h3>

Reinforcement Learning is a type of machine learning algorithm where the <em>machine/agent</em> in an <em>environment </em>learns ideal behavior in order to maximize its performance. 
Simple reward feedback is required for the agent to learn its behavior, this is known as the <em>reinforcement signal</em>.
<h3><strong>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/pacman.png">
</strong></h3>
Let's take <em>pacman</em> for example. 

As long as pacman keeps eating food, it earns points but when it crashes against a monster it loses it's life. 

Thus pacman learns that it needs to eat more food and avoid monsters so as to improve it's performance.
<h3><strong>Implementing Machine Learning with R:</strong></h3><h3><strong>Linear Regression:</strong></h3>
We'll be working with the diamonds data-set to implement linear regression algorithm:


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/diamond.png">

Description of the data-set:


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/diamond_description.png">

Prior to building any model on the data, we are supposed to split the data into "train" and "test" sets. 

The model will be built on the "train" set and it's accuracy will be checked on the "test" set.

We need to load the "caTools" package to split the data into two sets.

<code>library(caTools)</code>
"caTools" package provides a function "sample.split()" which helps in splitting the data.

<code>sample.split(diamonds$price,SplitRatio = 0.65)-&gt;split_index</code>
65% of the observations from price column have been assigned the "true" label and the rest 35% have been assigned "false" label.

<code>subset(diamonds,split_index==T)-&gt;train
subset(diamonds,split_index==F)-&gt;test</code>
All the observations which have "true" label have been stored in the "<em>train" object</em> and those observations having "false" label have been assigned to the "test" set.

Now that the splitting is done and we have our "train" and "test" sets, it's time to build the linear regression model on the training set.

We'll be using the "lm()" function to build the linear regression model on the "train" data. 

We are determining the <em>price</em> of the diamonds with respect to all other variables of the data-set. 

The built model is stored in the object "mod_regress".

<code>lm(price~.,data = train)-&gt;mod_regress</code><em> </em>
Now, that we have built the model, we need to make predictions on the "test" set. 

"predict()" function is used to get predictions. 

It takes two arguments: the <em>built model</em> and the <em>test set. 
</em>The predicted results are stored in the "result_regress" object.

<code>predict(mod_regress,test)-&gt;result_regress</code>
Let's bind the actual price values from the "test" data-set and the predicted values into a single data-set using the "cbind()" function. 

The new data-frame is stored in "Final_Data"

<code>cbind(Actual=test$price,Predicted=result_regress)-&gt;Final_Data</code> 
<code>as.data.frame(Final_Data)-&gt;Final_Data</code>
A glance at the "Final_Data" which comprises of actual values and predicted values:


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data.png">

Let's find the error by subtracting the predicted values from the actual values and add this error as a new column to the "Final_Data":

<code>(Final_Data$Actual- Final_Data$Predicted)-&gt;error</code>
<code>cbind(Final_Data,error)-&gt;Final_Data</code>
A glance at the "Final_Data" which also comprises of the error in prediction:


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data2.png">

Now, we'll go ahead and calculate "<em>Root Mean Square Error" </em>which gives an aggregate error for all the predictions

<code>rmse1&lt;-sqrt(mean(Final_Data$error^2))</code> 
<code>rmse1</code>

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rmse1.png">

Going ahead, let's build another model, so that we can compare the accuracy of both these models and determine which is a better one.

We'll build a new linear regression model on the "train" set but this time, we'll be dropping the &#8216;x' and &#8216;y' columns from the independent variables i.e. 

the "price" of the diamonds is determined by all the columns except &#8216;x' and &#8216;y'.

The model built is stored in "mod_regress2": 

<code>lm(price~.-y-z,data = train)-&gt;mod_regress2</code>
The predicted results are stored in "result_regress2"<br> 

<code>predict(mod_regress2,test)-&gt;result_regress2</code>
Actual and Predicted values are combined and stored in "Final_Data2":

<code>cbind(Actual=test$price,Predicted=result_regress2)-&gt;Final_Data2</code> 
<code>as.data.frame(Final_Data2)-&gt;Final_Data2</code>
Let's also add the error in prediction to "Final_Data2"

<code>(Final_Data2$Actual- Final_Data2$Predicted)-&gt;error2</code>
<code>cbind(Final_Data2,error2)-&gt;Final_Data2</code>
A glance at "Final_Data2":


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data2.png">

Finding Root Mean Square Error to get the aggregate error:

<code>rmse2&lt;-sqrt(mean(Final_Data2$error^2))</code>

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rmse2.png">

We see that "rmse2" is marginally less than "rmse1" and hence the second model is marginally better than the first model.
<h3><strong>Classification:</strong></h3>
We'll be working with the "car_purchase" data-set to implement <em>recursive partitioning </em>which is a classification algorithm.


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/car-1.png">

Let's split the data into "train" and "test" sets using "sample.split()" function from "caTools" package.

<code>library(caTools)</code>
65% of the observations from &#8216;Purchased' column will be assigned "TRUE" labels and the rest will be assigned "FALSE" labels.

<code>sample.split(car_purchase$Purchased,SplitRatio = 0.65)-&gt;split_values</code>
All those observations which have "TRUE" label will be stored into &#8216;train' data and those observations having "FALSE" label will be assigned to &#8216;test' data.

<code>subset(car_purchase,split_values==T)-&gt;train_data</code>
<code>subset(car_purchase,split_values==F)-&gt;test_data</code>
Time to build the Recursive Partitioning algorithm:

We'll start off by loading the &#8216;rpart' package:

<code>library(rpart)</code>
"Purchased" column will be the dependent variable and all other columns are the independent variables i.e. 

we are determining whether the person has bought the car or not with respect to all other columns. 

The model is built on the "train_data" and the result is stored in "mod1".

<code>rpart(Purchased~.,data = train_data)-&gt;mod1</code>
Let's plot the result:

<code>plot(mod1,margin = 0.1)</code> <code>text(mod1,pretty = T,cex=0.8)</code>
<strong>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rplot.png">
</strong>

Now, let's go ahead and predict the results on "test_data". 

We are giving the built rpart model "mod1" as the first argument, the test set "test_data" as the second argument and prediction type as "class" for the third argument. 

The result is stored in &#8216;result1' object. 

<code>predict(mod1,test_data,type = "class")-&gt;result1</code>
Let's evaluate the accuracy of the model using "confusionMatrix()" function from caret package.

<code>library(caret)</code> <code></code><code>confusionMatrix(table(test_data$Purchased,result1))</code>
<strong>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/confusion.png">
</strong>

The confusion matrix tells us that out of the 90 observations where the person did not buy the car, 79 observations have been rightly classified as "No" and 11 have been wrongly classified as "YES". 

Similarly, out of the 50 observations where the person actually bought the car, 47 have been rightly classified as "YES" and 3 have been wrongly classified as "NO".

We can find the accuracy of the model by dividing the correct predictions with total predictions i.e. 

(79+47)/(79+47+11+3).
<h3><strong>K-Means Clustering:</strong></h3>
We'll work with "iris" data-set to implement k-means clustering:


<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/iris.png">

Let's remove the "Species" column and create a new data-set which comprises only the first four columns from the &#8216;iris' data-set.

<code>iris[1:4]-&gt;iris_k</code>
Let us take the number of clusters to be 3. 

"Kmeans()" function takes the input data and the number of clusters in which the data is to be clustered. 

The syntax is : kmeans( data, k) where k is the number of cluster centers.

<code>kmeans(iris_k,3)-&gt;k1</code>
Analyzing the clustering:

<code>str(k1)</code>

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture15.png">

The str() function gives the structure of the kmeans which includes various parameters like withinss, betweenss, etc, analyzing which you can find out the performance of kmeans.

betweenss : Between sum of squares i.e. 

Intracluster similarity

withinss : Within sum of square i.e. 

Intercluster similarity

totwithinss : Sum of all the withinss of all the clusters i.e.Total intra-cluster similarity

A good clustering will have a lower value of "tot.withinss" and higher value of "betweenss" which depends on the number of clusters &lsquo;k&rsquo; chosen initially. 

The time is ripe to become an expert in Machine Learning to take advantage of new opportunities that come your way. 

This brings us to the end of this "<em><strong>Machine Learning with R</strong></em>" blog. 

I hope this blog was informative fruitful.

<h2>put the whole if else statement in one line</h2>
if (TRUE) 1 else 3

You have to use {} for allows the if statement to have more than one line. 

<h2>quit R</h2>
if logout, 
quit("yes")

<h2>Run R scripts from the Windows command line (CMD)</h2>
This use library RDCOMClient to send summary information to colleges with Microsoft Outlook

There are two ways to do that.

use batch file looks like this.
"C:\Program Files\R\R-3.4.3\bin\Rscript.exe" C:\Users\myusername\Documents\R\Send_Outlook_Email.R

The second one looks like this.
"C:\Program Files\R\R-3.4.3\bin\R.exe" CMD BATCH C:\Users\myusername\Documents\R\Send_Outlook_Email.R

Remember to use quotation marks when there is space in the file path.

<h2>Locate the position of patterns in a string</h2>
library("stringr")
fruit &lt;- "apple banana pear pineapple"
str_locate(fruit, "ea")
str_locate_all(fruit, "ea")

str_locate, an integer matrix.
First column gives start postion of match, and second column gives end position

str_locate_all a list of integer matrices.

<h3>trim string</h3>
x &lt;- "This string is moderately long"
strtrim(x, 20)

<h2>find the max length of string in array</h2>
setwd("D:/Dropbox/MyDocs/R misc Jobs/Learning Exercise/QuizData")
WordTableFIle <&lt;- readLines("EnglishWordList.txt", encoding="UTF-8", warn = FALSE)
WordTable <&lt;- matrix(unlist(strsplit(WordTableFIle, split = "\\t")), ncol=1, byrow=TRUE) # make it one column
maxNum = max(nchar(WordTable))
maxNum
WordTable[which(nchar(WordTable) == maxNum)]

<h2>Convert Character Vector between Encodings</h2>
iconv(x, from, to, sub=NA)

‘i’ stands for ‘internationalization’.

Usage
iconv(x, from, to, sub=NA)

Arguments
x	A character vector.
from	A character string describing the current encoding.

to	A character string describing the target encoding.

sub	character string. If not NA it is used to replace any non-convertible bytes in the input.
(This would normally be a single character, but can be more.
If "byte", the indication is "&lt;xx>" with the hex code of the byte.

Details
The names of encodings and which ones are available (and indeed, if any are) is platform-dependent. 
On systems that support R's iconv you can use "" for the encoding of the current locale, as well as "latin1" and "UTF-8".


iconvlist()
On many platforms iconvlist provides an alphabetical list of the supported encodings. 
On others, the information is on the man page for iconv(5) or elsewhere in the man pages (and beware that the system command iconv may not support the same set of encodings as the C functions R calls). 
Unfortunately, the names are rarely common across platforms.

Elements of x which cannot be converted (perhaps because they are invalid or because they cannot be represented in the target encoding) will be returned as NA unless sub is specified.

Some versions of iconv will allow transliteration by appending //TRANSLIT to the to encoding: see the examples.

Value
A character vector of the same length and the same attributes as x.

Note
Not all platforms support these functions. 

See Also
localeToCharset, file.

Examples
## Not run: 
iconvlist()

## convert from Latin-2 to UTF-8: two of the glibc iconv variants.
iconv(x, "ISO_8859-2", "UTF-8")
iconv(x, "LATIN2", "UTF-8")

## Both x below are in latin1 and will only display correctly in a
## latin1 locale.
(x &lt;- "fa\xE7ile")
charToRaw(xx &lt;- iconv(x, "latin1", "UTF-8"))
## in a UTF-8 locale, print(xx)

iconv(x, "latin1", "ASCII")          #   NA
iconv(x, "latin1", "ASCII", "?")     # "fa?ile"
iconv(x, "latin1", "ASCII", "")      # "faile"
iconv(x, "latin1", "ASCII", "byte")  # "faile"

# Extracts from R help files
(x &lt;- c("Ekstr\xf8m", "J\xf6reskog", "bi\xdfchen Z\xfcrcher"))
iconv(x, "latin1", "ASCII//TRANSLIT")
iconv(x, "latin1", "ASCII", sub="byte")
## End(Not run)

<h2>encoding error with read_html</h2>
<a href="https://stackoverflow.com/users/4350463/hodgenovice" class="whitebut ">hodgenovice R expert</a>
<a href="https://stackoverflow.com/questions/45290452/encoding-error-with-read-html" class="whitebut ">encoding error with read_html</a>
url = "http://www.chinanews.com/scroll-news/news1.html"
thekeyword = "新闻"
read_page = lapply(unique(iconvlist()), function(encoding_attempt) {

  # Optional print statement to show progress since this takes time
  # print(match(encoding_attempt, iconvlist()) / length(iconvlist()))

  read_attempt = tryCatch(expr=read_html(url, encoding=encoding_attempt),
                           error=function(condition) NA,
                           warning=function(condition) message(condition))
  read_attempt = as.character(read_attempt)
  fisrtLine = grep(thekeyword, read_attempt)
  if(length(fisrtLine)>0){
    cat(encoding_attempt, "\n")
    cat(read_attempt[fisrtLine], "\n")
  }
})

names(read_page) = unique(iconvlist())


# 2. See which encodings correctly display some complex characters
read_phrase = lapply(read_page, function(encoded_page) 
  if(!is.na(encoded_page))
    html_text(html_nodes(encoded_page, ".content_right")))

# ended up with encodings which could be sensible...
encoding_shortlist = names(read_phrase)[read_phrase == "新闻"]
encoding_shortlist

sink("testResult.txt")
print(read_page)
sink()

retriveFile &lt;- as.character(read_html(url, warn=F, encoding = "UTF-16"))
fisrtLine = grep(thekeyword, retriveFile)
fisrtLine

<h2>Object-oriented programming (OOP)</h2>
a programming paradigm based on the concept of "objects", 

which can contain data, in the form of fields (often known as attributes or properties), 

and code, in the form of procedures (often known as methods). 

A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of "this" or "self"). 

In OOP, computer programs are designed by making them out of objects that interact with one another.

OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.

Many of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. 

Significant object-oriented languages include Java, C++, C#, Python, R, PHP, JavaScript, Ruby, Perl, Object Pascal, Objective-C, Dart, Swift, Scala, Kotlin, Common Lisp, MATLAB, and Smalltalk.


<h2>Reference classes</h2>
<!-- http://www.inside-r.org/r-doc/methods/ReferenceClasses -->
R has three object oriented (OO) systems: [[S3]], [[S4]] and Reference Classes (where the latter were for a while referred to as [[R5]], yet their official name is Reference Classes). 
This page describes this new reference-based class system.

Reference Classes (or refclasses) are new in R 2.12. 
They fill a long standing need for mutable objects that had previously been filled by non-core packages like R.oo, proto and mutatr. 
While the core functionality is solid, reference classes are still under active development and some details will change. 
The most up-to-date documentation for Reference Classes can always be found in ?ReferenceClasses.

There are two main differences between reference classes and S3 and S4:

Refclass objects use message-passing OO
Refclass objects are mutable: the usual R copy on modify semantics do not apply

These properties makes this object system behave much more like Java and C#. 
Surprisingly, the implementation of reference classes is almost entirely in R code - they are a combination of S4 methods and environments. 
This is a testament to the flexibility of S4.

Particularly suited for: simulations where you’re modelling complex state, GUIs.

Note that when using reference based classes we want to minimise side effects, and use them only where mutable state is absolutely required. 
The majority of functions should still be “functional”, and side effect free. 
This makes code easier to reason about (because you don’t need to worry about methods changing things in surprising ways), and easier for other R programmers to understand.

Limitations: can’t use enclosing environment - because that’s used for the object.

<h3>Classes and instances</h3>
Creating a new reference based class is straightforward: you use setRefClass. 
Unlike setClass from S4, you want to keep the results of that function around, because that’s what you use to create new objects of that type:

# Or keep reference to class around.
Person = setRefClass("Person")
Person$new()

A reference class has three main components, given by three arguments to setRefClass:

contains, the classes which the class inherits from. 
These should be other reference class objects:

setRefClass("Polygon")
setRefClass("Regular")

# Specify parent classes
setRefClass("Triangle", contains = "Polygon")
setRefClass("EquilateralTriangle", 
  contains = c("Triangle", "Regular"))

fields are the equivalent of slots in S4. 
They can be specified as a vector of field names, or a named list of field types:

setRefClass("Polygon", fields = c("sides"))
setRefClass("Polygon", fields = list(sides = "numeric"))

The most important property of refclass objects is that they are mutable, or equivalently they have reference semantics:

    Polygon = setRefClass("Polygon", fields = c("sides"))
    square = Polygon$new(sides = 4)
    
    triangle = square
    triangle$sides = 3
    
    square$sides        

methods are functions that operate within the context of the object and can modify its fields. 
These can also be added after object creation, as described below.

setRefClass("Dist")
setRefClass("DistUniform", c("a", "b"), "Dist", methods = list(
  mean = function() {
    (a + b) / 2
  }
))

You can also add methods after creation:

# Instead of creating a class all at once:
Person = setRefClass("Person", methods = list(
  say_hello = function() message("Hi!")
))

# You can build it up piece-by-piece
Person = setRefClass("Person")
Person$methods(say_hello = function() message("Hi!"))

It’s not currently possible to modify fields because adding fields would invalidate existing objects that didn’t have those fields.

The object returned by setRefClass (or retrieved later by getRefClass) is called a generator object. 
It has methods:

new for creating new objects of that class. 
The new method takes named arguments specifying initial values for the fields

methods for modifying existing or adding new methods

help for getting help about methods

fields to get a list of fields defined for class

lock locks the named fields so that their value can only be set once

accessors a convenience method that automatically sets up accessors of the form getXXX and setXXX.

<h3>Methods</h3>
Refclass methods are associated with objects, not with functions, and are called using the special syntax obj$method(arg1, arg2, ...). 
(You might recall we’ve seen this construction before when we called functions stored in a named list). 
Methods are also special because they can modify fields. 
This is different

We’ve also seen this construct before, when we used closures to create mutable state. 
Reference classes work in a similar manner but give us some extra functionality:

inheritance
a way of documenting methods
a way of specifying fields and their types

Modify fields with &lt;=. 
Will call accessor functions if defined.

Special fields: .self (Don’t use fields with names starting with . 
as these may be used for special purposes in future versions.)

initialize

<h3>Common methods</h3>
Because all refclass classes inherit from the same superclass, envRefClass, they a have common set of methods:

obj$callSuper:

obj$copy: creates a copy of the current object. 
This is necessary because Reference Classes classes don’t behave like most R objects, which are copied on assignment or modification.

obj$field: named access to fields. 
Equivalent to slots for S4. 
obj$field("xxx") the same as obj$xxx. 
obj$field("xxx", 5) the same as obj$xxx = 5

obj$import(x) coerces into this object, and obj$export(Class) coerces a copy of obj into that class. 
These should be super classes.

obj$initFields

<h1>R S3 Class</h1>
In this article, you will learn to work with S3 classes (one of the three class systems in R programming).

S3 class is the most popular and prevalent class in R programming language.

Most of the classes that come predefined in R are of this type. 
The fact that it is simple and easy to implement is the reason behind this.

<h2>How to define S3 class and create S3 objects?</h2>
S3 class has no formal, predefined definition.

Basically, a list with its class attribute set to some class name, is an S3 object. 
The components of the list become the member variables of the object.

Following is a simple example of how an S3 object of class student can be created.

> # create a list with required components
> s = list(name = "John", age = 21, GPA = 3.5)
> # name the class appropriately
> class(s) = "student"
> # That's it! we now have an object of class "student"
> s
$name
[1] "John"
$age
[1] 21
$GPA
[1] 3.5
attr(,"class")
[1] "student"

This might look awkward for programmers coming from C++, Python etc. 
where there are formal class definitions and objects have properly defined attributes and methods.

In R S3 system, it's pretty ad hoc. 
You can convert an object's class according to your will with objects of the same class looking completely different. 
It's all up to you.

<h2>How to use constructors to create objects?</h2>
It is a good practice to use a function with the same name as class (not a necessity) to create objects.

This will bring some uniformity in the creation of objects and make them look similar.

We can also add some integrity check on the member attributes. 
Here is an example. 
Note that in this example we use the attr() function to set the class attribute of the object.

# a constructor function for the "student" class
student = function(n,a,g) {
# we can add our own integrity checks
if(g>4 || g&lt;0)  stop("GPA must be between 0 and 4")
value = list(name = n, age = a, GPA = g)
# class can be set using class() or attr() function
attr(value, "class") = "student"
value
}

Here is a sample run where we create objects using this constructor.

> s = student("Paul", 26, 3.7)
> s
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7
attr(,"class")
[1] "student"
> class(s)
[1] "student"
> s = student("Paul", 26, 5)
Error in student("Paul", 26, 5) : GPA must be between 0 and 4
> # these integrity check only work while creating the object using constructor
> s = student("Paul", 26, 2.5)
> # it's up to us to maintain it or not
> s$GPA = 5

<h2>Methods and Generic Functions</h2>
In the above example, when we simply write the name of the object, its internals get printed.

In interactive mode, writing the name alone will print it using the print() function.

> s
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7
attr(,"class")
[1] "student"

Furthermore, we can use print() with vectors, matrix, data frames, factors etc. 
and they get printed differently according to the class they belong to.

How does print() know how to print these variety of dissimilar looking object?

The answer is, print() is a generic function. 
Actually, it has a collection of a number of methods. 
You can check all these methods with methods(print).

> methods(print)
[1] print.acf*                                   
[2] print.anova*
...
[181] print.xngettext*                             
[182] print.xtabs*                                 
Non-visible functions are asterisked

We can see methods like print.data.frame and print.factor in the above list.

When we call print() on a data frame, it is dispatched to print.data.frame().

If we had done the same with a factor, the call would dispatch to print.factor(). 
Here, we can observe that the method names are in the form generic_name.class_name(). 
This is how R is able to figure out which method to call depending on the class.

Printing our object of class "student" looks for a method of the form print.student(), but there is no method of this form.

So, which method did our object of class "student" call?

It called print.default(). 
This is the fallback method which is called if no other match is found. 
Generic functions have a default method.

There are plenty of generic functions like print(). 
You can list them all with methods(class="default").

> methods(class="default")
[1] add1.default*            aggregate.default*      
[3] AIC.default*             all.equal.default
...

<h2>How to write your own method?</h2>
Now let us implement a method print.student() ourself.

print.student = function(obj) {
cat(obj$name, "\n")
cat(obj$age, "years old\n")
cat("GPA:", obj$GPA, "\n")
}

Now this method will be called whenever we print() an object of class "student".

In S3 system, methods do not belong to object or class, they belong to generic functions. 
This will work as long as the class of the object is set.

> # our above implemented method is called
> s
Paul 
26 years old
GPA: 3.7 
> # removing the class attribute will restore as previous
> unclass(s)
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7

<h2>Writing Your Own Generic Function</h2>
It is possible to make our own generic function like print() or plot(). 
Let us first look at how these functions are implemented.

> print
function (x, ...) 
UseMethod("print")
&lt;bytecode: 0x0674e230>
&lt;environment: namespace:base>
> plot
function (x, y, ...) 
UseMethod("plot")
&lt;bytecode: 0x04fe6574>
&lt;environment: namespace:graphics>

We can see that they have a single call to UseMethod() with the name of the generic function passed to it. 
This is the dispatcher function which will handle all the background details. 
It is this simple to implement a generic function.

For the sake of example, we make a new generic function called grade.

grade = function(obj) {
UseMethod("grade")
}

A generic function is useless without any method. 
Let us implement the default method.

grade.default = function(obj) {
cat("This is a generic function\n")
}

Now let us make method for our class "student".

grade.student = function(obj) {
cat("Your grade is", obj$GPA, "\n")
}

A sample run.

> grade(s)
Your grade is 3.7

In this way, we implemented a generic function called grade and later a method for our class.

<h1>R Inheritance</h1>
In this article, you’ll learn everything about inheritance in R. 
More specifically, how to create inheritance in S3, S4 and Reference classes, and use them efficiently in your program.

Inheritance is one of the key features of object-oriented programming which allows us to define a new class out of existing classes.

This is to say, we can derive new classes from existing base classes and adding new features. 
We don’t have to write from scratch. 
Hence, inheritance provides reusability of code.

Inheritance forms a hierarchy of class just like a family tree. 
Important thing to note is that the attributes define for a base class will automatically be present in the derived class.

Moreover, the methods for the base class will work for the derived.


<img class="alignnone size-full wp-image-79" src="https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance.jpg" alt="Inheritance in R Programming" srcset="https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance.jpg 740w, https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance-300x162.jpg 300w" sizes="(max-width: 740px) 100vw, 740px">

Below, we discuss how inheritance is carried out for the three different class systems in R programming language.

<h2>Inheritance in S3 Class</h2>
S3 classes do not have any fixed definition. 
Hence attributes of S3 objects can be arbitrary.

Derived class, however, inherit the methods defined for base class. 
Let us suppose we have a function that creates new objects of class student as follows.

student = function(n,a,g) {
value = list(name=n, age=a, GPA=g)
attr(value, "class") = "student"
value
}

Furthermore, we have a method defined for generic function print() as follows.

print.student = function(obj) {
cat(obj$name, "\n")
cat(obj$age, "years old\n")
cat("GPA:", obj$GPA, "\n")
}

Now we want to create an object of class InternationalStudent which inherits from student.

This is be done by assigning a character vector of class names like class(obj) = c(child, parent).

> # create a list
> s = list(name="John", age=21, GPA=3.5, country="France")
> # make it of the class InternationalStudent which is derived from the class student
> class(s) = c("InternationalStudent","student")
> # print it out
> s
John 
21 years old
GPA: 3.5

We can see above that, since we haven’t defined any method of the form print.InternationalStudent(), the method print.student() got called. 
This method of class student was inherited.

Now let us define print.InternationalStudent().

print.InternationalStudent = function(obj) {
cat(obj$name, "is from", obj$country, "\n")
}

This will overwrite the method defined for class student as shown below.

> s
John is from France

We can check for inheritance with functions like inherits() or is().

> inherits(s,"student")
[1] TRUE
> is(s,"student")
[1] TRUE

<h2>Inheritance in S4 Class</h2>
Since S4 classes have proper definition, derived classes will inherit both attributes and methods of the parent class.

Let us define a class student with a method for the generic function show().

# define a class called student
setClass("student",
slots=list(name="character", age="numeric", GPA="numeric")
)
# define class method for the show() generic function
setMethod("show",
"student",
function(object) {
cat(object@name, "\n")
cat(object@age, "years old\n")
cat("GPA:", object@GPA, "\n")
}
)

Inheritance is done during the derived class definition with the argument contains as shown below.

# inherit from student
setClass("InternationalStudent",
slots=list(country="character"),
contains="student"
)

Here we have added an attribute country, rest will be inherited from the parent.

> s = new("InternationalStudent",name="John", age=21, GPA=3.5, country="France")
> show(s)
John 
21 years old
GPA: 3.5

We see that method define for class student got called when we did show(s).

We can define methods for the derived class which will overwrite methods of the base class, like in the case of S3 systems.

<h2>Inheritance in Reference Class</h2>
Inheritance in reference class is very much similar to that of the S4 class. 
We define in the contains argument, from which base class to derive from.

Here is an example of student reference class with two methods inc_age() and dec_age().

student = setRefClass("student",
fields=list(name="character", age="numeric", GPA="numeric"),
methods=list(
inc_age = function(x) {
age &lt;= age + x
},
dec_age = function(x) {
age &lt;= age - x
}
)
)

Now we will inherit from this class. 
We also overwrite dec_age() method to add an integrity check to make sure age is never negative.

InternationalStudent = setRefClass("InternationalStudent",
fields=list(country="character"),
contains="student",
methods=list(
dec_age = function(x) {
if((age - x)&lt;0)  stop("Age cannot be negative")
age &lt;= age - x
}
)
)

Let us put it to test.

> s = InternationalStudent(name="John", age=21, GPA=3.5, country="France")
> s$dec_age(5)
> s$age
[1] 16
> s$dec_age(20)
Error in s$dec_age(20) : Age cannot be negative
> s$age
[1] 16

In this way, we are able to inherit from the parent class.


<h2>R Reference Class</h2>
In this article, you will learn to work with reference classes in R programming which is one of the three class systems (other two are S3 and S4).

Reference class in R programming is similar to the object oriented programming we are used to seeing in common languages like C++, Java, Python etc.

Unlike <a title="R S3 Class" href="/r-programming/S3-class">S3</a> and <a title="R S4 class" href="/r-programming/S4-class">S4 classes</a>, methods belong to class rather than generic functions. 
Reference class are internally implemented as S4 classes with an environment added to it.

<h3>How to define a reference class?</h3>
Defining reference class is similar to defining a S4 class. 
Instead of setClass() we use the setRefClass() function.

> setRefClass("student")

Member variables of a class, if defined, need to be included in the class definition. 
Member variables of reference class are called fields (analogous to slots in S4 classes).

Following is an example to define a class called student with 3 fields, name, age and GPA.

> setRefClass("student", fields = list(name = "character", age = "numeric", GPA = "numeric"))

<h3>How to create a reference objects?</h3>
The function setRefClass() returns a generator function which is used to create objects of that class.

> student = setRefClass("student",
fields = list(name = "character", age = "numeric", GPA = "numeric"))
> # now student() is our generator function which can be used to create new objects
> s = student(name = "John", age = 21, GPA = 3.5)
> s
Reference class object of class "student"
Field "name":
[1] "John"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h3>How to access and modify fields?</h3>
Fields of the object can be accessed using the $ operator.

> s$name
[1] "John"
> s$age
[1] 21
> s$GPA
[1] 3.5

Similarly, it is modified by reassignment.

> s$name = "Paul"
> s
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h4>Warning Note</h4>

In R programming, objects are copied when assigned to new variable or passed to a function (pass by value). 
For example.

> # create list a and assign to b
> a = list("x" = 1, "y" = 2)
> b = a
> # modify b
> b$y = 3
> # a remains unaffected
> a
$x
[1] 1
$y
[1] 2
> # only b is modified
> b
$x
[1] 1
$y
[1] 3

But this is not the case with reference objects. 
Only a single copy exist and all variables reference to the same copy. 
Hence the name, reference.

> # create reference object a and assign to b
> a = student(name = "John", age = 21, GPA = 3.5)
> b = a
> # modify b
> b$name = "Paul"
> # a and b both are modified
> a
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5
> b
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

This can cause some unwanted change in values and be the source of strange bugs. 
We need to keep this in mind while working with reference objects. 
To make a copy, we can use the copy() method made availabe to us.

> # create reference object a and assign a’s copy to b
> a = student(name = "John", age = 21, GPA = 3.5)
> b = a$copy()
> # modify b
> b$name = "Paul"
> # a remains unaffected
> a
Reference class object of class "student"
Field "name":
[1] "John"
Field "age":
[1] 21
Field "GPA":
[1] 3.5
> # only b is modified
> b
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h3>Reference Methods</h3>
Methods are defined for a reference class and do not belong to generic functions as in S3 and S4 classes.

All reference class have some methods predefined because they all are inherited from the superclass envRefClass.

> student
Generator for class "student":
Class fields:
Name:       name       age       GPA
Class: character   numeric   numeric
Class Methods:  
"callSuper", "copy", "export", "field", "getClass", "getRefClass", 
"import", "initFields", "show", "trace", "untrace", "usingMethods"
Reference Superclasses:  
"envRefClass"

We can see class methods like copy(), field() and show() in the above list. 
We can create our own methods for the class.

This can be done during the class definition by passing a list of function definitions to methods argument of setRefClass().

student = setRefClass("student",
fields = list(name = "character", age = "numeric", GPA = "numeric"),
methods = list(
inc_age = function(x) {
age &lt;= age + x
},
dec_age = function(x) {
age &lt;= age - x
}
)
)

In the above section of our code, we defined two methods called inc_age() and dec_age(). 
These two method modify the field age.

Note that we have to use the non-local assignment operator &lt;= since age isn’t in the method’s local environment. 
This is important.

Using the simple assignment operator = would have created a local variable called age, which is not what we want. 
R will issue a warning in such case.

Here is a sample run where we use the above defined methods.

> s = student(name = "John", age = 21, GPA = 3.5)
> s$inc_age(5)
> s$age
[1] 26
> s$dec_age(10)
> s$age
[1] 16

<h2>R S4 Class</h2>
In this article, you’ll learn everything about S4 classes in R; how to define them, create them, access their slots, and use them efficiently in your program.

Unlike <a title="R S3 class" href="/r-programming/S3-class">S3 classes</a> and <a title="R object" href="/r-programming/object-class-introduction">objects</a> which lacks formal definition, we look at S4 class which is stricter in the sense that it has a formal definition and a uniform way to create objects.

This adds safety to our code and prevents us from accidentally making naive mistakes.

<h3>How to define S4 Class?</h3>
S4 class is defined using the setClass() function.

In R terminology, member variables are called slots. 
While defining a class, we need to set the name and the slots (along with class of the slot) it is going to have.

<h4>Example 1: Definition of S4 class</h4>
setClass("student", slots=list(name="character", age="numeric", GPA="numeric"))

In the above example, we defined a new class called student along with three slots it’s going to have name, age and GPA.

There are other optional arguments of setClass() which you can explore in the help section with ?setClass.

<h3>How to create S4 objects?</h3>
S4 objects are created using the new() function.

<h4>Example 2: Creation of S4 object</h4>
> # create an object using new()
> # provide the class name and value for slots
> s = new("student",name="John", age=21, GPA=3.5)
> s
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.5

We can check if an object is an S4 object through the function isS4().

> isS4(s)
[1] TRUE

The function setClass() returns a generator function.

This generator function (usually having same name as the class) can be used to create new objects. 
It acts as a constructor.

> student = setClass("student", slots=list(name="character", age="numeric", GPA="numeric"))
> student
class generator function for class “student” from package ‘.GlobalEnv’
function (...) 
new("student", ...)

Now we can use this constructor function to create new objects.

Note above that our constructor in turn uses the new() function to create objects. 
It is just a wrap around.

<h4>Example 3: Creation of S4 objects using generator function</h4>
> student(name="John", age=21, GPA=3.5)
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.5

<h3>How to access and modify slot?</h3>
Just as components of a <a title="R list" href="/r-programming/list">list</a> are accessed using $, slot of an object are accessed using @.

<h4>Accessing slot</h4>
> s@name
[1] "John"
> s@GPA
[1] 3.5
> s@age
[1] 21

<h4>Modifying slot directly</h4>
A slot can be modified through reassignment.

> # modify GPA
> s@GPA = 3.7
> s
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.7

<h4>Modifying slots using slot() function</h4>
Similarly, slots can be access or modified using the slot() function.

> slot(s,"name")
[1] "John"
> slot(s,"name") = "Paul"
> s
An object of class "student"
Slot "name":
[1] "Paul"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.7

<h3>Methods and Generic Functions</h3>
As in the case of S3 class, methods for S4 class also belong to generic functions rather than the class itself. 
Working with S4 generics is pretty much similar to S3 generics.

You can list all the S4 generic functions and methods available, using the function showMethods().

<h4>Example 4: List all generic functions</h4>
> showMethods()
Function: - (package base)
Function: != (package base)
...
Function: trigamma (package base)
Function: trunc (package base)

Writing the name of the object in interactive mode prints it. 
This is done using the S4 generic function show().

You can see this function in the above list. 
This function is the S4 analogy of the S3 print() function.

<h4>Example 5: Check if a function is a generic function</h4>
> isS4(print)
[1] FALSE
> isS4(show)
[1] TRUE

We can list all the methods of show generic function using showMethods(show).

<h4>Example 6: List all methods of a generic function</h4>
> showMethods(show)
Function: show (package methods)
object="ANY"
object="classGeneratorFunction"
...
object="standardGeneric"
(inherited from: object="genericFunction")
object="traceable"

<h3>How to write your own method?</h3>
We can write our own method using setMethod() helper function.

For example, we can implement our class method for the show() generic as follows.

setMethod("show",
"student",
function(object) {
cat(object@name, "\n")
cat(object@age, "years old\n")
cat("GPA:", object@GPA, "\n")
}
)

Now, if we write out the name of the object in interactive mode as before, the above code is executed.

> s = new("student",name="John", age=21, GPA=3.5)
> s    # this is same as show(s)
John 
21 years old
GPA: 3.5

In this way we can write our own S4 class methods for generic functions.

<h2>Write text to a file</h2>
fileConn=file("output.txt")
writeLines(c("Hello","World"), fileConn)
close(fileConn)

sink("outfile.txt")
cat("hello")
cat("world")
sink()

cat("Hello",file="outfile.txt",sep="\n")
cat("World",file="outfile.txt",append=TRUE)
cat("hello","world",file="output.txt",sep="\n",append=TRUE)
file.show("outfile.txt")

txt = "Hallo\nWorld"
writeLines(txt, "outfile.txt")

library(tidyverse)
c('Hello', 'World') %>% write_lines( "output.txt")

writeLines() with sink()
sink("tempsink", type="output")
writeLines("Hello\nWorld")
sink()
file.show("tempsink", delete.file=TRUE)

text = c("Hello", "World")
write.table(text, file = "output.txt", col.names = F, row.names = F, quote = F)

<h2>Play birthday music</h2>
<a href="https://stackoverflow.com/questions/31782580/how-can-i-play-birthday-music-using-r" class="whitebut ">play-birthday-music</a>
library("dplyr")
library("audio")
notes = c(A = 0, B = 2, C = 3, D = 5, E = 7, F = 8, G = 10)
pitch = "D D E D G F# D D E D A G D D D5 B G F# E C5 C5 B G A G"
duration = c( rep( c(0.75, 0.25, 1, 1, 1, 2), 2),
              0.75, 0.25, 1, 1, 1, 1, 1, 0.75, 0.25, 1, 1, 1, 2)
bday = data_frame(pitch = strsplit(pitch, " ")[[1]], duration = duration)

bday =
  bday %>%
  mutate(octave = substring(pitch, nchar(pitch)) %>%
          {suppressWarnings(as.numeric(.))} %>%
           ifelse(is.na(.), 4, .),
           note = notes[substr(pitch, 1, 1)],
           note = note + grepl("#", pitch) -
           grepl("b", pitch) + octave * 12 + 12 * (note < 3),
           freq = 2 ^ ((note - 60) / 12) * 440)

tempo = 120
sample_rate = 44100 # this is MP3 sample freq, the freq resolution is 40Hz
                    # the A4 freq is 440Hz
                    # the A#4 freq is 466Hz
                    # the Ab4 freq is 415Hz

# A3 (220) A4 (440) A5 (880) C6 (1046.502)

make_sine = function(freq, duration) {
  wave = sin( seq(0, duration /tempo *60, 1 /sample_rate) *freq *2 *pi)
  fade = seq(0, 1, 50 /sample_rate)
  wave * c(fade, rep(1, length(wave) - 2 * length(fade)), rev(fade))
}

bday_wave = mapply(make_sine, bday$freq, bday$duration) %>% do.call("c", .)

play(bday_wave)

There's a few points to note.
The default octave for the notes is octave 4, where A4 is at 440 Hz (the note used to tune the orchestra).
Octaves change over at C, so C3 is one semitone higher than B2.
The reason for the fade in make_sine is that without it there are audible pops when starting and stopping notes.

simple way:
library("audio")
bday = load.wave(bday_file)
play(bday)

<h2>S4 objects, slot</h2>
A slot can be seen as a part, element or a "property" of S4 objects. 
Say you have a car object, then you can have the slots "price", "number of doors", "type of engine", "mileage".

Slots can be accessed in numerous ways :
> aCar@price
> slot(aCar,"typeEngine")

<h2>Read a UTF-8 text file with BOM</h2>
library("data.table")
theName = "file_name.csv"
thetempData = fread(theName , encoding = "UTF-8", stringsAsFactors = F)

<h2>Data Cleanup: Remove NA</h2>
data = data[!is.na(data)]

<h3>Identifying missing values</h3>
We can test for the presence of missing values via the is.na() function.

<code># remove na in r - test for missing values (is.na example)
test &lt;- c(1,2,3,NA)
is.na(test)</code>

In the example above, is.na() will return a vector indicating which elements have a na value.

<h3>na.omit() &#8211; remove rows with na from a list</h3>

This is the easiest option. 
The na.omit() function returns a list without any rows that contain na values. 

try na.omit() or na.exclude()
max( na.omit(vec) )

<code># remove na in r - remove rows - na.omit function / option
ompleterecords &lt;- na.omit(datacollected) </code>

Passing your data frame through the na.omit() function is a simple way to purge incomplete records from your analysis. 
It is an efficient way to remove na values in r.

<h3>complete.cases() &#8211; returns vector of rows with na values</h3>

The na.omit() function relies on the sweeping assumption that the dropped rows (removed the na values) are similar to the typical member of the dataset. 

We accomplish this with the complete.cases() function. 
This r function will examine a dataframe and return a vector of the rows which contain missing values. 
We can examine the dropped records and purge them if we wish.

<code># na in R - complete.cases example
fullrecords &lt;-  collecteddata[!complete.cases(collecteddata)] droprecords &lt;-  collecteddata[complete.cases(collecteddata)] </code>

<h3>Fix in place using&nbsp;na.rm</h3>

For certain statistical functions in R, you can guide the calculation around a missing value through including the na.rm parameter (na.rm=True). 
The rows with na values are retained in the dataframe but excluded from the relevant calculations. 
Support for this parameter varies by package and function, so please check the documentation for your specific package.

This is often the best option if you find there are significant trends in the observations with na values. 
Use the na.rm parameter to guide your code around the missing values and proceed from there. 
We prepared a guide to using na.rm.

<h3>NA Values and regression analysis</h3>

Removal of missing values can distort a regression analysis. 
This is particularly true if you are working with higher order or more complicated models. 
Fortunately, there are several options in the common packages for working around these issues.

If you are using the lm function, it includes a na.action option. 
As part of defining your model, you can indicate how the regression function should handle missing values. 
Two possible choices are na.omit and na.exclude. 
na.omit will omit all rows from the calculations. 
The na.exclude option removes na values from the R calculations but makes an additional adjustment (padding out vectors with missing values) to maintain the integrity of the residual analytics and predictive calculations. 
This is often more effective that procedures that delete rows from the calculations.

You also have the option of attempting to &#8220;heal&#8221; the data using custom procedures. 
In this situation, map is.na against the data set to generate a logical vector that identifies which rows need to be adjusted. 
From there, you can build your own &#8220;healing&#8221; logic.

<h2>create a empty zero-length vector</h2>
numeric()
logical()
character()
integer()
double()
raw()
complex() 
vector('numeric')
vector('character')
vector('integer')
vector('double')
vector('raw')
vector('complex')
All return 0 length vectors of the appropriate atomic modes.


<h2>Export R tables to HTML</h2>
library(tableHTML)
#create an html table 
tableHTML(mtcars)

#and to export in a file
write_tableHTML(tableHTML(mtcars), file = 'myfile.html')

<h2>h2o max_depth</h2>
Available in: GBM, DRF, XGBoost, Isolation Forest

Hyperparameter: yes

Description
This specifies the maximum depth to which each tree will be built. 
A single tree will stop splitting when there are no more splits that satisfy the min_rows parameter, if it reaches max_depth, or if there are no splits that satisfy this min_split_improvement parameter.

In general, deeper trees can seem to provide better accuracy on a training set because deeper trees can overfit your model to your data. 
Also, the deeper the algorithm goes, the more computing time is required. 
This is especially true at depths greater than 10. 
At depth 4, 8 nodes, for example, you need 8 * 100 * 20 trials to complete this splitting for the layer.

One way to determine an appropriate value for max_depth is to run a quick Cartesian grid search. 
Each model in the grid search will use early stopping to tune the number of trees using the validation set AUC, as before. 
The examples below are also available in the GBM Tuning Tutorials folder on GitHub.

The max_depth default value varies depending on the algorithm.

library(h2o)
h2o.init()
# import the titanic dataset
df = h2o.importFile(path = "http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv")
dim(df)
head(df)
tail(df)
summary(df, exact_quantiles = TRUE)

# pick a response for the supervised problem
response = "survived"

# the response variable is an integer.
# we will turn it into a categorical/factor for binary classification
df[[response]] = as.factor(df[[response]])

# use all other columns (except for the name) as predictors
predictors = setdiff(names(df), c(response, "name"))

# split the data for machine learning
splits = h2o.splitFrame(data = df,
                         ratios = c(0.6, 0.2),
                         destination_frames = c("train", "valid", "test"),
                         seed = 1234)
train = splits[[1]]
valid = splits[[2]]
test  = splits[[3]]

# Establish a baseline performance using a default GBM model trained on the 60% training split
# We only provide the required parameters, everything else is default
gbm = h2o.gbm(x = predictors, y = response, training_frame = train)

# Get the AUC on the validation set
h2o.auc(h2o.performance(gbm, newdata = valid))
# The AUC is over 94%, so this model is highly predictive!
[1] 0.9480135

# Determine the best max_depth value to use during a hyper-parameter search.
# Depth 10 is usually plenty of depth for most datasets, but you never know
hyper_params = list( max_depth = seq(1, 29, 2) )
# or hyper_params = list( max_depth = c(4, 6, 8, 12, 16, 20) ), which is faster for larger datasets

grid = h2o.grid(
  hyper_params = hyper_params,

  # full Cartesian hyper-parameter search
  search_criteria = list(strategy = "Cartesian"),

  # which algorithm to run
  algorithm = "gbm",

  # identifier for the grid, to later retrieve it
  grid_id = "depth_grid",

  # standard model parameters
  x = predictors,
  y = response,
  training_frame = train,
  validation_frame = valid,

  # more trees is better if the learning rate is small enough
  # here, use "more than enough" trees - we have early stopping
  ntrees = 10000,

  # smaller learning rate is better, but because we have learning_rate_annealing,
  # we can afford to start with a bigger learning rate
  learn_rate = 0.05,

  # learning rate annealing: learning_rate shrinks by 1% after every tree
  # (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,

  # sample 80% of rows per tree
  sample_rate = 0.8,

  # sample 80% of columns per split
  col_sample_rate = 0.8,

  # fix a random number generator seed for reproducibility
  seed = 1234,

  # early stopping once the validation AUC doesn't improve by at least
  # 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = "AUC",

  # score every 10 trees to make early stopping reproducible
  # (it depends on the scoring interval)
  score_tree_interval = 10)

# by default, display the grid search results sorted by increasing logloss
# (because this is a classification task)
grid

# sort the grid models by decreasing AUC
sorted_grid = h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sorted_grid

# find the range of max_depth for the top 5 models
top_depths = sortedGrid@summary_table$max_depth[1:5]
min_depth = min(as.numeric(top_depths))
max_depth = max(as.numeric(top_depths))

> sorted_grid
#H2O Grid Details
Grid ID: depth_grid
Used hyper parameters:
 -  max_depth
Number of models: 15
Number of failed models: 0
Hyper-Parameter Search Summary: ordered by decreasing auc
     max_depth           model_ids                auc
  1         13  depth_grid_model_6 0.9552831783601015
...
  15         1  depth_grid_model_0 0.9478162862778248

It appears that max_depth values of 9 to 27 are best suited for this dataset, which is unusually deep.

<h2>Median</h2>
The middle most value in a data series is called the median.
The median() function is used in R to calculate this value.

data = c( 1, 2, 2, 2,3,3, 4, 7, 9 )
median(data) # Find the median 3

<h2>find mode</h2>
the value that has highest number of occurrences in a dataset
R does not have a standard in-built mode function

x = c(1,2,1,2,3,4,5,5,4,3,2,3,4)
getModes = function(x) {
  ux = unique(x)
  tab = tabulate(match(x, ux))
  ux[tab == max(tab)]
}
getModes(x)  # 2 3 4   three modes here


<h2>h2o Course Prerequisites</h2>
sample codes
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/merging-data.html
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/max_depth.html.

familiar with pandas
http://pandas.pydata.org/pandas-docs/stable/10min.html

R and Python+Pandas
https://www.slideshare.net/ajayohri/python-for-r-users

Basic Stats
https://mathwithbaddrawings.com/2016/07/13/why-not-to-trust-statistics/
http://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm

most important to understand the normal distribution and standard deviation:
https://en.wikipedia.org/wiki/Standard_deviation


https://students.brown.edu/seeing-theory
linear regression

advice intermixed with xkcd cartoons on stats:
http://livefreeordichotomize.com/2016/12/15/hill-for-the-data-scientist-an-xkcd-story

Confusion Matrix
http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/


Bias/Variance
http://scott.fortmann-roe.com/docs/BiasVariance.html
https://elitedatascience.com/bias-variance-tradeoff
https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bias.2C_variance_and_mean_squared_error

<h2>droplevels</h2>
removes unused levels of a factor. 
x = factor(c(3, 4, 8, 1, 5, 4, 4, 5))        # Example factor vector
x = x[- 1]                                   # Delete first entry

Our example vector consists of five factor levels: 1, 3, 4, 5, and 8. 
However, the vector itself does not include the value 3. 
The factor level 3 might therefore be dropped. 

x_drop = droplevels(x)                       # Apply droplevels in R
x_drop
# 4 8 1 5 4 4 5
# Levels: 1 4 5 8

<h2>h2o samples</h2>
library(h2o)
h2o.init()

h2oiris = as.h2o( droplevels(iris[1:100,]))

h2oiris
class(h2oiris)
h2o.levels(h2oiris, 5)

write.csv( mtcars, file = 'mtcars.csv') # create local data
h2omtcars = h2o.importFile( path = 'mtcars.csv')
h2omtcars

h2obin = h2o.importFile( path = 'https://stats.idre.ucla.edu/stat/data/binary.csv') # load online data

gbmModel = h2o.gbm( x = c('Month', 'DayOfWeek', 'Distance'), y = 'IsDepDelayed', training_frame = airlinesTrainData) # train model use GBM

h2o.varimp(gbmModel) # find variable importance

xgBoostModel = h2o.xgboost( x = c('Month', 'DayOfWeek', 'Distance'), y = 'IsDepDelayed', training_frame = airlinesTrainData) # xgb model

h2o.predict( gbmModel, airlinesTrainData) # predict

# https://stats.idre.ucla.edu/other/dae/
# http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html
# http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#defining-a-gbm-model
# https://dzone.com/articles/how-do-you-measure-if-your-customer-churn-predicti

<h2>Gradient Boosting Machine</h2>
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. 

H2O's Gradient Boosting Machine (GBM) offers a Stochastic GBM, which can increase performance quite a bit compared to the original GBM implementation.

Now we will train a basic GBM model

The GBM model will infer the response distribution from the response encoding if not specified explicitly through the "distribution" argument.
A seed is required for reproducibility.

gbm_fit1 = h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit1", seed = 1)

Next we will increase the number of trees used in the GBM by setting "ntrees=500".

The default number of trees in an H2O GBM is 50, so this GBM will trained using ten times the default.

Increasing the number of trees in a GBM is one way to increase performance of the model, however, you have to be careful not to overfit your model to the training data by using too many trees.

To automatically find the optimal number of trees, you must use H2O's early stopping functionality.

This example will not do that, however, the following example will.

gbm_fit2 = h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit2",
    #validation_frame = valid, only used if stopping_rounds > 0
  ntrees = 500, seed = 1)

We will again set "ntrees = 500", however, this time we will use early stopping in order to prevent overfitting (from too many trees).

All of H2O's algorithms have early stopping available, however early stopping is not enabled by default (with the exception of Deep Learning).

There are several parameters that should be used to control early stopping.

The three that are common to all the algorithms are: "stopping_rounds", "stopping_metric" and "stopping_tolerance".

The stopping metric is the metric by which you'd like to measure performance, and so we will choose AUC here.

The "score_tree_interval" is a parameter specific to the Random Forest model and the GBM.

Setting "score_tree_interval = 5" will score the model after every five trees.

The parameters we have set below specify that the model will stop training after there have been three scoring intervals where the AUC has not increased more than 0.0005.

Since we have specified a validation frame, the stopping tolerance will be computed on validation AUC rather than training AUC. 

gbm_fit3 = h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit3",
  validation_frame = valid,  #only used if stopping_rounds > 0
  ntrees = 500, score_tree_interval = 5,      #used for early stopping
                stopping_rounds = 3,          #used for early stopping
                stopping_metric = "AUC",      #used for early stopping
                stopping_tolerance = 0.0005,  #used for early stopping
                seed = 1)

Let's compare the performance of the two GBMs

gbm_perf1 = h2o.performance(model = gbm_fit1, newdata = test)
gbm_perf2 = h2o.performance(model = gbm_fit2, newdata = test)
gbm_perf3 = h2o.performance(model = gbm_fit3, newdata = test)

# Print model performance
gbm_perf1
gbm_perf2
gbm_perf3

# Retreive test set AUC
h2o.auc(gbm_perf1)  # 0.682765594191
h2o.auc(gbm_perf2)  # 0.671854616713
h2o.auc(gbm_perf3)  # 0.68309902855


To examine the scoring history, use the "scoring_history" method on a trained model.
If "score_tree_interval" is not specified, it will score at various intervals, as we can see for "h2o.scoreHistory()" below.
However, regular 5-tree intervals are used for "h2o.scoreHistory()".
The "gbm_fit2" was trained only using a training set (no validation set), so the scoring history is calculated for training set performance metrics only.

h2o.scoreHistory(gbm_fit2)


When early stopping is used, we see that training stopped at 105 trees instead of the full 500.
Since we used a validation set in "gbm_fit3", both training and validation performance metrics are stored in the scoring history object.
Take a look at the validation AUC to observe that the correct stopping tolerance was enforced.

h2o.scoreHistory(gbm_fit3)

Look at scoring history for third GBM model

plot(gbm_fit3, timestep = "number_of_trees", metric = "AUC")
plot(gbm_fit3, timestep = "number_of_trees", metric = "logloss")

4. Deep Learning
H2O's Deep Learning algorithm is a multilayer feed-forward artificial neural network.
It can also be used to train an autoencoder.
In this example we will train a standard supervised prediction model.

Train a default DL
First we will train a basic DL model with default parameters.
The DL model will infer the response distribution from the response encoding if it is not specified explicitly through the "distribution" argument.
H2O's DL will not be reproducible if it is run on more than a single core, so in this example, the performance metrics below may vary slightly from what you see on your machine.
In H2O's DL, early stopping is enabled by default, so below, it will use the training set and default stopping parameters to perform early stopping.

dl_fit1 = h2o.deeplearning(x = x,
     y = y,
     training_frame = train,
     model_id = "dl_fit1",
     seed = 1)

Train a DL with new architecture and more epochs.

Next we will increase the number of epochs used in the GBM by setting "epochs=20" (the default is 10).
Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model to your training data.
To automatically find the optimal number of epochs, you must use H2O's early stopping functionality.
Unlike the rest of the H2O algorithms, H2O's DL will use early stopping by default, so for comparison we will first turn off early stopping.
We do this in the next example by setting "stopping_rounds=0".

dl_fit2 = h2o.deeplearning(
  x = x, y = y, training_frame = train, model_id = "dl_fit2",
     #validation_frame = valid, only used if stopping_rounds > 0
     epochs = 20, hidden= c(10,10),
     stopping_rounds = 0,  # disable early stopping
     seed = 1)
Train a DL with early stopping This example will use the same model parameters as "dl_fit2".
This time, we will turn on  early stopping and specify the stopping criterion.
We will also pass a validation set, as is recommended for early stopping.

dl_fit3 = h2o.deeplearning(
  x = x, y = y, training_frame = train, model_id = "dl_fit3",
     validation_frame = valid,  #in DL, early stopping is on by default
     epochs = 20, hidden = c(10,10),
     score_interval = 1,           #used for early stopping
     stopping_rounds = 3,          #used for early stopping
     stopping_metric = "AUC",      #used for early stopping
     stopping_tolerance = 0.0005,  #used for early stopping
     seed = 1)


Let's compare the performance of the three DL models
dl_perf1 = h2o.performance(model = dl_fit1, newdata = test)
dl_perf2 = h2o.performance(model = dl_fit2, newdata = test)
dl_perf3 = h2o.performance(model = dl_fit3, newdata = test)

Print model performance
dl_perf1
dl_perf2
dl_perf3

# Retreive test set AUC
h2o.auc(dl_perf1)  # 0.6774335
h2o.auc(dl_perf2)  # 0.678446
h2o.auc(dl_perf3)  # 0.6770498

# Scoring history
h2o.scoreHistory(dl_fit3)
# Scoring History: 
  timestamp   duration  training_speed   epochs
1 2016-05-03 10:33:29  0.000 sec                  0.00000
2 2016-05-03 10:33:29  0.347 sec 424697 rows/sec  0.86851
3 2016-05-03 10:33:30  1.356 sec 601925 rows/sec  6.09185
4 2016-05-03 10:33:31  2.348 sec 717617 rows/sec 13.05168
5 2016-05-03 10:33:32  3.281 sec 777538 rows/sec 20.00783
6 2016-05-03 10:33:32  3.345 sec 777275 rows/sec 20.00783

# iterations        samples training_MSE training_r2
1          0       0.000000  
2          1   99804.000000      0.14402     0.03691
3          7  700039.000000      0.14157     0.05333
4         15 1499821.000000      0.14033     0.06159
5         23 2299180.000000      0.14079     0.05853
6         23 2299180.000000      0.14157     0.05333
# training_logloss training_AUC training_lift
1                     
2          0.45930      0.66685       2.20727
3          0.45220      0.68133       2.59354
4          0.44710      0.67993       2.70390
5          0.45100      0.68192       2.81426
6          0.45220      0.68133       2.59354
# training_classification_error validation_MSE validation_r2
1             
2                       0.36145        0.14682       0.03426
3                       0.33647        0.14500       0.04619
4                       0.37126        0.14411       0.05204
5                       0.32868        0.14474       0.04793
6                       0.33647        0.14500       0.04619
# validation_logloss validation_AUC validation_lift
1    
2            0.46692        0.66582         2.53209
3            0.46256        0.67354         2.64124
4            0.45789        0.66986         2.44478
5            0.46292        0.67117         2.70672
6            0.46256        0.67354         2.64124
# validation_classification_error
1         
2  0.37197
3  0.34716
4  0.34385
5  0.36544
6  0.34716

# Look at scoring history for third DL model
plot(dl_fit3, timestep = "epochs", metric = "AUC")


5. Naive Bayes model
The Naive Bayes (NB) algorithm does not usually beat an algorithm like a Random Forest or GBM, however it is still a popular algorithm, especially in the text domain (when your input is text encoded as "Bag of Words", for example).
The Naive Bayes algorithm is for binary or multiclass classification problems only, not regression.
Therefore, your response must be a factor instead of a numeric.

First we will train a basic NB model with default parameters. 

nb_fit1 = h2o.naiveBayes(
  x = x, y = y, training_frame = train, model_id = "nb_fit1")

Train a NB model with Laplace Smoothing
One of the few tunable model parameters for the Naive Bayes algorithm is the amount of Laplace smoothing. 
The H2O Naive Bayes model will not use any Laplace smoothing by default.

nb_fit2 = h2o.naiveBayes(
  x = x, y = y, training_frame = train, model_id = "nb_fit2", laplace = 6)

Let's compare the performance of the two NB models
nb_perf1 = h2o.performance(model = nb_fit1, newdata = test)
nb_perf2 = h2o.performance(model = nb_fit2, newdata = test)

# Print model performance
nb_perf1
nb_perf2

# Retreive test set AUC
h2o.auc(nb_perf1)  # 0.6488014
h2o.auc(nb_perf2)  # 0.6490678

<h2>Confusion Matrix</h2>
<a href="http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" class="whitebut ">the basic yes/no confusion matrix</a>

<a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="whitebut ">Bias/Variance</a>

<a href="https://elitedatascience.com/bias-variance-tradeoff" class="whitebut ">the Bias-Variance Tradeoff</a>

<a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bias.2C_variance_and_mean_squared_error" class="whitebut ">shows biased estimator can be better than perfectly unbiased estimator</a>

<a href="https://www.coursera.org/learn/machine-learning-h2o/home/welcome" class="whitebut ">Practical Machine Learning on H2O</a>

<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/merging-data.html" class="whitebut ">Merging Two Datasets</a>

<a href="https://www.h2o.ai/download/" class="whitebut ">Downloads</a>

<h2>Print Strings without Quotes</h2>
   print(resultTable, quote = FALSE, row.names = FALSE, col.names = FALSE)
   write.table(resultTable, file = "/tmp/foo.csv", quote = FALSE,  row.names = FALSE, sep = "\t")

<h2>The apply family</h2>
<a href="https://nicercode.github.io/guides/repeating-things/" class="whitebut ">repeating-things</a>

There are several related function in R which allow you to apply some function to a series of objects (eg. vectors, matrices, dataframes or files). They include:
lapply
sapply
tapply
aggregate
mapply
apply

Each repeats a function or operation on a series of elements, but they differ in the data types they accept and return. What they all in common is that <strong>order of iteration is not important</strong>.  This is crucial. If each each iteration is independent, then you can cycle through them in whatever order you like. Generally, we argue that you should only use the generic looping functions <code>for</code>, <code>while</code>, and <code>repeat</code> when the order or operations <strong>is</strong> important. Otherwise reach for one of the apply tools.

<h2>lapply and sapply</h2>

<code>lapply</code> applies a function to each element of a list (or vector), collecting results in a list.  <code>sapply</code> does the same, but will try to <em>simplify</em> the output if possible.

Lists are a very powerful and flexible data structure that few people seem to know about. Moreover, they are the building block for other data structures, like <code>data.frame</code> and <code>matrix</code>. To access elements of a list, you use the double square bracket, for example <code>X[[4]]</code> returns the fourth element of the list <code>X</code>. If you don’t know what a list is, we suggest you <a href="http://cran.r-project.org/doc/manuals/R-intro.html#Lists-and-data-frames">read more about them</a>, before you proceed.
<h3>Basic syntax</h3>
<code>result &lt;- lapply(a list or vector, a function, ...)</code>

This code will also return a list, stored in <code>result</code>, with same number of elements as <code>X</code>.

<h3>Usage</h3>
lapply is great for building analysis pipelines, where you want to repeat a series of steps on a large number of similar objects.  The way to do this is to have a series of lapply statements, with the output of one providing the input to another:
<code>first.step &lt;- lapply(X, first.function) second.step &lt;- lapply(first.step, next.function)</code>

The challenge is to identify the parts of your analysis that stay the same and those that differ for each call of the function. The trick to using <code>lapply</code> is to recognise that only one item can differ between different function calls.

It is possible to pass in a bunch of additional arguments to your function, but these must be the same for each call of your function. For example, let’s say we have a function <code>test</code> which takes the path of a file, loads the data, and tests it against some hypothesised value H0. We can run the function on the file
“myfile.csv” as follows.

<code>result &lt;- test("myfile.csv", H0=1)</code>

We could then run the test on a bunch of files using lapply:

<code>files &lt;- c("myfile1.csv", "myfile2.csv", "myfile3.csv") result &lt;- lapply(files, test, H0=1)</code>

But notice, that in this example, the <strong>only this that differs</strong> between the runs is a single number in the file name. So we could save ourselves typing these by adding an extra step to generate the file names

<code>files &lt;- lapply(1:10, function(x){paste0("myfile", x, ".csv")}) result &lt;- lapply(files, test, H0=1)</code>

The nice things about that piece of code is that it would extend as long as we wanted, to 10000000 files, if needed.

<h3>Example - plotting temperature for many sites using open weather data</h3>

Let’s look at the weather in some eastern Australian cities over the last couple of days.  The website
<a href="http://openweathermap.org">openweathermap.com</a> provides access to all sorts of neat data, lots of it essentially real time.  We’ve parcelled up some on the nicercode website to use.  In theory, this sort of analysis script could use the weather data directly, but we don’t want to hammer their website too badly.  The code used to generate these files is <a href="https://gist.github.com/richfitz/5795029">here</a>.

We want to look at the temperatures over the last few days for the cities

<code>cities &lt;- c("Melbourne", "Sydney", "Brisbane", "Cairns")</code>

The data are stored in a url scheme where the Sydney data is at
<a href="http://nicercode.github.io/guides/repeating-things/data/Sydney.csv">http://nicercode.github.io/guides/repeating-things/data/Sydney.csv</a> and so on.  

The URLs that we need are therefore:

<code>urls &lt;-
 sprintf("http://nicercode.github.io/guides/repeating-things/data/%s.csv",
         cities) urls</code>

<code>[1] "http://nicercode.github.io/guides/repeating-things/data/Melbourne.csv"
[2] "http://nicercode.github.io/guides/repeating-things/data/Sydney.csv"   
[3] "http://nicercode.github.io/guides/repeating-things/data/Brisbane.csv" 
[4] "http://nicercode.github.io/guides/repeating-things/data/Cairns.csv"   </code>

We can write a function to download a file if it does not exist:

<code>download.maybe &lt;- function(url, refetch=FALSE, path=".") {
 dest &lt;- file.path(path, basename(url))
 if (refetch || !file.exists(dest))
   download.file(url, dest)
 dest
}</code>
and then run that over the urls:

<code>path &lt;- "data" dir.create(path, showWarnings=FALSE) files &lt;- sapply(urls, download.maybe, path=path) names(files) &lt;- cities</code>

Notice that we never specify the order of which file is downloaded in which order; we just say “apply this function (<code>download.maybe</code>) to this list of urls.  We also pass the <code>path</code> argument to every function call.  So it was as if we’d written

<code>download.maybe(urls[[1]], path=path) download.maybe(urls[[2]], path=path) download.maybe(urls[[3]], path=path) download.maybe(urls[[4]], path=path)</code>
but much less boring, and scalable to more files.

The first column, <code>time</code> of each file is a string representing date and time, which needs processing into R’s native time format (dealing with times in R (or frankly, in any language) is a complete pain).  In a real case, there might be many steps involved in processing each file.  We can make a function like this:

<code>load.file &lt;- function(filename) {
 d &lt;- read.csv(filename, stringsAsFactors=FALSE)
 d$time &lt;- as.POSIXlt(d$time)
 d
}</code>
that reads in a file given a filename, and then apply that function to each filename using <code>lapply</code>:

<code>data &lt;- lapply(files, load.file) names(data) &lt;- cities</code>

We now have a <strong>list</strong>, where each element is a <code>data.frame</code> of weather data:

<code>head(data$Sydney)</code>

<code>             time  temp temp.min temp.max
1 2013-06-13 23:00:00 12.66     8.89    16.11
2 2013-06-14 00:00:00 15.90    12.22    20.00
3 2013-06-14 02:00:00 18.44    16.11    20.00
4 2013-06-14 03:00:00 18.68    16.67    20.56
5 2013-06-14 04:00:00 19.41    17.78    22.22
6 2013-06-14 05:00:00 19.10    17.78    22.22</code>

We can use <code>lapply</code> or <code>sapply</code> to easy ask the same question to each element of this list.  For example, how many rows of data are there?

<code>sapply(data, nrow)</code>

<code>Melbourne    Sydney  Brisbane    Cairns 
      97        99        99        80 </code>

What is the hottest temperature recorded by city?

<code>sapply(data, function(x) max(x$temp))</code>

<code>Melbourne    Sydney  Brisbane    Cairns 
   12.85     19.41     22.00     31.67 </code>
or, estimate the autocorrelation function for each set:

<code>autocor &lt;- lapply(data, function(x) acf(x$temp, lag.max=24))</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-221.png" alt="plot of chunk unnamed-chunk-22" /> 
<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-222.png" alt="plot of chunk unnamed-chunk-22" /> 
<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-223.png" alt="plot of chunk unnamed-chunk-22" /> 
<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-224.png" alt="plot of chunk unnamed-chunk-22" />

<code>plot(autocor$Sydney, main="Sydney")</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-225.png" alt="plot of chunk unnamed-chunk-22" />

<code>plot(autocor$Cairns, main="Cairns")</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-226.png" alt="plot of chunk unnamed-chunk-22" />

I find that for loops can be easier to plot data, partly because there is nothing to <em>collect</em> (or combine) at each iteration.

<code>xlim &lt;- range(sapply(data, function(x) range(x$time))) ylim &lt;- range(sapply(data, function(x) range(x[-1]))) plot(data[[1]]$time, data[[1]]$temp, ylim=ylim, type="n",
    xlab="Time", ylab="Temperature") cols &lt;- 1:4 for (i in seq_along(data))
 lines(data[[i]]$time, data[[i]]$temp, col=cols[i])</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-23.png" alt="plot of chunk unnamed-chunk-23" />

<code>plot(data[[1]]$time, data[[1]]$temp, ylim=ylim, type="n",
    xlab="Time", ylab="Temperature") mapply(function(x, col) lines(x$time, x$temp, col=col),
      data, cols)</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-24.png" alt="plot of chunk unnamed-chunk-24" />

<code>$Melbourne
NULL

$Sydney
NULL

$Brisbane
NULL

$Cairns
NULL</code>

<h3>Parallelising your code</h3>

Another great feature of lapply is that is <strong>makes it really easy to parallelise your code</strong>. All computers now contain multiple CPUs, and these can all be put to work using the great <a href="http://www.rforge.net/multicore/">multicore package</a>.

<code>result &lt;- lapply(x, f)   #apply f to x using a single core and lapply
library(multicore) result &lt;- mclapply(x, f) #same thing using all the cores in your machine</code>

<h2>tapply and aggregate</h2>

In the case above, we had naturally “split” data; we had a vector of city names that led to a list of different data.frames of weather data.  Sometimes the “split” operation depends on a factor.  For example, you might have an experiment where you measured the size of plants at different levels of added fertiliser - you then want to know the mean height as a function of this treatment.

However, we’re actiually going to use some data on <a href="https://github.com/audy/smalldata">ratings of seinfeld episodes</a>, taken from the [Internet movie Database]
(http://www.reddit.com/r/dataisbeautiful/comments/1g7jw2/seinfeld_imdb_episode_ratings_oc/).

<code>library(downloader) if (!file.exists("seinfeld.csv"))
 download("https://raw.github.com/audy/smalldata/master/seinfeld.csv",
          "seinfeld.csv") dat &lt;- read.csv("seinfeld.csv", stringsAsFactors=FALSE)</code>

Columns are Season (number), Episode (number), Title (of the episode), Rating (according to IMDb) and Votes (to construct the rating).

<code>head(dat)</code>

<code>  Season Episode             Title Rating Votes
1      1       2      The Stakeout    7.8   649
2      1       3       The Robbery    7.7   565
3      1       4    Male Unbonding    7.6   561
4      1       5     The Stock Tip    7.8   541
5      2       1 The Ex-Girlfriend    7.7   529
6      2       1        The Statue    8.1   509</code>

Make sure it’s sorted sensibly

<code>dat &lt;- dat[order(dat$Season, dat$Episode),]</code>

Biologically, this could be Site / Individual / ID / Mean size /
Things measured.

Hypothesis: Seinfeld used to be funny, but got progressively less good as it became too mainstream.  Or, does the mean episode rating per season decrease?

Now, we want to calculate the average rating per season:

<code>mean(dat$Rating[dat$Season == 1])</code>

<code>[1] 7.725</code>

<code>mean(dat$Rating[dat$Season == 2])</code>

<code>[1] 8.158</code>
and so on until:

<code>mean(dat$Rating[dat$Season == 9])</code>

<code>[1] 8.323</code>

As with most things, we <em>could</em> automate this with a for loop:

<code>seasons &lt;- sort(unique(dat$Season)) rating  &lt;- numeric(length(seasons)) for (i in seq_along(seasons))
 rating[i] &lt;- mean(dat$Rating[dat$Season == seasons[i]])</code>

That’s actually not that horrible to do.  But we it could be nicer.  We first <strong>split</strong> the ratings by season:

<code>ratings.split &lt;- split(dat$Rating, dat$Season) head(ratings.split)</code>

<code>$`1`
[1] 7.8 7.7 7.6 7.8

$`2`
[1] 7.7 8.1 8.0 7.9 7.8 8.5 8.7 8.5 8.0 8.0 8.4 8.3

$`3`
[1] 8.3 7.5 7.8 8.1 8.3 7.3 8.7 8.5 8.5 8.6 8.1 8.4 8.5 8.7 8.6 7.8 8.3
[18] 8.6 8.7 8.6 8.0 8.5 8.6

$`4`
[1] 8.4 8.3 8.6 8.5 8.7 8.6 8.1 8.2 8.7 8.4 8.3 8.7 8.5 8.6 8.3 8.2 8.4
[18] 8.5 8.4 8.7 8.7 8.4 8.5

$`5`
[1] 8.6 8.4 8.4 8.4 8.3 8.2 8.1 8.5 8.5 8.3 8.0 8.1 8.6 8.3 8.4 8.5 7.9
[18] 8.0 8.5 8.7 8.5

$`6`
[1] 8.1 8.4 8.3 8.4 8.2 8.3 8.5 8.4 8.3 8.2 8.1 8.4 8.6 8.2 7.5 8.4 8.2
[18] 8.5 8.3 8.4 8.1 8.5 8.2</code>

Then use sapply to loop over this list, computing the mean

<code>rating &lt;- sapply(ratings.split, mean)</code>

Then if we wanted to apply a different function (say, compute the per-season standard error) we could just do:

<code>se &lt;- function(x)
 sqrt(var(x) / length(x)) rating.se &lt;- sapply(ratings.split, se)
plot(rating ~ seasons, ylim=c(7, 9), pch=19) arrows(seasons, rating - rating.se, seasons, rating + rating.se,
      code=3, angle=90, length=0.02)</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-34.png" alt="plot of chunk unnamed-chunk-34" />

But there’s still repetition there.  Let’s abstract that away a bit.

Suppose we want a:
 1. response variable (like Rating was)
 2. grouping variable (like Season was)
 3. function to apply to each level

This just writes out <em>exactly</em> what we had before

<code>summarise.by.group &lt;- function(response, group, func) {
 response.split &lt;- split(response, group)
 sapply(response.split, func)
}</code>

We can compute the mean rating by season again:

<code>rating.new &lt;- summarise.by.group(dat$Rating, dat$Season, mean)</code>
which is the same as what we got before:

<code>identical(rating.new, rating)</code>

<code>[1] TRUE</code>

Of course, we’re not the first people to try this.  This is <strong>exactly</strong> what the <code>tapply</code> function does (but with a few bells and whistles, especially around missing values, factor levels, additional arguments and multiple grouping factors at once).

<code>tapply(dat$Rating, dat$Season, mean)</code>

<code>1     2     3     4     5     6     7     8     9 
7.725 8.158 8.304 8.465 8.343 8.283 8.441 8.423 8.323 </code>

So using <code>tapply</code>, you can do all the above manipulation in a single line.

There are a couple of limitations of <code>tapply</code>.

The first is that getting the season out of <code>tapply</code> is quite hard.  We could do:

<code>as.numeric(names(rating))</code>

<code>[1] 1 2 3 4 5 6 7 8 9</code>

But that’s quite ugly, not least because it involves the conversion numeric -&gt; string -&gt; numeric.

Better could be to use

<code>sort(unique(dat$Season))</code>

<code>[1] 1 2 3 4 5 6 7 8 9</code>

But that requires knowing what is going on inside of <code>tapply</code> (that unique levels are sorted and data are returned in that order).

I suspect that this approach:

<code>first &lt;- function(x) x[[1]] tapply(dat$Season, dat$Season, first)</code>

<code>1 2 3 4 5 6 7 8 9 
1 2 3 4 5 6 7 8 9 </code>
is probably the most fool-proof, but it’s certainly not pretty.

However, the returned format is extremely flexible.  If you do:

The <code>aggregate</code> function provides a simplfied interface to <code>tapply</code> that avoids this issue.  It has two interfaces: the first is similar to what we used before, but the grouping variable now must be a list or data frame:

<code>aggregate(dat$Rating, dat["Season"], mean)</code>

<code>  Season     x
1      1 7.725
2      2 8.158
3      3 8.304
4      4 8.465
5      5 8.343
6      6 8.283
7      7 8.441
8      8 8.423
9      9 8.323</code>

(note that <code>dat["Season"]</code> returns a one-column data frame).  The column ‘x’ is our response variable, Rating, grouped by season.  We can get its name included in the column names here by specifying the first argument as a <code>data.frame</code> too:

<code>aggregate(dat["Rating"], dat["Season"], mean)</code>

<code>  Season Rating
1      1  7.725
2      2  8.158
3      3  8.304
4      4  8.465
5      5  8.343
6      6  8.283
7      7  8.441
8      8  8.423
9      9  8.323</code>

The other interface is the formula interface, that will be familiar from fitting linear models:

<code>aggregate(Rating ~ Season, dat, mean)</code>

<code>  Season Rating
1      1  7.725
2      2  8.158
3      3  8.304
4      4  8.465
5      5  8.343
6      6  8.283
7      7  8.441
8      8  8.423
9      9  8.323</code>

This interface is really nice; we can get the number of votes here too.

<code>aggregate(cbind(Rating, Votes) ~ Season, dat, mean)</code>

<code>  Season Rating Votes
1      1  7.725 579.0
2      2  8.158 533.0
3      3  8.304 496.7
4      4  8.465 497.0
5      5  8.343 452.5
6      6  8.283 385.7
7      7  8.441 408.0
8      8  8.423 391.4
9      9  8.323 415.0</code>

If you have multiple grouping variables, you can write things like:
&lt;div class='bogus-wrapper'&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;div class=&#8221;highlight&#8221;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#8221;gutter&#8221;&gt;&lt;pre class=&#8221;line-numbers&#8221;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class='code'&gt;&lt;pre&gt;<code>aggregate(response ~ factor1 + factor2, dat, function)</code>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;

to apply a function to each pair of levels of <code>factor1</code> and <code>factor2</code>.

<h2>replicate</h2>

This is great in Monte Carlo simulation situations.  For example.
Suppose that you flip a fair coin n times and count the number of heads:

<code>trial &lt;- function(n)
 sum(runif(n) &lt; 0.5) # could have done a binomial draw...</code>

You can run the trial a bunch of times:

<code>trial(10)</code>

<code>[1] 4</code>

<code>trial(10)</code>

<code>[1] 4</code>

<code>trial(10)</code>

<code>[1] 6</code>
and get a feel for the results.  If you want to replicate the trial
100 times and look at the distribution of results, you could do:

<code>replicate(100, trial(10))</code>

<code>  [1] 4 4 5 6 8 5 5 7 3 5 6 4 4 3 5 3 6 7 2 6 6 4 5 4 4 4 4 5 6 5 4 2 6 5 6
[36] 5 6 8 5 6 4 5 4 5 5 5 4 7 3 5 5 6 4 6 4 6 4 4 4 6 3 5 5 7 6 7 5 3 4 4
[71] 5 6 8 5 6 2 5 7 6 3 5 9 3 7 6 4 5 3 7 3 3 7 6 8 5 4 6 7 4 3</code>
and then you could plot these:

<code>plot(table(replicate(10000, trial(50))))</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-49.png" alt="plot of chunk unnamed-chunk-49" />

<h3>for loops</h3>

“<code>for</code>” loops shine where the output of one iteration depends on the result of the previous iteration.

Suppose you wanted to model random walk.  Every time step, with 50% probability move left or right.

Start at position 0

<code>x &lt;- 0</code>

Move left or right with probability p (0.5 = unbiased)

<code>p &lt;- 0.5</code>

Update the position

<code>x &lt;- x + if (runif(1) &lt; p) -1 else 1</code>

Let’s abstract the update into a function:

<code>step &lt;- function(x, p=0.5)
 x + if (runif(1) &lt; p) -1 else 1</code>

Repeat a bunch of times:

<code>x &lt;- step(x) x &lt;- step(x)</code>

To find out where we got to after 20 steps:

<code>for (i in 1:20)
 x &lt;- step(x)</code>

If we want to collect where we’re up to at the same time:

<code>nsteps &lt;- 200 x &lt;- numeric(nsteps + 1) x[1] &lt;- 0 # start at 0 for (i in seq_len(nsteps))
 x[i+1] &lt;- step(x[i]) plot(x, type="l")</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-56.png" alt="plot of chunk unnamed-chunk-56" />

Pulling <em>that</em> into a function:

<code>random.walk &lt;- function(nsteps, x0=0, p=0.5) {
 x &lt;- numeric(nsteps + 1)
 x[1] &lt;- x0
 for (i in seq_len(nsteps))
   x[i+1] &lt;- step(x[i])
 x
}</code>

We can then do 30 random walks:

<code>walks &lt;- replicate(30, random.walk(100)) matplot(walks, type="l", lty=1, col=rainbow(nrow(walks)))</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-58.png" alt="plot of chunk unnamed-chunk-58" />

Of course, in this case, if we think in terms of vectors we can actually implement random walk using implicit vectorisation:

<code>random.walk &lt;- function(nsteps, x0=0, p=0.5)
 cumsum(c(x0, ifelse(runif(nsteps) &lt; p, -1, 1)))
walks &lt;- replicate(30, random.walk(100)) matplot(walks, type="l", lty=1, col=rainbow(nrow(walks)))</code>


<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-59.png" alt="plot of chunk unnamed-chunk-59" />

Which reinforces one of the advantages of thinking in terms of functions: you can change the implementation detail without the rest of the program changing.

<h2>increase the max print rows limit</h2>
getOption("max.print")
options(max.print=999999)

<h2>parallel processing: foreach package</h2>
loops are incredibly inefficient at processing data in R.

iters&lt;-10  #number of iterations in the loop
ls&lt;-vector('list',length=iters)  #vector for appending output
strt&lt;-Sys.time()   #start time
for(i in 1:iters){   #loop
    cat(i,'\n')    #counter
    to.ls&lt;-rnorm(1e6)
    to.ls&lt;-summary(to.ls)
    ls[[i]]&lt;-to.ls    #export
}

print(Sys.time()-strt)   #end time
# Time difference of 2.944168 secs

repeated the above code with an increasing number of iterations, 10 to 100 at intervals of 10.

iters&lt;-seq(10,100,by=10)   #iterations to time
times&lt;-numeric(length(iters))  #output time vector for  iteration sets
for(val in 1:length(iters)){   #loop over iteration sets
    cat(val,' of ', length(iters),'\n')
    to.iter&lt;-iters[val]
    ls&lt;-vector('list',length=to.iter)    #vector for appending output
    strt&lt;-Sys.time()    #start time
    for(i in 1:to.iter){    #same for loop as before
        cat(i,'\n')
        to.ls&lt;-rnorm(1e6)
        to.ls&lt;-summary(to.ls)
        ls[[i]]&lt;-to.ls          #export
    }
    times[val]&lt;-Sys.time()-strt    #end time
}

library(ggplot2)   #plot the times
to.plo&lt;-data.frame(iters,times)
ggplot(to.plo,aes(x=iters,y=times)) + 
    geom_point() +
    geom_smooth() + 
    theme_bw() + 
    scale_x_continuous('No. of loop iterations') + 
    scale_y_continuous ('Time in seconds')


<img src="https://beckmw.files.wordpress.com/2014/01/seq_time1.jpg">
Fig: Processing time as a function of number of iterations for a simple loop.

The processing time increases linearly with the number of iterations.  
Again, processing time is not extensive for the above example.  
Suppose we wanted to run the example with ten thousand iterations.  
We can predict how long that would take based on the linear relationship between time and iterations.

mod&lt;-lm(times~iters)    #predict times
predict(mod,newdata=data.frame(iters=1e4))/60
# 45.75964


This is all well and good if we want to wait around for 45 minutes.  
Running the loop in parallel would greatly decrease this time.  
I want to first illustrate the problem of running loops in sequence before I show how this can done using the foreach package.  
If the above code is run with <code>1e4</code> iterations, a quick look at the performance metrics in the task manager (Windows 7 OS) gives you an idea of how hard your computer is working to process the code.  
My machine has eight processors and you can see that only a fraction of them are working while the script is running.


<img src="https://beckmw.files.wordpress.com/2014/01/proc1.jpg">
Fig: Resources used during sequential processing of a <code>for</code> loop.

Running the code using foreach will make full use of the computer's processors.
Individual chunks of the loop are sent to each processor so that the entire process can be run in parallel rather than in sequence.  
Here's how to run the code with <code>1e4</code> iterations in parallel.
That is, each processor gets a finite set of the total number of iterations, i.e., iterations 1&#8211;100 goes to processor one, iterations 101&#8211;200 go to processor two, etc. The output from each processor is then comiled after the iterations are completed.  

#import packages
library(foreach)
library(doParallel)
iters&lt;-1e4   #number of iterations

#setup parallel backend to use 8 processors
cl&lt;-makeCluster(8)
registerDoParallel(cl)

#start time
strt&lt;-Sys.time()

#loop
ls&lt;-foreach(icount(iters)) %dopar% {
    to.ls&lt;-rnorm(1e6)
    to.ls&lt;-summary(to.ls)
    to.ls
    }

print(Sys.time()-strt)
stopCluster(cl)

#Time difference of 10.00242 mins

Running the loop in parallel decreased the processing time about four-fold.  
Although the loop generally looks the same as the sequential version, several parts of the code have changed.  
First, we are using the <code>foreach</code> function rather than <code>for</code> to define our loop.  
The syntax for specifying the iterator is slightly different with <code>foreach</code> as well, i.e., <code>icount(iters)</code> tells the function to repeat the loop a given number of times based on the value assigned to <code>iters</code>.  
Additionally, the convention <code>%dopar%</code> specifies that the code is to be processed in parallel if a backend has been registered (using <code>%do%</code> will run the loop sequentially).  
The functions <code>makeCluster</code> and <code>registerDoParallel</code> from the <a href="http://cran.r-project.org/web/packages/doParallel/index.html" title="doParallel">doParallel</a> package are used to create the parallel backend.  
Another important issue is the method for recombining the data after the chunks are processed. By default, <code>foreach</code> will append the output to a list which we've saved to an object.  
The default method for recombining output can be changed using the <code>.combine</code> argument.  
Also be aware that packages used in the evaluated expression must be included with the <code>.packages</code> argument.
The processors should be working at full capacity if the the loop is executed properly.  
Note the difference here compared to the first loop that was run in sequence.


<img src="https://beckmw.files.wordpress.com/2014/01/proc2.jpg">
Fig: Resources used during parallel processing of a <code>for</code> loop.

A few other issues are worth noting when using the foreach package.  
These are mainly issues I've encountered and I'm sure others could contribute to this list.  
The foreach package does not work with all types of loops.  
For example, I chose the above example to use a large number (<code>1e6</code>) of observations with the <code>rnorm</code> function.  
I can't say for certain the exact type of data that works best, but I have found that functions     hat take a long time when run individually are generally handled very well.  
Interestingly, decreasing the number of observations and increasing the number of iterations may cause the processors to not run at maximum efficiency (try <code>rnorm(100)</code> with <code>1e5</code> iterations). I also haven't had much success running repeated models in parallel.  
The functions work but the processors never seem to reach max efficiency.  
The system statistics should cue you off as to whether or not the functions are working.
I also find it bothersome that monitoring progress seems is an issue with parallel loops.  
A simple call using <code>cat</code> to return the iteration in the console does not work with parallel loops.  
The most practical solution I've found is described <a href="http://vikparuchuri.com/blog/monitoring-progress-inside-foreach-loop/" title="here">here</a>, which involves exporting information to a separate file that tells you how far the loop has progressed.  
Also, be very aware of your RAM when running processes in parallel.  
I've found that it's incredibly easy to max out the memory, which not only causes the function to stop working correctly, but also makes your computer run like garbage.  
Finally, I'm a little concerned that I might be destroying my processors by running them at maximum capacity.  
The fan always runs at full blast leading me to believe that critical meltdown is imminent.  
I'd be pleased to know if this is an issue or not.
That's it for now.  
I have to give credit to <a href="http://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf" title="this tutorial">this tutorial</a> for a lot of the information in this post.  
<h3>Vectorised</h3>
E = sapply(1:10000, function(n) {max.eig(5, 1)})
summary(E)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.7615  1.9150  2.2610  2.3160  2.6470  5.2800

Here eigenvalues are calculated from 10000 function calls, all of which use the same parameters. 
The distribution of the resulting eigenvalues is plotted in the histogram below. 
Generating these data took a couple of seconds on my middle-of-the-range laptop. 
Not a big wait. 
But it was only using one of the four cores on the machine, so in principle it could have gone faster.

We can make things more interesting by varying the dimensions of the matrix.
sapply(1:5, function(n) {max.eig(n, 1)})

Or changing both the dimensions (taking on integral values between 1 and 5) and the standard deviation (running through 1, 2 and 3).
sapply(1:5, function(n) {sapply(1:3, function(m) {max.eig(n, m)})})

The results are presented in an intuitive matrix. Everything up to this point is being done serially.
<h3>Enter foreach</h3>

library(foreach)

At first sight, the foreach library provides a slightly different interface for vectorisation. We’ll start off with simple repetition.
times(10) %do% max.eig(5, 1)

That just executes the function with the same arguments 10 times over. If we want to systematically vary the parameters, then instead of times() we use foreach().
foreach(n = 1:5) %do% max.eig(n, 1)

The results are returned as a list, which is actually more reminiscent of the behaviour of lapply() than sapply(). But we can get something more compact by using the .combine option.
foreach(n = 1:5, .combine = c) %do% max.eig(n, 1)


That’s better. Now, what about varying both the dimensions and standard deviation? We can string together multiple calls to foreach() using the %:% nesting operator.
foreach(n = 1:5) %:% foreach(m = 1:3) %do% max.eig(n, m)

I have omitted the output because it consists of nested lists: it’s long and somewhat ugly. But again we can use the .combine option to make it more compact.
foreach(n = 1:5, .combine = rbind) %:% foreach(m = 1:3) %do% max.eig(n, m)

foreach(n = 1:5, .combine = cbind) %:% foreach(m = 1:3) %do% max.eig(n, m)

You can choose between combining using cbind() or rbind() depending on whether you want the output from the inner loop to form the columns or rows of the output. 
There’s lots more magic to be done with .combine. 
You can find the details in the informative article <a href="http://r.adu.org.za/web/packages/foreach/vignettes/foreach.pdf" target="_blank">Using The foreach Package</a> by Steve Weston.

You can also use foreach() to loop over multiple variables simultaneously.
foreach(n = 1:5, m = 1:5) %do% max.eig(n, m)

But this is still all serial…

<h3>Filtering</h3>
One final capability before we move on to parallel execution, is the ability to add in a filter within the foreach() statement.

library(numbers)
foreach(n = 1:100, .combine = c) %:% when (isPrime(n)) %do% n

Here we identify the prime numbers between 1 and 100 by simply looping through the entire sequence of values and selecting only those that satisfy the condition in the when() clause. Of course, there are more efficient ways to do this, but this notation is rather neat.
<h3>Going Parallel</h3>
Making the transition from serial to parallel is as simple as changing %do% to %dopar%.
foreach(n = 1:5) %dopar% max.eig(n, 1)

Warning message:
executing %dopar% sequentially: no parallel backend registered

The warning gives us pause for thought: maybe it was not quite that simple? Yes, indeed, there are additional requirements. You need first to choose a parallel backend. And here, again, there are a few options. We will start with the most accessible, which is the multicore backend.
<h3>Multicore</h3>
Multicore processing is provided by the doMC library. You need to load the library and tell it how many cores you want to use.
library(doMC)
registerDoMC(cores=4)

Let’s make a comparison between serial and parallel execution times.

library(rbenchmark)

benchmark(
+     foreach(n = 1:50) %do% max.eig(n, 1),
+     foreach(n = 1:50) %dopar% max.eig(n, 1)
+ )

The overall execution time is reduced, but not by the factor of 4 that one might expect. 
This is due to the additional burden of having to distribute the job over the multiple cores. 
The tradeoff between communication and computation is one of the major limitations of parallel computing, but if computations are lengthy and there is not too much data to move around then the gains can be excellent.

On a single machine you are limited by the number of cores. But if you have access to a cluster then you can truly take things to another level.
<h3>Cluster</h3>
The foreach() functionality can be applied to a cluster using the doSNOW library. We will start by using doSNOW to create a collection of R instances on a single machine using a SOCK cluster.

library(doSNOW)
cluster = makeCluster(4, type = "SOCK")
registerDoSNOW(cluster)

benchmark(
+     foreach(n = 1:50) %do% max.eig(n, 1),
+     foreach(n = 1:50) %dopar% max.eig(n, 1)
+ )

stopCluster(cluster)

There is an improvement in execution time which is roughly comparable to what we got with the multicore implementation. Note that when you are done, you need to shut down the cluster.

Next we will create an <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface" target="_blank">MPI</a> cluster consisting of 20 threads.
cluster = makeCluster(20, type = "MPI")
#
registerDoSNOW(cluster)
#
benchmark(
+     foreach(n = 1:100) %do% max.eig(n, 1),
+     foreach(n = 1:100) %dopar% max.eig(n, 1)
+ )


There is an improvement in performance, with the parallel job running roughly 3 times as quickly.

How about a slightly more complicated example? We will try running some bootstrap calculations. We start out with the serial implementation.
random.data = matrix(rnorm(1000000), ncol = 1000)

bmed = function(d, n) median(d[n])

library(boot)
#
sapply(1:100, function(n) {sd(boot(random.data[, n], bmed, R = 10000)$t)})

First we generated a big array of normally distributed random numbers. Then we used sapply to calculate bootstrap estimates for the standard deviation of the median for each columns of the matrix.

The parallel implementation requires a little more work: first we need to make the global data (the random matrix and the bootstrap function) available across the cluster.
clusterExport(cluster, c("random.data", "bmed"))

Then we spread the jobs out over the cluster nodes. We will do this first using clusterApply(), which is part of the snow library and is the cluster analogue of sapply(). It returns a list, so to get a nice compact representation we use unlist().
results = clusterApply(cluster, 1:100, function(n) {
+     library(boot)
+     sd(boot(random.data[, n], bmed, R = 10000)$t)
+ })
head(unlist(results))


The foreach implementation is a little neater.
results = foreach(n = 1:100, .combine = c) %dopar% {
    library(boot); sd(boot(random.data[, n], bmed, R = 10000)$t)
}
head(results)

stopCluster(cluster)

The key in both cases is that the boot library must be loaded on each of the cluster nodes as well so that its functionality is available. Simply loading the library on the root node is not enough!

<h2>repeating timer by r asynchronously</h2>
The future package:

library("future")
plan(multiprocess)

myfun = function() {
  future(fun2())

  return(1+1)
}
Unless fun2() is function used purely for its side effects, you typically want to retrieve the value of that future expression, which you do as:

f = future(fun2())
y = fun3()
v = value(f)
z = v + y
An alternative is to use the %=% operator as in:

v %=% fun2()
y = fun3()
z = v + y
FYI, if you use

plan(cluster, workers = c("n1", "n3", "remote.server.org"))
then the future expression is resolved on one of those machines. Using

plan(future.BatchJobs::batchjobs_slurm)
will cause it to be resolved via a Slurm job scheduler queue.

<a href="https://rstudio.github.io/promises/articles/futures.html" class="whitebut ">Launching tasks with future</a>
<a href="https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html" class="whitebut ">A Future for R</a>
<a href="https://appsilon.com/an-example-of-how-to-use-the-new-r-promises-package/" class="whitebut ">example of new R promises package</a>

<h2>Set a timer in R to execute a program</h2>
executing same code block every 15 seconds:

interval = 15
x = data.frame()

repeat {
  startTime = Sys.time()
  x = rbind.data.frame(x, sum(data)) #replace this line with your code/functions
  sleepTime = startTime + interval - Sys.time()
  if (sleepTime > 0)
    Sys.sleep(sleepTime)
}

Or:
print_test=function(x){
    if(condition)
    {
        Sys.sleep(x);
        cat("hello world");
        print_test(x);
    }
}
print_test(15)

<h2>What Is a Formula in R?</h2>
Formula allow you to capture two things:

An unevaluated expression
The context or environment in which the expression was created

In R the tilde operator ~ characterizes formulas With this operator, you say: "capture the meaning of this code, without evaluating it" You can think of a formula in R as a "quoting" operator

<code># A formula
d &lt;- y ~ x + b</code>

The variable on the left-hand side of a tilde (~) is called the "dependent variable", while the variables on the right-hand side are called the "independent variables" and are joined by plus signs +.

You can access the elements of a formula with the help of the square brackets: [[and ]].

<code>f &lt;- y ~ x + b 
# Retrieve the elements at index 1 and 2
f[[1]] ## "~"
f[[2]] ## y
f[[3]] ## x + b</code>

<h3>Why Use Formulae in R?</h3>
Formulas are powerful, general-purpose tools that allow you to capture the values of variables without evaluating them so that they can be interpreted by the function

Also, you use these R objects to express a relationship between variables.

For example, in the first line of code in the code chunk below, you say "y is a function of x, a, and b"

<code>y ~ x + a + b
## y ~ x + a + b</code>

More complex formulas like the code chunk below:

<code>Sepal.Width ~ Petal.Width | Species
## Sepal.Width ~ Petal.Width | Species</code>

Where you mean to say "the sepal width is a function of petal width, conditioned on species"

<h3>Using Formulas in R</h3>

<h2>How To Create a Formula in R</h2>
1.With the help of ~ operator 2.Some times you need or want to create a formula from an R object, such as a string. In such cases, you can use the formula or as.formula() function

<code>"y ~ x1 + x2"
## [1] "y ~ x1 + x2"
h &lt;- as.formula("y ~ x1 + x2")
h &lt;- formula("y ~ x1 + x2")</code>

<h2>How To Concatenate Formulae</h2>
To glue or bring multiple formulas together, you have two option:

Create separate variables for each formula and then use list()

<code># Create variables
i &lt;- y ~ x
j &lt;- y ~ x + x1
k &lt;- y ~ x + x1 + x2

# Concatentate
formulae &lt;- list(as.formula(i),as.formula(j),as.formula(k))</code>

Use the lapply() function, where you pass in a vector with all of your formulas as a first argument and as.formula as the function that you want to apply to each element of that vector

<code># Join all with "c()"
l &lt;- c(i, j, k)

# Apply "as.formula" to all elements of "f"
lapply(l, as.formula)
[[1]] ## y ~ x
[[2]] ## y ~ x + x1
[[3]] ## y ~ x + x1 + x2</code>

<h3>Formula Operators</h3>
"+" for joining
"-" for removing terms
":" for interaction
"*" for crossing
"%in%" for nesting
"^" for limit crossing to the specified degree

<code># Use multiple independent variables
y ~ x1 + x2 ## y ~ x1 + x2
# Ignore objects in an analysis
y ~ x1 - x2 ## y ~ x1 - x2</code>

What if you want to actually perform an arithmetic operation? you have a couple of solutions:

1.You can calculate and store all of the variables in advance 2.You use the I() or "as-is" operator: y ~ x + I(x^2)

<h3>How To Inspect Formulas in R</h3>
You saw functions such as attributes(), typeof(), class(), etc

To examine and compare different formulae, you can use the terms() function:

<code>m &lt;- formula("y ~ x1 + x2")
terms(m)
## y ~ x1 + x2
## attr(,"variables")
## list(y, x1, x2)
## attr(,"factors")
##    x1 x2
## y   0  0
## x1  1  0
## x2  0  1
## attr(,"term.labels")
## [1] "x1" "x2"
## attr(,"order")
## [1] 1 1
## attr(,"intercept")
## [1] 1
## attr(,"response")
## [1] 1
## attr(,".Environment")
## &lt;environment: R_GlobalEnv&gt;
class(m)
## [1] "formula"
typeof(m)
## [1] "language"
attributes(m)
## $class
## [1] "formula"
## 
## $.Environment
## &lt;environment: R_GlobalEnv&gt;</code>

If you want to know the names of the variables in the model, you can use all.vars.

<code>print(all.vars(m))
## [1] "y"  "x1" "x2"</code>

To modify formulae without converting them to character you can use the update() function:

<code>update(y ~ x1 + x2, ~. + x3)
## y ~ x1 + x2 + x3
y ~ x1 + x2 + x3
## y ~ x1 + x2 + x3</code>

Double check whether you variable is a formula by passing it to the is.formula() function.

<code># Load "plyr"
library(plyr)

# Check "m"
is.formula(m)
## [1] TRUE</code>

<h3>When To Use Formulas</h3>
1.Modeling Functions
2.Graphical Functions in R

<h3>R Formula Packages</h3>
1.Formula Package
2.formula.tools

<h2>dplyr samples</h2>
<a href="https://github.com/Apress/r-data-science-quick-reference" class="whitebut ">R data science quick reference</a>

library(tidyverse)
iris_df = as_tibble(iris)
print(iris_df, n = 3)
head(iris_df$Species)

## ============
iris_df %>% select(Sepal.Length, Species) %>% print(n = 3)
iris_df %>% select(-Species) %>% print(n = 3)
iris_df %>% select(-Species, -Sepal.Length) %>% print(n = 3)

<h2>get rid of all non-ASCII characters.</h2>

Texts = c("Let the stormy clouds chase, everyone from the place ☁  ♪ ♬",
    "See you soon brother ☮ ",
    "A boring old-fashioned message" ) 

gsub("[^\x01-\x7F]", "", Texts)
[1] "Let the stormy clouds chase, everyone from the place    "
[2] "See you soon brother  "                                  
[3] "A boring old-fashioned message"

Details: You can specify character classes in regex's with [ ]. 
When the class description starts with ^ it means everything except these characters. 
Here, I have specified everything except characters 1-127, i.e. 
everything except standard ASCII and I have specified that they should be replaced with the empty string.

<h2>Display a popup from a batch file</h2>
The goal is to display a popup, the calling batch file must stop and wait the popup closing.

<h3>Using powershell</h3>
echo calling popup
powershell [Reflection.Assembly]::LoadWithPartialName("""System.Windows.Forms""");[Windows.Forms.MessageBox]::show("""rgagnon.com""", """HowTo""",0)>nul
echo we are back!

<h3>Using MHTA</h3>
echo calling popup
mshta javascript:alert("rgagnon.com\n\nHowTo!");close();
echo we are back!
Regular CMD
echo calling popup
START /WAIT CMD /C "ECHO rgagnon.com && ECHO HowTo && ECHO. && PAUSE"
echo we are back!

<h3>Using JScript</h3>
 @if (@x)==(@y) @end /***** jscript comment ******
     @echo off
     echo calling popup
     cscript //E:JScript //nologo "%~f0" "%~nx0" %*
     echo we are back!
     exit /b 0
 @if (@x)==(@y) @end ******  end comment *********/

var wshShell = WScript.CreateObject("WScript.Shell");
wshShell.Popup("HowTo", -1, "rgagnon.com", 16);

<h2>select after the nth word form a string</h2>
library(stringr)
a="starting anything goes from here now"
res = gsub("^(\\w+\\s){1}","",a)
res
res = gsub("^(\\w+\\s){3}","",a)
res
res = gsub("^(\\w+\\s){4}","",a)
res

<h2>Significant network analysis packages</h2>
<a href="https://www.jessesadler.com/post/network-analysis-with-r/" class="redbut goldbs orangets">Introduction to Network Analysis with R</a>

<a href="http://statnet.org/" class="whitebut ">statnet</a>
<a href="https://igraph.org/" class="whitebut ">igraph</a>
<a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="whitebut ">tidygraph</a>
<a href="https://www.data-imaginist.com/2017/ggraph-introduction-layouts/" class="whitebut ">ggraph</a>
<a href="http://www.viznetwork.com/" class="whitebut ">vizNetwork</a>
<a href="https://christophergandrud.github.io/networkD3/" class="whitebut ">networkD3</a>

<h3>Basic Managerial Applications of Network Analysis</h3>
Plans the projects by analyzing the project activities.

Projects are broken down to individual tasks or activities, which are arranged in logical sequence. 

It is also decided that which tasks will be performed simultaneously and which other sequentially.

A network diagram is prepared, which presents visually the relationship between all the activities involved and the cost for different activities. 

Network analysis helps designing, planning, coordi­nating, controlling and in decision-making in order to accom­plish the project economically in the minimum available time with the limited available resources. 

The network analysis fulfills the objectives of reducing total time, cost, idle resources, interruptions and conflicts. 

Managerial applications of network analysis are as follows:
Assembly line scheduling,
Research and development,
Inventory planning and control,
Shifting of manufacturing plant from one site to another,
Launching of new products and advertising campaigns,
Control of traffic flow in cities,
Budget and audit procedures,
Launching space programmes,
Installation of new equipments,
Long-range planning and developing staffing plans, etc.

Network techniques:
A number of network techniques:
PERT- Programme Evaluation and Review Technique
CPM- Critical Path Method
RAMS- Resource Allocation and Multi-project Scheduling
PEP- Programme Evolution Procedure
COPAC- Critical Operating Production Allocation Control
MAP- Manpower Allocation Procedure
RPSM- Resource Planning and Scheduling Method
LCS- Least Cost Scheduling
MOSS- Multi-Operation Scheduling System
PCS- Project Control System
GERT- Graphical Evaluation Review Technique.

<h2>shows exactly two decimal places for the number</h2>
format(round(x, 2), nsmall = 2)

<h2>R Customizing Startup</h2>
<a href="https://github.com/rstudio/rstudio/issues/5454" class="whitebut ">Tab for spaces setting</a>
<a href="https://github.com/rstudio/rstudio/issues/4448" class="whitebut ">Spaces per tab option</a>
<a href="https://www.statmethods.net/interface/customizing.html" class="whitebut ">R Customizing Startup</a>

<h2>R courses</h2>
<a href="https://www.datacamp.com/courses/free-introduction-to-r">free-introduction-to-r</a>
<a href="https://www.datacamp.com/courses/intermediate-r">intermediate-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-the-tidyverse">introduction-to-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/data-manipulation-with-dplyr">data-manipulation-with-dplyr</a>
<a href="https://www.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2">introduction-to-data-visualization-with-ggplot2</a>
<a href="https://www.datacamp.com/courses/introduction-to-importing-data-in-r">introduction-to-importing-data-in-r</a>
<a href="https://www.datacamp.com/courses/cleaning-data-in-r">cleaning-data-in-r</a>
<a href="https://www.datacamp.com/courses/joining-data-with-dplyr">joining-data-with-dplyr</a>
<a href="https://www.datacamp.com/courses/intermediate-data-visualization-with-ggplot2">intermediate-data-visualization-with-ggplot2</a>
<a href="https://www.datacamp.com/courses/exploratory-data-analysis-in-r">exploratory-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/correlation-and-regression-in-r">correlation-and-regression-in-r</a>
<a href="https://www.datacamp.com/courses/supervised-learning-in-r-classification">supervised-learning-in-r-classification</a>
<a href="https://www.datacamp.com/courses/introduction-to-data-in-r">introduction-to-data-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-regression-in-r">introduction-to-regression-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-statistics-in-r">introduction-to-statistics-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-writing-functions-in-r">introduction-to-writing-functions-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-r-for-finance">introduction-to-r-for-finance</a>
<a href="https://www.datacamp.com/courses/case-study-exploratory-data-analysis-in-r">case-study-exploratory-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-importing-data-in-r">intermediate-importing-data-in-r</a>
<a href="https://www.datacamp.com/courses/multiple-and-logistic-regression-in-r">multiple-and-logistic-regression-in-r</a>
<a href="https://www.datacamp.com/courses/building-web-applications-with-shiny-in-r">building-web-applications-with-shiny-in-r</a>
<a href="https://www.datacamp.com/courses/data-visualization-in-r">data-visualization-in-r</a>
<a href="https://www.datacamp.com/courses/supervised-learning-in-r-regression">supervised-learning-in-r-regression</a>
<a href="https://www.datacamp.com/courses/writing-efficient-r-code">writing-efficient-r-code</a>
<a href="https://www.datacamp.com/courses/working-with-dates-and-times-in-r">working-with-dates-and-times-in-r</a>
<a href="https://www.datacamp.com/courses/unsupervised-learning-in-r">unsupervised-learning-in-r</a>
<a href="https://www.datacamp.com/courses/machine-learning-with-caret-in-r">machine-learning-with-caret-in-r</a>
<a href="https://www.datacamp.com/courses/time-series-analysis-in-r">time-series-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/manipulating-time-series-data-with-xts-and-zoo-in-r">manipulating-time-series-data-with-xts-and-zoo-in-r</a>
<a href="https://www.datacamp.com/courses/cluster-analysis-in-r">cluster-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/data-manipulation-with-datatable-in-r">data-manipulation-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/reporting-with-rmarkdown">reporting-with-rmarkdown</a>
<a href="https://www.datacamp.com/courses/working-with-data-in-the-tidyverse">working-with-data-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/forecasting-in-r">forecasting-in-r</a>
<a href="https://www.datacamp.com/courses/string-manipulation-with-stringr-in-r">string-manipulation-with-stringr-in-r</a>
<a href="https://www.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r">fundamentals-of-bayesian-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-r-for-finance">intermediate-r-for-finance</a>
<a href="https://www.datacamp.com/courses/introduction-to-portfolio-analysis-in-r">introduction-to-portfolio-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-probability-in-r">foundations-of-probability-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-inference-in-r">foundations-of-inference-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-text-analysis-in-r">introduction-to-text-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/joining-data-with-datatable-in-r">joining-data-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/credit-risk-modeling-in-r">credit-risk-modeling-in-r</a>
<a href="https://www.datacamp.com/courses/modeling-with-data-in-the-tidyverse">modeling-with-data-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/working-with-web-data-in-r">working-with-web-data-in-r</a>
<a href="https://www.datacamp.com/courses/parallel-programming-in-r">parallel-programming-in-r</a>
<a href="https://www.datacamp.com/courses/visualizing-geospatial-data-in-r">visualizing-geospatial-data-in-r</a>
<a href="https://www.datacamp.com/courses/importing-and-managing-financial-data-in-r">importing-and-managing-financial-data-in-r</a>
<a href="https://www.datacamp.com/courses/linear-algebra-for-data-science-in-r">linear-algebra-for-data-science-in-r</a>
<a href="https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models-in-r">hierarchical-and-mixed-effects-models-in-r</a>
<a href="https://www.datacamp.com/courses/case-study-exploring-baseball-pitching-data-in-r">case-study-exploring-baseball-pitching-data-in-r</a>
<a href="https://www.datacamp.com/courses/case-studies-manipulating-time-series-data-in-r">case-studies-manipulating-time-series-data-in-r</a>
<a href="https://www.datacamp.com/courses/differential-expression-analysis-with-limma-in-r">differential-expression-analysis-with-limma-in-r</a>
<a href="https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r">analyzing-election-and-polling-data-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-tensorflow-in-r">introduction-to-tensorflow-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-statistical-modeling-in-r">intermediate-statistical-modeling-in-r</a>
<a href="https://www.datacamp.com/courses/structural-equation-modeling-with-lavaan-in-r">structural-equation-modeling-with-lavaan-in-r</a>
<a href="https://www.datacamp.com/courses/bond-valuation-and-analysis-in-r">bond-valuation-and-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/garch-models-in-r">garch-models-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-functional-programming-with-purrr">foundations-of-functional-programming-with-purrr</a>
<a href="https://www.datacamp.com/courses/anomaly-detection-in-r">anomaly-detection-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-interactive-data-visualization-with-plotly-in-r">intermediate-interactive-data-visualization-with-plotly-in-r</a>
<a href="https://www.datacamp.com/courses/network-analysis-in-the-tidyverse">network-analysis-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/market-basket-analysis-in-r">market-basket-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/financial-analytics-in-r">financial-analytics-in-r</a>
<a href="https://www.datacamp.com/courses/visualizing-big-data-with-trelliscope-in-r">visualizing-big-data-with-trelliscope-in-r</a>
<a href="https://www.datacamp.com/courses/choice-modeling-for-marketing-in-r">choice-modeling-for-marketing-in-r</a>
<a href="https://www.datacamp.com/courses/handling-missing-data-with-imputations-in-r">handling-missing-data-with-imputations-in-r</a>
<a href="https://www.datacamp.com/courses/forecasting-product-demand-in-r">forecasting-product-demand-in-r</a>
<a href="https://www.datacamp.com/courses/defensive-r-programming">defensive-r-programming</a>
<a href="https://www.datacamp.com/courses/intermediate-functional-programming-with-purrr">intermediate-functional-programming-with-purrr</a>
<a href="https://www.datacamp.com/courses/analyzing-us-census-data-in-r">analyzing-us-census-data-in-r</a>
<a href="https://www.datacamp.com/courses/life-insurance-products-valuation-in-r">life-insurance-products-valuation-in-r</a>
<a href="https://www.datacamp.com/courses/mixture-models-in-r">mixture-models-in-r</a>
<a href="https://www.datacamp.com/courses/data-visualization-with-lattice-in-r">data-visualization-with-lattice-in-r</a>
<a href="https://www.datacamp.com/courses/fraud-detection-in-r">fraud-detection-in-r</a>
<a href="https://www.datacamp.com/courses/designing-and-analyzing-clinical-trials-in-r">designing-and-analyzing-clinical-trials-in-r</a>
<a href="https://www.datacamp.com/courses/chip-seq-with-bioconductor-in-r">chip-seq-with-bioconductor-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-regular-expressions-in-r">intermediate-regular-expressions-in-r</a>
<a href="https://www.datacamp.com/courses/survey-and-measurement-development-in-r">survey-and-measurement-development-in-r</a>
<a href="https://www.datacamp.com/courses/feature-engineering-in-r">feature-engineering-in-r</a>
<a href="https://www.datacamp.com/courses/human-resources-analytics-exploring-employee-data-in-r">human-resources-analytics-exploring-employee-data-in-r</a>
<a href="https://www.datacamp.com/courses/scalable-data-processing-in-r">scalable-data-processing-in-r</a>
<a href="https://www.datacamp.com/courses/practicing-statistics-interview-questions-in-r">practicing-statistics-interview-questions-in-r</a>
<a href="https://www.datacamp.com/courses/practicing-machine-learning-interview-questions-in-r">practicing-machine-learning-interview-questions-in-r</a>
<a href="https://www.datacamp.com/courses/human-resources-analytics-predicting-employee-churn-in-r">human-resources-analytics-predicting-employee-churn-in-r</a>
<a href="https://www.datacamp.com/courses/optimizing-r-code-with-rcpp">optimizing-r-code-with-rcpp</a>
<a href="https://www.datacamp.com/courses/predictive-analytics-using-networked-data-in-r">predictive-analytics-using-networked-data-in-r</a>
<a href="https://www.datacamp.com/courses/case-studies-network-analysis-in-r">case-studies-network-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/building-response-models-in-r">building-response-models-in-r</a>
<a href="https://www.datacamp.com/courses/business-process-analytics-in-r">business-process-analytics-in-r</a>
<a href="https://www.datacamp.com/courses/interactive-data-visualization-with-rbokeh">interactive-data-visualization-with-rbokeh</a>
<a href="https://www.datacamp.com/courses/r-for-sas-users">r-for-sas-users</a>
<a href="https://www.datacamp.com/courses/probability-puzzles-in-r">probability-puzzles-in-r</a>
<a href="https://www.datacamp.com/courses/time-series-with-datatable-in-r">time-series-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/data-privacy-and-anonymization-in-r">data-privacy-and-anonymization-in-r</a>
<a href="https://www.datacamp.com/courses/course-creation-at-datacamp">course-creation-at-datacamp</a>
<a href="https://www.datacamp.com/projects/796">projects/796</a>
<a href="https://www.datacamp.com/projects/78">projects/78</a>
<a href="https://www.datacamp.com/projects/758">projects/758</a>
<a href="https://www.datacamp.com/projects/74">projects/74</a>
<a href="https://www.datacamp.com/projects/738">projects/738</a>
<a href="https://www.datacamp.com/projects/712">projects/712</a>
<a href="https://www.datacamp.com/projects/697">projects/697</a>
<a href="https://www.datacamp.com/projects/691">projects/691</a>
<a href="https://www.datacamp.com/projects/68">projects/68</a>
<a href="https://www.datacamp.com/projects/677">projects/677</a>
<a href="https://www.datacamp.com/projects/673">projects/673</a>
<a href="https://www.datacamp.com/projects/668">projects/668</a>
<a href="https://www.datacamp.com/projects/664">projects/664</a>
<a href="https://www.datacamp.com/projects/643">projects/643</a>
<a href="https://www.datacamp.com/projects/638">projects/638</a>
<a href="https://www.datacamp.com/projects/62">projects/62</a>
<a href="https://www.datacamp.com/projects/614">projects/614</a>
<a href="https://www.datacamp.com/projects/584">projects/584</a>
<a href="https://www.datacamp.com/projects/567">projects/567</a>
<a href="https://www.datacamp.com/projects/561">projects/561</a>
<a href="https://www.datacamp.com/projects/552">projects/552</a>
<a href="https://www.datacamp.com/projects/547">projects/547</a>
<a href="https://www.datacamp.com/projects/515">projects/515</a>
<a href="https://www.datacamp.com/projects/511">projects/511</a>
<a href="https://www.datacamp.com/projects/496">projects/496</a>
<a href="https://www.datacamp.com/projects/49">projects/49</a>
<a href="https://www.datacamp.com/projects/489">projects/489</a>
<a href="https://www.datacamp.com/projects/478">projects/478</a>
<a href="https://www.datacamp.com/projects/464">projects/464</a>
<a href="https://www.datacamp.com/projects/458">projects/458</a>
<a href="https://www.datacamp.com/projects/445">projects/445</a>
<a href="https://www.datacamp.com/projects/438">projects/438</a>
<a href="https://www.datacamp.com/projects/435">projects/435</a>
<a href="https://www.datacamp.com/projects/41">projects/41</a>
<a href="https://www.datacamp.com/projects/309">projects/309</a>
<a href="https://www.datacamp.com/projects/208">projects/208</a>
<a href="https://www.datacamp.com/projects/182">projects/182</a>
<a href="https://www.datacamp.com/projects/177">projects/177</a>
<a href="https://www.datacamp.com/projects/166">projects/166</a>
<a href="https://www.datacamp.com/projects/139">projects/139</a>

<h2>Customizing Startup the R environment</h2>
R will always source the Rprofile.site file first. 
On Windows, the file is in the C:\Program Files\R\R-n.n.n\etc directory. 

You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory.

At startup, R will source the Rprofile.site file. 
It will then look for a .Rprofile file to source in the current working directory. 
If it doesn't find it, it will look for one in the user's home directory. 

There are two special functions you can place in these files. 
.First( ) will be run at the start of the R session and .Last( ) will be run at the end of the session.

# Sample Rprofile.site file

# Things you might want to change
# options(papersize="a4")
# options(editor="notepad")
# options(pager="internal")

# R interactive prompt
# options(prompt="> ")
# options(continue="+ ")

# to prefer Compiled HTML
help options(chmhelp=TRUE)
# to prefer HTML help
# options(htmlhelp=TRUE)

# General options
options(tab.width = 2)
options(width = 130)
options(graphics.record=TRUE)

.First = function(){
 library(Hmisc)
 library(R2HTML)
 cat("\nWelcome at", date(), "\n")
}

.Last = function(){
 cat("\nGoodbye at ", date(), "\n")
}

<h2>Managing R</h2>
with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf
Upon startup, R and RStudio look for a few different files you can use to control the behavior of your R session, for example by setting options or environment variables. 
In the context of RStudio Team, these settings are often used to set RStudio Server Pro to search for packages in an RStudio Package Manager repository.

This article is a practical guide to how to set particular options on R startup. 
General information on how to manage R package environments is available at <a href="https://environments.rstudio.com" target="_blank" rel="noopener">environments.rstudio.com</a> , and a deeper treatment of R process startup is available in <a href="https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/" target="_blank" rel="noopener">this article</a>.&nbsp;

Here is a summary table of how to control R options and environment variables on startup. 
More details are below.

<table border="black">
<tbody>
<tr><td>File</td><td>Who Controls</td><td>Level</td><td>Limitations</td></tr>
<tr><td><code>.Rprofile</code></td><td>User or Admin</td><td>User or Project</td><td>None, sourced as R code.</td></tr>
<tr><td><code>.Renviron</code></td><td>User or Admin</td><td>User or Project</td><td>Set environment variables only.</td></tr>
<tr><td><code>Rprofile.site</code></td><td>Admin</td><td>Version of R</td><td>None, sourced as R code.</td></tr>
<tr><td><code>Renviron.site</code></td><td>Admin</td><td>Version of R</td><td>Set environment variables only.</td></tr>
<tr><td><code>rsession.conf</code></td><td>Admin</td><td>Server</td><td>Only RStudio settings, only single repository.</td></tr>
<tr><td><code>repos.conf</code></td><td>Admin</td><td>Server</td><td>Only for setting repositories.</td></tr>
</tbody>
</table>
<h3><code>.Rprofile</code></h3>

<code>.Rprofile</code> files are user-controllable files to set options and environment variables. 
<code>.Rprofile</code> files can be either at the user or project level. 
User-level <code>.Rprofile</code> files live in the base of the user's home directory, and project-level <code>.Rprofile</code> files live in the base of the project directory.&nbsp;

R will source only one <code>.Rprofile</code> file. 
So if you have both a project-specific <code>.Rprofile</code> file and a user <code>.Rprofile</code> file that you want to use, you explicitly source the user-level <code>.Rprofile</code> at the top of your project-level <code>.Rprofile</code> with <code>source("~/.Rprofile")</code>.

<code>.Rprofile</code> files are sourced as regular R code, so setting environment variables must be done inside a <code>Sys.setenv(key = "value")</code> call.&nbsp;

One easy way to edit your <code>.RProfile</code> file is to use the <code>usethis::edit_r_profile()</code> function from within an R session. 
You can specify whether you want to edit the user or project level <code>.Rprofile.</code>

<h3><code>.Renviron</code></h3>

<code>.Renviron</code> is a user-controllable file that can be used to create environment variables. 
This is especially useful to avoid including credentials like API keys inside R scripts. 
This file is written in a key-value format, so environment variables are created in the format:

Key1=value1
Key2=value2
...

And then <code>Sys.getenv("Key1")</code> will return <code>"value1"</code> in an R session.

Like with the <code>.Rprofile</code> file, <code>.Renviron</code> files can be at either the user or project level. 
If there is a project-level <code>.Renviron</code>, the user-level file will not be sourced. 
The <code>usethis</code> package includes a helper function for editing <code>.Renviron</code> files from an R session with <code>usethis::edit_r_environ()</code>.

<h3><code>Rprofile.site</code> and <code>Renviron.site</code></h3>

Both <code>.Rprofile</code> and <code>.Renviron</code> files have equivalents that apply server wide. 
<code>Rprofile.site</code>&nbsp;and<code>Renviron.site</code> (no leading dot) files are managed by admins on RStudio Server and are specific to a particular version of R.&nbsp;The most common settings for these&nbsp;files involve access to package repositories. 
For example, using the <a href="https://environments.rstudio.com/shared.html" target="_blank" rel="noopener">shared-baseline</a> package management strategy is generally done from an <code>Rprofile.site</code>.

Users can override settings in these files&nbsp;with their individual <code>.Rprofile</code>&nbsp;files.

These files are set for each version of R and should be located in <code>R_HOME/etc/</code>. 
You can find<code>R_HOME</code> by running the command&nbsp;<code>R.home(component
  = "home")</code> in a session of that version of R. 
So, for example, if you find that <code>R_HOME</code> is <code>/opt/R/3.6.2/lib/R</code>, the<code>Rprofile.site</code> for R 3.6.2 would go in <code>/opt/R/3.6.2/lib/R/etc/Rprofile.site</code>.

<h3><code>rsession.conf</code> and <code>repos.conf</code></h3>

RStudio Server allows server admins to configure particular server-wide R package repositories via the <code>rsession.conf</code> and <code>repos.conf</code> files. 
Only one repository can be configured in <code>rsession.conf</code>. 
If multiple repositories are needed, <code>repos.conf</code> should be used. 
Details on configuring RStudio Server with these files are in this <a href="https://support.rstudio.com/hc/en-us/articles/360009863114-Configuring-RStudio-Server-to-use-RStudio-Package-Manager" target="_blank" rel="noopener">support article</a>.

<h2>R startup mechanism is as follows</h2>
Unless --no-environ was given on the command line, R searches for site and user files to process for setting environment variables. 
The name of the site file is the one pointed to by the environment variable R_ENVIRON; if this is unset, ‘R_HOME/etc/Renviron.site’ is used (if it exists, which it does not in a ‘factory-fresh’ installation). 
The name of the user file can be specified by the R_ENVIRON_USER environment variable; if this is unset, the files searched for are ‘.Renviron’ in the current or in the user's home directory (in that order). 
See ‘Details’ for how the files are read.

Then R searches for the site-wide startup profile file of R code unless the command line option --no-site-file was given. 
The path of this file is taken from the value of the R_PROFILE environment variable (after tilde expansion). 
If this variable is unset, the default is ‘R_HOME/etc/Rprofile.site’, which is used if it exists (it contains settings from the installer in a ‘factory-fresh’ installation). 
This code is sourced into the base package. 
Users need to be careful not to unintentionally overwrite objects in base, and it is normally advisable to use local if code needs to be executed: see the examples.

Then, unless --no-init-file was given, R searches for a user profile, a file of R code. 
The path of this file can be specified by the R_PROFILE_USER environment variable (and tilde expansion will be performed). 
If this is unset, a file called ‘.Rprofile’ is searched for in the current directory or in the user's home directory (in that order). 
The user profile file is sourced into the workspace.

Note that when the site and user profile files are sourced only the base package is loaded, so objects in other packages need to be referred to by e.g. 
utils::dump.frames or after explicitly loading the package concerned.

R then loads a saved image of the user workspace from ‘.RData’ in the current directory if there is one (unless --no-restore-data or --no-restore was specified on the command line).

Next, if a function .First is found on the search path, it is executed as .First(). 
Finally, function .First.sys() in the base package is run. 
This calls require to attach the default packages specified by options("defaultPackages"). 
If the methods package is included, this will have been attached earlier (by function .OptRequireMethods()) so that namespace initializations such as those from the user workspace will proceed correctly.

A function .First (and .Last) can be defined in appropriate ‘.Rprofile’ or ‘Rprofile.site’ files or have been saved in ‘.RData’. 
If you want a different set of packages than the default ones when you start, insert a call to options in the ‘.Rprofile’ or ‘Rprofile.site’ file. 
For example, options(defaultPackages = character()) will attach no extra packages on startup (only the base package) (or set R_DEFAULT_PACKAGES=NULL as an environment variable before running R). 
Using options(defaultPackages = "") or R_DEFAULT_PACKAGES="" enforces the R system default.

On front-ends which support it, the commands history is read from the file specified by the environment variable R_HISTFILE (default ‘.Rhistory’ in the current directory) unless --no-restore-history or --no-restore was specified.

The command-line option --vanilla implies --no-site-file, --no-init-file, --no-environ and (except for R CMD) --no-restore Under Windows, it also implies --no-Rconsole, which prevents loading the ‘Rconsole’ file.

Details
Note that there are two sorts of files used in startup: environment files which contain lists of environment variables to be set, and profile files which contain R code.

Lines in a site or user environment file should be either comment lines starting with #, or lines of the form name=value. 
The latter sets the environmental variable name to value, overriding an existing value. 
If value contains an expression of the form ${foo-bar}, the value is that of the environmental variable foo if that exists and is set to a non-empty value, otherwise bar. 
(If it is of the form ${foo}, the default is "".) This construction can be nested, so bar can be of the same form (as in ${foo-${bar-blah}}). 
Note that the braces are essential: for example $HOME will not be interpreted.

Leading and trailing white space in value are stripped. 
value is then processed in a similar way to a Unix shell: in particular the outermost level of (single or double) quotes is stripped, and backslashes are removed except inside quotes.

On systems with sub-architectures (mainly Windows), the files ‘Renviron.site’ and ‘Rprofile.site’ are looked for first in architecture-specific directories, e.g. 
‘R_HOME/etc/i386/Renviron.site’. 
And e.g. 
‘.Renviron.i386’ will be used in preference to ‘.Renviron’.

Note
It is not intended that there be interaction with the user during startup code. 
Attempting to do so can crash the R process.

The startup options are for Rgui, Rterm and R but not for Rcmd: attempting to use e.g. 
--vanilla with the latter will give a warning or error.

Unix versions of R have a file ‘R_HOME/etc/Renviron’ which is read very early in the start-up processing. 
It contains environment variables set by R in the configure process, and is not used on R for Windows.

R CMD check and R CMD build do not always read the standard startup files, but they do always read specific Renviron files. 
The location of these can be controlled by the environment variables R_CHECK_ENVIRON and R_BUILD_ENVIRON. 
If these are set their value is used as the path for the Renviron file; otherwise, files ‘~/.R/check.Renviron’ or ‘~/.R/build.Renviron’ or sub-architecture-specific versions are employed.

If you want ‘~/.Renviron’ or ‘~/.Rprofile’ to be ignored by child R processes (such as those run by R CMD check and R CMD build), set the appropriate environment variable R_ENVIRON_USER or R_PROFILE_USER to (if possible, which it is not on Windows) "" or to the name of a non-existent file.

See Also
For the definition of the ‘home’ directory on Windows see the ‘rw-FAQ’ Q2.14. 
It can be found from a running R by Sys.getenv("R_USER").

.Last for final actions at the close of an R session. 
commandArgs for accessing the command line arguments.

There are examples of using startup files to set defaults for graphics devices in the help for windows.options.

An Introduction to R for more command-line options: those affecting memory management are covered in the help file for Memory.

readRenviron to read ‘.Renviron’ files.

For profiling code, see Rprof.

Examples
## Not run: 
## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", "onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First = function() cat("\n   Welcome to R!\n\n")
.Last = function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, set a CRAN mirror
  old = getOption("defaultPackages"); r = getOption("repos")
  r["CRAN"] = "http://my.local.cran"
  options(defaultPackages = c(old, "MASS"), repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols = Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), "\n")
# coo\bardoh\exabc"def'

## End(Not run)

<h2>R-Studio size/positioning</h2>
<a href="https://community.rstudio.com/t/r-studio-size-positioning-changing-from-screen-to-screen/28906/3" class="whitebut ">R-Studio size/positioning changing from screen to screen</a>

The following worked (windows):

Note that launching RStudio in this way will only disable GPU rendering for that particular RStudio session (that is, only for RStudio sessions that see that environment variable active). If you'd like to make this change more permanently, you can directly modify RStudio Desktop's options file. The option file is located at:

Windows
%APPDATA%\Roaming\RStudio\desktop.ini

In each case, you can modify the entry called desktop.renderingEngine and set it to software to force software rendering. For example:

[General]
desktop.renderingEngine=software

<h2>Introduction to V8 for R</h2>
V8 is Google’s open source, high performance JavaScript engine. It is written in C++ and implements ECMAScript as specified in ECMA-262, 5th edition. The V8 R package builds on the C++ library to provide a completely standalone JavaScript engine within R:

<code># Create a new context
ct &lt;- v8()

# Evaluate some code
ct$eval("var foo = 123")
ct$eval("var bar = 456")
ct$eval("foo + bar")</code>

<code>[1] "579"</code>

A major advantage over the other foreign language interfaces is that V8 requires no compilers, external executables or other run-time dependencies. The entire engine is contained within a 6MB package (2MB zipped) and works on all major platforms.

<code># Create some JSON
cat(ct$eval("JSON.stringify({x:Math.random()})"))</code>

<code>{"x":0.5580623043314792}</code>

<code># Simple closure
ct$eval("(function(x){return x+1;})(123)")</code>

<code>[1] "124"</code>

However note that V8 by itself is just the naked JavaScript engine. Currently, there is no DOM (i.e. no <em>window</em> object), no network or disk IO, not even an event loop. Which is fine because we already have all of those in R. In this sense V8 resembles other foreign language interfaces such as Rcpp or rJava, but then for JavaScript.

<h3>Loading JavaScript Libraries</h3>
The <code>ct$source</code> method is a convenience function for loading JavaScript libraries from a file or url.

<code>ct$source(system.file("js/underscore.js", package="V8"))
ct$source("https://cdnjs.cloudflare.com/ajax/libs/crossfilter/1.3.11/crossfilter.min.js")</code>

<h3>Data Interchange</h3>
By default all data interchange between R and JavaScript happens via JSON using the bidirectional mapping implemented in the <a href="http://arxiv.org/abs/1403.2805">jsonlite</a> package.

<code>ct$assign("mydata", mtcars)
ct$get("mydata")</code>

<code>                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
...
Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</code>

Alternatively use <code>JS()</code> to assign the value of a JavaScript expression (without converting to JSON):

<code>ct$assign("foo", JS("function(x){return x*x}"))
ct$assign("bar", JS("foo(9)"))
ct$get("bar")</code>

<code>[1] 81</code>

<h3>Function Calls</h3>
The <code>ct$call</code> method calls a JavaScript function, automatically converting objects (arguments and return value) between R and JavaScript:

<code>ct$call("_.filter", mtcars, JS("function(x){return x.mpg &lt; 15}"))</code>

<code>                     mpg cyl disp  hp drat    wt  qsec vs am gear carb
Duster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4
Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4
Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4</code>

It looks a bit like <code>.Call</code> but then for JavaScript instead of C.

<h3>Interactive JavaScript Console</h3>
A fun way to learn JavaScript or debug a session is by entering the interactive console:

<code># Load some data
data(diamonds, package = "ggplot2")
ct$assign("diamonds", diamonds)
ct$console()</code>

From here you can interactively work in JavaScript without typing <code>ct$eval</code> every time:

<code>var cf = crossfilter(diamonds)
var price = cf.dimension(function(x){return x.price})
var depth = cf.dimension(function(x){return x.depth})
price.filter([2000, 3000])
output = depth.top(10)</code>

To exit the console, either press <code>ESC</code> or type <code>exit</code>. Afterwards you can retrieve the objects back into R:

<code>output &lt;- ct$get("output")
print(output)</code>

<h3>warnings, errors and console.log</h3>
Evaluating invalid JavaScript code results in a SyntaxError:

<code># A common typo
ct$eval('var foo &lt;- 123;')</code>

<code>Error in context_eval(join(src), private$context, serialize): SyntaxError: Unexpected token '&lt;'</code>

JavaScript runtime exceptions are automatically propagated into R errors:

<code># Runtime errors
ct$eval("123 + doesnotexit")</code>

<code>Error in context_eval(join(src), private$context, serialize): ReferenceError: doesnotexit is not defined</code>

Within JavaScript we can also call back to the R console manually using <code>console.log</code>, <code>console.warn</code> and <code>console.error</code>. This allows for explicitly generating output, warnings or errors from within a JavaScript application.

<code>ct$eval('console.log("this is a message")')</code>

<code>this is a message</code>

<code>ct$eval('console.warn("Heads up!")')</code>

<code>Warning: Heads up!</code>

<code>ct$eval('console.error("Oh no! An error!")')</code>

<code>Error in context_eval(join(src), private$context, serialize): Oh no! An error!</code>

A example of using <code>console.error</code> is to verify that external resources were loaded:

<code>ct &lt;- v8()
ct$source("https://cdnjs.cloudflare.com/ajax/libs/crossfilter/1.3.11/crossfilter.min.js")
ct$eval('var cf = crossfilter || console.error("failed to load crossfilter!")')</code>

<h3>The Global Namespace</h3>
Unlike what you might be used to from Node or your browser, the global namespace for a new context if very minimal. By default it contains only a few objects: <code>global</code> (a reference to itself), <code>console</code> (for <code>console.log</code> and friends) and <code>print</code> (an alias of console.log needed by some JavaScript libraries)

<code>ct &lt;- v8(typed_arrays = FALSE);
ct$get(JS("Object.keys(global)"))</code>

<code>[1] "print"   "console" "global" </code>

If typed arrays are enabled it contains some additional functions:

<code>ct &lt;- v8(typed_arrays = TRUE);
ct$get(JS("Object.keys(global)"))</code>

<code>[1] "print"   "console" "global" </code>

A context always has a global scope, even when no name is set. When a context is initiated with <code>global = NULL</code>, it can still be reached by evaluating the <code>this</code> keyword within the global scope:

<code>ct2 &lt;- v8(global = NULL, console = FALSE)
ct2$get(JS("Object.keys(this).length"))</code>

<code>[1] 1</code>

<code>ct2$assign("cars", cars)
ct2$eval("var foo = 123")
ct2$eval("function test(x){x+1}")
ct2$get(JS("Object.keys(this).length"))</code>

<code>[1] 4</code>

<code>ct2$get(JS("Object.keys(this)"))</code>

<code>[1] "print" "cars"  "foo"   "test" </code>

To create your own global you could use something like:

<code>ct2$eval("var __global__ = this")
ct2$eval("(function(){var bar = [1,2,3,4]; __global__.bar = bar; })()")
ct2$get("bar")</code>

<code>[1] 1 2 3 4</code>

<h3>Syntax Validation</h3>
V8 also allows for validating JavaScript syntax, without actually evaluating it.

<code>ct$validate("function foo(x){2*x}")</code>

<code>[1] TRUE</code>

<code>ct$validate("foo = function(x){2*x}")</code>

<code>[1] TRUE</code>

This might be useful for all those R libraries that generate browser graphics via templated JavaScript. Note that JavaScript does not allow for defining anonymous functions in the global scope:

<code>ct$validate("function(x){2*x}")</code>

<code>[1] FALSE</code>

To check if an anonymous function is syntactically valid, prefix it with <code>!</code> or wrap in <code>()</code>. These are OK:

<code>ct$validate("(function(x){2*x})")</code>

<code>[1] TRUE</code>

<code>ct$validate("!function(x){2*x}")</code>

<code>[1] TRUE</code>

<h3>Callback To R</h3>
A recently added feature is to interact with R from within JavaScript using the <code>console.r</code> API`. This is most easily demonstrated via the interactive console.

<code>ctx &lt;- v8()
ctx$console()</code>

From JavaScript we can read/write R objects via <code>console.r.get</code> and <code>console.r.assign</code>. The final argument is an optional list specifying arguments passed to <code>toJSON</code> or <code>fromJSON</code>.

<code>// read the iris object into JS
var iris = console.r.get("iris")
var iris_col = console.r.get("iris", {dataframe : "col"})

//write an object back to the R session
console.r.assign("iris2", iris)
console.r.assign("iris3", iris, {simplifyVector : false})</code>

To call R functions use <code>console.r.call</code>. The first argument should be a string which evaluates to a function. The second argument contains a list of arguments passed to the function, similar to <code>do.call</code> in R. Both named and unnamed lists are supported. The return object is returned to JavaScript via JSON.

<code>//calls rnorm(n=2, mean=10, sd=5)
var out = console.r.call('rnorm', {n: 2,mean:10, sd:5})
var out = console.r.call('rnorm', [2, 20, 5])

//anonymous function
var out = console.r.call('function(x){x^2}', {x:12})</code>

There is also an <code>console.r.eval</code> function, which evaluates some code. It takes only a single argument (the string to evaluate) and does not return anything. Output is printed to the console.

<code>console.r.eval('sessionInfo()')</code>

Besides automatically converting objects, V8 also propagates exceptions between R, C++ and JavaScript up and down the stack. Hence you can catch R errors as JavaScript exceptions when calling an R function from JavaScript or vice versa. If nothing gets caught, exceptions bubble all the way up as R errors in your top-level R session.

<code>//raise an error in R
console.r.call('stop("ouch!")')

//catch error from JavaScript
try {
  console.r.call('stop("ouch!")')
} catch (e) {
  console.log("Uhoh R had an error: " + e)
}
//# Uhoh R had an error: ouch!</code>

<h2>sprintf Function</h2>
<strong>Basic R Syntax of sprintf:</strong>
sprintf("%f", x)

<strong>Definition of sprintf:</strong>

The sprintf function returns character objects containing a formatted combination of input values.

<h3>Example 1: Format Decimal Places with sprintf Function in R</h3>
x &lt;- 123.456               # Create example data

The default number of decimal places is six digits after the decimal point

sprintf("%f", x)           # sprintf with default specification
#  "123.456000"

We can control the number of decimal places by adding a point and a number between the percentage sign and the f. 
For instance, we can print ten digits after the decimal point…

sprintf("%.10f", x)        # sprintf with ten decimal places
# "123.4560000000"

…or we can round our numeric input value to only two digits after the decimal point:

sprintf("%.2f", x)         # sprintf with two rounded decimal places
# "123.46"

<strong>Note:</strong> The output of sprintf is a <a href="http://www.r-tutor.com/r-introduction/basic-data-types/character" rel="noopener noreferrer" target="_blank">character string</a> and not a numeric value as the input was.

<h3>Example 2: Format Places Before Decimal Point</h3>
sprintf also enables the formatting of the number of digits before the decimal separator. 
We can tell sprintf to print all digits before the decimal point, but no digits after the decimal point…

sprintf("%1.0f", x)        # sprintf without decimal places
# "123"

…or we can print a certain amount of leading blanks before our number without decimal places (as illustrated by the quotes below)…

sprintf("%10.0f", x)       # sprintf with space before number
# "       123"

…or with decimal places…

sprintf("%10.1f", x)       # Space before number &amp; decimal places
# "     123.5"

…or we can print blanks at the right side of our output by writing a minus sign in front of the number within the sprintf function:

sprintf("%-15f", x)        # Space on right side
# "123.456000     "

<h3>Example 3: Print Non-Numeric Values with sprintf (e.g. 
+ or %)</h3>
It is also possible to combine numeric with non-numeric inputs. 
The following R code returns a plus sign in front of our example number…

sprintf("%+f", x)          # Print plus sign before number
# "+123.456000"

…and the following R code prints a percentage sign at the end of our number:

paste0(sprintf("%f", x),   # Print %-sign at the end of number
       "%")
# "123.456000%"

<h3>Example 4: Control Scientific Notation</h3>
The sprintf R function is also used to <a href="https://statisticsglobe.com/disable-exponential-scientific-notation-in-r">control exponential notation in R</a>. 
The following syntax returns our number as scientific notation with a lower case e…

sprintf("%e", x)           # Exponential notation
# "1.234560e+02"

…and the following code returns an upper case E to the RStudio console:

sprintf("%E", x)           # Exponential with upper case E
# "1.234560e+02"

<h3>Example 5: Control Amount of Decimal Zeros</h3>
We can also control the amount of decimal zeros that we want to print to the RStudio console. 
The following R code prints our example number without any decimal zeros…

sprintf("%g", x)           # sprintf without decimal zeros
# "123.456"

…the following R code returns our example number * 1e10 in scientific notation…

sprintf("%g", 1e10 * x)    # Scientific notation
# "1.23456e+12"

…and by adding a number before the g within the sprintf function we can control the amount of decimal zeros that we want to print:

sprintf("%.13g", 1e10 * x) # Fixed decimal zeros
# "1234560000000"

<h3>Example 6: Several Input Values for sprintf Function</h3>
So far, we have only used a single numeric value (i.e. our example data object x) as input for sprintf. 
However, the sprintf command allows as many input values as we want.

Furthermore, we can print these input values within more complex character strings. 
Have a look at the following sprintf example:

sprintf("Let's create %1.0f more complex example %1.0f you.", 1, 4)
# "Let's create 1 more complex example 4 you."

The first specification (i.e. %1.0f) within the previous R code corresponds to the input value 1 and the second specification corresponds to the input value 4.

Of cause we could use sprintf in even more complex settings. 
Have a look at the sprintf examples of the R help documentation, if you are interested in more complex examples:


<img src="https://statisticsglobe.com/wp-content/uploads/2019/04/sprintf-r-help-documentation-more-sophisticated-examples.png">

<em><strong>Figure 1: Complex sprintf Examples in R Help Documentation.</strong></em>

Examples
## be careful with the format: most things in R are floats
## only integer-valued reals get coerced to integer.

sprintf("%s is %f feet tall\n", "Sven", 7.1)      # OK
try(sprintf("%s is %i feet tall\n", "Sven", 7.1)) # not OK
try(sprintf("%s is %i feet tall\n", "Sven", 7))   # OK

## use a literal % :
sprintf("%.0f%% said yes (out of a sample of size %.0f)", 66.666, 3)

## no truncation:
sprintf("%1.f",101)
## re-use one argument three times, show difference between %x and %X
xx = sprintf("%1$d %1$x %1$X", 0:15)
xx = matrix(xx, dimnames=list(rep("", 16), "%d%x%X"))
noquote(format(xx, justify="right"))

## More sophisticated:

sprintf("min 10-char string '%10s'",
        c("a", "ABC", "and an even longer one"))

n = 1:18
sprintf(paste("e with %2d digits = %.",n,"g",sep=""), n, exp(1))

## Using arguments out of order
sprintf("second %2$1.0f, first %1$5.2f, third %3$1.0f", pi, 2, 3)

## Using asterisk for width or precision
sprintf("precision %.*f, width '%*.3f'", 3, pi, 8, pi)

## Asterisk and argument re-use, 'e' example reiterated:
sprintf("e with %1$2d digits = %2$.*1$g", n, exp(1))

## re-cycle arguments 
sprintf("%s %d", "test", 1:3)

sprintf(fmt, …)
Create a character string that contains values from R objects. 

fmt – A character string with some occurrences of %s, which will be places that object values are inserted.
… – The R objects to be inserted. 
The number of items should correspond to the number of %s occurrences in fmt.

Example. 
x = 2349
sprintf("Substitute in a string or number: %s", x)
"Substitute in a string or number: 2349"

sprintf("Can have multiple %s occurrences %s", x, "- got it?")
"Can have multiple 2349 occurrences - got it?"

Creating Custom Themes for RStudio
<a href="https://github.com/gadenbuie/rsthemes" class="whitebut ">rsthemes</a>

Creating an rstheme
Another straightforward method would be to copy an existing rstheme and then modify the values.

Because of the structure of the elements being styled, not all the CSS rule sets may end up being used. 
Below is a table that describes the most relevant selectors, which tmTheme scope they correspond to, if any, and how they impact the style of RStudio.

Selector	Scope	Description
.ace_bracket		Overrides default styling for matching bracket highlighting provided by Ace.
.ace_comment	comment	Changes the color and style of comments.
.ace_constant	constant	Changes the color and style of constants like TRUE, FALSE, and numeric literals.
.ace_constant.ace_language	constant.language	Changes the color and style of language constants like TRUE and FALSE. 
This rule set will override rules in .ace_constant for language constants. 
Also in RMarkdown files, everything surrounded in *.
.ace_constant.ace_numeric	constant.numeric	Changes the color and style of numeric literals. 
This value will override the settings in the “constant” scope, if set. 
Also in RMarkdown files, everything surrounded in **.
.ace_cusor		Changes the color and style of the text cursor in the editor window.
.ace_editor		Changes the default color and background of the RStudio editor windows. 
This selector will usually be the first in a list of other selectors for the same rule set, such as .rstudio-themes-flat.ace_editor_theme and so on.
.ace_gutter		Changes the color and style of the gutter: the panel on the left-hand side of the editor which holds line numbers, breakpoints, and fold widgets.
.ace_gutter-active-line		Changes the color and style of the gutter at the active line in the editor.
.ace_heading		Changes the color and style of headings in RMarkdown documents.
.ace_indent-guide		Changes the color and style of the indent guide, which can be enabled or disabled through Global Options > Code > Display > Show indent guides.
.ace_invisible		Changes the color and style of invisible characters, which can be enabled or disabled through Global Options > Code Display > Show whitespace characters.
.ace_keyword	keyword	Changes the color and style of keywords like function, if, else, stop, and operators.
.ace_keyword.ace_operator	keyword.operator	Changes the color and style of operators like (, ), =, +, and -. 
This value will override the settings in the .ace_keyword block for operators, if set.
.ace_meta.ace_tag	meta.tag	Changes the color and style of metadata tags in RMarkdown documents, like title and output.
.ace_marker-layer .ace_active-debug-line	marker-layer.active_debug_line	Changes the color and style of the highlighting on the line of code which is currently being debugged.
.ace_marker-layer .ace_bracket		Changes the color and style of the highlighting on matching brackets.
.ace_marker-layer .ace_selection		Changes the color and style of the highlighting for the currently selected line or block of lines.
.ace_markup.ace_heading	markup.heading	Changes the color and style of the characters that start a heading in RMarkdown documents.
.ace_print-margin		Changes the color and style, if applicable, of the line-width margin that can be enabled or disabled through Global Options > Code > Display > Show margin.
.ace_selection.ace_start		Changes the color and style of the highlighting for the start of the currently selected block of lines.
.ace_string	string	Changes the color and style of string literals.
.ace_support.ace_function	support.function	Changes the color and style of code blocks in RMarkdown documents.
In addition to these rule sets, you will also find a number of rule sets related to the Terminal pane, with selectors that include .terminal or selectors that begin with .xterm. 
It is possible to change these values as well, but it may be advisable to keep a back up copy of your original theme in case you don’t like any of the changes. 
There are also a number of classes that can be used to modify parts of RStudio unrelated to the editor. 
These classes are all prefixed with rstheme_, with the exception of dataGridHeader and themedPopupPanel. 
Any classes you find in the html of RStudio which are not prefixed with rstheme_, ace_, or explicitly listed in this article are subject to change at anytime, and so are unsafe to use in custom themes.

Since an rstheme is just CSS, anything that you can do with CSS you can do in an rstheme.

Testing Changes to a Theme
If you’re modifying a theme which has already been added to RStudio, you may need to restart RStudio desktop in order to make the changes take effect.

Sharing a Theme
Once you’re satisfied with your theme, you can easily share it with anyone by simply sharing the tmTheme or rstheme file. 
You can find rstheme files in 
C:\Users\(your user account)\Documents\.R\rstudio\themes on Windows

You can also use the theme related functions provided in the RStudio API to save a local copy of your converted theme.

If you upload your rstheme file to a URL-addressable location, you can also share a snippet of code that anyone can run to try your theme:

rstudioapi::addTheme("http://your/theme/path/theme.rstheme", apply = TRUE)
This will download, install, and apply the theme immediately on the user’s machine.

<h2>theme in RStudio</h2>
<a href="https://tmtheme-editor.herokuapp.com/#!/editor/theme/Monokai" class="whitebut ">Monokai theme</a>
Select a theme from this theme editor and save it to your machine as a .tmTheme file. 
Add the theme in RStudio. 
RStudio automatically creates an .rstheme file from that. 
the rstheme file in Users/Eric/Documents/.R/rstudio/themes folder (Windows machine). 
Open the file in text editor and find the .ace entry that matches the one you want to change. 
This will have an rgba value similar to: background-color: rgba(238, 252, 81, 0.8);
Note that the last value is the alpha (transparency) value and should be between 0 and 1. 

<a href="https://stackoverflow.com/questions/40369595/altering-rstudio-editor-theme" class="whitebut ">Altering RStudio Editor Theme</a>
<a href="https://stackoverflow.com/questions/25582588/any-way-to-change-colors-in-rstudio-to-something-other-than-default-options" class="whitebut ">change colors in Rstudio</a>
<a href="https://stackoverflow.com/questions/37635237/editing-r-studio-theme-in-cache-css-theme-file-ace-editor" class="whitebut ">Editing R studio theme in cache.css theme file</a>

There's a much faster way to deal with this and 100% doable.

Open RStudio with your favourite Editor theme and open an .R script

Inspect the Source layout (Right-click>Inspect) and Ctrl + f an unique class selector such as .ace_comment. 
In the matched CSS rules box in the side pane copy an attribute as unique as possible (i.e. 
color: #0088FF; I use Cobalt theme).

Go to RStudio's install path and dive into /www/rstudio/. 
As jorloff rightly said, you'll find a bunch of files like this: VERYUGLYNAME.cache.css. 
Open all of them with your favourite text editor as administrator.

Find in files: Ctrl+ Shift + f (in sublime text) and type the unique attribute value you previously chosed. 
BOOM, there you have it.

Now delight yourself editing your crazy style, but remember to back it up first!

As Jonathan said, RStudio's editor is based on ACE themes, so all clases have the ace_ prefix. 
Take your time inspecting and understanding the editor hierarchy. 
I recommend you to take some time inspecting the html code to understand its structure. 
The editor starts in id="rstudio_source_text_editor"


i am new to R Studio and i would like to share how i was able to customize the color scheme of R Studio:

How to change the color of comments in Rstudio

Rstudio Pane Appearance > Set editor theme to monokai
Right click on editor pane > Inspect > find the specific file name (i.e. 
838C7F60FB885BB7E5EED7F698E453B9.cache.css)
Open drive C > open Progam Files folder > open Rstudio folder
Open www folder > rstudio folder > find the 838C7F60FB885BB7E5EED7F698E453B9.cache.css (name of the theme you want to change)
Make a backup copy of the original
Change .ace_comment {color: #75715E} to .ace_comment {color: #F92672} > save to another location (don't change file name)
Copy the recently saved code and paste it in rstudio folder (step 4) > replace the original 838C7F60FB885BB7E5EED7F698E453B9.cache.css file with the modified 838C7F60FB885BB7E5EED7F698E453B9.cache.csss file
Click continue
Quit Rstudio
Open Rstudio
Check if the color of comment has changed from nightsand(#75715E) to orchid(#F92672)


I am using RStudio 1.0.136. 
According to all the posts, right click on the Editor -> Inspect. 
The Web Inspector comes up and shows the Elements tab. 
Then click the Sources tab, select "Only enable for this session", click "Enable Debugging" button. 
You will see the code for the theme xxxxxxx.cache.css file. 
If nothing in the editor, try the left top "Show Navigator" button right under the "Elements" menu. 
Select the .css file in the list and it should open.

My line number seems dim. 
So changed color: #222; to color: #818222; in this section: (forgive my bad color sense). 
And you can see the color change right away! How amazing!

.ace_gutter {
  background-color: #3d3d3d;
  background-image: -moz-linear-gradient(left, #3D3D3D, #333);
  background-image: -ms-linear-gradient(left, #3D3D3D, #333);
  background-image: -webkit-gradient(linear, 0 0, 0 100%, from(#3D3D3D), to(#333));
  background-image: -webkit-linear-gradient(left, #3D3D3D, #333);
  background-image: -o-linear-gradient(left, #3D3D3D, #333);
  background-image: linear-gradient(left, #3D3D3D, #333);
  background-repeat: repeat-x;
  border-right: 1px solid #4d4d4d;
  text-shadow: 0px 1px 1px #4d4d4d;
  color: #818222;
}
@skan mentioned selected words are too dim. 
I have the same problem. 
So here I found it:

.ace_marker-layer .ace_selected-word {
  border-radius: 4px;
  border: 8px solid #ff475d;
  box-shadow: 0 0 4px black;
}
I changed border: 8px solid #ff475d;. 
It is now very bright, or may be too bright. 
Anyway, it works. 
Thanks for every one. 
And hope this can help.

This is for current session only. 
Now you know which .css to modify and what you should do, it will be easy to modify the original .css file to keep it permanent.

<h2>file or folder is locked after script quited</h2>
switch to other folder before quit
setwd("C:/Users/User/Desktop")

<h2>Write file as UTF-8 encoding</h2>
<a href="https://tomizonor.wordpress.com/2013/04/17/file-utf8-windows/" class="whitebut ">Write file as UTF-8 encoding in R for Windows</a>

While the R uses UTF-8 encoding as default on Linux and Mac OS, the R for Windows does not use UTF-8 as default. So reading and writing UTF-8 files are something troublesome on Windows. In this article, I will show you a small script to help UTF-8 encoding.

options("encoding" = "UTF-8")
t2 = "®"
getOption("encoding")
Encoding(t2) = "UTF-8"
sink("test.txt")
cat("123")
cat(t2)
sink()

Sys.getlocale('LC_CTYPE')

writeLines(Sys.setlocale("LC_CTYPE", locale), con)
Sys.setlocale("LC_CTYPE")
<h2>S4 Classes</h2>
<h3>The Basic Idea</h3>
The S4 approach differs from the S3 approach to creating a class in that it is a more rigid definition. 
The idea is that an object is created using the <em>setClass</em> command. 
The command takes a number of options. 
Many of the options are not required, but we make use of several of the optional arguments because they represent good practices with respect to object oriented programming.
We first construct a trivial, contrived class simply to demonstrate the basic idea. 
Next we demonstrate how to create a method for an S4 class. 
This example is a little more involved than what we saw in the section on S3 classes.
In this example, the name of the class is <em>FirstQuadrant</em>, and the class is used to keep track of an <em>(x,y)</em> coordinate pair in the first quadrant. 
There is a restriction that both values must be greater than or equal to zero. 
There are two data elements, called <em>slots</em>, and they are called <em>x</em> and <em>y</em>. 
The default values for the coordinate is the origin, <em>x=0</em> and <em>y=0</em>.
######################################################################
# Create the first quadrant class
#
# This is used to represent a coordinate in the first quadrant.
FirstQuadrant = setClass(
    # Set the name for the class
    "FirstQuadrant",

    # Define the slots
    slots = c( x = "numeric", y = "numeric" ),

    # Set the default values for the slots. (optional)
    prototype=list( x = 0.0, y = 0.0 ),

    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object){
        if((object@x &lt; 0) || (object@y &lt; 0)) {
            return("A negative number for one of the coordinates was given.")
        }
        return(TRUE)
    }
)



Note that the way to access one of the data elements is to use the “@” symbol. 
An example if given below. 
In the example three elements of the class defined above are created. 
The first uses the default values for the slots, the second overrides the defaults, and finally an attempt is made to create a coordinate in the second quadrant.
> x = FirstQuadrant()
> x
An object of class "FirstQuadrant"
Slot "x":
[1] 0
Slot "y":
[1] 0
> y = FirstQuadrant(x=5,y=7)
> y
An object of class "FirstQuadrant"
Slot "x":
[1] 5
Slot "y":
[1] 7
> y@x
[1] 5
> y@y
[1] 7
> z = FirstQuadrant(x=3,y=-2)
Error in validObject(.Object) :
      invalid class “FirstQuadrant” object: A negative number for one of the coordinates was given.
> z
Error: object 'z' not found



In the next example we create a method that is associated with the class. 
The method is used to set the values of a coordinate. 
The first step is to reserve the name using the <em>setGeneric</em> command, and then the <em>setMethod</em> command is used to define the function to be called when the first argument is an object from the <em>FirstQuadrant</em> class.
# create a method to assign the value of a coordinate setGeneric(name="setCoordinate",
    def=function(theObject,xVal,yVal)
    {
        standardGeneric("setCoordinate")
    }
    )
setMethod(f="setCoordinate",
    signature="FirstQuadrant",
    definition=function(theObject,xVal,yVal)
    {
       theObject@x = xVal
       theObject@y = yVal
       return(theObject)
    }
    )



It is important to note that R generally passes objects as values. 
For this reason the methods defined above return the updated object. 
When the method is called, it is used to replace the former object with the updated object.
> z = FirstQuadrant(x=2.5,y=10)
> z
An object of class "FirstQuadrant"
Slot "x":
[1] 2.5
Slot "y":
[1] 10
> z = setCoordinate(z,-3.0,-5.0)
> z
An object of class "FirstQuadrant"
Slot "x":
[1] -3
Slot "y":
[1] -5



Note that the <em>validity</em> function given in the original class definition is not called. 
It is called when an object is first defined. 
It can be called later, but only when an explicit request is made using the <em>validObject</em> command.


<h3>Creating an S4 Class</h3>
An S4 class is created using the <em>setClass()</em> command. 
At a minimum the name of the class is specified and the names of the data elements
(slots) is specified. 
There are a number of other options, and just as a matter of good practice we also specify a function to verify that the data is consistent (validation), and we specify the default values (the prototype). 
In the last section of this page,
S4 inheritance, we include an additional parameter used to specify a class hierarchy.
In this section we look at another example, and we examine some of the functions associated with S4 classes. 
The example we define will be used to motivate the use of methods associated with a class, and it will be used to demonstrate inheritance later. 
The idea is that we want to create a program to simulate a cellular automata model of a predator-prey system.
We do not develop the whole code here but concentrate on the data structures. 
In particular we will create a base class for the agents. 
In the next section we will create the basic methods for the class. 
In the inheritance section we will discuss how to build on the class to create different predators and different prey species. 
The basic structure of the class is shown in Figure 1.


<img src="https://www.cyclismo.org/tutorial/R/_images/s4AgentClass.png" />
Figure 1.

Diagram of the base class, Agent, used for the agents in a simulation.

The methods for this class are defined in the following section. 
Here we define the class and its slots, and the code to define the class is given below:
######################################################################
# Create the base Agent class
#
# This is used to represent the most basic agent in a simulation.
Agent = setClass(
    # Set the name for the class
    "Agent",
    # Define the slots
    slots = c(
        location = "numeric",
        velocity   = "numeric",
        active   = "logical"
        ),
    # Set the default values for the slots. 
(optional)
    prototype=list(
        location = c(0.0,0.0),
        active   = TRUE,
        velocity = c(0.0,0.0)
        ),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>100.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    }
    )



Now that the code to define the class is given we can create an object whose class is Agent.
> a = Agent()
> a
An object of class "Agent"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE



Before we define the methods for the class a number of additional commands are explored. 
The first set of functions explored are the
<em>is.object</em> and the <em>isS4</em> commands. 
The <em>is.object</em> command determines whether or not a variable refers to an object. 
The <em>isS4</em>
command determines whether or not the variable is an S4 object. 
The reason both are required is that the <em>isS4</em> command alone cannot determine if a variable is an S3 object. 
You need to determine if the variable is an object and then decide if it is S4 or not.
> is.object(a)
[1] TRUE
> isS4(a)
[1] TRUE



The next set of commands are used to get information about the data elements, or slots, within an object. 
The first is the <em>slotNames</em>
command. 
This command can take either an object or the name of a class. 
It returns the names of the slots associated with the class as strings.
> slotNames(a)
[1] "location" "velocity" "active"
> slotNames("Agent")
[1] "location" "velocity" "active"



The <em>getSlots</em> command is similar to the <em>slotNames</em> command. 
It takes the name of a class as a string. 
It returns a vector whose entries are the types associated with the slots, and the names of the entries are the names of the slots.
> getSlots("Agent")
     location  velocity    active
"numeric" "numeric" "logical"
> s = getSlots("Agent")
> s[1]
     location
"numeric"
> s[[1]]
[1] "numeric"
> names(s)
[1] "location" "velocity" "active"



The next command examined is the <em>getClass</em> command. 
It has two forms. 
If you give it a variable that is an S4 class it returns a list of slots for the class associated with the variable. 
If you give it a character string with the name of a class it gives the slots and their data types.
> getClass(a)
An object of class "Agent"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> getClass("Agent")
Class "Agent" [in ".GlobalEnv"]
Slots:
Name:  location velocity   active
Class:  numeric  numeric  logical



The final command examined is the <em>slot</em> command. 
It can be used to get or set the value of a slot in an object. 
It can be used in place of the “@” operator.
> slot(a,"location")
[1] 0 0
> slot(a,"location") = c(1,5)
> a
An object of class "Agent"
Slot "location":
[1] 1 5
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE





<h3>Creating Methods</h3>
We now build on the Agent class defined above. 
Once the class and its data elements are defined we can define the methods associated with the class. 
The basic idea is that if the name of a function has not been defined, the name must first be reserved using the <em>setGeneric</em>
function. 
The <em>setMethod</em> can then be used to define which function is called based on the class names of the objects sent to it.
We define the methods associated with the Agent method given in the previous section. 
Note that the <em>validity</em> function for an object is only called when it is first created and when an explicit call to the
<em>validObject</em> function is made. 
We make use of the <em>validObject</em>
command in the methods below that are used to change the value of a data element within an object.
# create a method to assign the value of the location setGeneric(name="setLocation",
    def=function(theObject,position)
    {
        standardGeneric("setLocation")
    }
    )
setMethod(f="setLocation",
    signature="Agent",
    definition=function(theObject,position)
    {
       theObject@location = position
       validObject(theObject)
       return(theObject)
    }
    )
# create a method to get the value of the location setGeneric(name="getLocation",
    def=function(theObject)
    {
        standardGeneric("getLocation")
    }
    )
setMethod(f="getLocation",
    signature="Agent",
    definition=function(theObject)
    {
       return(theObject@location)
    }
    )

# create a method to assign the value of active setGeneric(name="setActive",
    def=function(theObject,active)
    {
        standardGeneric("setActive")
    }
    )
setMethod(f="setActive",
    signature="Agent",
    definition=function(theObject,active)
    {
       theObject@active = active
       validObject(theObject)
       return(theObject)
    }
    )
# create a method to get the value of active setGeneric(name="getActive",
    def=function(theObject)
    {
        standardGeneric("getActive")
    }
    )
setMethod(f="getActive",
    signature="Agent",
    definition=function(theObject)
    {
       return(theObject@active)
    }
    )

# create a method to assign the value of velocity setGeneric(name="setVelocity",
    def=function(theObject,velocity)
    {
        standardGeneric("setVelocity")
    }
    )
setMethod(f="setVelocity",
    signature="Agent",
    definition=function(theObject,velocity)
    {
       theObject@velocity = velocity
       validObject(theObject)
       return(theObject)
    }
    )
# create a method to get the value of the velocity setGeneric(name="getVelocity",
    def=function(theObject)
    {
        standardGeneric("getVelocity")
    }
    )
setMethod(f="getVelocity",
    signature="Agent",
    definition=function(theObject)
    {
       return(theObject@velocity)
    }
    )



With these definitions the data elements are encapsulated and can be accessed and set using the methods given above. 
It is generally good practice in object oriented programming to keep your data private and not show them to everybody willy nilly.
> a = Agent()
> getVelocity(a)
[1] 0 0
> a = setVelocity(a,c(1.0,2.0))
> getVelocity(a)
[1] 1 2



The last topic examined is the idea of overloading functions. 
In the examples above the signature is set to a single element. 
The signature is a vector of characters and specifies the data types of the argument list for the method to be defined. 
Here we create two new methods. 
The name of the method is <em>resetActivity</em>, and there are two versions.
The first version accepts two arguments whose types are <em>Agent</em> and
<em>logical</em>. 
This version of the method will set the activity slot to a given value. 
The second version accepts two arguments whose types are
<em>Agent</em> and <em>numeric</em>. 
This version will set the activity to TRUE and then set the energy level to the value passed to it. 
Note that the names of the variables in the argument list must be exactly the same.
# create a method to reset the velocity and the activity setGeneric(name="resetActivity",
    def=function(theObject,value)
    {
        standardGeneric("resetActivity")
    }
    )
setMethod(f="resetActivity",
    signature=c("Agent","logical"),
    definition=function(theObject,value)
    {
       theObject = setActive(theObject,value)
       theObject = setVelocity(theObject,c(0.0,0.0))
       return(theObject)
    }
    )
setMethod(f="resetActivity",
    signature=c("Agent","numeric"),
    definition=function(theObject,value)
    {
       theObject = setActive(theObject,TRUE)
       theObject = setVelocity(theObject,value)
       return(theObject)
    }
    )



This definition of the function yields two options for the
<em>resetActivity</em> function. 
The decision to determine which function to call depends on two arguments and their type. 
For example, if the first argument is from the Agent class and the second is a value of
TRUE or FALSE, then the first version of the function is called. 
Otherwise, if the second argument is a number the second version of the function is called.
> a = Agent()
> a
An object of class "Agent"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> a = resetActivity(a,FALSE)
> getActive(a)
[1] FALSE
>  a = resetActivity(a,c(1,3))
> getVelocity(a)
[1] 1 3





<h3>Inheritance</h3>
A class’ inheritance hiearchy can be specified when the class is defined using the <em>contains</em> option. 
The <em>contains</em> option is a vector that lists the classes the new class inherits from. 
In the following example we build on the Agent class defined in the previous section. 
The idea is that we need agents that represent a predator and two prey. 
We will focus on two predators for this example.
The hierarchy for the classes is shown in
Figure 2.. 
 In this example we have one Prey class that is derived from the Agent class. 
There are two predator classes, Bobcat and Lynx. 
The Bobcat class is derived from the Agent class, and the Lynx class is derived from the Bobcat class. 
We will keep this very simple, and the only methods associated with the new classes is a <em>move</em> method. 
For our purposes it will only print out a message and set the values of the position and velocity to demonstrate the order of execution of the methods associated with the classes.


<img src="https://www.cyclismo.org/tutorial/R/_images/s4AgentPredPrey.png" />
Figure 2.

Diagram of the predator and prey classes derived from the Agent class.

The first step is to create the three new classes.
######################################################################
# Create the Prey class
#
# This is used to represent a prey animal
Prey = setClass(
    # Set the name for the class
    "Prey",
    # Define the slots - in this case it is empty...
    slots = character(0),
    # Set the default values for the slots. 
(optional)
    prototype=list(),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>70.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    },
    # Set the inheritance for this class
    contains = "Agent"
    )

######################################################################
# Create the Bobcat class
#
# This is used to represent a smaller predator
Bobcat = setClass(
    # Set the name for the class
    "Bobcat",
    # Define the slots - in this case it is empty...
    slots = character(0),
    # Set the default values for the slots. 
(optional)
    prototype=list(),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>85.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    },
    # Set the inheritance for this class
    contains = "Agent"
    )
######################################################################
# Create the Lynx class
#
# This is used to represent a larger predator
Lynx = setClass(
    # Set the name for the class
    "Lynx",
    # Define the slots - in this case it is empty...
    slots = character(0),
    # Set the default values for the slots. 
(optional)
    prototype=list(),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>95.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    },
    # Set the inheritance for this class
    contains = "Bobcat"
    )



The inheritance is specified using the <em>contains</em> option in the
<em>setClass</em> command. 
Note that this can be a vector allowing for multiple inheritance. 
We choose not to use that to keep things simpler. 
If you are feeling like you need more self-loathing in your life you should try it out and experiment.
Next we define a method, <em>move</em>, for the new classes. 
We will include methods for the Agent, Prey, Bobcat, and Lynx classes. 
The methods do not really do anything but are used to demonstrate the idea of how methods are executed.
# create a method to move the agent.
setGeneric(name="move",
    def=function(theObject)
    {
        standardGeneric("move")
    }
    )
setMethod(f="move",
    signature="Agent",
    definition=function(theObject)
    {
       print("Move this Agent dude")
       theObject = setVelocity(theObject,c(1,2))
       validObject(theObject)
       return(theObject)
    }
    )
setMethod(f="move",
    signature="Prey",
    definition=function(theObject)
    {
       print("Check this Prey before moving this dude")
       theObject = callNextMethod(theObject)
       print("Move this Prey dude")
       validObject(theObject)
       return(theObject)
    }
    )
setMethod(f="move",
    signature="Bobcat",
    definition=function(theObject)
    {
       print("Check this Bobcat before moving this dude")
       theObject = setLocation(theObject,c(2,3))
       theObject = callNextMethod(theObject)
       print("Move this Bobcat dude")
       validObject(theObject)
       return(theObject)
    }
    )
setMethod(f="move",
    signature="Lynx",
    definition=function(theObject)
    {
       print("Check this Lynx before moving this dude")
       theObject = setActive(theObject,FALSE)
       theObject = callNextMethod(theObject)
       print("Move this Lynx dude")
       validObject(theObject)
       return(theObject)
    }
    )



There are a number of things to note. 
First each method calls the
<em>callNextMethod</em> command. 
This command will execute the next version of the same method for the previous class in the hierarchy. 
Note that
I have included the arguments (in the same order) as those called by the original function. 
Also note that the function returns a copy of the object and is used to update the object passed to the original function.
Another thing to note is that the methods associated with the Lync,
Bobcat, and Agent classes arbitrarily change the values of the position, velocity, and activity for the given object. 
This is done to demonstrate the changes that take place and reinforce the necessity for using the <em>callNextMethod</em> function the way it is used here.
Finally, it should be noted that the <em>validObject</em> command is called in every method. 
You should try adding a print statement in the validity function. 
You might find that the order is a bit odd. 
You should experiment with this and play with it. 
There are times you do not get the expected results so be careful!
We now give a brief example to demonstrate the order that the functions are called. 
In the example we create a Bobcat object and then call the <em>move</em> method. 
We next create a Lynx object and do the same. 
We print out the slots for both agents just to demonstrate the values that are changed.
> robert = Bobcat()
> robert
An object of class "Bobcat"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> robert = move(robert)
[1] "Check this Bobcat before moving this dude"
[1] "Move this Agent dude"
[1] "Move this Bobcat dude"
> robert
An object of class "Bobcat"
Slot "location":
[1] 2 3
Slot "velocity":
[1] 1 2
Slot "active":
[1] TRUE
>
>
>
> lionel = Lynx()
> lionel
An object of class "Lynx"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> lionel = move(lionel)
[1] "Check this Lynx before moving this dude"
[1] "Check this Bobcat before moving this dude"
[1] "Move this Agent dude"
[1] "Move this Bobcat dude"
[1] "Move this Lynx dude"
> lionel
An object of class "Lynx"
Slot "location":
[1] 2 3
Slot "velocity":
[1] 1 2
Slot "active":
[1] FALSE

<h2>convert named character to vector</h2>
a = unname(resultTable[,1][which(klineWave[,7]==TRUE)])

<h2>Neural Network Models in R</h2>
Neural Network (or Artificial Neural Network) has the ability to learn by examples. 

<h3>Activation Functions</h3>
Activation function defines the output of a neuron in terms of a local induced field. 
 Activation functions are a single line of code that gives the neural nets non-linearity and expressiveness. 
There are many activation functions. 
Some of them are as follows (<a href="https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/]">Source</a>):

<strong>Identity function</strong> is a function that maps input to the same output value. 
It is a linear operator in vector space. 
Also, known straight line function where activation is proportional to the input.
In <strong>Binary Step Function</strong>, if the value of Y is above a certain value known as the threshold, the output is True(or activated), and if it’s less than the threshold, then the output is false (or not activated). 
It is very useful in the classifier.
<strong>Sigmoid Function</strong> called S-shaped functions. 
Logistic and hyperbolic tangent functions are commonly used sigmoid functions. 
There are two types of sigmoid functions.
<strong>Binary Sigmoid Function</strong> is a logistic function where the output values are either binary or vary from 0 to 1.
<strong>Bipolar Sigmoid Function</strong> is a logistic function where the output value varies from -1 to 1. 
Also known as Hyperbolic Tangent Function or tanh.

<strong>Ramp Function:</strong> The name of the ramp function is derived from the appearance of its graph. 
It maps negative inputs to 0 and positive inputs to the same output.
<strong>ReLu</strong> stands for the rectified linear unit (ReLU). 
It is the most used activation function in the world. 
It output 0 for negative values of x.

<center>
<img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/4_jouacz.png" /></center>

<h3>Implementation of a Neural Network in R</h3>
<h3>Install required package</h3>
Let's first install the neuralnet library:

<code class="lang-python"># install package
install.packages("neuralnet")
</code>

<code>Updating HTML index of packages in '.Library'
Making 'packages.html' ... 
done
</code>
<h3>Create training dataset</h3>
<center>
<img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/5_lrfb0r.png" /></center>

Let's create your own dataset. 
Here you need two kinds of attributes or columns in your data: Feature and label. 
In the table shown above, you can see the technical knowledge, communication skills score and placement status of the student. 
So the first two columns(Technical Knowledge Score and Communication Skills Score) are features and third column(Student Placed) is the binary label.

<code class="lang-python"># creating training data set
TKS=c(20,10,30,20,80,30)
CSS=c(90,20,40,50,50,80)
Placed=c(1,0,0,0,1,1)
# Here, you will combine multiple columns or features into a single set of data
df=data.frame(TKS,CSS,Placed)
</code>

Let's build a NN classifier model using the neuralnet library.

First, import the neuralnet library and create NN classifier model by passing argument set of label and features, dataset, number of neurons in hidden layers, and error calculation.

<code class="lang-python"># load library
require(neuralnet)

# fit neural network
nn=neuralnet(Placed~TKS+CSS,data=df, hidden=3,act.fct = "logistic",
               linear.output = FALSE)
</code>

Here,

<code>  - Placed~TKS+CSS, Placed is label annd TKS and CSS are features.
 - df is dataframe,
 - hidden=3: represents single layer with 3 neurons respectively.
 - act.fct = "logistic" used for smoothing the result.
 - linear.ouput=FALSE: set FALSE for apply act.fct otherwise TRUE
</code>
<h3>Plotting Neural Network</h3>
Let's plot your neural net model.

<code class="lang-python"># plot neural network
plot(nn)
</code>

<center>
<img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672260/6_y5jnhr.png" /></center>

<h3>Create test dataset</h3>
Create test dataset using two features Technical Knowledge Score and Communication Skills Score

<code class="lang-python"># creating test set
TKS=c(30,40,85)
CSS=c(85,50,40)
test=data.frame(TKS,CSS)
</code>

<h3>Predict the results for the test set</h3>
Predict the probability score for the test data using the compute function.

<code class="lang-python">## Prediction using neural network
Predict=compute(nn,test)
Predict$net.result
</code>

0.9928202080
0.3335543925
0.9775153014

Now, Convert probabilities into binary classes.

<code class="lang-python"># Converting probabilities into binary classes setting threshold level 0.5
prob &lt;- Predict$net.result
pred &lt;- ifelse(prob&gt;0.5, 1, 0)
pred
</code>

1
0
1

Predicted results are 1,0, and 1.

<h3>Pros and Cons</h3>
Neural networks are more flexible and can be used with both regression and classification problems. 
Neural networks are good for the nonlinear dataset with a large number of inputs such as images. 
Neural networks can work with any number of inputs and layers. 
Neural networks have the numerical strength that can perform jobs in parallel.

There are more alternative algorithms such as SVM, Decision Tree and Regression are available that are simple, fast, easy to train, and provide better performance. 
Neural networks are much more of the black box, require more time for development and more computation power. 
Neural Networks requires more data than other Machine Learning algorithms. 
NNs can be used only with numerical inputs and non-missing value datasets. 
A well-known neural network researcher said <i> "A neural network is the second best way to solve any problem. 
The best way is to actually understand the problem,"</i>

<h3>Use-cases of NN</h3>
NN's wonderful properties offer many applications such as:

<strong>Pattern Recognition:</strong>  neural networks are very suitable for pattern recognition problems such as facial recognition, object detection, fingerprint recognition, etc.
<strong>Anomaly Detection:</strong> neural networks are good at pattern detection, and they can easily detect the unusual patterns that don’t fit in the general patterns.
<strong>Time Series Prediction:</strong> Neural networks can be used to predict time series problems such as stock price, weather forecasting.
<strong>Natural Language Processing:</strong> Neural networks offer a wide range of applications in Natural Language Processing tasks such as text classification, Named Entity Recognition (NER), Part-of-Speech Tagging, Speech Recognition, and Spell Checking.



<h2>Neural Net Package Examples</h2>
<code>library("neuralnet")</code>
Going to create a neural network to perform square rooting 
Type ?neuralnet for more information on the neuralnet library

Generate 50 random numbers uniformly distributed between 0 and 100 And store them as a dataframe

<code>traininginput &lt;-  as.data.frame(runif(50, min=0, max=100))
trainingoutput &lt;- sqrt(traininginput)</code>

Column bind the data into one variable

<code>trainingdata &lt;- cbind(traininginput,trainingoutput)
colnames(trainingdata) &lt;- c("Input","Output")</code>

Train the neural network Going to have 10 hidden layers Threshold is a numeric value specifying the threshold for the partial derivatives of the error function as stopping criteria.

<code>net.sqrt &lt;- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01)</code>

Plot the neural network
<code>plot(net.sqrt, rep = "best")</code>

Test the neural network on some training data

<code>testdata &lt;- as.data.frame((1:10)^2) #Generate some squared numbers
net.results &lt;- compute(net.sqrt, testdata) #Run them through the neural network</code>

Lets see what properties net.sqrt has

<code>ls(net.results)</code>

<code>## [1] "net.result" "neurons"</code>

Lets see the results

<code>print(net.results$net.result)</code>

<code>##              [,1]
##  [1,] 0.995651087
##  [2,] 2.004949735
##  [3,] 2.997236258
##  [4,] 4.003559121
##  [5,] 4.992983838
##  [6,] 6.004351125
##  [7,] 6.999959828
##  [8,] 7.995941860
##  [9,] 9.005608807
## [10,] 9.971903887</code>

Lets display a better version of the results

<code>cleanoutput &lt;- cbind(testdata,sqrt(testdata),
                     as.data.frame(net.results$net.result))
colnames(cleanoutput) &lt;- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)</code>

<code>##    Input Expected Output Neural Net Output
## 1      1               1       0.995651087
## 2      4               2       2.004949735
## 3      9               3       2.997236258
## 4     16               4       4.003559121
## 5     25               5       4.992983838
## 6     36               6       6.004351125
## 7     49               7       6.999959828
## 8     64               8       7.995941860
## 9     81               9       9.005608807
## 10   100              10       9.971903887</code>

<h3><code>sin</code> function</h3>
Generate random data and the dependent variable

<code>x &lt;- sort(runif(50, min = 0, max = 4*pi))
y &lt;- sin(x)

data &lt;- cbind(x,y)</code>

Create the neural network responsible for the sin function

<code>library(neuralnet)
sin.nn &lt;- neuralnet(y ~ x, data = data, hidden = 5, stepmax = 100000, learningrate = 10e-6,  
                    act.fct = 'logistic', err.fct = 'sse', rep = 5, lifesign = "minimal", 
                    linear.output = T)</code>

<code>## hidden: 5    thresh: 0.01    rep: 1/5    steps: stepmax  min thresh: 0.01599376894
## hidden: 5    thresh: 0.01    rep: 2/5    steps:    7943  error: 0.41295  time: 0.73 secs
## hidden: 5    thresh: 0.01    rep: 3/5    steps:   34702  error: 0.02068  time: 3.13 secs
## hidden: 5    thresh: 0.01    rep: 4/5    steps:    4603  error: 0.4004   time: 0.41 secs
## hidden: 5    thresh: 0.01    rep: 5/5    steps:    3582  error: 0.26375  time: 0.34 secs</code>

<code>## Warning: algorithm did not converge in 1 of 5 repetition(s) within the
## stepmax</code>

Visualize the neural network

<code>plot(sin.nn, rep = "best")</code>

Generate data for the prediction of the using the neural net;

<code>testdata&lt;- as.data.frame(runif(10, min=0, max=(4*pi)))
testdata</code>

<code>##    runif(10, min = 0, max = (4 * pi))
## 1                         1.564816433
## 2                         4.692188270
## 3                        10.942269605
## 4                        11.432769193
## 5                         1.528565797
## 6                         4.277983023
## 7                         7.863112004
## 8                         3.233025098
## 9                         4.212822393
## 10                       11.584672483</code>

Calculate the real value using the <code>sin</code> function

<code>testdata.result &lt;- sin(testdata)</code>

Make the prediction

<code>sin.nn.result &lt;- compute(sin.nn, testdata)
sin.nn.result$net.result</code>

<code>##                 [,1]
##  [1,]  1.04026644587
##  [2,] -0.99122081475
##  [3,] -0.77154683268
##  [4,] -0.80702735515
##  [5,]  1.03394587608
##  [6,] -0.91997356615
##  [7,]  1.02031970677
##  [8,] -0.08226873533
##  [9,] -0.89463523567
## [10,] -0.81283835083</code>

Compare with the real values:

<code>better &lt;- cbind(testdata, sin.nn.result$net.result, testdata.result, (sin.nn.result$net.result-testdata.result))
colnames(better) &lt;- c("Input", "NN Result", "Result", "Error")

better</code>

<code>##           Input      NN Result         Result           Error
## 1   1.564816433  1.04026644587  0.99998212049  0.040284325379
## 2   4.692188270 -0.99122081475 -0.99979597259  0.008575157839
## 3  10.942269605 -0.77154683268 -0.99857964177  0.227032809091
## 4  11.432769193 -0.80702735515 -0.90594290260  0.098915547446
## 5   1.528565797  1.03394587608  0.99910842368  0.034837452408
## 6   4.277983023 -0.91997356615 -0.90712021799 -0.012853348159
## 7   7.863112004  1.02031970677  0.99995831846  0.020361388309
## 8   3.233025098 -0.08226873533 -0.09130510334  0.009036368006
## 9   4.212822393 -0.89463523567 -0.87779026852 -0.016844967152
## 10 11.584672483 -0.81283835083 -0.83144207031  0.018603719479</code>

Calculate the RMSE:

<code>library(Metrics)
rmse(better$Result, better$`NN Result`)</code>

<code>## [1] 0.08095028855</code>

Plot the results:

<code>plot(x,y)
plot(sin, 0, (4*pi), add=T)
x1 &lt;- seq(0, 4*pi, by=0.1)
lines(x1, compute(sin.nn, data.frame(x=x1))$net.result, col="green")</code>


<h3>A classification problem</h3>
Using the <code>iris</code> dataset

<code>data(iris)
iris.dataset &lt;- iris</code>

Check what is inside the dataset:

<code>head(iris.dataset)</code>

<code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code>

Change the dataset so we are able to predict classes:

<code>iris.dataset$setosa &lt;- iris.dataset$Species=="setosa"
iris.dataset$virginica = iris.dataset$Species == "virginica"
iris.dataset$versicolor = iris.dataset$Species == "versicolor"</code>

Separate into train and test data:

<code>train &lt;- sample(x = nrow(iris.dataset), size = nrow(iris)*0.5)
train</code>

<code>##  [1] 116   3 137 124 100  48  28 123  99  54 129 128  96  11  97 115  53
## [18]   8 133  85  91  70  60  45 113 119  69 126 114  86 109 140  58  13
## [35]  77  57   7  61   9 111 141  39 120  98 104  88  83 106  20 147  74
## [52] 122  93  72  73 146   4  38   1  22 118 103  51  21  80  82  25  78
## [69] 148 143  14  50  23  84  40</code>

<code>iristrain &lt;- iris.dataset[train,]
irisvalid &lt;- iris.dataset[-train,]
print(nrow(iristrain))</code>

<code>## [1] 75</code>

<code>print(nrow(irisvalid))</code>

<code>## [1] 75</code>

Build the Neural Network for the classification:

<code>nn &lt;- neuralnet(setosa+versicolor+virginica ~ Sepal.Length + Sepal.Width, data=iristrain, hidden=3,  
                rep = 2, err.fct = "ce", linear.output = F, lifesign = "minimal", stepmax = 1000000)</code>

<code>## hidden: 3    thresh: 0.01    rep: 1/2    steps:   77918  error: 54.96826 time: 9.41 secs
## hidden: 3    thresh: 0.01    rep: 2/2    steps:   53687  error: 54.24648 time: 6.25 secs</code>

Let’s check the neural network that we just built

<code>plot(nn, rep="best")</code>

Let’s try to make the prediction:

<code>comp &lt;- compute(nn, irisvalid[-3:-8])
pred.weights &lt;- comp$net.result
idx &lt;- apply(pred.weights, 1, which.max)
pred &lt;- c('setosa', 'versicolor', 'virginica')[idx]
table(pred, irisvalid$Species)</code>

<code>##             
## pred         setosa versicolor virginica
##   setosa         28          0         0
##   versicolor      1         13         5
##   virginica       0          9        19</code>

<h3>AND operation</h3>
<code>AND &lt;- c(rep(0,3),1)
OR &lt;- c(0,rep(1,3))
binary.data &lt;- data.frame(expand.grid(c(0,1), c(0,1)), AND)
net &lt;- neuralnet(AND~Var1+Var2, binary.data, hidden=0, rep=10, err.fct="ce", linear.output=FALSE)
</code>

Now to validate the predictions:

<code>input &lt;- data.frame(expand.grid(c(0,1), c(0,1)))
net.results &lt;- compute(net, input)
cbind(round(net.results$net.result), AND)</code>

<code>##        AND
## [1,] 0   0
## [2,] 0   0
## [3,] 0   0
## [4,] 1   1</code>

<h3>sqrt example</h2>
inputData = as.data.frame(runif(550,0,100))
outputData = sqrt(inputData)
trainData = cbind(inputData, outputData)
colnames(trainData) = c("In","Out")
sqrtModel = neuralnet(Out~In,trainData,hidden = 10, threshold = 0.01)

testData = as.data.frame((0.5:9)^2)
testResult = compute(sqrtModel, testData)
testResult$net.result
testResult$net.result^2
pctError = 100-(100*testResult$net.result^2/testData)


<h2>neuralnet examples</h2>
<a href="https://www.rdocumentation.org/packages/neuralnet/versions/1.44.2/topics/neuralnet" class="whitebut ">neuralnet: Training of neural networks</a>
<a href="https://www.datacamp.com/community/tutorials/neural-network-models-r" class="whitebut ">Neural Network Models Activation Functions</a>
<a href="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788397872/1/ch01lvl1sec24/simple-example-using-r-neural-net-library-neuralnet" class="whitebut ">Simple example neuralnet()</a>
<a href="https://www.r-bloggers.com/2015/09/fitting-a-neural-network-in-r-neuralnet-package/" class="whitebut ">Fitting a neural network</a>
<a href="https://rstudio-pubs-static.s3.amazonaws.com/341320_a4aea4aa0f6c47f2b7242e0bee322683.html" class="whitebut ">Neural Net Package Examples</a>
<a href="https://www.r-bloggers.com/2015/09/fitting-a-neural-network-in-r-neuralnet-package/" class="whitebut ">Fitting a neural network</a>
<a href="https://medium.com/@brscntyz/neural-network-in-r-e275302b6e44" class="whitebut ">Neural Network in R</a>
<a href="https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network-in-r/" class="whitebut ">Creating & Visualizing Neural Network in R</a>

Examples
require(neuralnet)
TKS=c(20,10,30,20,80,30,10,30,20,80,30,30,10,30,20,80,30,30)
CSS=c(90,20,40,50,50,80,50,50,80,50,50,20,40,50,50,80,50,50)
Placed=c(1,0,0,0,1,1)
df=data.frame(TKS,CSS,Placed)

nn=neuralnet(
   Placed~TKS+CSS,
   data=df, hidden=3, act.fct = "logistic",
   linear.output = FALSE)

TKS=c(10,20,55,25,30,20)
CSS=c(15,50,30,35,20,80)
test=data.frame(TKS,CSS)

Predict=compute(nn,test)
ifelse(Predict$net.result>0.5, 1, 0)

Examples
# Binary classification
nn <- neuralnet(Species == "setosa" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)

# Multiclass classification
nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)

# Custom activation function
softplus <- function(x) log(1 + exp(x))
nn <- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width, iris, 
                linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)


<h2>Commonly used Machine Learning Algorithms (with Python and R Codes)</h2>
<h3>Overview</h3>
1. Linear Regression
2. Logistic Regression
3. Decision Tree
4. SVM (Support Vector Machine)
5. Naive Bayes
6. kNN (k- Nearest Neighbors)
7. K-Means
8. Random Forest
9. Dimensionality Reduction Algorithms
10. Gradient Boosting Algorithms

Major focus on commonly used <a href="https://www.analyticsvidhya.com/machine-learning/?utm_source=blog&amp;utm_medium=commonly-used-machine-learning-algorithms" target="_blank" rel="noopener noreferrer">machine learning</a> algorithms
Algorithms covered- Linear regression, logistic regression, Naive Bayes, kNN, Random forest, etc.
Learn both theory and implementation of these algorithms in R and python

<h3>Introduction</h3>
We are probably living in the most defining period of human history. 
The period when computing moved from large mainframes to PCs to cloud. 
But what makes it defining is not what has happened, but what is coming our way in years to come.

What makes this period exciting and enthralling for someone like me is the democratization of the various tools and techniques, which followed the boost in computing. 
Welcome to the world of <a href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog&amp;utm_medium=essentialMLalgorithmsarticle">data science</a>!

Today, as a data scientist, I can build data-crunching machines with complex algorithms for a few dollars per hour. 
But reaching here wasn't easy! I had my dark days and nights.

<em>Are you a beginner looking for a place to start your data science journey? Presenting two comprehensive courses, full of knowledge and data science learning, curated just for you to learn data science (using Python) from scratch:</em>

<em><a href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog&amp;utm_medium=essentialMLalgorithmsarticle" target="_blank" rel="noopener noreferrer">Introduction to Data Science</a></em>
<em><a href="https://courses.analyticsvidhya.com/bundles/data-science-beginners-with-interview" target="_blank" rel="noopener noreferrer">Certified Program: Data Science for Beginners (with Interviews)</a></em>

<h3>Who can benefit the most from this guide?</h3>
<h5>What I am giving out today is probably the most valuable guide, I have ever created.</h5>

The idea behind creating this guide is to simplify the journey of aspiring data scientists and <a href="https://www.analyticsvidhya.com/machine-learning/?utm_source=blog&amp;utm_medium=commonly-used-machine-learning-algorithms" target="_blank" rel="noopener noreferrer">machine learning</a> enthusiasts across the world. 
Through this guide, I will enable you to work on machine learning problems and gain from experience. 
<strong>I am providing a high-level understanding of various machine learning algorithms along with R &amp; Python codes to run them. 
These should be sufficient to get your hands dirty.</strong>

Essentials of machine learning algorithms with implementation in R and Python

I have deliberately skipped the statistics behind these techniques, as you don't need to understand them at the start. 
So, if you are looking for statistical understanding of these algorithms, you should look elsewhere. 
But, if you are looking to equip yourself to start building machine learning project, you are in for a treat.

<h3>Broadly, there are 3 <a href="https://www.analyticsvidhya.com/machine-learning/?utm_source=blog&amp;utm_medium=commonly-used-machine-learning-algorithms" target="_blank" rel="noopener noreferrer">types of Machine Learning</a> Algorithms</h3>
<h4>1. Supervised Learning</h4>

<strong>How it works:</strong> This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). 
Using these set of variables, we generate a function that map inputs to desired outputs. 
The training process continues until the model achieves a desired level of accuracy on the training data. 
Examples of Supervised Learning: Regression, <a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="noopener noreferrer">Decision Tree</a>, <a href="https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/" target="_blank" rel="noopener noreferrer">Random Forest</a>, KNN, Logistic Regression etc.

<h4>2. Unsupervised Learning</h4>

<strong>How it works: </strong>In this algorithm, we do not have any target or outcome variable to predict / estimate. 
It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. 
Examples of Unsupervised Learning: Apriori algorithm, K-means.

<h4>3. Reinforcement Learning:</h4>

<strong>How it works:</strong> Using this algorithm, the machine is trained to make specific decisions. 
It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. 
This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. 
Example of Reinforcement Learning: Markov Decision Process

<h3><strong>List of Common Machine Learning Algorithms</strong></h3>

Here is the list of commonly used machine learning algorithms. 
These algorithms can be applied to almost any data problem:

Linear Regression
Logistic Regression
Decision Tree
SVM
Naive Bayes
kNN
K-Means
Random Forest
Dimensionality Reduction Algorithms
Gradient Boosting algorithms
GBM
XGBoost
LightGBM
CatBoost

<h3>1. Linear Regression</h3>

It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). 
Here, we establish relationship between independent and dependent variables by fitting a best line. 
This best fit line is known as regression line and represented by a linear equation Y= a *X + b.

The best way to understand linear regression is to relive this experience of childhood. 
Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. 
This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.

In this equation:

Y – Dependent Variable
a – Slope
X – Independent variable
b – Intercept

These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.

Look at the below example. 
Here we have identified the best fit line having linear equation <strong>y=0.2811x+13.9</strong>. 
Now using this equation, we can find the weight, knowing the height of a person.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Linear_Regression.png">

Linear Regression is mainly of two types: Simple Linear Regression and Multiple Linear Regression. 
Simple Linear Regression is characterized by one independent variable. 
And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. 
While finding the best fit line, you can fit a polynomial or curvilinear regression. 
And these are known as polynomial or curvilinear regression.

Here's a coding window to try out your hand and build your own linear regression model in Python:

<strong>R Code</strong>

#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays
x_train &lt;- input_variables_values_training_datasets
y_train &lt;- target_variables_values_training_datasets
x_test &lt;- input_variables_values_test_datasets
x &lt;- cbind(x_train,y_train)
# Train the model using the training sets and check score
linear &lt;- lm(y_train ~ ., data = x)
summary(linear)
#Predict Output
predicted= predict(linear,x_test)

<h3>2. Logistic Regression</h3>

Don't get confused by its name! It is a classification not a regression algorithm. 
It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). 
In simple words, it predicts the probability of occurrence of an event by fitting data to a <a href="https://en.wikipedia.org/wiki/Logistic_function" target="_blank" rel="nofollow noopener noreferrer">logit function</a>. 
Hence, it is also known as <strong>logit regression</strong>. 
Since, it predicts the probability, its output values lies between 0 and 1 (as expected).

Again, let us try and understand this through a simple example.

Let's say your friend gives you a puzzle to solve. 
There are only 2 outcome scenarios – either you solve it or you don't. 
Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. 
The outcome to this study would be something like this – if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. 
On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. 
This is what Logistic Regression provides you.

Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.

odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk

Above, p is the probability of presence of the characteristic of interest. 
It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).

Now, you may ask, why take a log? For the sake of simplicity, let's just say that this is one of the best mathematical way to replicate a step function. 
I can go in more details, but that will beat the purpose of this article.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Logistic_Regression.png">
Build your own logistic regression model in Python here and check the accuracy:

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Logistic-Regression?lite=true">﻿</iframe>

<strong>R Code</strong>

x &lt;- cbind(x_train,y_train)
# Train the model using the training sets and check score
logistic &lt;- glm(y_train ~ ., data = x,family='binomial')
summary(logistic)
#Predict Output
predicted= predict(logistic,x_test)

<h4>Furthermore..</h4>

There are many different steps that could be tried in order to improve the model:

including interaction terms
removing features
<a href="https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/" target="_blank" rel="noopener noreferrer">regularization techniques</a>
using a non-linear model

<h3>3. Decision Tree</h3>

This is one of my favorite algorithm and I use it quite frequently. 
It is a type of supervised learning algorithm that is mostly used for classification problems. 
Surprisingly, it works for both categorical and continuous dependent variables. 
In this algorithm, we split the population into two or more homogeneous sets. 
This is done based on most significant attributes/ independent variables to make as distinct groups as possible. 
For more details, you can read: <a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="noopener noreferrer">Decision Tree Simplified</a>.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/IkBzK.png">

source: <a href="http://stats.stackexchange.com" target="_blank" rel="nofollow noopener noreferrer">statsexchange</a>

In the image above, you can see that population is classified into four different groups based on multiple attributes to identify &#8216;if they will play or not'. 
To split the population into different heterogeneous groups, it uses various techniques like Gini, Information Gain, Chi-square, entropy.

The best way to understand how decision tree works, is to play Jezzball – a classic game from Microsoft (image below). 
Essentially, you have a room with moving walls and you need to create walls such that maximum area gets cleared off with out the balls.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/download.jpg">

So, every time you split the room with a wall, you are trying to create 2 different populations with in the same room. 
Decision trees work in very similar fashion by dividing a population in as different groups as possible.

<em>More</em>: <a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="noopener noreferrer">Simplified Version of Decision Tree Algorithms</a>

Let's get our hands dirty and code our own decision tree in Python!

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Decision-Tree?lite=true">﻿</iframe>

<strong>R Code</strong>

library(rpart)
x &lt;- cbind(x_train,y_train)
# grow tree 
fit &lt;- rpart(y_train ~ ., data = x,method="class")
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>4. SVM (Support Vector Machine)</h3>

It is a classification method. 
In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.

For example, if we only had two features like Height and Hair length of an individual, we'd first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as <strong>Support Vectors</strong>)


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM1.png">

Now, we will find some <em>line</em> that splits the data between the two differently classified groups of data. 
This will be the line such that the distances from the closest point in each of the two groups will be farthest away.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM2-300x204.png">

In the example shown above, the line which splits the data into two differently classified groups is the <em>black</em> line, since the two closest points are the farthest apart from the line. 
This line is our classifier. 
Then, depending on where the testing data lands on either side of the line, that's what class we can classify the new data as.

More: <a href="https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/" target="_blank" rel="noopener noreferrer">Simplified Version of Support Vector Machine</a>

<strong>Think of this algorithm as playing JezzBall in n-dimensional space. 
The tweaks in the game are:</strong>

You can draw lines/planes at any angles (rather than just horizontal or vertical as in the classic game)
The objective of the game is to segregate balls of different colors in different rooms.
And the balls are not moving.

Try your hand and design an SVM model in Python through this coding window:

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Support-Vector-Machine?lite=true">﻿</iframe>
<h5></h5>

<strong>R Code</strong>

library(e1071)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-svm(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>5. Naive Bayes</h3>

It is a classification technique based on <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="nofollow noopener noreferrer">Bayes’ theorem</a> with an assumption of independence between predictors. 
In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. 
For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. 
Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.

<a href="https://courses.analyticsvidhya.com/courses/naive-bayes?utm_source=blog&amp;utm_medium=common-machine-learning-algorithms">Naive Bayesian</a> model is easy to build and particularly useful for very large data sets. 
Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.

Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). 
Look at the equation below:<br />

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_rule-300x172.png">

Here,

<i>P</i>(<i>c|x</i>) is the posterior probability of <i>class</i> (<i>target</i>) given <i>predictor</i> (<i>attribute</i>). 

<i>P</i>(<i>c</i>) is the prior probability of <i>class</i>. 

<i>P</i>(<i>x|c</i>) is the likelihood which is the probability of <i>predictor</i> given <i>class</i>. 

<i>P</i>(<i>x</i>) is the prior probability of <i>predictor</i>.

<strong>Example: </strong>Let's understand it using an example. 
Below I have a training data set of weather and corresponding target variable &#8216;Play'. 
Now, we need to classify whether players will play or not based on weather condition. 
Let's follow the below steps to perform it.

Step 1: Convert the data set to frequency table

Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41.png">

Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. 
The class with the highest posterior probability is the outcome of prediction.

<strong>Problem: </strong>Players will pay if weather is sunny, is this statement is correct?

We can solve it using above discussed method, so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)

Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64

Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.

<a href="https://courses.analyticsvidhya.com/courses/naive-bayes?utm_source=blog&amp;utm_medium=common-machine-learning-algorithms">Naive Bayes</a> uses a similar method to predict the probability of different class based on various attributes. 
This algorithm is mostly used in text classification and with problems having multiple classes.

Code a Naive Bayes classification model in Python:

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Naive-Bayes?lite=true">﻿</iframe>

<strong>R Code</strong>

library(e1071)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-naiveBayes(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>6. kNN (k- Nearest Neighbors)</h3>

It can be used for both classification and regression problems. 
However, it is more widely used in classification problems in the industry. 
K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. 
The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.

These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. 
First three functions are used for continuous function and fourth one (Hamming) for categorical variables. 
If K = 1, then the case is simply assigned to the class of its nearest neighbor. 
At times, choosing K turns out to be a challenge while performing kNN modeling.

More: <a href="http://Introduction to k-nearest neighbors : Simplified">Introduction to k-nearest neighbors : Simplified</a>.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/KNN.png">

KNN can easily be mapped to our real lives. 
If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!

<strong>Things to consider before selecting kNN:</strong>

KNN is computationally expensive
Variables should be normalized else higher range variables can bias it
Works on pre-processing stage more before going for kNN like an outlier, noise removal

<h5>Python Code</h5>
'''
The following code is for the K-Nearest Neighbors
Created by - ANALYTICS VIDHYA
'''
# importing required libraries
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# read the train and test dataset
train_data = pd.read_csv('train-data.csv')
test_data = pd.read_csv('test-data.csv')

# shape of the dataset
print('Shape of training data :',train_data.shape)
print('Shape of testing data :',test_data.shape)

# Now, we need to predict the missing target variable in the test data
# target variable - Survived

# seperate the independent and target variable on training data
train_x = train_data.drop(columns=['Survived'],axis=1)
train_y = train_data['Survived']

# seperate the independent and target variable on testing data
test_x = test_data.drop(columns=['Survived'],axis=1)
test_y = test_data['Survived']

'''
Create the object of the K-Nearest Neighbor model
You can also add other parameters and test your code here
Some parameters are : n_neighbors, leaf_size
Documentation of sklearn K-Neighbors Classifier: 

https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

 '''
model = KNeighborsClassifier()  

# fit the model with the training data
model.fit(train_x,train_y)

# Number of Neighbors used to predict the target
print('\nThe number of neighbors used to predict the target : ',model.n_neighbors)

# predict the target on the train dataset
predict_train = model.predict(train_x)
print('\nTarget on train data',predict_train) 

# Accuray Score on train dataset
accuracy_train = accuracy_score(train_y,predict_train)
print('accuracy_score on train dataset : ', accuracy_train)

# predict the target on the test dataset
predict_test = model.predict(test_x)
print('Target on test data',predict_test) 

# Accuracy Score on test dataset
accuracy_test = accuracy_score(test_y,predict_test)
print('accuracy_score on test dataset : ', accuracy_test)
<strong>R Code</strong>

library(knn)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-knn(y_train ~ ., data = x,k=5)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>7. K-Means</h3>

It is a type of unsupervised algorithm which solves the clustering problem. 
Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). 
Data points inside a cluster are homogeneous and heterogeneous to peer groups.

Remember figuring out shapes from ink blots? k means is somewhat similar this activity. 
You look at the shape and spread to decipher how many different clusters / population are present!


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/splatter_ink_blot_texture_by_maki_tak-d5p6zph-284x300.jpg">

<strong>How K-means forms cluster:</strong>

K-means picks k number of points for each cluster known as centroids.
Each data point forms a cluster with the closest centroids i.e. 
k clusters.
Finds the centroid of each cluster based on existing cluster members. 
Here we have new centroids.
As we have new centroids, repeat step 2 and 3. 
Find the closest distance for each data point from new centroids and get associated with new k-clusters. 
Repeat this process until convergence occurs i.e. 
centroids does not change.

<strong>How to determine value of K:</strong>

In K-means, we have clusters and each cluster has its own centroid. 
Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster. 
Also, when the sum of square values for all the clusters are added, it becomes total within sum of square value for the cluster solution.

We know that as the number of cluster increases, this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k, and then much more slowly after that. 
Here, we can find the optimum number of cluster.


<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Kmenas-1024x516.png">

<h5>Python Code</h5>
<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/K-Means?lite=true">﻿</iframe>

<strong>R Code</strong>

library(cluster)
fit &lt;- kmeans(X, 3) # 5 cluster solution

<h3>8. Random Forest</h3>

Random Forest is a trademark term for an ensemble of decision trees. 
In Random Forest, we've collection of decision trees (so known as &#8220;Forest&#8221;). 
To classify a new object based on attributes, each tree gives a classification and we say the tree &#8220;votes&#8221; for that class. 
The forest chooses the classification having the most votes (over all the trees in the forest).

Each tree is planted &amp; grown as follows:

If the number of cases in the training set is N, then sample of N cases is taken at random but <em>with replacement</em>. 
This sample will be the training set for growing the tree.
If there are M input variables, a number m&lt;&lt;M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. 
The value of m is held constant during the forest growing.
Each tree is grown to the largest extent possible. 
There is no pruning.

For more details on this algorithm, comparing with decision tree and tuning model parameters, I would suggest you to read these articles:

<a href="https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/">Introduction to Random forest – Simplified</a>

<a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/">Comparing a CART model to Random Forest (Part 1)</a>

<a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/">Comparing a Random Forest to a CART model (Part 2)</a>

<a href="https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/">Tuning the parameters of your Random Forest model</a>

<strong>Python Code:</strong>

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Random-Forest?lite=true">﻿</iframe>

<strong>R Code</strong>

library(randomForest)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;- randomForest(Species ~ ., x,ntree=500)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>9. Dimensionality Reduction Algorithms</h3>

In the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. 
Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.

For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.

As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. 
How'd you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others.

To know more about this algorithms, you can read &#8220;<a href="https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/">Beginners Guide To Learn Dimension Reduction Techniques</a>&#8220;.

<h5>Python Code</h5>
<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Dimensionality-Reduction-PCA?lite=true">﻿</iframe>
<h5>R Code</h5>

library(stats)
pca &lt;- princomp(train, cor = TRUE)
train_reduced  &lt;- predict(pca,train)
test_reduced  &lt;- predict(pca,test)

<h3>10. Gradient Boosting Algorithms</h3>
<h4>10.1. 
GBM</h4>

GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. 
Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. 
It combines multiple weak or average predictors to a build strong predictor. 
These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.

More: <a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/" target="_blank" rel="noopener noreferrer">Know about Boosting algorithms in detail</a>

<h5>Python Code</h5>
<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Gradient-Boosting?lite=true">﻿</iframe>
<h5>R Code</h5>

library(caret)
x &lt;- cbind(x_train,y_train)
# Fitting model
fitControl &lt;- trainControl( method = "repeatedcv", number = 4, repeats = 4)
fit &lt;- train(y ~ ., data = x, method = "gbm", trControl = fitControl,verbose = FALSE)
predicted= predict(fit,x_test,type= "prob")[,2] 

GradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the <a href="http://discuss.analyticsvidhya.com/t/what-is-the-fundamental-difference-between-randomforest-and-gradient-boosting-algorithms/2341" target="_blank" rel="noopener noreferrer">difference between these two algorithms</a>.

<h4>10.2. 
XGBoost</h4>

Another classic gradient boosting algorithm that's known to be the decisive choice between winning and losing in some Kaggle competitions.

The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques.

The support includes various objective functions, including regression, classification and ranking.

One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. 
This helps to reduce overfit modelling and has a massive support for a range of languages such as Scala, Java, R, Python, Julia and C++.

Supports distributed and widespread training on many machines that encompass GCE, AWS, Azure and Yarn clusters. 
XGBoost can also be integrated with Spark, Flink and other cloud dataflow systems with a built in cross validation at each iteration of the boosting process.

To learn more about XGBoost and parameter tuning, visit <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a>.

<strong>Python Code:</strong>

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" class="lazy" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/XGBoost?lite=true">﻿</iframe>

R Code:

require(caret)

x &lt;- cbind(x_train,y_train)

# Fitting model

TrainControl &lt;- trainControl( method = "repeatedcv", number = 10, repeats = 4)

model&lt;- train(y ~ ., data = x, method = "xgbLinear", trControl = TrainControl,verbose = FALSE)

OR 

model&lt;- train(y ~ ., data = x, method = "xgbTree", trControl = TrainControl,verbose = FALSE)

predicted &lt;- predict(model, x_test)

<h4>10.3. LightGBM</h4>

LightGBM is a gradient boosting framework that uses tree based learning algorithms. 
It is designed to be distributed and efficient with the following advantages:

Faster training speed and higher efficiency
Lower memory usage
Better accuracy
Parallel and GPU learning supported
Capable of handling large-scale data

The framework is a fast and high-performance gradient boosting one based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. 
It was developed under the Distributed Machine Learning Toolkit Project of Microsoft.

Since the LightGBM is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. 
So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.

Also, it is surprisingly very fast, hence the word &#8216;Light'.

Refer to the article to know more about LightGBM: <a href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/">https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/</a>

Python Code:

data = np.random.rand(500, 10) # 500 entities, each contains 10 features
label = np.random.randint(2, size=500) # binary target

train_data = lgb.Dataset(data, label=label)
test_data = train_data.create_valid('test.svm')

param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}
param['metric'] = 'auc'

num_round = 10
bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])

bst.save_model('model.txt')

# 7 entities, each contains 10 features
data = np.random.rand(7, 10)
ypred = bst.predict(data)

R Code:

library(RLightGBM)
data(example.binary)
#Parameters

num_iterations &lt;- 100
config &lt;- list(objective = "binary",  metric="binary_logloss,auc", learning_rate = 0.1, num_leaves = 63, tree_learner = "serial", feature_fraction = 0.8, bagging_freq = 5, bagging_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_leaf = 5.0)

#Create data handle and booster
handle.data &lt;- lgbm.data.create(x)

lgbm.data.setField(handle.data, "label", y)

handle.booster &lt;- lgbm.booster.create(handle.data, lapply(config, as.character))

#Train for num_iterations iterations and eval every 5 steps

lgbm.booster.train(handle.booster, num_iterations, 5)

#Predict
pred &lt;- lgbm.booster.predict(handle.booster, x.test)

#Test accuracy
sum(y.test == (y.pred &gt; 0.5)) / length(y.test)

#Save model (can be loaded again via lgbm.booster.load(filename))
lgbm.booster.save(handle.booster, filename = "/tmp/model.txt")

If you're familiar with the Caret package in R, this is another way of implementing the LightGBM.

require(caret)
require(RLightGBM)
data(iris)

model &lt;-caretModel.LGBM()

fit &lt;- train(Species ~ ., data = iris, method=model, verbosity = 0)
print(fit)
y.pred &lt;- predict(fit, iris[,1:4])

library(Matrix)
model.sparse &lt;- caretModel.LGBM.sparse()

#Generate a sparse matrix
mat &lt;- Matrix(as.matrix(iris[,1:4]), sparse = T)
fit &lt;- train(data.frame(idx = 1:nrow(iris)), iris$Species, method = model.sparse, matrix = mat, verbosity = 0)
print(fit)

<h4>10.4. Catboost</h4>

CatBoost is a recently open-sourced machine learning algorithm from Yandex. 
It can easily integrate with deep learning frameworks like Google's TensorFlow and Apple's Core ML.

The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be.

Make sure you handle missing data well before you proceed with the implementation.

Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.

Learn more about Catboost from this article: <a href="https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/">https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/</a>

Python Code:

import pandas as pd
import numpy as np

from catboost import CatBoostRegressor

#Read training and testing files
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

#Imputing missing values for both train and test
train.fillna(-999, inplace=True)
test.fillna(-999,inplace=True)

#Creating a training set for modeling and validation set to check model performance
X = train.drop(['Item_Outlet_Sales'], axis=1)
y = train.Item_Outlet_Sales

from sklearn.model_selection import train_test_split

X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)
categorical_features_indices = np.where(X.dtypes != np.float)[0]

#importing library and building model
from catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')

model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)

submission = pd.DataFrame()

submission['Item_Identifier'] = test['Item_Identifier']
submission['Outlet_Identifier'] = test['Outlet_Identifier']
submission['Item_Outlet_Sales'] = model.predict(test)

R Code:

set.seed(1)

require(titanic)

require(caret)

require(catboost)

tt &lt;- titanic::titanic_train[complete.cases(titanic::titanic_train),]

data &lt;- as.data.frame(as.matrix(tt), stringsAsFactors = TRUE)

drop_columns = c("PassengerId", "Survived", "Name", "Ticket", "Cabin")

x &lt;- data[,!(names(data) %in% drop_columns)]y &lt;- data[,c("Survived")]

fit_control &lt;- trainControl(method = "cv", number = 4,classProbs = TRUE)

grid &lt;- expand.grid(depth = c(4, 6, 8),learning_rate = 0.1,iterations = 100, l2_leaf_reg = 1e-3,            rsm = 0.95, border_count = 64)

report &lt;- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = TRUE, preProc = NULL,tuneGrid = grid, trControl = fit_control)

print(report)

importance &lt;- varImp(report, scale = FALSE)

print(importance)

<h3>Projects</h3>

Now, its time to take the plunge and actually play with some other real datasets. 
So are you ready to take on the challenge? Accelerate your data science journey with the following Practice Problems:

<table border="1">
<tbody>
<tr><td>

<img class="lazy" data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2016/01/food_500x250-300x150.jpg"></td><td><a href="https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=common-machine-learning-algorithms&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem: Food Demand Forecasting Challenge</a></td><td>Predict the demand of meals for a meal delivery company</td></tr>
<tr><td>

<img class="lazy" data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2016/01/hr_500x250-300x150.jpg"></td><td><a href="https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=common-machine-learning-algorithms&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem: HR Analytics Challenge</a></td><td>Identify the employees most likely to get promoted</td></tr>
<tr><td>

<img class="lazy" data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2016/01/upvote_500x250-300x150.jpg"></td><td><a href="https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=common-machine-learning-algorithms&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem: Predict Number of Upvotes</a></td><td>Predict number of upvotes on a query asked at an online question &amp; answer platform</td></tr>
</tbody>
</table>

<h2>Files Manipulation</h2>
Creating Files and Directories
dir.create("new_folder")
file.create("new_text_file.txt")

Copying a file / folder
file.copy("source_file.txt", "destination_folder")

# list all files in current directory
list.files()
 
# list all files in another directory
list.files("C:/path/to/somewhere/else")

# delete a file
unlink("some_file.csv")

# check if a file exists
file.exists("C:/path/to/file/some_file.txt")

Get the base name of a file
basename("C:/path/to/file.txt")

get the directory name of a file
dirname("C:/path/to/file.txt")

Get a file’s extension
library(tools)
file_ext("C:/path/to/file.txt") # returns "txt"

launch a file
shell.exec("C:/path/to/file/some_file.txt")
 
# or file.show to launch a file
file.show("C:/path/to/file/some_file.txt")

file.rename
 file.rename(statusList, paste0(format(Sys.Date(), format="%m%d")," ",format(Sys.time(), "%H%M")," ",statusList))

<h2>loadhistory</h2>
loadhistory(file = ".Rhistory")
savehistory(file = ".Rhistory")

<h4>Basics</h4>
<h2>Vectors, lists, matrices, data frames</h2>

# Goals: A first look at R objects - vectors, lists, matrices, data frames.

# To make vectors "x" "y" "year" and "names"
x = c(2,3,7,9)
y = c(9,7,3,2)
year = 1990:1993
names = c("payal", "shraddha", "kritika", "itida")
# Accessing the 1st and last elements of y --
y[1]
y[length(y)]

# To make a list "person" --
person = list(name="payal", x=2, y=9, year=1990)
person
# Accessing things inside a list --
person$name
person$x

# To make a matrix, pasting together the columns "year" "x" and "y"
# The verb cbind() stands for "column bind"
cbind(year, x, y)

# To make a "data frame", which is a list of vectors of the same length --
D = data.frame(names, year, x, y)
nrow(D)
# Accessing one of these vectors
D$names
# Accessing the last element of this vector
D$names[nrow(D)]
# Or equally,
D$names[length(D$names)]


<h2>Sorting</h2>

# Goal: To do sorting.
#
# The approach here needs to be explained. If `i' is a vector of
# integers, then the data frame D[i,] picks up rows from D based
# on the values found in `i'.
#
# The order() function makes an integer vector which is a correct
# ordering for the purpose of sorting.

D = data.frame(x=c(1,2,3,1), y=c(7,19,2,2))
D

# Sort on x
indexes = order(D$x)
D[indexes,]

# Print out sorted dataset, sorted in reverse by y
D[rev(order(D$y)),]


<h2>Prices and returns</h2>

# Goal: Prices and returns

# I like to multiply returns by 100 so as to have "units in percent".
# In other words, I like it for 5% to be a value like 5 rather than 0.05.

---------------
# I. Simulate random-walk prices, switch between prices & returns.
---------------
# Simulate a time-series of PRICES drawn from a random walk
# where one-period returns are i.i.d. N(mu, sigma^2).
ranrw = function(mu, sigma, p0=100, T=100) {
  cumprod(c(p0, 1 + (rnorm(n=T, mean=mu, sd=sigma)/100)))
}
prices2returns = function(x) {
  100*diff(log(x))
}
returns2prices = function(r, p0=100) {
  c(p0, p0 * exp(cumsum(r/100)))
}

cat("Simulate 25 points from a random walk starting at 1500 --\n")
p = ranrw(0.05, 1.4, p0=1500, T=25)
    # gives you a 25-long series, starting with a price of 1500, where
    # one-period returns are N(0.05,1.4^2) percent.
print(p)

cat("Convert to returns--\n")
r = prices2returns(p)
print(r)

cat("Go back from returns to prices --\n")
goback = returns2prices(r, 1500)
print(goback)

---------------
# II. Plenty of powerful things you can do with returns....
---------------
summary(r); sd(r)                       # summary statistics
plot(density(r))                        # kernel density plot
acf(r)                                  # Autocorrelation function
ar(r)                                   # Estimate a AIC-minimising AR model
Box.test(r, lag=2, type="Ljung")        # Box-Ljung test
library(tseries)
runs.test(factor(sign(r)))              # Runs test
bds.test(r)                             # BDS test.

---------------
# III. Visualisation and the random walk
---------------
# I want to obtain intuition into what kinds of price series can happen,
# given a starting price, a mean return, and a given standard deviation.
# This function simulates out 10000 days of a price time-series at a time,
# and waits for you to click in the graph window, after which a second
# series is painted, and so on. Make the graph window very big and
# sit back and admire.
# The point is to eyeball many series and thus obtain some intuition
# into what the random walk does.
visualisation = function(p0, s, mu, labelstring) {
  N = 10000
  x = (1:(N+1))/250                        # Unit of years
  while (1) {
    plot(x, ranrw(mu, s, p0, N), ylab="Level", log="y",
         type="l", col="red", xlab="Time (years)",
         main=paste("40 years of a process much like", labelstring))
    grid()
    z=locator(1)
  }
}

# Nifty -- assuming sigma of 1.4% a day and E(returns) of 13% a year
visualisation(2600, 1.4, 13/250, "Nifty")

# The numerical values here are used to think about what the INR/USD
# exchange rate would have looked like if it started from 31.37, had
# a mean depreciation of 5% per year, and had the daily vol of a floating
# exchange rate like EUR/USD.
visualisation(31.37, 0.7, 5/365, "INR/USD (NOT!) with daily sigma=0.7")
# This is of course not like the INR/USD series in the real world -
# which is neither a random walk nor does it have a vol of 0.7% a day.

# The numerical values here are used to think about what the USD/EUR
# exchange rate, starting with 1, having no drift, and having the observed
# daily vol of 0.7. (This is about right).
visualisation(1, 0.7, 0, "USD/EUR with no drift")

---------------
# IV. A monte carlo experiment about the runs test
---------------
# Measure the effectiveness of the runs test when faced with an
# AR(1) process of length 100 with a coeff of 0.1
set.seed(101)
one.ts = function() {arima.sim(list(order = c(1,0,0), ar = 0.1), n=100)}
table(replicate(1000, runs.test(factor(sign(one.ts())))$p.value &lt; 0.05))
# We find that the runs test throws up a prob value of below 0.05
# for 91 out of 1000 experiments.
# Wow! :-)
# To understand this, you need to look up the man pages of:
#    set.seed, arima.sim, sign, factor, runs.test, replicate, table.
# e.g. say ?replicate


<h2>Writing functions</h2>

# Goals: To write functions
#        To write functions that send back multiple objects.

# FIRST LEARN ABOUT LISTS --
X = list(height=5.4, weight=54)
print("Use default printing --")
print(X)
print("Accessing individual elements --")
cat("Your height is ", X$height, " and your weight is ", X$weight, "\n")

# FUNCTIONS --
square = function(x) {
  return(x*x)
}
cat("The square of 3 is ", square(3), "\n")

                 # default value of the arg is set to 5.
cube = function(x=5) {
  return(x*x*x);
}
cat("Calling cube with 2 : ", cube(2), "\n")    # will give 2^3
cat("Calling cube        : ", cube(), "\n")     # will default to 5^3.

# LEARN ABOUT FUNCTIONS THAT RETURN MULTIPLE OBJECTS --
powers = function(x) {
  parcel = list(x2=x*x, x3=x*x*x, x4=x*x*x*x);
  return(parcel);
}

X = powers(3);
print("Showing powers of 3 --"); print(X);

# WRITING THIS COMPACTLY (4 lines instead of 7)

powerful = function(x) {
  return(list(x2=x*x, x3=x*x*x, x4=x*x*x*x));
}
print("Showing powers of 3 --"); print(powerful(3));

# In R, the last expression in a function is, by default, what is
# returned. So you could equally just say:
powerful = function(x) {list(x2=x*x, x3=x*x*x, x4=x*x*x*x)}


<h2>Amazing R vector notation</h2>

# Goal: The amazing R vector notation.

cat("EXAMPLE 1: sin(x) for a vector --\n")
# Suppose you have a vector x --
x = c(0.1,0.6,1.0,1.5)

# The bad way --
n = length(x)
r = numeric(n)
for (i in 1:n) {
  r[i] = sin(x[i])
}
print(r)

# The good way -- don't use loops --
print(sin(x))


cat("\n\nEXAMPLE 2: Compute the mean of every row of a matrix --\n")
# Here's another example. It isn't really about R; it's about thinking in
# matrix notation. But still.
# Let me setup a matrix --
N=4; M=100;
r = matrix(runif(N*M), N, M)

# So I face a NxM matrix
#               [r11 r12 ... r1N]
#               [r21 r22 ... r2N]
#               [r32 r32 ... r3N]
# My goal: each column needs to be reduced to a mean.

# Method 1 uses loops:
mean1 = numeric(M)
for (i in 1:M) {
  mean1[i] = mean(r[,i])
}

# Alternatively, just say:
mean2 = rep(1/N, N) %*% r               # Pretty!

# The two answers are the same --
all.equal(mean1,mean2[,])
#
# As an aside, I should say that you can do this directly by using
# the rowMeans() function. But the above is more about pedagogy rather
# than showing you how to get rowmeans.


cat("\n\nEXAMPLE 3: Nelson-Siegel yield curve\n")
# Write this asif you're dealing with scalars --
# Nelson Siegel function
nsz = function(b0, b1, b2, tau, t) {
  tmp = t/tau
  tmp2 = exp(-tmp)
  return(b0 + ((b1+b2)*(1-tmp2)/(tmp)) - (b2*tmp2))
}

timepoints = c(0.01,1:5)

# The bad way:
z = numeric(length(timepoints))
for (i in 1:length(timepoints)) {
  z[i] = nsz(14.084,-3.4107,0.0015,1.8832,timepoints[i])
}
print(z)

# The R way --
print(z = nsz(14.084,-3.4107,0.0015,1.8832,timepoints))


cat("\n\nEXAMPLE 3: Making the NPV of a bond--\n")
# You know the bad way - sum over all cashflows, NPVing each.
# Now look at the R way.
C = rep(100, 6)
nsz(14.084,-3.4107,0.0015,1.8832,timepoints)        # Print interest rates
C/((1.05)^timepoints)                               # Print cashflows discounted &#64; 5%
C/((1 + (0.01*nsz(14.084,-3.4107,0.0015,1.8832,timepoints))^timepoints)) # Using NS instead of 5%
# NPV in two different ways --
C %*% (1 + (0.01*nsz(14.084,-3.4107,0.0015,1.8832,timepoints)))^-timepoints
sum(C * (1 + (0.01*nsz(14.084,-3.4107,0.0015,1.8832,timepoints)))^-timepoints)
# You can drop back to a flat yield curve at 5% easily --
sum(C * 1.05^-timepoints)

# Make a function for NPV --
npv = function(C, timepoints, r) {
  return(sum(C * (1 + (0.01*r))^-timepoints))
}
npv(C, timepoints, 5)

# Bottom line: Here's how you make the NPV of a bond with cashflows C
# at timepoints timepoints when the zero curve is a Nelson-Siegel curve --
npv(C, timepoints, nsz(14.084,-3.4107,0.0015,1.8832,timepoints))
# Wow!

# ---------------------------------------------------------------------------
# Elegant vector notation is amazingly fast (in addition to being beautiful)
N = 1e5
x = runif(N, -3,3)
y = runif(N)

method1 = function(x,y) {
  tmp = NULL
  for (i in 1:N) {
    if (x[i] &lt; 0) tmp = c(tmp, y[i])
  }
  tmp
}

method2 = function(x,y) {
  y[x &lt; 0]
}

s1 = system.time(ans1 = method1(x,y))
s2 = system.time(ans2 = method2(x,y))
all.equal(ans1,ans2)
s1/s2           # On my machine it's 2000x faster


<h2>Amazing R indexing notation</h2>

# Goal: To show amazing R indexing notation, and the use of is.na()

x = c(2,7,9,2,NA,5)                 # An example vector to play with.

# Give me elems 1 to 3 --
x[1:3]

# Give me all but elem 1 --
x[-1]

# Odd numbered elements --
indexes = seq(1,6,2)
x[indexes]
# or, more compactly,
x[seq(1,6,2)]

# Access elements by specifying "on" / "off" through booleans --
require = c(TRUE,TRUE,FALSE,FALSE,FALSE,FALSE)
x[require]
# Short vectors get reused! So, to get odd numbered elems --
x[c(TRUE,FALSE)]

# Locate missing data --
is.na(x)

# Replace missing data by 0 --
x[is.na(x)] = 0
x

# Similar ideas work for matrices --
y = matrix(c(2,7,9,2,NA,5), nrow=2)
y

# Make a matrix containing columns 1 and 3 --
y[,c(1,3)]

# Let us see what is.na(y) does --
is.na(y)
str(is.na(y))
# So is.na(y) gives back a matrix with the identical structure as that of y.
# Hence I can say
y[is.na(y)] = -1
y


<h2>Making latex tabular objects</h2>

# Goal: To make latex tabular out of an R matrix

# Setup a nice R object:
m = matrix(rnorm(8), nrow=2)
rownames(m) = c("Age", "Weight")
colnames(m) = c("Person1", "Person2", "Person3", "Person4")
m

# Translate it into a latex tabular:
library(xtable)
xtable(m, digits=rep(3,5))

# Production latex code that goes into a paper or a book --
print(xtable(m,
             caption="String",
             label="t:"),
             type="latex",
             file="blah.gen",
             table.placement="tp",
             latex.environments=c("center", "footnotesize"))
# Now you do \input{blah.gen} in your latex file.
# You're lazy, and want to use R to generate latex tables for you?
data = cbind(
              c(7,9,11,2),
              c(2,4,19,21)
              )
colnames(data) = c("a","b")
rownames(data) = c("x","y","z","a")
xtable(data)

# or you could do
data = rbind(
              c(7,2),
              c(9,4),
              c(11,19),
              c(2,21)
              )
# and the rest goes through identically.


<h2>Associative arrays / hashes</h2>

# Goal: Associative arrays (as in awk) or hashes (as in perl).
#       Or, more generally, adventures in R addressing.

# Here's a plain R vector:
x = c(2,3,7,9)
# But now I tag every elem with labels:
names(x) = c("kal","sho","sad","aja")
# Associative array operations:
x["kal"] = 12
# Pretty printing the entire associative array:
x

# This works for matrices too:
m = matrix(runif(10), nrow=5)
rownames(m) = c("violet","indigo","blue","green","yellow")
colnames(m) = c("Asia","Africa")
# The full matrix --
m
# Or even better --
library(xtable)
xtable(m)

# Now address symbolically --
m[,"Africa"]
m["indigo",]
m["indigo","Africa"]

# The "in" operator, as in awk --
for (colour in c("yellow", "orange", "red")) {
  if (colour %in% rownames(m)) {
    cat("For Africa and ", colour, " we have ", m[colour, "Africa"], "\n")
  } else {
    cat("Colour ", colour, " does not exist in the hash.\n")
  }
}

# This works for data frames also --
D = data.frame(m)
D
# Look closely at what happened --
str(D)                                  # The colours are the rownames(D).

# Operations --
D$Africa
D[,"Africa"]
D["yellow",]
# or
subset(D, rownames(D)=="yellow")

colnames(D) = c("Antarctica","America")
D
D$America


<h2>Matrix notation (portfolio computations in financial economics)</h2>

# Goal: Utilise matrix notation
#       We use the problems of portfolio analysis as an example.

# Prices of 4 firms to play with, at weekly frequency (for calendar 2004) --
p = structure(c(300.403, 294.604, 291.038, 283.805, 270.773, 275.506, 292.271, 292.837, 284.872, 295.037, 280.939, 259.574, 250.608, 268.84, 266.507, 263.94, 273.173, 238.609, 230.677, 192.847, 219.078, 201.846, 210.279, 193.281, 186.748, 197.314, 202.813, 204.08, 226.044, 242.442, 261.274, 269.173, 256.05, 259.75, 243, 250.3, 263.45, 279.5, 289.55, 291.95, 302.1, 284.4, 283.5, 287.8, 298.3, 307.6, 307.65, 311.9, 327.7, 318.1, 333.6, 358.9, 385.1, 53.6, 51.95, 47.65, 44.8, 44.85, 44.3, 47.1, 44.2, 41.8, 41.9, 41, 35.3, 33.35, 35.6, 34.55, 35.55, 40.05, 35, 34.85, 28.95, 31, 29.25, 29.05, 28.95, 24.95, 26.15, 28.35, 29.4, 32.55, 37.2, 39.85, 40.8, 38.2, 40.35, 37.55, 39.4, 39.8, 43.25, 44.75, 47.25, 49.6, 47.6, 46.35, 49.4, 49.5, 50.05, 50.5, 51.85, 56.35, 54.15, 58, 60.7, 62.7, 293.687, 292.746, 283.222, 286.63, 259.774, 259.257, 270.898, 250.625, 242.401, 248.1, 244.942, 239.384, 237.926, 224.886, 243.959, 270.998, 265.557, 257.508, 258.266, 257.574, 251.917, 250.583, 250.783, 246.6, 252.475, 266.625, 263.85, 249.925, 262.9, 264.975, 273.425, 275.575, 267.2, 282.25, 284.25, 290.75, 295.625, 296.25, 291.375, 302.225, 318.95, 324.825, 320.55, 328.75, 344.05, 345.925, 356.5, 368.275, 374.825, 373.525, 378.325, 378.6, 374.4, 1416.7, 1455.15, 1380.97, 1365.31, 1303.2, 1389.64, 1344.05, 1266.29, 1265.61, 1312.17, 1259.25, 1297.3, 1327.38, 1250, 1328.03, 1347.46, 1326.79, 1286.54, 1304.84, 1272.44, 1227.53, 1264.44, 1304.34, 1277.65, 1316.12, 1370.97, 1423.35, 1382.5, 1477.75, 1455.15, 1553.5, 1526.8, 1479.85, 1546.8, 1565.3, 1606.6, 1654.05, 1689.7, 1613.95, 1703.25, 1708.05, 1786.75, 1779.75, 1906.35, 1976.6, 2027.2, 2057.85, 2029.6, 2051.35, 2033.4, 2089.1, 2065.2, 2091.7), .Dim = c(53, 4), .Dimnames = list(NULL, c("TISCO", "SAIL", "Wipro", "Infosys")))
# Shift from prices to returns --
r = 100*diff(log(p))

# Historical expected returns --
colMeans(r)
# Historical correlation matrix --
cor(r)
# Historical covariance matrix --
S = cov(r)
S

# Historical portfolio variance for a stated portfolio of 20%,20%,30%,30% --
w = c(.2, .2, .3, .3)
t(w) %*% S %*% w

# The portfolio optimisation function in tseries --
library(tseries)
optimised = portfolio.optim(r)         # This uses the historical facts from r
optimised$pw                            # Weights
optimised$pm                            # Expected return using these weights
optimised$ps                            # Standard deviation of optimised port.


<h2>Handling missing data</h2>

# Goal:
#       A stock is traded on 2 exchanges.
#       Price data is missing at random on both exchanges owing to non-trading.
#       We want to make a single price time-series utilising information
#          from both exchanges. I.e., missing data for exchange 1 will
#          be replaced by information for exchange 2 (if observed).

# Let's create some example data for the problem.
e1 = runif(15)                         # Prices on exchange 1
e2 = e1 + 0.05*rnorm(15)               # Prices on exchange 2.
cbind(e1, e2)
# Blow away 5 points from each at random.
e1[sample(1:15, 5)] = NA
e2[sample(1:15, 5)] = NA
cbind(e1, e2)

# Now how do we reconstruct a time-series that tries to utilise both?
combined = e1                          # Do use the more liquid exchange here.
missing = is.na(combined)
combined[missing] = e2[missing]        # if it's also missing, I don't care.
cbind(e1, e2, combined)
# There you are.



<h4>Reading files</h4>
<h2>Reading a file with a few columns of numbers, and look at what is there.</h2>

# Goal: To read in a simple data file, and look around it's contents.

# Suppose you have a file "x.data" which looks like this:
#        1997,3.1,4
#        1998,7.2,19
#        1999,1.7,2
#        2000,1.1,13
# To read it in --

A = read.table("x.data", sep=",",
                col.names=c("year", "my1", "my2"))
nrow(A)                                 # Count the rows in A

summary(A$year)                         # The column "year" in data frame A
                                        # is accessed as A$year

A$newcol = A$my1 + A$my2               # Makes a new column in A
newvar = A$my1 - A$my2                 # Makes a new R object "newvar"
A$my1 = NULL                           # Removes the column "my1"

# You might find these useful, to "look around" a dataset --
str(A)
summary(A)
library(Hmisc)          # This requires that you've installed the Hmisc package
contents(A)
describe(A)


<h2>Reading a file involving dates</h2>

# Goal: To read in a simple data file where date data is present.

# Suppose you have a file "x.data" which looks like this:
#        1997-07-04,3.1,4
#        1997-07-05,7.2,19
#        1997-07-07,1.7,2
#        1997-07-08,1.1,13

A = read.table("x.data", sep=",",
                col.names=c("date", "my1", "my2"))
A$date = as.Date(A$date, format="%Y-%m-%d")
       # Say ?strptime to learn how to use "%" to specify
       # other date formats. Two examples --
       # "15/12/2002"  needs "%d/%m/%Y"
       # "03 Jun 1997" needs "%d %b %Y"

       # Actually, if you're using the ISO 8601 date format, i.e.
       # "%Y-%m-%d", that's the default setting and you don't need to
       # specify the format.

A$newcol = A$my1 + A$my2               # Makes a new column in A
newvar = A$my1 - A$my2                 # Makes a new R object "newvar"
A$my1 = NULL                           # Delete the `my1' column
summary(A)                              # Makes summary statistics


<h2>Reading in a file made by CMIE's <i>Business Beacon</i> program</h2>

# Goal: To read in files produced by CMIE's "Business Beacon".
#       This assumes you have made a file of MONTHLY data using CMIE's
#       Business Beacon program. This contains 2 columns: M3 and M0.

A = read.table(
                # Generic to all BB files --
                sep="|",                # CMIE's .txt file is pipe delimited
                skip=3,                 # Skip the 1st 3 lines
                na.strings=c("N.A.","Err"),  # The ways they encode missing data
                # Specific to your immediate situation --
                file="bb_data.text",
                col.names=c("junk", "date", "M3", "M0")
                )
A$junk = NULL                          # Blow away this column

# Parse the CMIE-style "Mmm yy" date string that's used on monthly data
A$date = as.Date(paste("1", as.character(A$date)), format="%d %b %Y")



Reading and writing both ascii files and binary files. Also, measure speed of these.

# Goal: Reading and writing ascii files, reading and writing binary files.
#       And, to measure how much faster it is working with binary files.

# First manufacture a tall data frame:
                # FYI -- runif(10) yields 10 U(0,1) random numbers.
B = data.frame(x1=runif(100000), x2=runif(100000), x3=runif(100000))
summary(B)

# Write out ascii file:
write.table(B, file = "/tmp/foo.csv", sep = ",", col.names = NA)
# Read in this resulting ascii file:
C=read.table("/tmp/foo.csv", header = TRUE, sep = ",", row.names=1)
# Write a binary file out of dataset C:
save(C, file="/tmp/foo.binary")
# Delete the dataset C:
rm(C)
# Restore from foo.binary:
load("/tmp/foo.binary")
summary(C)                              # should yield the same results
                                        # as summary(B) above.


# Now we time all these operations --
cat("Time creation of dataset:\n")
system.time({
  B = data.frame(x1=runif(100000), x2=runif(100000), x3=runif(100000))
})

cat("Time writing an ascii file out of dataset B:\n")
system.time(
            write.table(B, file = "/tmp/foo.csv", sep = ",", col.names = NA)
            )

cat("Time reading an ascii file into dataset C:\n")
system.time(
            {C=read.table("/tmp/foo.csv", header = TRUE, sep=",", row.names=1)
           })

cat("Time writing a binary file out of dataset C:\n")
system.time(save(C, file="/tmp/foo.binary"))

cat("Time reading a binary file + variablenames from /tmp/foo.binary:\n")
system.time(load("/tmp/foo.binary"))    # and then read it in from binary file


<h2>Sending an R data object to someone else</h2>file.

# Goals: Lots of times, you need to give an R object to a friend,
#        or embed data into an email.

# First I invent a little dataset --
set.seed(101)   # To make sure you get the same random numbers as me
                # FYI -- runif(10) yields 10 U(0,1) random numbers.
A = data.frame(x1=runif(10), x2=runif(10), x3=runif(10))
# Look at it --
print(A)

# Writing to a binary file that can be transported
save(A, file="/tmp/my_data_file.rda")   # You can give this file to a friend
load("/tmp/my_data_file.rda")

# Plan B - you want pure ascii, which can be put into an email --
dput(A)
# This gives you a block of R code. Let me utilise that generated code
# to create a dataset named "B".
B = structure(list(x1 = c(0.372198376338929, 0.0438248154241592,
0.709684018278494, 0.657690396532416, 0.249855723232031, 0.300054833060130,
0.584866625955328, 0.333467143354937, 0.622011963743716, 0.54582855431363
), x2 = c(0.879795730113983, 0.706874740775675, 0.731972594512627,
0.931634427979589, 0.455120594473556, 0.590319729177281, 0.820436094887555,
0.224118480458856, 0.411666829371825, 0.0386105608195066), x3 = c(0.700711545301601,
0.956837461562827, 0.213352001970634, 0.661061500199139, 0.923318882007152,
0.795719761401415, 0.0712125543504953, 0.389407767681405, 0.406451216200367,
0.659355078125373)), .Names = c("x1", "x2", "x3"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")

# Verify that A and B are near-identical --
A-B
# or,
all.equal(A,B)


<h2>Make a "zoo" object, for handling time-series data.</h2>

# Goal: Make a time-series object using the "zoo" package

A = data.frame(date=c("1995-01-01", "1995-01-02", "1995-01-03", "1995-01-06"),
                x=runif(4),
                y=runif(4))
A$date = as.Date(A$date) # yyyy-mm-dd is the default format
# So far there's nothing new - it's just a data frame. I have hand-
# constructed A but you could equally have obtained it using read.table().

# I want to make a zoo matrix out of the numerical columns of A
library(zoo)
B = A
B$date = NULL
z = zoo(as.matrix(B), order.by=A$date)
rm(A, B)

# So now you are holding "z", a "zoo" object. You can do many cool
# things with it.
# See http://www.google.com/search?hl=en&q=zoo+quickref+achim&btnI=I%27m+Feeling+Lucky

# To drop down to a plain data matrix, say
C = coredata(z)
rownames(C) = as.character(time(z))
# Compare --
str(C)
str(z)

# The above is a tedious way of doing these things, designed to give you
# an insight into what is going on. If you just want to read a file
# into a zoo object, a very short path is something like:
#        z = read.zoo(filename, format="%d %b %Y")


<h2>Exporting and importing data.</h2>

# Goal: All manner of import and export of datasets.

# Invent a dataset --
A = data.frame(
                name=c("a","b","c"),
                ownership=c("Case 1","Case 1","Case 2"),
                listed.at=c("NSE",NA,"BSE"),
                   # Firm "b" is unlisted.
                is.listed=c(TRUE,FALSE,TRUE),
                   # R convention - boolean variables are named "is.something"
                x=c(2.2,3.3,4.4),
                date=as.Date(c("2004-04-04","2005-05-05","2006-06-06"))
              )

# To a spreadsheet through a CSV file --
write.table(A,file="demo.csv",sep = ",",col.names = NA,qmethod = "double")
B = read.table("demo.csv", header = TRUE, sep = ",", row.names = 1)

# To R as a binary file --
save(A, file="demo.rda")
load("demo.rda")

# To the Open XML standard for transport for statistical data --
library(StatDataML)
writeSDML(A, "/tmp/demo.sdml")
B = readSDML("/tmp/demo.sdml")

# To Stata --
library(foreign)
write.dta(A, "/tmp/demo.dta")
B = read.dta("/tmp/demo.dta")

# foreign::write.foreign() also has a pathway to SAS and SPSS.


<h2>Reading .gz .bz2 files and URLs</h2>

# Goal: Special cases in reading files

# Reading in a .bz2 file --
read.table(bzfile("file.text.bz2"))           # Requires you have ./file.text.bz2

# Reading in a .gz file --
read.table(gzfile("file.text.gz"))            # Requires you have ./file.text.bz2

# Reading from a pipe --
mydata = read.table(pipe("awk -f filter.awk input.txt"))

# Reading from a URL --
read.table(url("http://www.mayin.org/ajayshah/A/demo.text"))

# This also works --
read.table("http://www.mayin.org/ajayshah/A/demo.text")

# Hmm, I couldn't think of how to read a .bz2 file from a URL. How about:
read.table(pipe("links -source http://www.mayin.org/ajayshah/A/demo.text.bz2 | bunzip2"))

# Reading binary files from a URL --
load(url("http://www.mayin.org/ajayshah/A/nifty_weekly_returns.rda"))


<h2>Directly reading Microsoft Excel files</h2>

# Goal: Reading in a Microsoft .xls file directly

library(gdata)
a = read.xls("file.xls", sheet=2)                # This reads in the 2nd sheet

# Look at what the cat dragged in
str(a)

# If you have a date column, you'll want to fix it up like this:
a$date = as.Date(as.character(a$X), format="%d-%b-%y")
a$X = NULL


# Also see http://tolstoy.newcastle.edu.au/R/help/06/04/25674.html for
# another path.



<h4>Graphs</h4>
<h2>A grid of multiple pictures on one screen</h2>

# Goal: To make a panel of pictures.

par(mfrow=c(3,2))                       # 3 rows, 2 columns.

# Now the next 6 pictures will be placed on these 6 regions. :-)

# Let me take some pains on the 1st
plot(density(runif(100)), lwd=2)
text(x=0, y=0.2, "100 uniforms")        # Showing you how to place text at will
abline(h=0, v=0)
              # All these statements effect the 1st plot.

x=seq(0.01,1,0.01)
par(col="blue")                         # default colour to blue.

# 2 --
plot(x, sin(x), type="l")
lines(x, cos(x), type="l", col="red")

# 3 --
plot(x, exp(x), type="l", col="green")
lines(x, log(x), type="l", col="orange")

# 4 --
plot(x, tan(x), type="l", lwd=3, col="yellow")

# 5 --
plot(x, exp(-x), lwd=2)
lines(x, exp(x), col="green", lwd=3)

# 6 --
plot(x, sin(x*x), type="l")
lines(x, sin(1/x), col="pink")


<h2>Making PDF files that go into books/papers</h2>

# Goal: Make pictures in PDF files that can be put into a paper.

xpts = seq(-3,3,.05)

# Here is my suggested setup for a two-column picture --
pdf("demo2.pdf", width=5.6, height=2.8, bg="cadetblue1", pointsize=8)
par(mai=c(.6,.6,.2,.2))
plot(xpts, sin(xpts*xpts), type="l", lwd=2, col="cadetblue4",
     xlab="x", ylab="sin(x*x)")
grid(col="white", lty=1, lwd=.2)
abline(h=0, v=0)

# My suggested setup for a square one-column picture --
pdf("demo1.pdf", width=2.8, height=2.8, bg="cadetblue1", pointsize=8)
par(mai=c(.6,.6,.2,.2))
plot(xpts, sin(xpts*xpts), type="l", lwd=2, col="cadetblue4",
     xlab="x", ylab="sin(x*x)")
grid(col="white", lty=1, lwd=.2)
abline(h=0, v=0)




<h2>A histogram with tails in red</h2>

# Goal: A histogram with tails shown in red.

# This happened on the R mailing list on 7 May 2004.
# This is by Martin Maechler &lt;maechler&#64;stat.math.ethz.ch&gt;, who was
# responding to a slightly imperfect version of this by
# "Guazzetti Stefano" &lt;Stefano.Guazzetti&#64;ausl.re.it&gt;

x = rnorm(1000)
hx = hist(x, breaks=100, plot=FALSE)
plot(hx, col=ifelse(abs(hx$breaks) &lt; 1.669, 4, 2))
         # What is cool is that "col" is supplied a vector.


<h2>z=f(x,y) using contour lines and colours</h2>

# Goal: Visualisation of 3-dimensional (x,y,z) data using contour
#       plots and using colour to represent the 3rd dimension.
#       The specific situation is: On a grid of (x,y) points, you have
#       evaluated f(x,y). Now you want a graphical representation of
#       the resulting list of (x,y,z) points that you have.

# Setup an interesting data matrix of (x,y,z) points:
points = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268, 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88, 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342, 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148, 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222, 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034, 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8, 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008, 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2, 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074, 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002, 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054, 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002, 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032, 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002, 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026, 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002, 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028, 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002, 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054, 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x", "y", "z")))

# Understand this object --
summary(points)
  # x is a grid from 0 to 1
  # y is a grid from 20 to 200
  # z is the interesting object which will be the 3rd dimension.

# Solution using contourplot() from package 'lattice'
library(lattice)
d3 = data.frame(points)
contourplot(z ~ x+y, data=d3)
## or nicer
contourplot(z ~ x+y, data=d3, cuts=20, region = TRUE)
## or using logit - transformed z values:
contourplot(qlogis(z) ~ x+y, data=d3, pretty=TRUE, region = TRUE)

# An interesting alternative is levelplot()
levelplot(z ~ x+y, pretty=TRUE, contour=TRUE, data=d3)

# There is a contour() function in R. Even though it sounds obvious
# for the purpose, it is a bit hard to use.
# contour() wants 3 inputs: vectors of x and y values, and a matrix of
# z values, where the x values correspond to the rows of z, and the y
# values to the columns.  A collection of points like `points' above
# needs to be turned into such a grid. It might sound odd, but contour()
# image() and persp() have used this kind of input for the longest time.
#
# For irregular data, there's an interp function in the akima package
# that can convert from irregular data into the grid format.
#
# The `points' object that I have above - a list of (x,y,z) points -
# fits directly into the mentality of lattice::contourplot() but not
# into the requirements of contour()


<h2>Show recessions using filled  colour in a macro time-series plot</h2>

# Goal: Display of a macroeconomic time-series, with a filled colour
#       bar showing a recession.

years = 1950:2000
timeseries = cumsum(c(100, runif(50)*5))
hilo = range(timeseries)
plot(years, timeseries, type="l", lwd=3)
# A recession from 1960 to 1965 --
polygon(x=c(1960,1960, 1965,1965),
        y=c(hilo, rev(hilo)),
        density=NA, col="orange", border=NA)
lines(years, timeseries, type="l", lwd=3) # paint again so line comes on top

# alternative method -- though not as good looking --
# library(plotrix)
# gradient.rect(1960, hilo[1], 1965, hilo[2],
#               reds=c(0,1), greens=c(0,0), blues=c(0,0),
#               gradient="y")



Plotting two series on one graph, one  with a left y axis and another with a right y axis.

# Goal: Display two series on one plot, one with a left y axis
#       and another with a right y axis.

y1 = cumsum(rnorm(100))
y2 = cumsum(rnorm(100, mean=0.2))

par(mai=c(.8, .8, .2, .8))
plot(1:100, y1, type="l", col="blue", xlab="X axis label", ylab="Left legend")
par(new=TRUE)
plot(1:100, y2, type="l", ann=FALSE, yaxt="n")
axis(4)
legend(x="topleft", bty="n", lty=c(1,1), col=c("blue","black"),
       legend=c("String 1 (left scale)", "String 2 (right scale)"))



<h4>Probability and statistics</h4>
<h2>Tables, joint and marginal distributions</h2>

# Goal: Joint distributions, marginal distributions, useful tables.

# First let me invent some fake data
set.seed(102)                           # This yields a good illustration.
x = sample(1:3, 15, replace=TRUE)
education = factor(x, labels=c("None", "School", "College"))
x = sample(1:2, 15, replace=TRUE)
gender = factor(x, labels=c("Male", "Female"))
age = runif(15, min=20,max=60)

D = data.frame(age, gender, education)
rm(x,age,gender,education)
print(D)

# Table about education
table(D$education)

# Table about education and gender --
table(D$gender, D$education)
# Joint distribution of education and gender --
table(D$gender, D$education)/nrow(D)

# Add in the marginal distributions also
addmargins(table(D$gender, D$education))
addmargins(table(D$gender, D$education))/nrow(D)

# Generate a good LaTeX table out of it --
library(xtable)
xtable(addmargins(table(D$gender, D$education))/nrow(D),
       digits=c(0,2,2,2,2))             # You have to do | and \hline manually.

# Study age by education category
by(D$age, D$gender, mean)
by(D$age, D$gender, sd)
by(D$age, D$gender, summary)

# Two-way table showing average age depending on education & gender
a = matrix(by(D$age, list(D$gender, D$education), mean), nrow=2)
rownames(a) = levels(D$gender)
colnames(a) = levels(D$education)
print(a)
# or, of course,
print(xtable(a))


<h2>`Moving window' standard deviation</h2>

# Goal: To do `moving window volatility' of returns.

library(zoo)

# Some data to play with (Nifty on all fridays for calendar 2004) --
p = structure(c(1946.05, 1971.9, 1900.65, 1847.55, 1809.75, 1833.65, 1913.6, 1852.65, 1800.3, 1867.7, 1812.2, 1725.1, 1747.5, 1841.1, 1853.55, 1868.95, 1892.45, 1796.1, 1804.45, 1582.4, 1560.2, 1508.75, 1521.1, 1508.45, 1491.2, 1488.5, 1537.5, 1553.2, 1558.8, 1601.6, 1632.3, 1633.4, 1607.2, 1590.35, 1609, 1634.1, 1668.75, 1733.65, 1722.5, 1775.15, 1820.2, 1795, 1779.75, 1786.9, 1852.3, 1872.95, 1872.35, 1901.05, 1996.2, 1969, 2012.1, 2062.7, 2080.5), index = structure(c(12419, 12426, 12433, 12440, 12447, 12454, 12461, 12468, 12475, 12482, 12489, 12496, 12503, 12510, 12517, 12524, 12531, 12538, 12545, 12552, 12559, 12566, 12573, 12580, 12587, 12594, 12601, 12608, 12615, 12622, 12629, 12636, 12643, 12650, 12657, 12664, 12671, 12678, 12685, 12692, 12699, 12706, 12713, 12720, 12727, 12734, 12741, 12748, 12755, 12762, 12769, 12776, 12783), class = "Date"), frequency = 0.142857142857143, class = c("zooreg", "zoo"))

# Shift to returns --
r = 100*diff(log(p))
head(r)
summary(r)
sd(r)

# Compute the moving window vol --
vol = sqrt(250) * rollapply(r, 20, sd, align = "right")

# A pretty plot --
plot(vol, type="l", ylim=c(0,max(vol,na.rm=TRUE)),
     lwd=2, col="purple", xlab="2004",
     ylab=paste("Annualised sigma, 20-week window"))
grid()
legend(x="bottomleft", col=c("purple", "darkgreen"),
       lwd=c(2,2), bty="n", cex=0.8,
       legend=c("Annualised 20-week vol (left scale)", "Nifty (right scale)"))
par(new=TRUE)
plot(p, type="l", lwd=2, col="darkgreen",
     xaxt="n", yaxt="n", xlab=", ylab=")
axis(4)


<h2>Quartiles/deciles tables/graphs.</h2>

Requires this data file

# Get the data in place --
load(file="demo.rda")
summary(firms)

# Look at it --
plot(density(log(firms$mktcap)))
plot(firms$mktcap, firms$spread, type="p", cex=.2, col="blue", log="xy",
     xlab="Market cap (Mln USD)", ylab="Bid/offer spread (bps)")
m=lm(log(spread) ~ log(mktcap), firms)
summary(m)

# Making deciles --
library(gtools)
library(gdata)
                                      # for deciles (default=quartiles)
size.category = quantcut(firms$mktcap, q=seq(0, 1, 0.1), labels=F)
table(size.category)
means = aggregate(firms, list(size.category), mean)
print(data.frame(means$mktcap,means$spread))

# Make a picture combining the sample mean of spread (in each decile)
# with the weighted average sample mean of the spread (in each decile),
# where weights are proportional to size.
wtd.means = by(firms, size.category,
  function(piece) (sum(piece$mktcap*piece$spread)/sum(piece$mktcap)))
lines(means$mktcap, means$spread, type="b", lwd=2, col="green", pch=19)
lines(means$mktcap, wtd.means, type="b", lwd=2, col="red", pch=19)
legend(x=0.25, y=0.5, bty="n",
       col=c("blue", "green", "red"),
       lty=c(0, 1, 1), lwd=c(0,2,2),
       pch=c(0,19,19),
       legend=c("firm", "Mean spread in size deciles",
         "Size weighted mean spread in size deciles"))

# Within group standard deviations --
aggregate(firms, list(size.category), sd)

# Now I do quartiles by BOTH mktcap and spread.
size.quartiles = quantcut(firms$mktcap, labels=F)
spread.quartiles = quantcut(firms$spread, labels=F)
table(size.quartiles, spread.quartiles)
# Re-express everything as joint probabilities
table(size.quartiles, spread.quartiles)/nrow(firms)
# Compute cell means at every point in the joint table:
aggregate(firms, list(size.quartiles, spread.quartiles), mean)

# Make pretty two-way tables
aggregate.table(firms$mktcap, size.quartiles, spread.quartiles, nobs)
aggregate.table(firms$mktcap, size.quartiles, spread.quartiles, mean)
aggregate.table(firms$mktcap, size.quartiles, spread.quartiles, sd)
aggregate.table(firms$spread, size.quartiles, spread.quartiles, mean)
aggregate.table(firms$spread, size.quartiles, spread.quartiles, sd)


<h2>Distribution of sample mean and sample median</h2>

# Goal: Show the efficiency of the mean when compared with the median
#       using a large simulation where both estimators are applied on
#       a sample of U(0,1) uniformly distributed random numbers.

one.simulation = function(N=100) {     # N defaults to 100 if not supplied
  x = runif(N)
  return(c(mean(x), median(x)))
}

# Simulation --
results = replicate(100000, one.simulation(20)) # Gives back a 2x100000 matrix

# Two kernel densities --
k1 = density(results[1,])              # results[1,] is the 1st row
k2 = density(results[2,])

# A pretty picture --
xrange = range(k1$x, k2$x)
plot(k1$x, k1$y, xlim=xrange, type="l", xlab="Estimated value", ylab=")
grid()
lines(k2$x, k2$y, col="red")
abline(v=.5)
legend(x="topleft", bty="n",
       lty=c(1,1),
       col=c("black", "red"),
       legend=c("Mean", "Median"))


<h2>The bootstrap</h2>

  Getting started with the `boot' package in R for bootstrap inference


The package <code>boot</code> has elegant and powerful support for
bootstrapping. In order to use it, you have to repackage your
estimation function as follows.


R has very elegant and abstract notation in array indexes. Suppose
there is an integer vector <code>OBS</code> containing the elements 2,
3, 7, i.e. that <code>OBS = c(2,3,7);</code>. Suppose x is a
vector. Then the notation <code>x[OBS]</code> is a vector containing
elements x[2], x[3] and x[7]. This beautiful notation works for x as a
dataset (data frame) also. Here are demos:


# For vectors --
> x = c(10,20,30,40,50)
> d = c(3,2,2)
> x[d]
[1] 30 20 20

# For data frames --
> D = data.frame(x=seq(10,50,10), y=seq(500,100,-100))
> t(D)
    1   2   3   4   5
x  10  20  30  40  50
y 500 400 300 200 100
> D[d,]
     x   y
3   30 300
2   20 400
2.1 20 400



Now for the key point: how does the R boot package work? The R
package <code>boot</code> repeatedly calls your estimation function,
and each time, the bootstrap sample is supplied using an integer
vector of indexes like above. Let me show you two examples of how you
would write estimation functions which are compatible with the
package:


samplemean = function(x, d) {
  return(mean(x[d]))
}

samplemedian = function(x, d) {
  return(median(x[d]))
}



The estimation function (that you write) consumes data
<code>x</code> and a vector of indices <code>d</code>. This function
will be called many times, one for each bootstrap replication. Every
time, the data `x' will be the same, and the bootstrap sample `d' will
be different.


At each call, the boot package will supply a fresh set of indices
d. The notation x[d] allows us to make a brand-new vector (the
bootstrap sample), which is given to mean() or median(). This reflects
sampling with replacement from the original data vector.


Once you have written a function like this, here is how you would
obtain bootstrap estimates of the standard deviation of the
distribution of the median:


    b = boot(x, samplemedian, R=1000)           # 1000 replications



The object `b' that is returned by boot() is interesting and
useful. Say ?boot to learn about it. For example, after making
<code>b</code> as shown above, you can say:

    print(sd(b$t[,1]))



Here, I'm using the fact that b$t is a matrix containing 1000 rows
which holds all the results of estimation. The 1st column in it is the
only thing being estimated by samplemedian(), which is the sample
median.


The default plot() operator does nice things when fed with this
object. Try it: say <code>plot(b)</code>
<h2>Dealing with data frames</h2>


Here is an example, which uses the bootstrap to report the ratio of
two standard deviations:


library(boot)

sdratio = function(D, d) {
  E=D[d,]
  return(sd(E$x)/sd(E$y))
}

x = runif(100)
y = 2*runif(100)
D = data.frame(x, y)

b = boot(D, sdratio, R=1000)
cat("Standard deviation of sdratio = ", sd(b$t[,1]), "\n")
ci = boot.ci(b, type="basic")
cat("95% CI from ", ci$basic[1,4], " - ", ci$basic[1,5], "\n")



Note the beautiful syntax <code>E = D[d,]</code> which gives you a
data frame E using the rows out of data frame D that are specified by
the integer vector d.
<h2>Sending more stuff to your estimation function</h2>


Many times, you want to send additional things to your estimation
function. You're allowed to say whatever you want to boot(), after you
have supplied the two mandatory things that he wants. Here's an
example: the trimmed mean.


The R function mean() is general, and will also do a trimmed
mean. If you say mean(x, 0.1), then it will remove the most extreme
10% of the data at both the top and the bottom, and report the mean of
the middle 80%. Suppose you want to explore the sampling
characteristics of the trimmed mean using boot(). You would write this:


trimmedmean = function(x, d, trim=0) {
  return(mean(x[d], trim/length(x)))
}



Here, I'm defaulting trim to 0. And, I'll allowing the caller to
talk in the units of observations, not fractions of the data. So the
user would say "5" to trim off the most extreme 5 observations at the
top and the bottom. I convert that into fractions before feeding this
to mean().


Here's how you would call boot() using this:


    b = boot(x, trimmedmean, R=1000, trim=5)



This sends the extra argument trim=5 to boot, which sends it on to
our trimmedmean() function.
<h2>Finding out more</h2>


The boot() function is very powerful. The above examples only
scratch the surface. Among other things, it does things like the block
bootstrap for time-series data, randomly censored data, etc. The
manual can be accessed by saying:


library(boot)
?boot



but what you really need is the article <i>Resampling Methods in R:
The <code>boot</code> package</i> by Angelo J. Canty, which appeared
in the December 2002 issue of <a href="http://cran.r-project.org/doc/Rnews"><i>R News</i></a>.


Also see the web appendix to <i>An R and S-PLUS Companion to
Applied Regression</i> by John Fox [<a href="http://socserv.mcmaster.ca/jfox/Books/Companion/appendix-bootstrapping.pdf">pdf</a>],
and a tutorial by Patrick Burns [<a href="http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html">html</a>].

Return to <a href="../index.html"><i>R by example</i></a>

<a href="http://www.mayin.org/ajayshah">Ajay Shah</a>
ajayshah at mayin dot org

<h2>Notes on boot()</h2>

# Goals: Do bootstrap inference, as an example, for a sample median.

library(boot)

samplemedian = function(x, d) {        # d is a vector of integer indexes
  return(median(x[d]))                  # The genius is in the x[d] notation
}

data = rnorm(50)                          # Generate a dataset with 50 obs
b  =  boot(data, samplemedian, R=2000)    # 2000 bootstrap replications
cat("Sample median has a sigma of ", sd(b$t[,1]), "\n")
plot(b)

# Make a 99% confidence interval
boot.ci(b, conf=0.99, type="basic")


<h2>Doing MLE with your own likelihood function</h2>

  Roll your own likelihood function with R

This document assumes you know something about maximum likelihood
estimation. It helps you get going in terms of doing MLE in R. All
through this, we will use the "ordinary least squares" (OLS) model
(a.k.a. "linear regression" or "classical least squares" (CLS)) as the
simplest possible example. <a href="ols-lf.pdf">Here are the formulae
for the OLS likelihood, and the notation that I use.</a>


There are two powerful optimisers in R: optim() and nlminb().
This note only uses optim(). You should also explore nlminb().


You might find it convenient to <a href="mlefiles.tar.bz2">snarf
a tarfile of all the .R programs involved in this page.</a>
<h2>Writing the likelihood function</h2>


You have to write an R function which computes out the likelihood
function. As always in R, this can be done in several different
ways.


One issue is that of restrictions upon parameters. When the search
algorithm is running, it may stumble upon nonsensical values - such as
a sigma below 0 - and you do need to think about this. One traditional
way to deal with this is to "transform the parameter space". As an
example, for all positive values of sigma, log(sigma) ranges from
-infinity to +infinity. So it's safe to do an unconstrained search
using log(sigma) as the free parameter.

<a href="html/ols_lfn.html">Here is the OLS likelihood, written in a few ways.</a>


Confucius he said, when you write a likelihood function, do take
the trouble of also writing it's gradient (the vector of first
derivatives). You don't absolutely need it, but it's highly
recommended. In my toy experiment, this seems to be merely a question
of speed - using the analytical gradient makes the MLE go faster. But
the OLS likelihood is unique and simple; it is globally quasiconcave
and has a clear top. There could not be a simpler task for a
maximisation routine. In more complex situations, numerical
derivatives are known to give more unstable searches, while analytical
derivatives give more reliable answers.
<h2>A simulation setup</h2>


To use the other files on this page, you need to take my <a href="html/simulated_setup.html">simulation setup file</a>.
<h2>Comparing these alternatives</h2>


Now that I've written the OLS likelihood function in a few ways,
it's natural to ask: Do they all give the same answer? And, which is
the fastest?


I <a href="html/lfn_testing_timing.html">wrote a simple R program in order
to learn these things</a>. This gives the result:


True theta =  2 4 6 
OLS theta =  2.004311 3.925572 6.188047 

Kick the tyres --
                 lf1() lf1() in logs    lf2()    lf3()
A weird theta 1864.956      1864.956 1864.956 1864.956
True theta    1766.418      1766.418 1766.418 1766.418
OLS theta     1765.589      1765.589 1765.589 1765.589
Cost (ms)        0.450         0.550    1.250    1.000

Derivatives -- first let me do numerically --
  Derivative in sigma     --  10.92756 
  Derivative in intercept -- -8.63967 
  Derivative in slope     -- -11.82872 
  Analytical derivative in sigma -- 10.92705 
  Analytical derivative in beta  -- -8.642051 -11.82950 



This shows us that of the 4 ways of writing it, ols.lf1() is the
fastest, and that there is a fair match between my claimed analytical
gradient and numerical derivatives.

<h2>A minimal program which does the full MLE</h2>
Using this foundation, I can jump to a self-contained and <a href="html/minimal.html">minimal R program which does the full job</a>. It
gives this result:


True theta =  2 4 6 
OLS theta =  2.004311 3.925572 6.188047 

Gradient-free (constrained optimisation) --
$par
[1] 2.000304 3.925571 6.188048
$value
[1] 1765.588
$counts
function gradient 
      18       18 
$convergence
[1] 0
$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

Using the gradient (constrained optimisation) --
$par
[1] 2.000303 3.925571 6.188048
$value
[1] 1765.588
$counts
function gradient 
      18       18 
$convergence
[1] 0
$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

You say you want a covariance matrix?
MLE results --
          Coefficient  Std. Err.        t
Sigma        2.000303 0.08945629 22.36068
Intercept    3.925571 0.08792798 44.64530
X            6.188048 0.15377325 40.24138
Compare with the OLS results --
            Estimate Std. Error  t value      Pr(>|t|)
(Intercept) 3.925572 0.08801602 44.60065 7.912115e-240
X[, 2]      6.188047 0.15392722 40.20112 6.703474e-211



The file minimal.R also generates this picture:

<h2>Measurement about the full MLE</h2>


The R optim() function has many different paths to MLE. I wrote <a href="html/mle_testing_timing.html">a simple R program</a> in order to learn
about these. This yields the result:


True theta =  2 4 6 
OLS theta =  2.004311 3.925572 6.188047 

                        Hit rate   Cost
L-BFGS-B, analytical         100   25.1
BFGS, analytical             100   33.1
Nelder-Mead, trans.          100   59.2
Nelder-Mead                  100   60.5
L-BFGS-B, numerical          100   61.2
BFGS, trans., numerical      100   68.5
BFGS, numerical              100   71.2
SANN                          99 4615.5
SANN, trans.                  96 4944.9



The algorithms compared above are:

<i>L-BFGS-B, analytical</i>. This uses L-BFGS-B which is a
variant of BFGS which allows "box" constraints (you can specify a
permitted range for each parameter). This uses the ols.gradient()
function to do analytical derivatives. It is the fastest (25.1
milliseconds on my machine) and works 100% of the time.

<i>BFGS, analytical</i>. This uses BFGS instead of L-BFGS-B --
i.e. no constraints are permitted. Analytical derivatives are used.

<i>Nelder-Mead, trans.</i>. Nelder-Mead is a derivative-free
algorithm. It does not need you to write the gradient. This variant
uses the log() transformation in order to ensure that sigma is positive.

<i>Nelder-Mead</i> This is Nelder-Mead without the transformation.

<i>L-BFGS-B, numerical</i> This is the same L-BFGS-B but instead
of giving him analytical derivative, I leave optim() to fend for himself
with numerical derivatives. A worse than doubling of cost!

<i>BFGS, trans., numerical</i> This uses plain BFGS, with
the log() transformation to ensure that sigma stays positive, but using
numerical derivatives.

<i>BFGS, numerical</i> This is plain BFGS, with no transformation
to ensure a sane sigma, and using numerical derivatives.

<i>SANN</i> This is a stochastic search algorithm based on
simulated annealing. As you see, it failed for 1% of the runs. It is
very costly. The attraction is that it might be more effective at
finding global maxima and in "staying out of troublesome territory".

<i>SANN trans.</i> This uses the log() transform for sigma
and does the search using simulated annealing.

<h2>Notes on MLE</h2>
# Goal: To do OLS by MLE.
# OLS likelihood function
# Note: I am going to write the LF using sigma2=sigma^2 and not sigma.
ols.lf1 = function(theta, y, X) {
  beta = theta[-1]
  sigma2 = theta[1]
  if (sigma2 &lt;= 0) return(NA)
  n = nrow(X)
  e = y - X%*%beta                                  # t() = matrix transpose
  logl = ((-n/2)*log(2*pi)) - ((n/2)*log(sigma2)) - ((t(e)%*%e)/(2*sigma2))
  return(-logl) # since optim() does minimisation by default.
}

# Analytical derivatives
ols.gradient = function(theta, y, X) {
  beta = theta[-1]
  sigma2 = theta[1]
  e = y - X%*%beta
  n = nrow(X)

  g = numeric(length(theta))
  g[1] = (-n/(2*sigma2)) + (t(e)%*%e)/(2*sigma2*sigma2) # d logl / d sigma
  g[-1] = (t(X) %*% e)/sigma2                           # d logl / d beta

  return(-g)
}

X = cbind(1, runif(1000))
theta.true = c(2,4,6) # error variance = 2, intercept = 4, slope = 6.
y = X %*% theta.true[-1] + sqrt(theta.true[1]) * rnorm(1000)

# Estimation by OLS --
d = summary(lm(y ~ X[,2]))
theta.ols = c(sigma2 = d$sigma^2, d$coefficients[,1])
cat("OLS theta = ", theta.ols, "\n\n")

cat("\nGradient-free (constrained optimisation) --\n")
optim(c(1,1,1), method="L-BFGS-B", fn=ols.lf1,
      lower=c(1e-6,-Inf,-Inf), upper=rep(Inf,3), y=y, X=X)

cat("\nUsing the gradient (constrained optimisation) --\n")
optim(c(1,1,1), method="L-BFGS-B", fn=ols.lf1, gr=ols.gradient,
      lower=c(1e-6,-Inf,-Inf), upper=rep(Inf,3), y=y, X=X)

cat("\n\nYou say you want a covariance matrix?\n")
p = optim(c(1,1,1), method="L-BFGS-B", fn=ols.lf1, gr=ols.gradient,
           lower=c(1e-6,-Inf,-Inf), upper=rep(Inf,3), hessian=TRUE,
           y=y, X=X)
inverted = solve(p$hessian)
results = cbind(p$par, sqrt(diag(inverted)), p$par/sqrt(diag(inverted)))
colnames(results) = c("Coefficient", "Std. Err.", "t")
rownames(results) = c("Sigma", "Intercept", "X")
cat("MLE results --\n")
print(results)
cat("Compare with the OLS results --\n")
d$coefficients

# Picture of how the loglikelihood changes if you perturb the sigma
theta = theta.ols
delta.values = seq(-1.5, 1.5, .01)
logl.values = as.numeric(lapply(delta.values,
                                 function(x) {-ols.lf1(theta+c(x,0,0),y,X)}))
plot(sqrt(theta[1]+delta.values), logl.values, type="l", lwd=3, col="blue",
     xlab="Sigma", ylab="Log likelihood")
grid()


<h2>The strange Cauchy distribution</h2>

# Goals: Scare the hell out of children with the Cauchy distribution.

# A function which simulates N draws from one of two distributions,
# and returns the mean obtained thusly.
one.simulation = function(N=100, distribution="normal") {
  if (distribution == "normal") {
    x = rnorm(N)
  } else {
    x = rcauchy(N)
  }
  mean(x)
}

k1 = density(replicate(1000, one.simulation(20)))
k2 = density(replicate(1000, one.simulation(20, distribution="cauchy")))

xrange = range(k1$x, k2$x)
plot(k1$x, k1$y, xlim=xrange, type="l", xlab="Estimated value", ylab=")
grid()
lines(k2$x, k2$y, col="red")
abline(v=.5)
legend(x="topleft", bty="n",
       lty=c(1,1),
       col=c("black", "red"),
       legend=c("Mean of Normal", "Mean of Cauchy"))
# The distribution of the mean of normals collapses into a point;
# that of the cauchy does not.

# Here's more scary stuff --
for (i in 1:10) {
  cat("Sigma of distribution of 1000 draws from mean of normal - ",
      sd(replicate(1000, one.simulation(20))), "\n")
}
for (i in 1:10) {
  cat("Sigma of distribution of 1000 draws from mean of cauchy - ",
      sd(replicate(1000, one.simulation(20, distribution="cauchy"))), "\n")
}

# Exercise for the reader: Compare the distribution of the median of
# the Normal against the distribution of the median of the Cauchy.


<h2>An example of simulation-based inference</h2>

# Goal: An example of simulation-based inference.
# This is in the context of testing for time-series dependence in
# stock market returns data.
# The code here does the idea of Kim, Nelson, Startz (1991).
# We want to use the distribution of realworld returns data, without
# needing assumptions about normality.
# The null is lack of dependence (i.e. an efficient market).
# So repeatedly, the data is permuted, and the sample ACF is computed.
# This gives us the distribution of the ACF under H0: independence, but
# while using the empirical distribution of the returns data.

# Weekly returns on Nifty, 1/1/2002 to 31/12/2003, 104 weeks of data.
r = c(-0.70031182197603, 0.421690133064168, -1.20098072984689, 0.143402360644984, 3.81836537549516, 3.17055939373247, 0.305580301919228, 1.23853814691852, 0.81584795095706, -1.51865139747764, -2.71223626421522, -0.784836480094242, 1.09180041170998, 0.397649587762761, -4.11309534220923, -0.263912425099111, -0.0410144239805454, 1.75756212770972, -2.3335373897992, -2.19228764624217, -3.64578978183987, 1.92535789661354, 3.45782867883164, -2.15532607229374, -0.448039988298987, 1.50124793565896, -1.45871585874362, -2.13459863369767, -6.2128068251802, -1.94482987066289, 0.751294815735637, 1.78244982829590, 1.61567494389745, 1.53557708728931, -1.53557708728931, -0.322061470004265, -2.28394919698225, 0.70399304137414, -2.93580952607737, 2.38125098034425, 0.0617697039252185, -4.14482733720716, 2.04397528093754, 0.576400673606603, 3.43072725191913, 2.96465382864843, 2.89833358015583, 1.85387040058336, 1.52136515035952, -0.637268376944444, 1.75418926224609, -0.804391905851354, -0.861816058320475, 0.576902488444109, -2.84259880663331, -1.35375536139417, 1.49096529042234, -2.05404881010045, 2.86868849528146, -0.258270670200478, -4.4515881438687, -1.73055019137092, 3.04427015714648, -2.94928202352018, 1.62081315773994, -6.83117945164824, -0.962715713711582, -1.75875847071740, 1.50330330252721, -0.0479705789653728, 3.68968303215933, -0.535807567290103, 3.94034871061182, 3.85787174417738, 0.932185956989873, 4.08598654183674, 2.27343783689715, 1.13958830440017, 2.01737201171230, -1.88131458327554, 1.97596267156648, 2.79857144562001, 2.22470306481695, 2.03212951411427, 4.95626853448883, 3.40400972901396, 3.03840139165246, -1.89863129741417, -3.70832135042951, 4.78478922155396, 4.3973589590097, 4.9667050392987, 2.99775078737081, -4.12349101552438, 3.25638269809945, 2.29683376253966, -2.64772825878214, -0.630835277076258, 4.72528848505451, 1.87368447333380, 3.17543946162564, 4.58174427843208, 3.23625985632168, 2.29777651227296)

# The 1st autocorrelation from the sample:
acf(r, 1, plot=FALSE)$acf[2]

# Obtain 1000 draws from the distribution of the 1st autocorrelation
# under the null of independence:
set.seed = 101
simulated = replicate(1000, acf(r[sample(1:104, replace=FALSE)], 1, plot=FALSE)$acf[2])
# At 95% --
quantile(simulated, probs=c(.025,.975))
# At 99% --
quantile(simulated, probs=c(.005,.995))

# So we can reject the null at 95% but not at 99%.

# A pretty picture.
plot(density(simulated), col="blue")
abline(v=0)
abline(v=quantile(simulated, probs=c(.025,.975)), lwd=2, col="purple")
abline(v=acf(r, 1, plot=FALSE)$acf[2], lty=2, lwd=4, col="yellow")


<h2>Four standard operations with standard distributions</h2>

# Goal: Standard computations with well-studied distributions.

# The normal distribution is named "norm". With this, we have:

# Normal density
dnorm(c(-1.96,0,1.96))

# Cumulative normal density
pnorm(c(-1.96,0,1.96))

# Inverse of this
qnorm(c(0.025,.5,.975))
pnorm(qnorm(c(0.025,.5,.975)))

# 1000 random numbers from the normal distribution
summary(rnorm(1000))


# Here's the same ideas, for the chi-squared distribution with 10 degrees
# of freedom.
dchisq(c(0,5,10), df=10)

# Cumulative normal density
pchisq(c(0,5,10), df=10)

# Inverse of this
qchisq(c(0.025,.5,.975), df=10)

# 1000 random numbers from the normal distribution
summary(rchisq(1000, df=10))


<h2>Two CDFs and a two-sample Kolmogorov-Smirnoff test</h2>

# Goal: Given two vectors of data,
#       superpose their CDFs
#       and show the results of the two-sample Kolmogorov-Smirnoff test

# The function consumes two vectors x1 and x2.
# You have to provide a pair of labels as `legendstrings'.
# If you supply an xlab, it's used
# If you specify log - e.g. log="x" - this is passed on to plot.
# The remaining args that you specify are sent on into ks.test()
two.cdfs.plot = function(x1, x2, legendstrings, xlab=", log=", ...) {
  stopifnot(length(x1)&gt;0,
            length(x2)&gt;0,
            length(legendstrings)==2)
  hilo = range(c(x1,x2))

  par(mai=c(.8,.8,.2,.2))
  plot(ecdf(x1), xlim=hilo, verticals=TRUE, cex=0,
       xlab=xlab, log=log, ylab="Cum. distribution", main=")
  grid()
  plot(ecdf(x2), add=TRUE, verticals=TRUE, cex=0, lwd=3)
  legend(x="bottomright", lwd=c(1,3), lty=1, bty="n",
         legend=legendstrings)

  k = ks.test(x1,x2, ...)
  text(x=hilo[1], y=c(.9,.85), pos=4, cex=.8,
     labels=c(
       paste("KS test statistic: ", sprintf("%.3g", k$statistic)),
       paste("Prob value: ", sprintf("%.3g", k$p.value))
       )
     )
  k
}

x1 = rnorm(100, mean=7, sd=1)
x2 = rnorm(100, mean=9, sd=1)

# Check error detection --
two.cdfs.plot(x1,x2)

# Typical use --
two.cdfs.plot(x1, x2, c("X1","X2"), xlab="Height (metres)", log="x")

# Send args into ks.test() --
two.cdfs.plot(x1, x2, c("X1","X2"), alternative="less")


<h2>Simulation to measure size and power of a test</h2>

# Goal: Simulation to study size and power in a simple problem.

set.seed(101)

# The data generating process: a simple uniform distribution with stated mean
dgp = function(N,mu) {runif(N)-0.5+mu}

# Simulate one FIXED hypothesis test for H0:mu=0, given a true mu for a sample size N
one.test = function(N, truemu) {
  x = dgp(N,truemu)
  muhat = mean(x)
  s = sd(x)/sqrt(N)
  # Under the null, the distribution of the mean has standard error s
  threshold = 1.96*s
  (muhat &lt; -threshold) || (muhat &gt; threshold)
} # Return of TRUE means reject the null

# Do one experiment, where the fixed H0:mu=0 is run Nexperiments times with a sample size N.
# We return only one number: the fraction of the time that H0 is rejected.
experiment = function(Nexperiments, N, truemu) {
  sum(replicate(Nexperiments, one.test(N, truemu)))/Nexperiments
}

# Measure the size of a test, i.e. rejections when H0 is true
experiment(10000, 50, 0)
# Measurement with sample size of 50, and true mu of 0.

# Power study: I.e. Pr(rejection) when H0 is false
# (one special case in here is when the H0 is actually true)

muvalues = seq(-.15,.15,.01)
  # When true mu &lt; -0.15 and when true mu &gt; 0.15,
  # the Pr(rejection) veers to 1 (full power) and it's not interesting.

# First do this with sample size of 50
results = NULL
for (truth in muvalues) {
  results = c(results, experiment(10000, 50, truth))
}
par(mai=c(.8,.8,.2,.2))
plot(muvalues, results, type="l", lwd=2, ylim=c(0,1),
     xlab="True mu", ylab="Pr(Rejection of H0:mu=0)")
abline(h=0.05, lty=2)

# Now repeat this with sample size of 100 (should yield a higher power)
results = NULL
for (truth in muvalues) {
  results = c(results, experiment(10000, 100, truth))
}
lines(muvalues, results, lwd=2, col="blue")
legend(x=-0.15, y=.2, lwd=c(2,1,2), lty=c(1,2,1), cex=.8,
       col=c("black","black","blue"), bty="n",
       legend=c("N=50", "Size, 0.05", "N=100"))



<h4>Regression</h4>
<h2>Doing OLS</h2>

# Goal: Simulate a dataset from the OLS model and obtain
#       obtain OLS estimates for it.

x = runif(100, 0, 10)                  # 100 draws from U(0,10)
y = 2 + 3*x + rnorm(100)               # beta = [2, 3] and sigma = 1

# You want to just look at OLS results?
summary(lm(y ~ x))

# Suppose x and y were packed together in a data frame --
D = data.frame(x,y)
summary(lm(y ~ x, D))

# Full and elaborate steps --
d = lm(y ~ x)
# Learn about this object by saying ?lm and str(d)
# Compact model results --
print(d)
# Pretty graphics for regression diagnostics --
par(mfrow=c(2,2))
plot(d)

d = summary(d)
# Detailed model results --
print(d)
# Learn about this object by saying ?summary.lm and by saying str(d)
cat("OLS gave slope of ", d$coefficients[2,1],
    "and a error sigma of ", d$sigma, "\n")


## I need to drop down to a smaller dataset now --
x = runif(10)
y = 2 + 3*x + rnorm(10)
m = lm(y ~ x)

# Now R supplies a wide range of generic functions which extract
# useful things out of the result of estimation of many kinds of models.

residuals(m)
fitted(m)
AIC(m)
AIC(m, k=log(10))                        # SBC
vcov(m)
logLik(m)


<h2>Dummy variables in regression</h2>

# Goal: "Dummy variables" in regression.

# Suppose you have this data:
people = data.frame(
  age =       c(21,62,54,49,52,38),
  education = c("college", "school", "none", "school", "college", "none"),
  education.code = c(  2,        1,      0,        1,         2,      0 )
  )
# Here people$education is a string categorical variable and
# people$education.code is the same thing, with a numerical coding system.
people

# Note the structure of the dataset --
str(people)
# The strings supplied for `education' have been treated (correctly) as
# a factor, but education.code is being treated as an integer and not as
# a factor.


# We want to do a dummy variable regression. Normally you would have:
#  1 Chosen college as the omitted category
#  2 Made a dummy for "none" named educationnone
#  3 Made a dummy for "school" named educationschool
#  4 Ran a regression like lm(age ~ educationnone + educationschool, people)
# But this is R. Things are cool:
lm(age ~ education, people)

# ! :-)
# When you feed him an explanatory variable like education, he does all
# these steps automatically. (He chose college as the omitted category).

# If you use an integer coding, then the obvious thing goes wrong --
lm(age ~ education.code, people)
# because he's thinking that education.code is an integer explanatory
# variable. So you need to:

lm(age ~ factor(education.code), people)
# (he choose a different omitted category)

# Alternatively, fix up the dataset --
people$education.code = factor(people$education.code)
lm(age ~ education.code, people)

#
# Bottom line:
# Once the dataset has categorical variables correctly represented as factors, i.e. as
str(people)
# doing OLS in R induces automatic generation of dummy variables while leaving one out:
lm(age ~ education, people)
lm(age ~ education.code, people)

# But what if you want the X matrix?
m = lm(age ~ education, people)
model.matrix(m)
# This is the design matrix that went into the regression m.


<h2>Generate latex tables of OLS results</h2>

# Goal: To make a latex table with results of an OLS regression.

# Get an OLS --
x1 = runif(100)
x2 = runif(100, 0, 2)
y = 2 + 3*x1 + 4*x2 + rnorm(100)
m = lm(y ~ x1 + x2)

# and print it out prettily --
library(xtable)
# Bare --
xtable(m)
xtable(anova(m))

# Better --
print.xtable(xtable(m, caption="My regression",
                    label="t:mymodel",
                    digits=c(0,3,2,2,3)),
             type="latex",
             file="xtable_demo_ols.tex",
             table.placement = "tp",
             latex.environments=c("center", "footnotesize"))

print.xtable(xtable(anova(m),
                    caption="ANOVA of my regression",
                    label="t:anova_mymodel"),
             type="latex",
             file="xtable_demo_anova.tex",
             table.placement = "tp",
             latex.environments=c("center", "footnotesize"))

# Read the documentation of xtable. It actually knows how to generate
# pretty latex tables for a lot more R objects than just OLS results.
# It can be a workhorse for making tabular out of matrices, and
# can also generate HTML.


<h2>`Least squares dummy variable' (LSDV) or `fixed effects' model</h2>

# Goals: Simulate a dataset from a "fixed effects" model, and
#        obtain "least squares dummy variable" (LSDV) estimates.
#
# We do this in the context of a familiar "earnings function" -
#  log earnings is quadratic in log experience, with parallel shifts by
#  education category.

# Create an education factor with 4 levels --
education = factor(sample(1:4,1000, replace=TRUE),
                    labels=c("none", "school", "college", "beyond"))
# Simulate an experience variable with a plausible range --
experience = 30*runif(1000)            # experience from 0 to 20 years
# Make the intercept vary by education category between 4 given values --
intercept = c(0.5,1,1.5,2)[education]

# Simulate the log earnings --
log.earnings = intercept +
  2*experience - 0.05*experience*experience + rnorm(1000)
A = data.frame(education, experience, e2=experience*experience, log.earnings)
summary(A)

# The OLS path to LSDV --
summary(lm(log.earnings ~ -1 + education + experience + e2, A))


<h2>Estimate beta of Sun Microsystems using data from Yahoo finance</h2><h2>Elaborate version</h2>

# Goal: Using data from Yahoo finance, estimate the beta of Sun Microsystems
#       for weekly returns.
# This is the `elaborate version' (36 lines), also see terse version (16 lines)

library(tseries)

# I know that the yahoo symbol for the common stock of Sun Microsystems
# is "SUNW" and for the S&P 500 index is "^GSPC".
prices = cbind(get.hist.quote("SUNW", quote="Adj", start="2003-01-01", retclass="zoo"),
                get.hist.quote("^GSPC", quote="Adj", start="2003-01-01", retclass="zoo"))
colnames(prices) = c("SUNW", "SP500")
prices = na.locf(prices)               # Copy last traded price when NA

# To make weekly returns, you must have this incantation:
nextfri.Date = function(x) 7 * ceiling(as.numeric(x - 1)/7) + as.Date(1)
# and then say
weekly.prices = aggregate(prices, nextfri.Date,tail,1)

# Now we can make weekly returns --
r = 100*diff(log(weekly.prices))

# Now shift out of zoo to become an ordinary matrix --
r = coredata(r)
rj = r[,1]
rM = r[,2]
d = lm(rj ~ rM)               # Market model estimation.
print(summary(d))

# Make a pretty picture
big = max(abs(c(rj, rM)))
range = c(-big, big)
plot(rM, rj, xlim=range, ylim=range,
     xlab="S&P 500 weekly returns (%)", ylab="SUNW weekly returns (%)")
grid()
abline(h=0, v=0)
lines(rM, d$fitted.values, col="blue")


<h2>Terse version.</h2>

# Goal : Terse version of estimating the beta of Sun Microsystems
#        using weekly returns and data from Yahoo finance.
#        By Gabor Grothendieck.

library(tseries)

getstock = function(x)
   c(get.hist.quote(x, quote = "Adj", start = "2003-01-01", compress = "w"))
r = diff(log(cbind(sp500 = getstock("^gspc"), sunw = getstock("sunw"))))

mm = lm(sunw ~ ., r)
print(summary(mm))

range = range(r, -r)
plot(r[,1], r[,2], xlim = range, ylim = range,
     xlab = "S&P 500 weekly returns (%)", ylab = "SUNW weekly returns (%)")
grid()
abline(mm, h = 0, v = 0, col = "blue")


<h2>Nonlinear regression</h2>

# Goal: To do nonlinear regression, in three ways
#       By just supplying the function to be fit,
#       By also supplying the analytical derivatives, and
#       By having him analytically differentiate the function to be fit.
#
# John Fox has a book "An R and S+ companion to applied regression"
# (abbreviated CAR).
# An appendix associated with this book, titled
#   "Nonlinear regression and NLS"
# is up on the web, and I strongly recommend that you go read it.
#
# This file is essentially from there (I have made slight changes).

# First take some data - from the CAR book --
library(car)
data(US.pop)
attach(US.pop)
plot(year, population, type="l", col="blue")

# So you see, we have a time-series of the US population. We want to
# fit a nonlinear model to it.

library(stats)                            # Contains nonlinear regression
time = 0:20
pop.mod = nls(population ~ beta1/(1 + exp(beta2 + beta3*time)),
  start=list(beta1=350, beta2=4.5, beta3=-0.3), trace=TRUE)
# You just write in the formula that you want to fit, and supply
# starting values. "trace=TRUE" makes him show iterations go by.

summary(pop.mod)
# Add in predicted values into the plot
lines(year, fitted.values(pop.mod), lwd=3, col="red")

# Look at residuals
plot(year, residuals(pop.mod), type="b")
abline(h=0, lty=2)

# Using analytical derivatives:
model = function(beta1, beta2, beta3, time) {
  m = beta1/(1+exp(beta2+beta3*time))
  term = exp(beta2 + beta3*time)
  gradient = cbind((1+term)^-1,
                    -beta1*(1+term)^-2 * term,
                    -beta1*(1+term)^-2 * term * time)
  attr(m, 'gradient') = gradient
  return(m)
}

summary(nls(population ~ model(beta1, beta2, beta3, time),
            start=list(beta1=350, beta2=4.5, beta3=-0.3)))

# Using analytical derivatives, using automatic differentiation (!!!):
model = deriv(~ beta1/(1 + exp(beta2+beta3*time)), # rhs of model
               c('beta1', 'beta2', 'beta3'), # parameter names
               function(beta1, beta2, beta3, time){} # arguments for result
               )
summary(nls(population ~ model(beta1, beta2, beta3, time),
            start=list(beta1=350, beta2=4.5, beta3=-0.3)))


<h2>Standard tests</h2>

# Goal: Some of the standard tests

# A classical setting --
x = runif(100, 0, 10)                  # 100 draws from U(0,10)
y = 2 + 3*x + rnorm(100)               # beta = [2, 3] and sigma is 1
d = lm(y ~ x)
# CLS results --
summary(d)

library(sandwich)
library(lmtest)
# Durbin-Watson test --
dwtest(d, alternative="two.sided")
# Breusch-Pagan test --
bptest(d)
# Heteroscedasticity and autocorrelation consistent (HAC) tests
coeftest(d, vcov=kernHAC)

# Tranplant the HAC values back in --
library(xtable)
sum.d = summary(d)
xtable(sum.d)
sum.d$coefficients[1:2,1:4] = coeftest(d, vcov=kernHAC)[1:2,1:4]
xtable(sum.d)


<h2>Using orthogonal polynomials</h2>

# Goal: Experiment with fitting nonlinear functional forms in
#       OLS, using orthogonal polynomials to avoid difficulties with
#       near-singular design matrices that occur with ordinary polynomials.
#       Shriya Anand, Gabor Grothendieck, Ajay Shah, March 2006.

# We will deal with noisy data from the d.g.p. y = sin(x) + e
x = seq(0, 2*pi, length.out=50)
set.seed(101)
y = sin(x) + 0.3*rnorm(50)
basicplot = function(x, y, minx=0, maxx=3*pi, title=") {
  plot(x, y, xlim=c(minx,maxx), ylim=c(-2,2), main=title)
  lines(x, sin(x), col="blue", lty=2, lwd=2)
  abline(h=0, v=0)
}
x.outsample = seq(0, 3*pi, length.out=100)

# Severe multicollinearity with ordinary polynomials
x2 = x*x
x3 = x2*x
x4 = x3*x
cor(cbind(x, x2, x3, x4))
# and a perfect design matrix using orthogonal polynomials
m = poly(x, 4)
all.equal(cor(m), diag(4))              # Correlation matrix is I.

par(mfrow=c(2,2))
# Ordinary polynomial regression --
  p = lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
  summary(p)
  basicplot(x, y, title="Polynomial, insample") # Data
  lines(x, fitted(p), col="red", lwd=3)  # In-sample
  basicplot(x, y, title="Polynomial, out-of-sample")
  predictions.p = predict(p, list(x = x.outsample))    # Out-of-sample
  lines(x.outsample, predictions.p, type="l", col="red", lwd=3)
  lines(x.outsample, sin(x.outsample), type="l", col="blue", lwd=2, lty=2)
  # As expected, polynomial fitting gives terrible results out of sample.

# These IDENTICAL things using orthogonal polynomials
  d = lm(y ~ poly(x, 4))
  summary(d)
  basicplot(x, y, title="Orth. poly., insample") # Data
  lines(x, fitted(d), col="red", lwd=3)  # In-sample
  basicplot(x, y, title="Orth. poly., out-of-sample")
  predictions.op = predict(d, list(x = x.outsample))    # Out-of-sample
  lines(x.outsample, predictions.op, type="l", col="red", lwd=3)
  lines(x.outsample, sin(x.outsample), type="l", col="blue", lwd=2, lty=2)

# predict(d) is magical! See ?SafePrediction
# The story runs at two levels. First, when you do an OLS model,
# predict()ion requires applying coefficients to an appropriate
# X matrix. But one level deeper, the polynomial or orthogonal-polynomial
# needs to be utilised for computing the X matrix based on the
# supplied x.outsample data.
# If you say p = poly(x, n)
# then you can say predict(p, new) where predict.poly() gets invoked.
# And when you say predict(lm()), the full steps are worked out for
# you automatically: predict.poly() is used to make an X matrix and
# then prediction based on the regression results is done.

all.equal(predictions.p, predictions.op) # Both paths are identical for this
                                         # (tame) problem.


<h2>A function that takes a model specification as an argument</h2>

# Goal: R syntax where model specification is an argument to a function.

# Invent a dataset
x = runif(100); y = runif(100); z = 2 + 3*x + 4*y + rnorm(100)
D = data.frame(x=x, y=y, z=z)

amodel = function(modelstring) {
  summary(lm(modelstring, D))
}

amodel(z ~ x)
amodel(z ~ y)



<h4>Time-series analysis</h4>
<h2>ARMA estimation, diagnostics, forecasting</h2>

# Goals: ARMA modeling - estimation, diagnostics, forecasting.


# 0. SETUP DATA
rawdata = c(-0.21,-2.28,-2.71,2.26,-1.11,1.71,2.63,-0.45,-0.11,4.79,5.07,-2.24,6.46,3.82,4.29,-1.47,2.69,7.95,4.46,7.28,3.43,-3.19,-3.14,-1.25,-0.50,2.25,2.77,6.72,9.17,3.73,6.72,6.04,10.62,9.89,8.23,5.37,-0.10,1.40,1.60,3.40,3.80,3.60,4.90,9.60,18.20,20.60,15.20,27.00,15.42,13.31,11.22,12.77,12.43,15.83,11.44,12.32,12.10,12.02,14.41,13.54,11.36,12.97,10.00,7.20,8.74,3.92,8.73,2.19,3.85,1.48,2.28,2.98,4.21,3.85,6.52,8.16,5.36,8.58,7.00,10.57,7.12,7.95,7.05,3.84,4.93,4.30,5.44,3.77,4.71,3.18,0.00,5.25,4.27,5.14,3.53,4.54,4.70,7.40,4.80,6.20,7.29,7.30,8.38,3.83,8.07,4.88,8.17,8.25,6.46,5.96,5.88,5.03,4.99,5.87,6.78,7.43,3.61,4.29,2.97,2.35,2.49,1.56,2.65,2.49,2.85,1.89,3.05,2.27,2.91,3.94,2.34,3.14,4.11,4.12,4.53,7.11,6.17,6.25,7.03,4.13,6.15,6.73,6.99,5.86,4.19,6.38,6.68,6.58,5.75,7.51,6.22,8.22,7.45,8.00,8.29,8.05,8.91,6.83,7.33,8.52,8.62,9.80,10.63,7.70,8.91,7.50,5.88,9.82,8.44,10.92,11.67)

# Make a R timeseries out of the rawdata: specify frequency & startdate
gIIP = ts(rawdata, frequency=12, start=c(1991,4))
print(gIIP)
plot.ts(gIIP, type="l", col="blue", ylab="IIP Growth (%)", lwd=2,
        main="Full data")
grid()

# Based on this, I decide that 4/1995 is the start of the sensible period.
gIIP = window(gIIP, start=c(1995,4))
print(gIIP)
plot.ts(gIIP, type="l", col="blue", ylab="IIP Growth (%)", lwd=2,
        main="Estimation subset")
grid()

# Descriptive statistics about gIIP
mean(gIIP); sd(gIIP); summary(gIIP);
plot(density(gIIP), col="blue", main="(Unconditional) Density of IIP growth")
acf(gIIP)


# 1. ARMA ESTIMATION
m.ar2 = arima(gIIP, order = c(2,0,0))
print(m.ar2)                       # Print it out


# 2. ARMA DIAGNOSTICS
tsdiag(m.ar2)                      # His pretty picture of diagnostics
## Time series structure in errors
print(Box.test(m.ar2$residuals, lag=12, type="Ljung-Box"));
## Sniff for ARCH
print(Box.test(m.ar2$residuals^2, lag=12, type="Ljung-Box"));
## Eyeball distribution of residuals
plot(density(m.ar2$residuals), col="blue", xlim=c(-8,8),
     main=paste("Residuals of AR(2)"))


# 3. FORECASTING
## Make a picture of the residuals
plot.ts(m.ar2$residual, ylab="Innovations", col="blue", lwd=2)
s = sqrt(m.ar2$sigma2)
abline(h=c(-s,s), lwd=2, col="lightGray")

p = predict(m.ar2, n.ahead = 12)         # Make 12 predictions.
print(p)

## Watch the forecastability decay away from fat values to 0.
## sd(x) is the naive sigma. p$se is the prediction se.
gain = 100*(1-p$se/sd(gIIP))
plot.ts(gain, main="Gain in forecast s.d.", ylab="Per cent",
        col="blue", lwd=2)

## Make a pretty picture that puts it all together
ts.plot(gIIP, p$pred, p$pred-1.96*p$se, p$pred+1.96*p$se,
        gpars=list(lty=c(1,1,2,2), lwd=c(2,2,1,1),
          ylab="IIP growth (%)", col=c("blue","red", "red", "red")))
grid()
abline(h=mean(gIIP), lty=2, lwd=2, col="lightGray")
legend(x="bottomleft", cex=0.8, bty="n",
       lty=c(1,1,2,2), lwd=c(2,1,1,2),
       col=c("blue", "red", "red", "lightGray"),
       legend=c("IIP", "AR(2) forecasts", "95% C.I.", "Mean IIP growth"))

<h2>Web Scrapping</h2><a href="https://aidenloe.github.io/webscrapping.html#scrape_from_discussion_forums" class="whitebut ">Web Scrapping</a>
<h3>Scrape content (Wiki)</h3>
We will be using the <code>RCurl</code> and <code>XML</code> package to help us with the scrapping.

Let’s use the Eurovision_Song_Contest as an example.

The <code>XML</code> package has plenty functions that can allow us to scrape the data.

Usually we are extracting information based on the tags of the web pages.

<code>##### SCRAPPING CONTENT OFF WEBSITES ######
require(RCurl)
require(XML)
# XPath is a language for querying XML 
# //Select anywhere in the document
# /Select from root
# @select attributes. Used in [] brackets

#### Wikipedia Example ####
url &lt;- "https://en.wikipedia.org/wiki/Eurovision_Song_Contest"
txt = getURL(url) # get the URL html code

# parsing html code into readable format
PARSED &lt;- htmlParse(txt)

# Parsing code using tags
xpathSApply(PARSED, "//h1")

# strops code and return content of the tag
xpathSApply(PARSED, "//h1", xmlValue) # h1 tag
xpathSApply(PARSED, "//h3", xmlValue) # h3 tag
xpathSApply(PARSED, "//a[@href]") # a tag with href attribute

# Go to url 
# Highlight references
# right click, inspect element
# Search for tags
xpathSApply(PARSED, "//span[@class='reference-text']",xmlValue) # parse notes and citations
xpathSApply(PARSED, "//cite[@class='citation news']",xmlValue) # parse citation news
xpathSApply(PARSED, "//span[@class='mw-headline']",xmlValue) # parse headlines
xpathSApply(PARSED, "//p",xmlValue) # parsing contents in p tag
xpathSApply(PARSED, "//cite[@class='citation news']/a/@href") # parse links under citation. xmlValue not needed. 
xpathSApply(PARSED, "//p/a/@href") # parse href links under all p tags
xpathSApply(PARSED, "//p/a/@*") # parse all atributes under all p tags

# Partial matches - subtle variations within or between pages. 
xpathSApply(PARSED, "//cite[starts-with(@class, 'citation news')]",xmlValue) # parse citataion news that starts with..
xpathSApply(PARSED, "//cite[contains(@class, 'citation news')]",xmlValue) # parse citataion news that contains.

# Parsing tree like structure
parsed&lt;-   htmlTreeParse(txt, asText = TRUE)</code>

<h3>Scrape content (BBC)</h3>
When you know the structure of the data.

All you need to do is to find the correct function to scrape.

<code>##### BBC Example ####
url &lt;- "https://www.bbc.co.uk/news/uk-england-london-46387998"
url &lt;- "https://www.bbc.co.uk/news/education-46382919"
txt = getURL(url) # get the URL html code

# parsing html code into readable format
PARSED &lt;- htmlParse(txt)
xpathSApply(PARSED, "//h1", xmlValue) # h1 tag
xpathSApply(PARSED, "//p", xmlValue) # p tag
xpathSApply(PARSED, "//p[@class='story-body__introduction']", xmlValue) # p tag body
xpathSApply(PARSED, "//div[@class='date date--v2']",xmlValue) # date, only the first is enough
xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content") # sometimes there is meta data. </code>

<h3>Create simple BBC scrapper</h3>
Sometimes, creating a function will make your life better and make your script look simpler.

<code>##### Create simple BBC scrapper #####
# scrape title, date and content
BBCscrapper1&lt;- function(url){
  txt = getURL(url) # get the URL html code
  PARSED &lt;- htmlParse(txt) # Parse code into readable format
  title &lt;- xpathSApply(PARSED, "//h1", xmlValue) # h1 tag
  paragraph &lt;- xpathSApply(PARSED, "//p", xmlValue) # p tag
  date &lt;- xpathSApply(PARSED, "//div[@class='date date--v2']",xmlValue) # date, only the first is enough
  date &lt;- date[1]
  return(cbind(title,date))
  #return(as.matrix(c(title,date)))
}

# Use function that was just created. 
BBCscrapper1("https://www.bbc.co.uk/news/education-46382919")</code>

<code>##      title                                                         
## [1,] "Ed Farmer: Expel students who defy initiations ban, says dad"
##      date              
## [1,] "29 November 2018"</code>

<h3>Keeping it neat</h3>
Using the <code>plyr</code> package helps to arrange the data in an organised way.

<code>## Putting the title and date into a dataframe
require(plyr)
#url
url&lt;- c("https://www.bbc.co.uk/news/uk-england-london-46387998", "https://www.bbc.co.uk/news/education-46382919")
## ldply: For each element of a list, apply function then combine results into a data frame
#put into a dataframe
ldply(url,BBCscrapper1)</code>

<code>##                                                          title
## 1              Man murdered widow, 80, in London allotment row
## 2 Ed Farmer: Expel students who defy initiations ban, says dad
##               date
## 1 29 November 2018
## 2 29 November 2018</code>

<h1>Web Scrapping (Part 2)</h1>
This example below is taken from code kindly written by David stillwell.

Some editing has been made to the original code.

<h3>Scrape from Wiki tables</h3>
You have learned how to scrape viewership on wikipedia and content on web pages.

This section is about scrapping data tables online.

<code># Install the packages that you don't have first. 
library("RCurl") # Good package for getting things from URLs, including https
library("XML") # Has a good function for parsing HTML data
library("rvest") #another package that is good for web scraping. We use it in the Wikipedia example

#####################
### Get a table of data from Wikipedia
## all of this happens because of the read_html function in the rvest package
# First, grab the page source
us_states = read_html("https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population") %&gt;% # piping
  # then extract the first node with class of wikitable
  html_node(".wikitable") %&gt;% 
  # then convert the HTML table into a data frame
  html_table()</code>

<h3>Scrape from online tables</h3>
If we can have two data tables that have at least one column with the same name, then we can merge them together.

The main idea is to link the data together to run simple analysis.

In this case we can get data about <a href="http://apps.saferoutesinfo.org/legislation_funding/state_apportionment.cfm">funding</a> given to various US states to support building infrastructure to improve students’ ability to walk and bike to school.

<code>######################
url &lt;- "http://apps.saferoutesinfo.org/legislation_funding/state_apportionment.cfm"
funding&lt;-htmlParse(url) #get the data

# find the table on the page and read it into a list object
funding&lt;- XML::readHTMLTable(funding,stringsAsFactors = FALSE)

funding.df &lt;- do.call("rbind", funding) #flatten data
# Contain empty spaces previously.
colnames(funding.df)[1]&lt;- c("State") # shorten colname to just State. 

# Match up the tables by State/Territory names
# so we have two data frames, x and y, and we're setting the columns we want to do the matching on by setting by.x and by.y
mydata = merge(us_states, funding.df, by.x="State, federal district, or territory", by.y="State")
# it looks pretty good, but note that we're down to 50 US States, because the others didn't match up by name
# e.g. "District of Columbia" in the us_states data, doesn't match "Dist. of Col." in the funding data

#Replace the total spend column name with a name that's easier to use.
colnames(mydata)[18] = "total_spend"

#  We need to remove commas so that R can treat it as a number.
mydata[,"Population estimate, July 1, 2017[4]"] = gsub(",", ", mydata[,"Population estimate, July 1, 2017[4]"]) 
mydata[,"Population estimate, July 1, 2017[4]"] = as.numeric(mydata[,"Population estimate, July 1, 2017[4]"]) #this converts it to a number data type

# Now we have to do the same thing with the funding totals, which are in a format like this: $17,309,568
mydata[,"total_spend"] = gsub(",", ", mydata[,"total_spend"]) #this removes all commas
mydata[,"total_spend"] = gsub("\\$", ", mydata[,"total_spend"]) #this removes all dollar signs. We have a \\ because the dollar sign is a special character.
mydata[,"total_spend"] = as.numeric(mydata[,"total_spend"]) #this converts it to a number data type

# Now we can do the plotting
options(scipen=9999) #stop it showing scientific notation
plot(mydata[,"Population estimate, July 1, 2017[4]"], mydata[,"total_spend"])</code>

<code>## What's does the correlation between state funding and state population look like?
cor(mydata[,"Population estimate, July 1, 2017[4]"], mydata[,"total_spend"]) # 0.9924265 - big correlation!</code>

<code>## [1] 0.9885666</code>

<h3>Plot funding data on map</h3>
Perhaps it might be more interesting to see how the data is like on a map.

We can utilise <code>map_data</code> function in the <code>ggplot</code> package to help us with that.

Again, with a bit of data manipulation, we can merge the data table that contains the longitude and latitude information together with the funding data across different states.

<code>require(ggplot2)
all_states &lt;- map_data("state") # states
colnames(mydata)[1] &lt;- "state" # rename to states
mydata$state &lt;- tolower(mydata$state) #set all to lower case
Total &lt;- merge(all_states, mydata, by.x="region", by.y = 'state') # merge data
# we have data for delaware but not lat, long data in the maps
i &lt;- which(!unique(all_states$region) %in% mydata$state) 

# Plot data
ggplot() + 
  geom_polygon(data=Total, aes(x=long, y=lat, group = group, fill=Total$total_spend),colour="white") + 
  scale_fill_continuous(low = "thistle2", high = "darkred", guide="colorbar") + 
  theme_bw()  + 
  labs(fill = "Funding for School" ,title = "Funding for School between 2005 to 2012", x=", y=") + 
  scale_y_continuous(breaks=c()) +
  scale_x_continuous(breaks=c()) +
  theme(panel.border =  element_blank(),
        text = element_text(size=20))</code>


<h2>XPath for Web Scraping</h2>
<a href="https://www.opencodez.com/how-to-guide/how-to-use-xpath-for-web-scraping-with-r.htm" class="whitebut ">XPath for Web Scraping</a>

We have already learned about Web Scraping Technology in our previous post <a href="https://www.opencodez.com/web-development/web-scraping-using-beautiful-soup-part-1.htm" target="_blank">Web Scraping Using Beautiful Soup in Python.</a> In addition to that, a learner/developer might also be interested in fetching nodes/elements from the HTML or XML document using XPaths.

<h3>XPath For Web Scraping with R:</h3>
This article essentially elaborates on XPath and explains how to use XPath for web scraping with <a href="https://en.wikipedia.org/wiki/R_(programming_language)" target="_blank">R Programming language</a>.

<h3>What is XPath</h3>
XPath stands for XML Path Language. 
It is a query language to extract nodes from HTML or XML documents.

<h3>Required Tools and Knowledge</h3>

R Programming Language
XML Package
HTML/XML

<h3>How to get XPath in Mozilla Firefox Browser</h3>
Let us see how to find out XPath of any element on <a href="https://www.opencodez.com/" target="_blank">www.opencodez.com</a> using the Mozilla Firefox browser. 
We want to identify the XPath for the heading text of the first article on the home page. 
When we right-click on the highlighted element, we can find the Inspect Element option. 
A screenshot is attached below.


<img class="lazy" src="https://www.opencodez.com/wp-content/uploads/2020/02/Title-Tag-Inspect-Element-1024x511.png">

Observing the element HTML Code, we can identify that our target text is contained in the &#8216;a' tag. 
(highlighted in blue at the lower section of the screenshot). 
Next, we need to right-click on the blue highlight. 
Another box with several options opens up. 
Click on &#8220;Copy&#8221; which will show us new options. 
There will be an XPath option also. 
Click on that. 
Have a look at it in the below screenshot.


<img class="lazy" src="https://www.opencodez.com/wp-content/uploads/2020/02/XPath-option-1024x526.png">

Copy this XPath in any text file and check how does it look like. 
The XPath copied is /html/body/div[2]/div/div/div/div[1]/div[1]/article[1]/header/h2/a.

<h3>Absolute and Relative XPath</h3>
<h3>Absolute Path &#8211;</h3>
The XPath provided above is called the absolute path. 
It starts with &#8216;/' and traverses from the root node to the target node. 
Let us take a look if this XPath is correctly identified by Firefox. 
The set of commands is provided below.

Absolute Path

library(XML)
url &lt;- "https://www.opencodez.com/"
source &lt;- readLines(url, encoding = "UTF-8")
parsed_doc &lt;- htmlParse(source, encoding = "UTF-8")
xpathSApply(parsed_doc, path = '/html/body/div[2]/div/div/div/div[1]/div[1]/article[1]/header/h2/a', xmlValue)

12345

library(XML)url &lt;- "https://www.opencodez.com/"source &lt;- readLines(url, encoding = "UTF-8")parsed_doc &lt;- htmlParse(source, encoding = "UTF-8")xpathSApply(parsed_doc, path = '/html/body/div[2]/div/div/div/div[1]/div[1]/article[1]/header/h2/a', xmlValue)

When we run the commands in R Studio, we find that the result is a NULL. 
The corrected XPath is provided below.

Absolute XPath

xpathSApply(parsed_doc, path = '/html/body/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/article[1]/header/h2/a', xmlValue)

xpathSApply(parsed_doc, path = '/html/body/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/article[1]/header/h2/a', xmlValue)

xpathSApply is a function available in the XML library in R. 
xmlValue is the argument we need to pass so that we get the value of the target node. 
In our case its the heading of the article.

<h3>Relative Path &#8211;</h3>
We can create a short and concise path to our target node by using &#8216;//' to jump between nodes. 
For example, the above absolute path can also be written as //h2/a. 
This path also points to our target &#8216;a' tag. 
There can be other ways to represent this path as well. 
Now let us take a look at the command to extract the heading text.

Relative XPath

xpathSApply(doc = parsed_doc, path ="//h2/a", xmlValue)[1]

xpathSApply(doc = parsed_doc, path ="//h2/a", xmlValue)[1]

The output is a character vector with 22 values in it. 
A snapshot of the output is provided below. 
Hence we need to fetch the first text by using [1] in the command.


<img class="lazy" src="https://www.opencodez.com/wp-content/uploads/2020/02/Character-output-1024x598.png">

<h3>Other ways to represent XPaths</h3>
<h3>Wildcard Operator * &#8211;</h3>
The wildcard operator * matches any (single) node with an arbitrary name at its position. 
In our case, a wildcard operated XPath will look like below.

Wildcard XPath

(xpathSApply(doc = parsed_doc, path ="//h2/*", xmlValue))[1]

(xpathSApply(doc = parsed_doc, path ="//h2/*", xmlValue))[1]

<h3>Wildcard Operator . 
and .. 
&#8211;</h3>
Here we are going to explain two more operators &#8216;.' and &#8216;..' and its usage in the XPath command. 
The . 
operator selects the current nodes (or self-axis) in a selected node-set. 
The .. 
operator selects the node one level up the hierarchy from the current node. 
Let us all try this for ourselves. 
Do share the commands or scenarios in the comments where any difficulty is faced.

<h3>Numerical Predicate &#8211;</h3>
Some predicates or functions can also be used to pinpoint nodes using position, last or count in the command. 
Our target node XPath will change in the below manner.

Numerical Predicates

#Position
xpathSApply(doc = parsed_doc, path ="//h2[position()=1]", xmlValue)[1]

#Last
xpathSApply(doc = parsed_doc, path ="//h2[last()]", xmlValue)[1]

#Count
xpathSApply(parsed_doc,"//h2[count(.//a)&gt;0]", xmlValue)[1]

#PositionxpathSApply(doc = parsed_doc, path ="//h2[position()=1]", xmlValue)[1] #LastxpathSApply(doc = parsed_doc, path ="//h2[last()]", xmlValue)[1] #CountxpathSApply(parsed_doc,"//h2[count(.//a)&gt;0]", xmlValue)[1]

<h4>Position &#8211;</h4>
We are trying to locate all the h2 tags which have got the first position in the node tree structure. 
As explained earlier, this will generate a character of 22 values. 
Our &#8216;position' command extracts the first value because of [1] in the command.

<h4>Last &#8211;</h4>
Similar to the above, all the h2 tags which are last in the tree structure will be extracted. 
The first value can be fetched using [1]. 
We can experiment to fetch other article headings by changing the value inside the box bracket.

<h4>Count &#8211;</h4>
The command looks a bit scary!! But don't be. 
It simply extracts all the h2 nodes which have got &#8216;a' tag present. 
If we observe the &#8216;h2' tags in the HTML code, we will notice that all the &#8216;h2' tags do have a child tag &#821#8216;a' and so this command is the same as the one we saw in the relative path section. 
With the presence of [1], it allows us to fetch the first article heading text.

If you were facing issues working with the . 
operator command as suggested earlier, this example should provide you with some understanding.

<h3>Text Predicate &#8211;</h3>
We can also locate some nodes with the manipulation of text related predicates. 
Explaining this will require a change in our target node. 
Text predicates help us in cases where we want to extract text which contains a specific word or characters or let say has a length condition on characters. 
Let us see some commands.

<h4>Contains &#8211;</h4>

Text Predicate

library(stringr)
xpathSApply(parsed_doc,"//a[contains(text(), '10')]", xmlValue)

library(stringr)xpathSApply(parsed_doc,"//a[contains(text(), '10')]", xmlValue)

The above command will throw a list of headings that have got 10 in its text. 
The output snapshot is provided below.


<img class="lazy" src="https://www.opencodez.com/wp-content/uploads/2020/02/10.png">

<h4>Starts-with &#8211;</h4>
When we want to fetch text of any attribute which starts with a particular string pattern, we can use starts-with predicate in the command. 
Attributes in any tag are addressed using &#8216;@' symbol. 
Have a look at the command. 
Do try to understand the output for this and let us know if you face any difficulty.

starts-with

xpathSApply(parsed_doc,"//a[starts-with(./@title, '10')]", xmlValue)

xpathSApply(parsed_doc,"//a[starts-with(./@title, '10')]", xmlValue)

<h3>XPath Node Relations &#8211;</h3>
A very interesting way to prepare XPaths is by understanding the tree analogy of the nodes in the HTML code structure. 
As is usual in describing tree-structured data formats, we employ notation based on family relationships (child, parent, grandparent, …) to describe the between-node relations. 
The construction of a proper XPath statement that employs this feature follows the pattern node1/<i>relation</i>::node2, where node2 has a specific <i>relation</i> to node1. 
 Let us see some examples.

<h3>Ancestor &#8211;</h3>

Ancestor

xpathSApply(parsed_doc,"//a/ancestor::article", xmlValue)

xpathSApply(parsed_doc,"//a/ancestor::article", xmlValue)

The command locates and fetches all the article tags which are an ancestor to a tag.

<h4>Child &#8211;</h4>

Child

xpathSApply(parsed_doc,"//div[position()=1]/child::article", xmlValue)

xpathSApply(parsed_doc,"//div[position()=1]/child::article", xmlValue)

The command locates and fetches all article tags which is a child of div tag in the first position in the tree structure.

There are many other such relations that we can utilize like a sibling, preceding-sibling, descendant, following, etc.

<h3>Conclusion &#8211;</h3>
I hope you found this step by step detailed guide on XPath for Web Scraping with R useful. 
There are many more options with which we can create XPaths apart from the ones we have explained in this article.

It is encouraged that the reader tries these commands themselves to practice and gain a deeper understanding of a faster smoother experience with Web Scraping. 
Do comment if you want to understand any specific XPath command, if you face any error or you want to know about any other concept related to Web Scraping.

<h2>web scraping</h2>
url &lt;- 'http://www.r-datacollection.com/materials/html/fortunes.html'

install.packages('XML')
library(XML)
parsed_doc &lt;- htmlParse(url)
parsed_doc &lt;- htmlParse(getURL(apple.news.url, .encoding = 'utf8'))

分析 html
這邊用絕對路徑抓取 Tag 裡的文字

xpathApply : 會多加 Tag 屬性

xpathSApply : 只抓取文字
xpathSApply(doc = parsed_doc, path = '/html/body/div/p/i')


相對路徑 ( 推薦 ) : 兩條斜線
xpathSApply(doc = parsed_doc, path = '//div/p/i')

/* 簡化 */
xpathSApply(parsed_doc, '//div/p/i')

萬用字元
&lt;div>底下所有 i
xpathSApply(parsed_doc, '//html/body/div/*/i')
[[1]]
&lt;i>'What we have is nice, but we need something very different'&lt;/i>
[[2]]
&lt;i>'R is wonderful, but it cannot work magic'&lt;/i>

抓取條件

.. : 上一層
xpathSApply(parsed_doc, '//title/..')
&lt;head>
  &lt;title>Collected R wisdoms&lt;/title>
&lt;/head>


| : 或
xpathSApply(parsed_doc, '//address | //title')
[[1]]
&lt;title>Collected R wisdoms&lt;/title>
[[2]]
&lt;address>
  &lt;a href="www.r-datacollectionbook.com">
    &lt;i>The book homepage&lt;/i>
  &lt;/a>
  &lt;a/>
&lt;/address>

利用變數 text2 節省時間，下次要用就直接呼叫
text2 &lt;- c(address = '//address', title = '//title')
xpathSApply(parsed_doc, text2)

節點關係

ancestor : 由當前節點向上，從父節點至根節點
xpathSApply(parsed_doc, '//a/ancestor::div')
xpathSApply(parsed_doc, '//a/ancestor::div//i')
/* 說明 */
a 節點以上是 div 的
a 節點以上是 div 的 i

節點底下有 title 的都抓下來，title 上面有兩層，所以會有兩組結果
xpathSApply(parsed_doc, '//title/ancestor::*')

ancestor-or-self : 由當前節點向上，從當前節點 到父節點直至根節點，就是多一組包含自己
xpathSApply(parsed_doc, '//title/ancestor-or-self::*')


attribute : 抓出屬性
xpathSApply(parsed_doc, '//a/attribute::*')
/* 結果 */
href 
"https://stat.ethz.ch/mailman/listinfo/r-help" 
href 
"www.r-datacollectionbook.com"
xpathSApply(parsed_doc, '//div/attribute::*')
/* 結果 */
id           lang           date           lang           date 
  "R Inventor"      "english"    "June/2003"      "english" "October/2011"


child : 子節點
xpathSApply(parsed_doc, '//div/child::h1')
/* 說明 */
div 底下是 h1 的

descendant : 底下所有子節點，不管第幾層

descendant-or-self : 多一個自己
xpathSApply(parsed_doc, '//div/descendant::*')

following : 當前節點的後續節點(子節點除外)
xpathSApply(parsed_doc, "//div/following::*")

following-sibling : 同一層兄弟底下
xpathSApply(parsed_doc, "//div/following-sibling::*")

namespace : 命名空間搜尋
xpathSApply(parsed_doc, "*[name() = 'div']")


parent : 父節點
xpathSApply(parsed_doc, "//i/parent::a")
/* 說明 */
i 的上一層是 a

preceding : 上一個兄弟節點
xpathSApply(parsed_doc, "//i/preceding::a")
/* 說明 */
i 上一個兄弟是 a

preceding-sibling : 同一層兄弟
xpathSApply(parsed_doc, "//body/preceding-sibling::*")

self : 自己
xpathSApply(parsed_doc, "//body/self::*")
屬性抓值

text() : 第一個結果包含文字
xpathSApply(parsed_doc,"//*[text()='The book homepage']")

attribute : 屬性，用 @ 表示
xpathSApply(parsed_doc,"//div[@id='R Inventor']")
xpathSApply(parsed_doc,"//div[@date='October/2011']")

string-length() : 字串的長度
xpathSApply(parsed_doc, '//h1[string-length() > 5]')
/* 說明 */
字串長度大於5

contains(str1,str2) : 包含屬性
xpathSApply(parsed_doc, "//*[contains(text(),'Source')]")
/* 說明 */
文字包含 Source

starts-with(str1,str2) : 字串開頭
xpathSApply(parsed_doc,"//i[starts-with(text(),'The')]")
/* 說明 */
開頭是 The

substring-before : 篩選條件之前

substring-after : 篩選條件之後
xpathSApply(parsed_doc,"//div[substring-before(@date,'/')='June']")
/* 說明 */
屬性是 date 裡面 / 之前是 June 的
xpathSApply(parsed_doc, "//a[substring-after(@href,'k.') = 'com']")

not() : 不包含的
xpathSApply(parsed_doc,"//div[not(contains(@id,'Inventor'))]")

local-name() : 網頁名稱
xpathSApply(parsed_doc,"//*[local-name()='address']")
[[1]]
&lt;address>
  &lt;a href="www.r-datacollectionbook.com">
    &lt;i>The book homepage&lt;/i>
  &lt;/a>
  &lt;a/>
&lt;/address>

count() : 節點個數
xpathSApply(parsed_doc,"//div[count(.//a)>0]")
/* 說明 */
.就是div自己，數字可改

position() : 位置
xpathSApply(parsed_doc,"//div/p[position()=1]")
/* 說明 */
div 底下第一個 p，數字可改

last() : 最後一個節點
xpathSApply(parsed_doc,"//div/p[last()]")

萃取函式

|  Function   |  回傳值  

| xmlValue    | 節點內容
| xmlName     | Tag名稱
| xmlAttrs    | 所有屬性
| xmlGetAttr  | 指定屬性
| xmlChildren | 子節點  
| xmlSize     | 節點數量

範例
xpathSApply(parsed_doc,"//title", xmlValue)
xpathSApply(parsed_doc,"//div",xmlAttrs)
xpathSApply(parsed_doc,"//div",xmlGetAttr,"lang")
xpathSApply(parsed_doc,"//div/*",xmlName)
xpathSApply(parsed_doc,"//div",xmlChildren)
xpathSApply(parsed_doc,"//body",xmlSize) => 7
自己寫函示

return : 回傳值

require() : 引用， 比如說 require ( stringr )
my-lower &lt;- function(x) {
  x &lt;- tolower(xmlName(x))
  x
}
xpathSApply(parsed_doc,"//div//i",fun = my-lower)

<h2>ggplot</h2>
library("ggplot2")
p = ggplot(mtcars) +
     geom_point(aes(x = wt, y = mpg, colour = factor(gear))) +
     facet_wrap(~am) +
     # Economist puts x-axis labels on the right-hand side
     scale_y_continuous(position = "right")

## Standard
p + theme_economist() + scale_colour_economist()

# Change axis lines to vertical
p + theme_economist(horizontal = FALSE) + scale_colour_economist() + coord_flip()

## White panel/light gray background
p + theme_economist_white() + scale_colour_economist()

## All white variant
p + theme_economist_white(gray_bg = FALSE) + scale_colour_economist()

## The Economist uses ITC Officina Sans
library("extrafont")
p + theme_economist(base_family="ITC Officina Sans") + scale_colour_economist()

## Verdana is a widely available substitute
p + theme_economist(base_family="Verdana") + scale_colour_economist()

<h2>A basic plot</h2>
# load data
managers_energy = read.csv("managers_energy_data.csv")
simulation = read.csv("simulation.csv")

Next, we'll make a basic ggplot. Compared to other plotting languages, ggplot syntax might seem weird at first. In ggplot, we build the plot one layer at a time. The first thing we do is create a blank canvas by calling the ggplot() command:

# blank ggplot
manager_plot =  ggplot() 

This creates a blank ggplot called manager_plot. To this canvas, we'll add different 'geometric objects'. In ggplot notation, these geometric objects are called a geom. The geom tells ggplot how we want the data represented. To represent the data using points, we use geom_point. To represent the data using lines, we use geom_line, and so on. Here we'll use points:

# basic ggplot syntax
manager_plot =  ggplot() + geom_point()

This is the basic syntax of a ggplot chart. We first evoke ggplot, and then add features to the plot using the + sign.

Next we need to add data. Inside geom_point, we tell ggplot to use managers_energy as the source data:

# add data
manager_plot =  ggplot() + geom_point(data = managers_energy)

We can also put the command data = managers_energy inside the ggplot() command, as in ggplot(data = managers_energy). Personally, I don't like to do this because my plots usually combine different datasets. Putting the data inside the ggplot() command locks the whole chart into using only that data.

Next, we tell ggplot about the 'aesthetics' we want, using the aes() command. We tell ggplot that the x-axis should plot energy_pc and the y-axis should plot managers_employment_share. This gives us the syntax for a basic ggplot:

# basic plot of managers vs. energy use
manager_plot =  ggplot() + geom_point(data = managers_energy,
                aes(x = energy_pc, y = managers_employment_share))

Refining the chart
The secret to good data visualization, I've found, is the refinements that come after you've created a basic chart. These refinements highlight the aspects of the data that you want to showcase.

First, let's refine the size of our data points. My philosophy is that the point size of scatter plots should vary inversely with the number of points. If you have only a few data observations, you want large points so you can see the data. But if you have many data observations (thousands or millions), you want to shrink the point size so that you can actually see all the data.

In our managers plot, we've go quite a few data observations. So let's shrink the point size from the ggplot default. To do this, we'll put size = 0.8 inside geom_point. For reasons that I'll discuss later, this size command doesn't go inside the aesthetic command aes().


# smaller point size
manager_plot = ggplot() +
                geom_point( data = managers_energy,
                size = 0.8,
                aes(x = energy_pc, y = managers_employment_share))

Reducing the point size in our scatter plot gives us:

fig_02_size
Smaller point size
The next thing I notice about the plot is that the data is crushed against the origin. When you see this happen, it's a good sign that you need to use logarithmic scales. Log scales spread the data out so that we can see variation in all the observations, not just the largest ones.

Let's tell ggplot to use logarithmic scales instead of linear scales:


# add log scales
manager_plot =  manager_plot +
                scale_x_log10() +
                scale_y_log10()

Here I'm using an interesting feature of ggplot --- it let's you recursively add layers to your plot. Having defined manager_plot, we tell ggplot to change the axes by adding commands to the original plot. To be honest, I don't use this recursive feature very often. But it's useful here because I can highlight the new code that I've adding with each refinement to the chart. Changing to log scales gives us:

fig_03_log_scale
Add log scales
Now the scatter plot looks much better. We can actually see the trend across countries.

Next, let's tweek the values on the axes. When log scales span only a few orders of magnitude, I like to add numbers in between the factors of ten. To change the axis numbers, we use the breaks command. To make custom breaks, we use the concatenate command c(). If I wanted axis labels of 1, 5, and 10, I'd write breaks = c(1, 5, 10). Here's the custom breaks that I'll use:


# better axis breaks
manager_plot = manager_plot +
  scale_x_log10(breaks = c(5,10,20,50,100,200,500,1000)) +
  scale_y_log10(breaks = c(0.1,0.2,0.5,1,2,5,10,20))

This gives a plot with better axis numbers:

fig_04_breaks
Better axis breaks
Next, let's fix our axis labels. By default, ggplot will use your variable names as the axis labels. This is rarely what you want in your final plot. To change the axis labels we use the command labs(). While we're at it, we'll add a title to the chart using ggtitle():


# descriptive labels and title
manager_plot = manager_plot +
  labs(x = "Energy use per capita (GJ)",
       y = "Managers (% of Total Employment)" ) +
  ggtitle("Managers Employment vs. Energy Use")

Now our plot has better labels:

fig_05_labels
Descriptive labels and title
Adding simulation data
To our empirical data, we'll now add the simulation data. We're going to use one of the nicest features of ggplot: the ability to use color to represent changes in a variable. To do this, we put the color command inside the aesthetics, aes().

In our simulation, we want energy_pc on the x-axis, managers_employment_share on the y-axis, and span_of_control in color. To plot this using points, we write:


#plot simulation data with span of control indicated by color
  geom_point(data = simulation,
             aes(x = energy_pc,
                 y = managers_employment_share,
                 color = span_of_control)
             ) 
 
The logic here is that any aesthetic getting mapped onto variables goes inside the aes() command. If I wanted point size to be a function of the span_of_control, I would write:


# point size as function of span of control
  geom_point(data = simulation,
             aes(x = energy_pc,
                 y = managers_employment_share,
                 size = span_of_control)
             ) 

But if I want to set the size of points to a single value, this goes outside the aes() command.


# point size has a single value
  geom_point(data = simulation,
             size = 0.1,
             aes(x = energy_pc,
                 y = managers_employment_share,
                 color = span_of_control)
             ) 

Let's add the simulation data to our management plot. We want the simulation data to appear under the empirical data, so we have to add it to the ggplot before adding the empirical data.

Because we don't want the simulation data to overwhelm the empirical data, we're going to make the simulation data partially transparent. This makes it feel like it's in the background.

In ggplot, we set the transparency of our points using the alpha command. alpha = 0 is completely transparent. alpha = 1 is completely opaque. We'll add alpha = 0.3 inside our geom. Heres the code with the simulation data added to the empirical data, along with all the refinements so far:


# add simulation data
manager_plot = ggplot() +
  geom_point(data = simulation,
             size = 0.1,
             alpha = 0.3,
             aes(x = energy_pc,
                 y = managers_employment_share,
                 color = span_of_control)
             ) +
  geom_point(data = managers_energy,
             size = 0.8,
             aes(x = energy_pc,
                 y = managers_employment_share)
             ) +
  scale_x_log10(breaks = c(5,10,20,50,100,200,500,1000)) +
  scale_y_log10(breaks = c(0.1,0.2,0.5,1,2,5,10,20)) +
  labs(x = "Energy use per capita (GJ)",
       y =  "Managers (% of Total Employment)") +
  ggtitle("Managers Employment vs. Energy Use") 

This code gives us:

fig_06_simulation
Add simulation data
More refinements
After adding the simulation data, we need to do more plot refining. First, the simulation data spans a far greater range than the empirical data. So now our empirical data is compressed into the corner of the chart. We don't want that.

We'll fix this by limiting the x-y range of the chart using the command coord_cartesian(). Inside the command we put the x and y range that we want. I'll restrict x to range from 5 to 1000 and y from 0.1 to 30. We use the concatenate function c() to denote these limits:


# limit plot range
manager_plot = manager_plot +
  coord_cartesian(xlim = c(5,1000), ylim = c(0.1,30)) 

Our plot now looks like this:

fig_07_cartesion
Limit plot range
Notice that ggplot has again used variable names to label the plot, this time for the color legend. We fix this using the labs() command. We want to label the color scale "Span of Control", so we write:


# descriptive label for color legend  
manager_plot = manager_plot +
  labs(color = "Span of Control")

We get:

fig_08_span_label
Descriptive label for color legend
Adding the label creates a new problem. The label is too long and compresses the graph. To fix this, we add a line split to the label using \n:


# line break in legend label
manager_plot = manager_plot +
  labs(color = "Span of\nControl")

We now get:

fig_09_span_label_line
Line break in legend label
Now let's refine the colors used by ggplot to represent the span of control. By default, ggplot uses shades of blue. I prefer to use the whole color spectrum. To represent the span of control using a rainbow with 8 colors, we write:


# rainbow colors for span of control
manager_plot = manager_plot +
  scale_color_gradientn( colours = rainbow(8) )

Now the chart is starting to pop!

fig_10_rainbow
Rainbow colors for span of control
But if we're picky (and we should be), we see that the rainbow on the color legend is upside down compared to the rainbow in the chart. Let's fix that by reversing the direction of the legend:


# reverse color legend
manager_plot = manager_plot +
  scale_color_gradientn(colours = rainbow(8),
                        guide = guide_colourbar(reverse = T))

Now the legend and the chart have matching rainbows:

fig_11_rainbow_reverse
Reverse direction of color legend
The plot theme
The default ggplot theme uses a grey background. We can change the theme using the theme command. I prefer the black and white theme, theme_bw():


# black and white theme
manager_plot = manager_plot + theme_bw()

Our plot now looks like this:

fig_12_black_white
Black and white theme
I also prefer serif fonts over sans-serif. Let's change the font to Times:


# change font to Times
manager_plot = manager_plot +
  theme(text=element_text(size = 10, family="Times"))

Our chart is looking close to the final version:

fig_13_times
Change font to Times
The last thing we'll do is add my personal theme that I use for all my plots. This theme removes the grid lines and flips the tick marks to the inside of the plot box. It also centeres the plot title and makes it bold. Here's the code:


theme(panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      plot.title = element_text(face="bold", size = rel(1), hjust = 0.5),
      axis.line = element_line(color = "black"),
      axis.title.x = element_text(vjust= 0, size=rel(0.9)),
      axis.title.y = element_text(vjust= 1.1, size=rel(0.9)),
      axis.text.x = element_text(margin=margin(5,5,0,0,"pt")),
      axis.text.y = element_text(margin=margin(3,5,0,3,"pt")),
      axis.ticks.length = unit(-0.7, "mm"),
      text=element_text(size = 10, family="Times"))

Putting all the steps together, here's the finished code for the graphic:


# all code with custom theme
manager_plot = ggplot() +
  geom_point(data = simulation,
             size = 0.1,
             alpha = 0.3,
             aes(x = energy_pc,
                 y = managers_employment_share,
                 color = span_of_control)
  ) +
  geom_point(data = managers_energy,
             size = 0.8,
             aes(x = energy_pc,
                 y = managers_employment_share)
  ) +
  scale_x_log10(breaks = c(5,10,20,50,100,200,500,1000)) +
  scale_y_log10(breaks = c(0.1,0.2,0.5,1,2,5,10,20)) +
  labs(x = "Energy use per capita (GJ)",
       y =  "Managers (% of Total Employment)",
       color = "Span of \nControl") +
  ggtitle("Managers Employment vs. Energy Use") +
  coord_cartesian(xlim = c(5,1000), ylim = c(0.1,30)) +
  scale_color_gradientn(colours = rainbow(8),
                        guide=guide_colourbar(reverse = T) ) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      plot.title = element_text(face="bold", size = rel(1), hjust = 0.5),
      axis.line = element_line(color = "black"),
      axis.title.x = element_text(vjust= 0, size=rel(0.9)),
      axis.title.y = element_text(vjust= 1.1, size=rel(0.9)),
      axis.text.x = element_text(margin=margin(5,5,0,0,"pt")),
      axis.text.y = element_text(margin=margin(3,5,0,3,"pt")),
      axis.ticks.length = unit(-0.7, "mm"),
      text=element_text(size = 10, family="Times"))

<a href="https://www.youtube.com/watch?v=r0n_p7POzDE" class="whitebut ">ggplot youtube</a>

<h2>download YouTube data in R using tuber and purrr</h2>
<a href="https://www.storybench.org/how-to-download-youtube-data-in-r-using-tuber-and-purrr/" class="whitebut ">download YouTube data</a>

Accessing YouTube's metadata such as views, likes, dislikes and comments is simple in R thanks to the<code><a href="https://soodoku.github.io/tuber/articles/tuber-ex.html">tuber</a></code>

<code>install.packages("tuber")
library(tuber) # youtube API
library(magrittr) # Pipes %>%, %T>% and equals(), extract().
library(tidyverse) # all tidyverse packages
library(purrr) # package for iterating/extracting data</code>

<h3>1) Enable the APIs</h3>

First head over to your <a href="https://console.developers.google.com/apis/dashboard">Google APIs dashboard</a> (you’ll need an account for this). 
Click on “ENABLE APIS AND SERVICES”.


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-00-google-api-2-1200x419.png">

This will bring up a laundry list of APIs, but we only need the four pertaining to YouTube (see below) and the Freebase API.

Click on the search bar and type in YouTube and you should see four options. 
Enable all of them.


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-01-youtube-api-1-1200x642.png">

<strong>IMPORTANT</strong> you’ll also have to search for and enable the Freebase API.

<h3>2) Create your credentials</h3>

After these have been enabled, you’ll need to create credentials for the API. 
Click on the Credentials label on the left side of your Google dashboard (there should be a little key icon next to it).


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-02-google-api-credentials-1-1200x373.png">

After clicking on the Credentials icon, you’ll need to select the OAuth client ID option.


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-03-create-credentials-1.png">

<h3>Create your OAuth</h3>

Here is where we name our app and indicate it’s an “Other” Application type.


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-04-create-youtube-api-r-1-1200x673.png">

We’re told we’re limited to 100 sensitive scope logins until the OAuth consent screen is published. 
That’s not a problem for us, so we can copy the client ID and client secret


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-05-oauth-cred-google-1.png">

After clicking on the copy icons, we save them into two objects in RStudio (<code>client_id</code> and <code>client_secret</code>).

<code>client_id &lt;- "XXXXXXXXX"
client_secret &lt;- "XXXXXXXXX"</code>

<h3>3) Authenticate the application</h3>

Now you can run <code>tuber</code>’s <code>yt_oauth()</code> function to authenticate your application. 
I included the token as a blank string (<code>token = ''</code>) because it kept looking for the <code>.httr-oauth</code> in my local directory (and I didn’t create one).

<code># use the youtube oauth 
yt_oauth(app_id = client_id,
         app_secret = client_secret,
         token = '')</code>

Provided you did everything correct, this <em>should</em> open your browser and ask you to sign into the Google account you set everything up with (see the images below). 
You’ll see the name of your application in place of “<em>Your application name</em>”.


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-06-sign-in-with-youtube.png">

After signing in, you’ll be asked if the YouTube application you created can access your Google account. 
If you approve, click “Allow.”


<img class="lazy" data-src="https://www.storybench.org/wp-content/uploads/2019/09/32-07-google-credentials-allow-1-878x1200.png">

This should give you a blank page with a cryptic, <code>Authentication
complete. 
Please close this page and return to R.</code> message.

<hr class="wp-block-separator"/>

<h3>Accessing YouTube data</h3>

Great! Now that we’re all set up, we will download some data into RStudio. 
Be sure to check out the <a href="https://soodoku.github.io/tuber/reference/index.html">reference page</a> and the <a href="https://developers.google.com/youtube/v3/docs/">YouTube API reference doc</a> on how to access various meta data from YouTube videos.

We’ll download some example data from <a href="https://www.youtube.com/playlist?list=PLG6HoeSC3raE-EB8r_vVDOs-59kg3Spvd">Dave Chappelle’s comedy central playlist</a>, which is a collection of 200 of his most popular skits.

<h3>Downloading the playlist data</h3>

We will be using the <code>playlistId</code> from the url to access the content from the videos. 
Here is some information on the <code>playlistId</code> parameter:

<blockquote class="wp-block-quote">The <code>playlistId</code> parameter specifies the unique ID of the playlist for<br>
  which you want to retrieve playlist items. 
Note that even though this<br>
  is an optional parameter, every request to retrieve playlist items<br>
  must specify a value for either the <code>id</code> parameter or the <code>playlistId</code><br>
  parameter.
</blockquote>

Dave Chappelle’s playlist is in the url below. 
We pass it to the<br>
<code>stringr::str_split()</code> function to get the <code>playlistId</code> out of it.

<code>dave_chappelle_playlist_id &lt;- stringr::str_split(
    string = "https://www.youtube.com/playlist?list=PLG6HoeSC3raE-EB8r_vVDOs-59kg3Spvd", 
    pattern = "=", 
    n = 2,
    simplify = TRUE)[ , 2]
dave_chappelle_playlist_id

[1] "PLG6HoeSC3raE-EB8r_vVDOs-59kg3Spvd"</code>

Ok–we have a vector for Dave Chappelle’s <code>playlistId</code> named <code>dave_chappelle_playlist_id</code>, now we can use the <code>tuber::get_playlist_items()</code> to collect the videos into a <code>data.frame</code>.

<code>DaveChappelleRaw &lt;- tuber::get_playlist_items(filter = 
             c(playlist_id = "PLG6HoeSC3raE-EB8r_vVDOs-59kg3Spvd"), 
                                           part = "contentDetails",
                                  # set this to the number of videos
                                              max_results = 200) </code>

We should check these data to see if there is one row per video from the playlist (recall that Dave Chappelle had 200 videos).

# check the data for Dave Chappelle
DaveChappelleRaw %&gt;% dplyr::glimpse(78)
    Observations: 200
    Variables: 6
    $ .id                             &lt;chr> "items1", "items2", "items3", "item…
    $ kind                            &lt;fct> youtube#playlistItem, youtube#playl…
    $ etag                            &lt;fct> "p4VTdlkQv3HQeTEaXgvLePAydmU/G-gTM9…
    $ id                              &lt;fct> UExHNkhvZVNDM3JhRS1FQjhyX3ZWRE9zLTU…
    $ contentDetails.videoId          &lt;fct> oO3wTulizvg, ZX5MHNvjw7o, MvZ-clcMC…
    $ contentDetails.videoPublishedAt &lt;fct> 2019-04-28T16:00:07.000Z, 2017-12-3…

<h3>Collecting statistics from a YouTube playlist</h3>

Now that we have all of the video <code>ids</code> (not <code>.id</code>), we can create a function that extracts the statistics for each video on the playlist. 
We’ll start by putting the video ids in a vector and call it <code>dave_chap_ids</code>.

dave_chap_ids = base::as.vector(DaveChappelleRaw$contentDetails.videoId)
dplyr::glimpse(dave_chap_ids)
chr [1:200] "oO3wTulizvg" "ZX5MHNvjw7o" "MvZ-clcMCec" "4trBQseIkkc" ...

<code>tuber</code> has a <code>get_stats()</code> function we will use with the vector we just created for the show ids.

<code># Function to scrape stats for all vids
get_all_stats &lt;- function(id) {
  tuber::get_stats(video_id = id)
} </code>

<h3>Using purrr to iterate and extract metadata </h3>

Now we introduce a bit of iteration from <a href="https://purrr.tidyverse.org/">the <code>purrr</code> package</a>. 
The <code>purrr</code> package provides tools for ‘functional programming,’ but that is a much bigger topic for a later post.

For now, just know that the <code>purrr::map_df()</code> function takes an object as .<code>x</code>, and whatever function is listed in <code>.f</code> gets applied over the <code>.x</code> object. 
Check out the code below:

# Get stats and convert results to data frame 
DaveChappelleAllStatsRaw = purrr::map_df(.x = dave_chap_ids, 
                                          .f = get_all_stats)

DaveChappelleAllStatsRaw %>% dplyr::glimpse(78)

Observations: 200
Variables: 6
$ id            &lt;chr> "oO3wTulizvg", "ZX5MHNvjw7o", "MvZ-clcMCec", "4trBQse…
$ viewCount     &lt;chr> "4446789", "19266680", "6233018", "8867404", "7860341…
$ likeCount     &lt;chr> "48699", "150691", "65272", "92259", "56584", "144625…
$ dislikeCount  &lt;chr> "1396", "6878", "1530", "2189", "1405", "3172", "1779…
$ favoriteCount &lt;chr> "0", "0", "0", "0", "0", "0", "0", "0", "0", "0", "0"…
$ commentCount  &lt;chr> "2098", "8345", "5130", "5337", "2878", "9071", "4613…

Fantastic! We have the <code>DaveChappelleRaw</code> and <code>DaveChappelleAllStatsRaw</code> in two <code>data.frame</code>s we can export (and timestamp!)

# export DaveChappelleRaw
readr::write_csv(x = as.data.frame(DaveChappelleRaw), 
                 path = paste0("data/", 
                               base::noquote(lubridate::today()),
                               "-DaveChappelleRaw.csv"))

# export DaveChappelleRaw
readr::write_csv(x = as.data.frame(DaveChappelleAllStatsRaw), 
                 path = paste0("data/", 
                               base::noquote(lubridate::today()),
                               "-DaveChappelleAllStatsRaw.csv"))

# verify
fs::dir_ls("data", regexp = "Dave")

Be sure to go through the following <code>purrr</code> tutorials if you want to learn more about functional programming:

<a href="http://r4ds.had.co.nz/iteration.html#the-map-functions">R for Data Science by H. Wickham &amp; G.
Grolemund</a>
<a href="https://jennybc.github.io/purrr-tutorial/">purrr Tutorial by J.
Bryan</a>
<a href="https://github.com/cwickham/purrr-tutorial">A purrr tutorial - useR! 2017 by C.
Wickham</a>
<a href="https://colinfay.me/happy-dev-purrr/">Happy dev with {purrr} - by C. Fay</a>

Also check out the <a href="https://www.storybench.org/how-to-access-apis-in-r/">previous post on using
APIs</a>.

<h2>R package library</h2>
to remove package: may not work!
remove.packages('dplyr')

to install dplyr, first install Rcpp, one by one
install.packages("Rcpp")

install.packages("dplyr")

this is the c compiler"
C:/RBuildTools/3.4/mingw_64/bin/g++  -I"D:/R-3.4.3/include" -DNDEBUG          -O2 -Wall  -mtune=generic -c slice.cpp -o slice.o

The downloaded source packages are in
‘C:\Users\User\AppData\Local\Temp\Rtmp0sfeZ8\downloaded_packages’

to manually deleting the dplyr folder
find the folder

Change the Default Library in Rstudio
view the current library path:

.libPaths gets/sets the library trees within which packages are looked for.

.libPaths()                 # all library trees R knows about
"<span class="red">D:/R-3.4.3/library</span>"

https://stackoverflow.com/questions/31707941/how-do-i-change-the-default-library-path-for-r-packages/42643674

<h2>Sorting by Multiple Columns</h2>
dataset[with(dataset, order(z, x)),]
dataset[with(dataset, order(-z, b)),]

with dplyr Package arrange Function

library("dplyr")
arrange(data, x2, x3)

with data.table Package

library("data.table")
data_ordered = data
setorder(data_ordered, x2, x3)
data_ordered

<h2>print frequency table vertically</h2>
options("encoding" = "native.enc")
thelist = readLines("thelist.txt", encoding="UTF-8")

thetable = sort(table(thelist),  decreasing = TRUE)

names(thetable)
for(i in 1:length(thetable)){
  cat(names(thetable[i]), thetable[i],"\n")
}

<h2>reorder table</h2>
activityList = readLines("testtrial.txt", encoding="UTF-8")
activityList <= matrix(unlist(strsplit(activityList, split = ",")), ncol=2, byrow=TRUE)
sink("result.txt")
write.table(activityList[order(activityList[,1]),], row.names=F, col.names=F, quote=F)
sink()

<h2>dump data</h2>
keywordList &lt;-
structure(list(structure(list(node = &lt;pointer: 0x0000000003a118b0>, 
    doc = &lt;pointer: 0x000000001166c5f0>), .Names = c("node", 
"doc"), class = "xml_node")), class = "xml_nodeset")

<h2>Appending a list to a list of lists</h2>
    histList = list()
    for(item in historyList){
      setwd(paste0(folderName,"/",item))
      alist = readLines("alarm history.txt")
      histList[[length(histList)+1]] = list(alist)
      setwd("..")
    }

L=list()
for (i in 1:3) { L=c(L, list(list(sample(1:3)))) }


<h2>find the mode name of vector</h2>
x = c(1,2,4,3,3,4,5,6,6,4,3,2,3,4)
freqtable = table(x)
names(freqtable)[freqtable == max(freqtable)]

<h2>分佈式計算</h2>
在全世界個人電腦用戶等的協作下，推進新冠病毒蛋白質結構分析的分佈式計算項目「Folding@home（FAH）」實現了最尖端超級計算機也尚未達到的「Exa級」計算能力。

分佈式計算又被稱為網格計算，除FAH以外，還有很多項目在採用，例如美國加州大學柏克萊分校的「SETI@home」，該項目1999年啟動，通過射電望遠鏡探測地球以外智慧生命體的信號證據，於今年3月結束。

在物聯網（IoT）時代，名為邊緣計算（EdgeComputing）的分佈式系統正在受到關注。
速度快、延遲少的新一代通信標準「5G」對其起到支撐作用。

<h2>Make R Studio plots only show up in new window</h2>
In RStudio, the default graphics device is normally "RStudioGD".
Change that to something else: the normal choices are "windows" on Windows

options(device = "windows")
dev.new()
Call dev.new() after changing the option

or try using the windows command before your plot call.
windows()

To open another window, run the command a second time to open a second window.

dev.off() will shut down the window (in the order they were opened by default).

Commenting the following lines in "RStudio\R\Tools.R"

# set our graphics device as the default and cause it to be created/set
.rs.addFunction( "initGraphicsDevice", function()
{
   # options(device="RStudioGD")
   # grDevices::deviceIsInteractive("RStudioGD")
  grDevices::deviceIsInteractive()
})

<h2>R Studio</h2><a href="https://www.youtube.com/watch?v=FuIRxqx370A">
<img class="lazy" data-src="https://i.ytimg.com/vi/FuIRxqx370A/hqdefault.jpg"></a>
<h2>How to install and use packages in R|| Install R Packages|| Use R Packages</h2><a href="https://www.youtube.com/watch?v=IW1eY2lt2ds">
<img class="lazy" data-src="https://i.ytimg.com/vi/IW1eY2lt2ds/hqdefault.jpg"></a>
<h2>Ways to get help in R || R Help|| Help Function in R</h2><a href="https://www.youtube.com/watch?v=unTjebI7mNw">
<img class="lazy" data-src="https://i.ytimg.com/vi/unTjebI7mNw/hqdefault.jpg"></a>
<h2>Describing data using Hmisc and psych package - R Programming</h2><a href="https://www.youtube.com/watch?v=2F1RioDkBUg">
<img class="lazy" data-src="https://i.ytimg.com/vi/2F1RioDkBUg/hqdefault.jpg"></a>
<h2>Creating bar chart in R (along with text function)</h2><a href="https://www.youtube.com/watch?v=AlE7xXCE0JU">
<img class="lazy" data-src="https://i.ytimg.com/vi/AlE7xXCE0JU/hqdefault.jpg"></a>
<h2>Data Aggregation and Structure (Simple or Pivot)|| Simple Table||Pivot Table</h2><a href="https://www.youtube.com/watch?v=guSQmfjdP_U">
<img class="lazy" data-src="https://i.ytimg.com/vi/guSQmfjdP_U/hqdefault.jpg"></a>
<h2>Creating Joins on Datasets or Data frames|| Datasets|| Data Frames</h2><a href="https://www.youtube.com/watch?v=tiFMmnoDzzc">
<img class="lazy" data-src="https://i.ytimg.com/vi/tiFMmnoDzzc/hqdefault.jpg"></a>
<h2>Creating bins or ranges from numeric data in R Programming ||  R Bins || R Ranges</h2><a href="https://www.youtube.com/watch?v=Ik0R_H__LZ4">
<img class="lazy" data-src="https://i.ytimg.com/vi/Ik0R_H__LZ4/hqdefault.jpg"></a>
<h2>Sorting and Ordering data</h2><a href="https://www.youtube.com/watch?v=pBEvB62FDaQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/pBEvB62FDaQ/hqdefault.jpg"></a>
<h2>Removing NA values from Dataset in R ||Removing NA values from Dataframes in R</h2><a href="https://www.youtube.com/watch?v=N6wrEULE6M0">
<img class="lazy" data-src="https://i.ytimg.com/vi/N6wrEULE6M0/hqdefault.jpg"></a>
<h2>Creating and adding calculated column to dataset / dataframe|| Dataset and Dataframe</h2><a href="https://www.youtube.com/watch?v=CsFRM1_heM4">
<img class="lazy" data-src="https://i.ytimg.com/vi/CsFRM1_heM4/hqdefault.jpg"></a>
<h2>Identifying duplicate rows in dataset and removing them</h2><a href="https://www.youtube.com/watch?v=LKoknpFOEUw">
<img class="lazy" data-src="https://i.ytimg.com/vi/LKoknpFOEUw/hqdefault.jpg"></a>
<h2>Hypothesis testing in theory</h2><a href="https://www.youtube.com/watch?v=RV8h9B4BV8k">
<img class="lazy" data-src="https://i.ytimg.com/vi/RV8h9B4BV8k/hqdefault.jpg"></a>
<h2>DataFrame and Matrix aggregation functions</h2><a href="https://www.youtube.com/watch?v=Mc2eNpOS530">
<img class="lazy" data-src="https://i.ytimg.com/vi/Mc2eNpOS530/hqdefault.jpg"></a>
<h2>Interpretation of statisitcal terms in Linear Regression</h2><a href="https://www.youtube.com/watch?v=QuXN9oKL48Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/QuXN9oKL48Y/hqdefault.jpg"></a>
<h2>Simple Linear Regression</h2><a href="https://www.youtube.com/watch?v=vFPqCPsN5FA">
<img class="lazy" data-src="https://i.ytimg.com/vi/vFPqCPsN5FA/hqdefault.jpg"></a>
<h2>Combining multiple datasets</h2><a href="https://www.youtube.com/watch?v=_qJpnrvYbK4">
<img class="lazy" data-src="https://i.ytimg.com/vi/_qJpnrvYbK4/hqdefault.jpg"></a>
<h2>Exploring Apply function</h2><a href="https://www.youtube.com/watch?v=3RXJhMq9jFY">
<img class="lazy" data-src="https://i.ytimg.com/vi/3RXJhMq9jFY/hqdefault.jpg"></a>
<h2>R Apply family functions</h2><a href="https://www.youtube.com/watch?v=pS4AkSBomBo">
<img class="lazy" data-src="https://i.ytimg.com/vi/pS4AkSBomBo/hqdefault.jpg"></a>
<h2>Ways to get help in R</h2><a href="https://www.youtube.com/watch?v=unTjebI7mNw">
<img class="lazy" data-src="https://i.ytimg.com/vi/unTjebI7mNw/hqdefault.jpg"></a>
<h2>Fetching Data from Vector &amp; Matrix|| R Vector|| R Matrix</h2><a href="https://www.youtube.com/watch?v=AWeAs622y6s">
<img class="lazy" data-src="https://i.ytimg.com/vi/AWeAs622y6s/hqdefault.jpg"></a>
<h2>R House keeping commands like ls and rm to manage objects|| R-Is|| R- rm||R Programming Tutorial</h2><a href="https://www.youtube.com/watch?v=2q-VNrRHr04">
<img class="lazy" data-src="https://i.ytimg.com/vi/2q-VNrRHr04/hqdefault.jpg"></a>
<h2>Sample Data in R | Sample datasets for data mining | sample data sets for statistical analysis</h2><a href="https://www.youtube.com/watch?v=Q3hSogORyZo">
<img class="lazy" data-src="https://i.ytimg.com/vi/Q3hSogORyZo/hqdefault.jpg"></a>
<h2>Creating Simple Dataset in R using Combine and Scan command|| R Programming</h2><a href="https://www.youtube.com/watch?v=10Jm0yAX03w">
<img class="lazy" data-src="https://i.ytimg.com/vi/10Jm0yAX03w/hqdefault.jpg"></a>
<h2>Basic Mathematics Operations in R|| R Programming</h2><a href="https://www.youtube.com/watch?v=P8uRRWywjiY">
<img class="lazy" data-src="https://i.ytimg.com/vi/P8uRRWywjiY/hqdefault.jpg"></a>
<h2>Code | Market Basket Analysis | Association Rules | R Programming</h2><a href="https://www.youtube.com/watch?v=2otyDYe_V0o">
<img class="lazy" data-src="https://i.ytimg.com/vi/2otyDYe_V0o/hqdefault.jpg"></a>
<h2>Automating Assocation Rules or Market Basket Analysis in Shiny | R Programming | Shiny</h2><a href="https://www.youtube.com/watch?v=jzdwMA5BVyI">
<img class="lazy" data-src="https://i.ytimg.com/vi/jzdwMA5BVyI/hqdefault.jpg"></a>
<h2>R Tutorial | Creating boxplot and enhance it with ggplot | R Programming</h2><a href="https://www.youtube.com/watch?v=48UvtcXTb9U">
<img class="lazy" data-src="https://i.ytimg.com/vi/48UvtcXTb9U/hqdefault.jpg"></a>
<h2>R ShinyTutorial  - Dispalying Notification Menu - R Programming Tutorials</h2><a href="https://www.youtube.com/watch?v=qRj9C2cjJpE">
<img class="lazy" data-src="https://i.ytimg.com/vi/qRj9C2cjJpE/hqdefault.jpg"></a>
<h2>R Shiny Tutorial | How to highlight new menu using badge | R Programming Tutorial</h2><a href="https://www.youtube.com/watch?v=ikA1uZwzy5w">
<img class="lazy" data-src="https://i.ytimg.com/vi/ikA1uZwzy5w/hqdefault.jpg"></a>
<h2>R Shiny Tutorial - Adding a new box to shiny dashboard and putting controls in it | R Programming</h2><a href="https://www.youtube.com/watch?v=hVLN6vZFqgY">
<img class="lazy" data-src="https://i.ytimg.com/vi/hVLN6vZFqgY/hqdefault.jpg"></a>
<h2>R Shiny Tutorial - Shiny Dashboards - Adding Tabs in a Box - R Programming Tutorial</h2><a href="https://www.youtube.com/watch?v=kxnvaptV2xw">
<img class="lazy" data-src="https://i.ytimg.com/vi/kxnvaptV2xw/hqdefault.jpg"></a>
<h2>RStudio and Git - an Example (Part 2)</h2><a href="https://www.youtube.com/watch?v=qcjpHFwCugE">
<img class="lazy" data-src="https://i.ytimg.com/vi/qcjpHFwCugE/hqdefault.jpg"></a>
<h2>RStudio and Git - an Overview (Part 1)</h2><a href="https://www.youtube.com/watch?v=KjLycV1IWqc">
<img class="lazy" data-src="https://i.ytimg.com/vi/KjLycV1IWqc/hqdefault.jpg"></a>
<h2>The Production Function Model, An Introduction - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=-rteAEb3iJM">
<img class="lazy" data-src="https://i.ytimg.com/vi/-rteAEb3iJM/hqdefault.jpg"></a>
<h2>A Change in the Rate of Depreciation (delta) - Solow Model Application Part 3 of 4</h2><a href="https://www.youtube.com/watch?v=-eanLMbhjac">
<img class="lazy" data-src="https://i.ytimg.com/vi/-eanLMbhjac/hqdefault.jpg"></a>
<h2>A Change in Technology - Solow Model Application - Part 4 of 4</h2><a href="https://www.youtube.com/watch?v=9-3TFXc5yxo">
<img class="lazy" data-src="https://i.ytimg.com/vi/9-3TFXc5yxo/hqdefault.jpg"></a>
<h2>Solow Model Transition Dynamics (Level vs. Growth Effects) - Part 5 of 5</h2><a href="https://www.youtube.com/watch?v=Bdr34lhGI-Q">
<img class="lazy" data-src="https://i.ytimg.com/vi/Bdr34lhGI-Q/hqdefault.jpg"></a>
<h2>Golden Rule Level of Capital &amp; Savings Rate - Solow Model</h2><a href="https://www.youtube.com/watch?v=HGxr6cwnPfo">
<img class="lazy" data-src="https://i.ytimg.com/vi/HGxr6cwnPfo/hqdefault.jpg"></a>
<h2>A Change in the Savings Rate (s) - Solow Model Application Part 2 of 4</h2><a href="https://www.youtube.com/watch?v=IiPWNiKhrL0">
<img class="lazy" data-src="https://i.ytimg.com/vi/IiPWNiKhrL0/hqdefault.jpg"></a>
<h2>A Reduction in the Capital Stock - War! - Solow Model Application Part 1 of 4</h2><a href="https://www.youtube.com/watch?v=K3XmPnV_j-I">
<img class="lazy" data-src="https://i.ytimg.com/vi/K3XmPnV_j-I/hqdefault.jpg"></a>
<h2>Solow Swan Model with Population Growth - Part 1 of 2</h2><a href="https://www.youtube.com/watch?v=cESaITRvS2o">
<img class="lazy" data-src="https://i.ytimg.com/vi/cESaITRvS2o/hqdefault.jpg"></a>
<h2>Solow Model with Technology Growth and Population Growth - Part 2 of 5</h2><a href="https://www.youtube.com/watch?v=eIXquvF3N_Q">
<img class="lazy" data-src="https://i.ytimg.com/vi/eIXquvF3N_Q/hqdefault.jpg"></a>
<h2>Solow Model Diagram - Adding Technology &amp; Population Growth - Part 3 of 5</h2><a href="https://www.youtube.com/watch?v=mDiVFk7wj3w">
<img class="lazy" data-src="https://i.ytimg.com/vi/mDiVFk7wj3w/hqdefault.jpg"></a>
<h2>Solow Model with Technology Growth and Population Growth - Part 1 of 5</h2><a href="https://www.youtube.com/watch?v=md0cjl51JTk">
<img class="lazy" data-src="https://i.ytimg.com/vi/md0cjl51JTk/hqdefault.jpg"></a>
<h2>Calculating Growth Rates of the Solow Swan Model - Part 4 of 5</h2><a href="https://www.youtube.com/watch?v=qQc94rpvNPI">
<img class="lazy" data-src="https://i.ytimg.com/vi/qQc94rpvNPI/hqdefault.jpg"></a>
<h2>Solow Swan Model with Population Growth - Part 2 of 2</h2><a href="https://www.youtube.com/watch?v=rjHrm06s0kY">
<img class="lazy" data-src="https://i.ytimg.com/vi/rjHrm06s0kY/hqdefault.jpg"></a>
<h2>Solow Model - Transition Dynamics &amp; Time Series (Part 4)</h2><a href="https://www.youtube.com/watch?v=-06UOz8or34">
<img class="lazy" data-src="https://i.ytimg.com/vi/-06UOz8or34/hqdefault.jpg"></a>
<h2>Solow Model Application   Effect of an Increase in the Savings Rate</h2><a href="https://www.youtube.com/watch?v=2h2BJIJRJMM">
<img class="lazy" data-src="https://i.ytimg.com/vi/2h2BJIJRJMM/hqdefault.jpg"></a>
<h2>Solow Model (Part 1 of Many)</h2><a href="https://www.youtube.com/watch?v=Dx7ZvAKfL9k">
<img class="lazy" data-src="https://i.ytimg.com/vi/Dx7ZvAKfL9k/hqdefault.jpg"></a>
<h2>Solow Model Example - The Effect of Destruction of Capital</h2><a href="https://www.youtube.com/watch?v=GqDln9wE3Lk">
<img class="lazy" data-src="https://i.ytimg.com/vi/GqDln9wE3Lk/hqdefault.jpg"></a>
<h2>Solow Model Problem - Change in the Rate of Depreciation</h2><a href="https://www.youtube.com/watch?v=PDZUeYH6nn0">
<img class="lazy" data-src="https://i.ytimg.com/vi/PDZUeYH6nn0/hqdefault.jpg"></a>
<h2>Solow Model - The Steady State Level of Capital (Part 2)</h2><a href="https://www.youtube.com/watch?v=PfTJQrL1QZc">
<img class="lazy" data-src="https://i.ytimg.com/vi/PfTJQrL1QZc/hqdefault.jpg"></a>
<h2>Solow Model Example - A Change in Population Growth Rate</h2><a href="https://www.youtube.com/watch?v=Row9WKvqA18">
<img class="lazy" data-src="https://i.ytimg.com/vi/Row9WKvqA18/hqdefault.jpg"></a>
<h2>Solow Model - Solow Diagram &amp; Convergence (Part 3)</h2><a href="https://www.youtube.com/watch?v=UwQBlJ6ve5U">
<img class="lazy" data-src="https://i.ytimg.com/vi/UwQBlJ6ve5U/hqdefault.jpg"></a>
<h2>Application of Solow Swan Model - Effect of an Increase in Technology Growth</h2><a href="https://www.youtube.com/watch?v=r3jI1dAIueU">
<img class="lazy" data-src="https://i.ytimg.com/vi/r3jI1dAIueU/hqdefault.jpg"></a>
<h2>Level-Log Regression &amp; Interpretation (What do the Regression Coefficient Estimate Results Mean?)</h2><a href="https://www.youtube.com/watch?v=L9ZL6_DB4fQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/L9ZL6_DB4fQ/hqdefault.jpg"></a>
<h2>Log-Level Regression &amp; Interpretation (What do the Regression Coefficient Estimate Results Mean?)</h2><a href="https://www.youtube.com/watch?v=wXC2kViEGz8">
<img class="lazy" data-src="https://i.ytimg.com/vi/wXC2kViEGz8/hqdefault.jpg"></a>
<h2>Log-Log Regression &amp; Interpretation (What do the Regression Coefficient Estimate Results Mean?)</h2><a href="https://www.youtube.com/watch?v=NZCSt9WkpkI">
<img class="lazy" data-src="https://i.ytimg.com/vi/NZCSt9WkpkI/hqdefault.jpg"></a>
<h2>Level-Level Regression &amp; Interpretation (What do Coefficient Estimate Results Mean?)</h2><a href="https://www.youtube.com/watch?v=TJACbJspao0">
<img class="lazy" data-src="https://i.ytimg.com/vi/TJACbJspao0/hqdefault.jpg"></a>
<h2>Do a Linear Regression (with free R Statistics Software)</h2><a href="https://www.youtube.com/watch?v=Ktks5K95uQM">
<img class="lazy" data-src="https://i.ytimg.com/vi/Ktks5K95uQM/hqdefault.jpg"></a>
<h2>IS-LM Model Diagrams - The Effect of Policy Mixes - Shifting Both the IS and LM Curves</h2><a href="https://www.youtube.com/watch?v=ZA66asJE0ew">
<img class="lazy" data-src="https://i.ytimg.com/vi/ZA66asJE0ew/hqdefault.jpg"></a>
<h2>ISLM Practice Problem Part 1 - Deriving the IS and LM Curves, and the IS-LM Diagram</h2><a href="https://www.youtube.com/watch?v=_19w5dcGhCo">
<img class="lazy" data-src="https://i.ytimg.com/vi/_19w5dcGhCo/hqdefault.jpg"></a>
<h2>IS-LM Equations - Deriving Aggregate Demand Equation</h2><a href="https://www.youtube.com/watch?v=5EPvwarCqDA">
<img class="lazy" data-src="https://i.ytimg.com/vi/5EPvwarCqDA/hqdefault.jpg"></a>
<h2>IS-LM Curves and Diagram and a Change in the Price Level</h2><a href="https://www.youtube.com/watch?v=yBBpE8PzoKU">
<img class="lazy" data-src="https://i.ytimg.com/vi/yBBpE8PzoKU/hqdefault.jpg"></a>
<h2>IS-LM Model &amp; Diagram - LM Curve Shift from a Monetary Shock (Money Supply Increase)</h2><a href="https://www.youtube.com/watch?v=b28lsOUFOtw">
<img class="lazy" data-src="https://i.ytimg.com/vi/b28lsOUFOtw/hqdefault.jpg"></a>
<h2>IS-LM Curves and Diagram - Fiscal Shock and a Shift to the IS Curve (Government Purchases Increase)</h2><a href="https://www.youtube.com/watch?v=vx6w5JFIjzw">
<img class="lazy" data-src="https://i.ytimg.com/vi/vx6w5JFIjzw/hqdefault.jpg"></a>
<h2>Macro Problem - Calculate the IS Curve &amp; LM Curve Equations - Equilibrium Interest Rate &amp; Output</h2><a href="https://www.youtube.com/watch?v=pAX7mR4ii5Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/pAX7mR4ii5Y/hqdefault.jpg"></a>
<h2>Macro Problem - Numerical Example with Money Demand and Supply - Find Equilibrium Interest Rate</h2><a href="https://www.youtube.com/watch?v=BX7b1EUn5_Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/BX7b1EUn5_Y/hqdefault.jpg"></a>
<h2>Solow Growth Model Diagram Problem - Shocks &amp; Effects on Steady State per-worker Capital &amp; Output</h2><a href="https://www.youtube.com/watch?v=MRwX8vvpHio">
<img class="lazy" data-src="https://i.ytimg.com/vi/MRwX8vvpHio/hqdefault.jpg"></a>
<h2>Solow Model Diagram Problem - Effect of Decrease in Population Growth (per capita Capital &amp; Output)</h2><a href="https://www.youtube.com/watch?v=ceqUYIYd2Og">
<img class="lazy" data-src="https://i.ytimg.com/vi/ceqUYIYd2Og/hqdefault.jpg"></a>
<h2>Solow Model Practice - Calculate the Steady State &amp; Compare Economies with Varying Saving Rates</h2><a href="https://www.youtube.com/watch?v=b0FZuvKvOyo">
<img class="lazy" data-src="https://i.ytimg.com/vi/b0FZuvKvOyo/hqdefault.jpg"></a>
<h2>Macro Problem - Central Bank Loss Function and Alternative Inflation Targets</h2><a href="https://www.youtube.com/watch?v=-FH4U0eBX80">
<img class="lazy" data-src="https://i.ytimg.com/vi/-FH4U0eBX80/hqdefault.jpg"></a>
<h2>Tax Cut and the Traditional View vs Ricardian View - Public, Private &amp; National Savings</h2><a href="https://www.youtube.com/watch?v=5BiR3caIL4s">
<img class="lazy" data-src="https://i.ytimg.com/vi/5BiR3caIL4s/hqdefault.jpg"></a>
<h2>Macro Practice - Political Business Cycle - Independent Central Bank and Inflation &amp; Unemployment</h2><a href="https://www.youtube.com/watch?v=l00xcKtVEQk">
<img class="lazy" data-src="https://i.ytimg.com/vi/l00xcKtVEQk/hqdefault.jpg"></a>
<h2>Macro Problem - Hysteresis vs the Natural Rate Hypothesis - Inflation and Unemployment Trade-off</h2><a href="https://www.youtube.com/watch?v=-7n5PgdLF5Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/-7n5PgdLF5Y/hqdefault.jpg"></a>
<h2>Macro Problem - Sticky Price Model and an Unanticipated Monetary Expansion (vs Anticipated)</h2><a href="https://www.youtube.com/watch?v=EDXaAyhVhl4">
<img class="lazy" data-src="https://i.ytimg.com/vi/EDXaAyhVhl4/hqdefault.jpg"></a>
<h2>Macro Practice - Social Security, Marginal Propensity to Consume &amp; Altruistically Linked Generations</h2><a href="https://www.youtube.com/watch?v=xk4U1KQi6Nk">
<img class="lazy" data-src="https://i.ytimg.com/vi/xk4U1KQi6Nk/hqdefault.jpg"></a>
<h2>Macro Problem - Trade-Off Between Inflation, Unemployment and GDP</h2><a href="https://www.youtube.com/watch?v=q3Uey9ladUg">
<img class="lazy" data-src="https://i.ytimg.com/vi/q3Uey9ladUg/hqdefault.jpg"></a>
<h2>What is Decreasing Returns to Scale (DRS)?  - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=vellgNFKztw">
<img class="lazy" data-src="https://i.ytimg.com/vi/vellgNFKztw/hqdefault.jpg"></a>
<h2>What is Increasing Returns to Scale (IRS)?   - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=5W7GUxomGpM">
<img class="lazy" data-src="https://i.ytimg.com/vi/5W7GUxomGpM/hqdefault.jpg"></a>
<h2>What is Constant Returns to Scale (CRS)?   - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=in6CK8sTQgk">
<img class="lazy" data-src="https://i.ytimg.com/vi/in6CK8sTQgk/hqdefault.jpg"></a>
<h2>Calculate Returns to Scale - Nine Different Prod. Func. Examples - Intermediate Macro economics</h2><a href="https://www.youtube.com/watch?v=gPyPvWxJOlc">
<img class="lazy" data-src="https://i.ytimg.com/vi/gPyPvWxJOlc/hqdefault.jpg"></a>
<h2>Returns to Scale Overview - Definition &amp; Discussion - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=AttvGU47Eg8">
<img class="lazy" data-src="https://i.ytimg.com/vi/AttvGU47Eg8/hqdefault.jpg"></a>
<h2>Fiscal Expansion + Classical Model of a Closed Economy in the Long Run - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=RJ7a5nEU5aA">
<img class="lazy" data-src="https://i.ytimg.com/vi/RJ7a5nEU5aA/hqdefault.jpg"></a>
<h2>Impact of Fiscal Contraction on Closed Economy Long Run Model - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=69mSo2pIXUk">
<img class="lazy" data-src="https://i.ytimg.com/vi/69mSo2pIXUk/hqdefault.jpg"></a>
<h2>Change In Investment Demand and the Loanable Funds Market - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=2j780pByEeI">
<img class="lazy" data-src="https://i.ytimg.com/vi/2j780pByEeI/hqdefault.jpg"></a>
<h2>The Classical Model - A Closed Economy in the Long Run &amp; Market for Loanable Funds -  Macroeconomics</h2><a href="https://www.youtube.com/watch?v=JlOs6AyYiTY">
<img class="lazy" data-src="https://i.ytimg.com/vi/JlOs6AyYiTY/hqdefault.jpg"></a>
<h2>Small Open Economy Model Overview - Example with a Drop in Consumer Confidence - Intermediate Macro</h2><a href="https://www.youtube.com/watch?v=8NsstM92wtM">
<img class="lazy" data-src="https://i.ytimg.com/vi/8NsstM92wtM/hqdefault.jpg"></a>
<h2>Real Wages Related to Labor Productivity, Laborss Share of Income (with Cobb-Douglas Prod Function)</h2><a href="https://www.youtube.com/watch?v=9baU77J-eJ8">
<img class="lazy" data-src="https://i.ytimg.com/vi/9baU77J-eJ8/hqdefault.jpg"></a>
<h2>Find Changes in the Fraction of Income to Labor, Output, Rental Price of Capital and the Real Wage</h2><a href="https://www.youtube.com/watch?v=FImvmUDNBFA">
<img class="lazy" data-src="https://i.ytimg.com/vi/FImvmUDNBFA/hqdefault.jpg"></a>
<h2>Cobb-Douglas Production Function Differentiation Example</h2><a href="https://www.youtube.com/watch?v=rYsNHYgIUL4">
<img class="lazy" data-src="https://i.ytimg.com/vi/rYsNHYgIUL4/hqdefault.jpg"></a>
<h2>Quickly find output, wages rental price of land, and laborss share of income</h2><a href="https://www.youtube.com/watch?v=hIgcnjU-7g8">
<img class="lazy" data-src="https://i.ytimg.com/vi/hIgcnjU-7g8/hqdefault.jpg"></a>
<h2>Laborss Share of Income - Intermediate Macroeconomics</h2><a href="https://www.youtube.com/watch?v=C-czP-S55hg">
<img class="lazy" data-src="https://i.ytimg.com/vi/C-czP-S55hg/hqdefault.jpg"></a>
<h2>The Production Function, Finding the Wage Rate, Rental Rate, and Laborss Share of Income</h2><a href="https://www.youtube.com/watch?v=5rLbBV--W7o">
<img class="lazy" data-src="https://i.ytimg.com/vi/5rLbBV--W7o/hqdefault.jpg"></a>
<h2>Shifts in both Supply and Demand Curves - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=EiYbrhFwErI">
<img class="lazy" data-src="https://i.ytimg.com/vi/EiYbrhFwErI/hqdefault.jpg"></a>
<h2>Shifts to Demand or Supply Curves - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=es_g3L1kmR8">
<img class="lazy" data-src="https://i.ytimg.com/vi/es_g3L1kmR8/hqdefault.jpg"></a>
<h2>Supply and Demand (and Equilibrium Price &amp; Quanitity) - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=op70yS_7du8">
<img class="lazy" data-src="https://i.ytimg.com/vi/op70yS_7du8/hqdefault.jpg"></a>
<h2>Continuous Time Rock Paper Scissors (RPS) - ConG Experiment Software</h2><a href="https://www.youtube.com/watch?v=41YlphE4J54">
<img class="lazy" data-src="https://i.ytimg.com/vi/41YlphE4J54/hqdefault.jpg"></a>
<h2>Discrete Time Rock Paper Scissors (RPS) - ConG Experiment Software</h2><a href="https://www.youtube.com/watch?v=fr2HoiOIwjA">
<img class="lazy" data-src="https://i.ytimg.com/vi/fr2HoiOIwjA/hqdefault.jpg"></a>
<h2>Break Even Price and Shut Down Price -- Calculate and Interpret</h2><a href="https://www.youtube.com/watch?v=0nrt-SFwvOI">
<img class="lazy" data-src="https://i.ytimg.com/vi/0nrt-SFwvOI/hqdefault.jpg"></a>
<h2>What is the Shut Down Price, Find the Break Even Price? - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=0dUetpTRxJ8">
<img class="lazy" data-src="https://i.ytimg.com/vi/0dUetpTRxJ8/hqdefault.jpg"></a>
<h2>Firm Entry and Exit - Will Firms Enter or Exit a Perfectly Competitive Market?</h2><a href="https://www.youtube.com/watch?v=IyJXO89Ss98">
<img class="lazy" data-src="https://i.ytimg.com/vi/IyJXO89Ss98/hqdefault.jpg"></a>
<h2>Example of a Firm in a Perfectly Competitive Market - Economic Profits and Firm Entry</h2><a href="https://www.youtube.com/watch?v=S3nulylUYQ0">
<img class="lazy" data-src="https://i.ytimg.com/vi/S3nulylUYQ0/hqdefault.jpg"></a>
<h2>The Firmss Supply Curve - Given Firm Costs Information, Draw and Interpret the Firm Supply Curve</h2><a href="https://www.youtube.com/watch?v=SuvaPSov21E">
<img class="lazy" data-src="https://i.ytimg.com/vi/SuvaPSov21E/hqdefault.jpg"></a>
<h2>Total Utility and Marginal Utility - Definition &amp; Overview - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=Yfslg-hFSiM">
<img class="lazy" data-src="https://i.ytimg.com/vi/Yfslg-hFSiM/hqdefault.jpg"></a>
<h2>How to Calculate Marginal Cost, Average Total Cost, Average Variable Cost, and Average Fixed Cost</h2><a href="https://www.youtube.com/watch?v=3-j-rZ6hz74">
<img class="lazy" data-src="https://i.ytimg.com/vi/3-j-rZ6hz74/hqdefault.jpg"></a>
<h2>Giffen Good Example - Price Change, Income and Substitution Effect - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=VRC-DVqKYCE">
<img class="lazy" data-src="https://i.ytimg.com/vi/VRC-DVqKYCE/hqdefault.jpg"></a>
<h2>Income and Substitution Effects with an Inferior Good - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=GB3VVQaNC4k">
<img class="lazy" data-src="https://i.ytimg.com/vi/GB3VVQaNC4k/hqdefault.jpg"></a>
<h2>Income Effect and Substitution Effect - a long rambling discussion</h2><a href="https://www.youtube.com/watch?v=1DCb6dIXYfM">
<img class="lazy" data-src="https://i.ytimg.com/vi/1DCb6dIXYfM/hqdefault.jpg"></a>
<h2>Marginal Utility Examples - Increasing, Diminishing and Constant - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=BS-3NXVpdaA">
<img class="lazy" data-src="https://i.ytimg.com/vi/BS-3NXVpdaA/hqdefault.jpg"></a>
<h2>Positive and Zero Marginal Utility Examples -- Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=_QzZEntl6XI">
<img class="lazy" data-src="https://i.ytimg.com/vi/_QzZEntl6XI/hqdefault.jpg"></a>
<h2>Calculating Marginal Utility - Example from Introduction to Microeconomics</h2><a href="https://www.youtube.com/watch?v=tmfXKaUjo5Q">
<img class="lazy" data-src="https://i.ytimg.com/vi/tmfXKaUjo5Q/hqdefault.jpg"></a>
<h2>Example of Optimal Consumption Bundle - Income and Substitution Effect</h2><a href="https://www.youtube.com/watch?v=zHrrsJlXVt8">
<img class="lazy" data-src="https://i.ytimg.com/vi/zHrrsJlXVt8/hqdefault.jpg"></a>
<h2>Substitution and Income Effect Examples - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=BEb3-fpgG4s">
<img class="lazy" data-src="https://i.ytimg.com/vi/BEb3-fpgG4s/hqdefault.jpg"></a>
<h2>Utility Maximization with Budget Line + Indifference Curves - Price and Income Changes</h2><a href="https://www.youtube.com/watch?v=airLmoNWNw8">
<img class="lazy" data-src="https://i.ytimg.com/vi/airLmoNWNw8/hqdefault.jpg"></a>
<h2>Consumer Budget Constraint Shift Example</h2><a href="https://www.youtube.com/watch?v=l1DFLAIgewk">
<img class="lazy" data-src="https://i.ytimg.com/vi/l1DFLAIgewk/hqdefault.jpg"></a>
<h2>Utility Maximization Example with Perfect Compliments - Price Change - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=tnehG-Qn_Rg">
<img class="lazy" data-src="https://i.ytimg.com/vi/tnehG-Qn_Rg/hqdefault.jpg"></a>
<h2>Utility Maximization Example with Inferior Good - Price Change, Income and Substitution Effect</h2><a href="https://www.youtube.com/watch?v=nR3dyoPjgjc">
<img class="lazy" data-src="https://i.ytimg.com/vi/nR3dyoPjgjc/hqdefault.jpg"></a>
<h2>Demand &amp; Supply Curves with an Excise Tab (Example, Texarkana Cigarettes - Intro to Microeconomics)</h2><a href="https://www.youtube.com/watch?v=DpAUgi9O2qE">
<img class="lazy" data-src="https://i.ytimg.com/vi/DpAUgi9O2qE/hqdefault.jpg"></a>
<h2>Example of Excise Tax with Supply &amp; Demand Curves - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=axgM7dpRbbw">
<img class="lazy" data-src="https://i.ytimg.com/vi/axgM7dpRbbw/hqdefault.jpg"></a>
<h2>Working Through Elasticity Examples - Demand Elasticity &amp; Income Elasticity - Intro to Micro</h2><a href="https://www.youtube.com/watch?v=zVuHHOUhp14">
<img class="lazy" data-src="https://i.ytimg.com/vi/zVuHHOUhp14/hqdefault.jpg"></a>
<h2>Price and Income Elasticity Word Problem - VW Beetles - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=fACUQeXNw6s">
<img class="lazy" data-src="https://i.ytimg.com/vi/fACUQeXNw6s/hqdefault.jpg"></a>
<h2>Price and Income Elasticity - Midpoint Method - Tourist T-shirts - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=u8TWHUbIzlk">
<img class="lazy" data-src="https://i.ytimg.com/vi/u8TWHUbIzlk/hqdefault.jpg"></a>
<h2>Quota - Quantity Control Example - Maine Lobster - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=CiKEmE7scwk">
<img class="lazy" data-src="https://i.ytimg.com/vi/CiKEmE7scwk/hqdefault.jpg"></a>
<h2>Price Floor Example - USDA &amp; Milk - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=Pe8uxhvJGnU">
<img class="lazy" data-src="https://i.ytimg.com/vi/Pe8uxhvJGnU/hqdefault.jpg"></a>
<h2>Price Controls - New York City Bread - Both Price Floor &amp; Price Ceiling - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=3jxCwP4N5ww">
<img class="lazy" data-src="https://i.ytimg.com/vi/3jxCwP4N5ww/hqdefault.jpg"></a>
<h2>Marginal Analysis Example - Marginal Cost &amp; Marginal Benefit - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=tKmzUYttrzY">
<img class="lazy" data-src="https://i.ytimg.com/vi/tKmzUYttrzY/hqdefault.jpg"></a>
<h2>Opportunity Cost Examples - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=ftJM9yf-HxQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/ftJM9yf-HxQ/hqdefault.jpg"></a>
<h2>Producer Surplus (Taxis) - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=8UrS9J4NCJc">
<img class="lazy" data-src="https://i.ytimg.com/vi/8UrS9J4NCJc/hqdefault.jpg"></a>
<h2>Consumer Surplus (Fun World) - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=cFRkcO6KRe8">
<img class="lazy" data-src="https://i.ytimg.com/vi/cFRkcO6KRe8/hqdefault.jpg"></a>
<h2>Producer and Consumer Surplus - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=bhucQFXX1Gs">
<img class="lazy" data-src="https://i.ytimg.com/vi/bhucQFXX1Gs/hqdefault.jpg"></a>
<h2>More Shifts to Demand and Supply Curves - Equilibrium Analysis - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=aybmcZtkM6k">
<img class="lazy" data-src="https://i.ytimg.com/vi/aybmcZtkM6k/hqdefault.jpg"></a>
<h2>Supply and Demand Curve Analysis Example (Trucks, Intro to Microeconomics)</h2><a href="https://www.youtube.com/watch?v=sKbSmLu9V0I">
<img class="lazy" data-src="https://i.ytimg.com/vi/sKbSmLu9V0I/hqdefault.jpg"></a>
<h2>Supply and Demand Curve Analysis (Pearl Jam) - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=qb9gDpzRptQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/qb9gDpzRptQ/hqdefault.jpg"></a>
<h2>Supply &amp; Demand Curve Shift Examples - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=TEsp7CzsOU0">
<img class="lazy" data-src="https://i.ytimg.com/vi/TEsp7CzsOU0/hqdefault.jpg"></a>
<h2>Normative vs. Positive Statement Examples - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=R0k3FvPsZW8">
<img class="lazy" data-src="https://i.ytimg.com/vi/R0k3FvPsZW8/hqdefault.jpg"></a>
<h2>Feasibility and Efficiency with Production Possibility Frontier (PPF) - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=tTyMOlgqvSw">
<img class="lazy" data-src="https://i.ytimg.com/vi/tTyMOlgqvSw/hqdefault.jpg"></a>
<h2>Production Possibility Frontier (PPF) - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=NeO4aKaP6EE">
<img class="lazy" data-src="https://i.ytimg.com/vi/NeO4aKaP6EE/hqdefault.jpg"></a>
<h2>Examples of Efficient and Inefficient Situations - Intro to Microeconomics</h2><a href="https://www.youtube.com/watch?v=ymU1Jvn03TE">
<img class="lazy" data-src="https://i.ytimg.com/vi/ymU1Jvn03TE/hqdefault.jpg"></a>
<h2>Equilibrium examples - intro to microeconomics</h2><a href="https://www.youtube.com/watch?v=6F-dvaddlGc">
<img class="lazy" data-src="https://i.ytimg.com/vi/6F-dvaddlGc/hqdefault.jpg"></a>
<h2>Marginal Analysis Examples &amp; Applications - intro to microeconomics</h2><a href="https://www.youtube.com/watch?v=v9NcBobA_Qw">
<img class="lazy" data-src="https://i.ytimg.com/vi/v9NcBobA_Qw/hqdefault.jpg"></a>
<h2>ConG - Software Demonstration</h2><a href="https://www.youtube.com/watch?v=lLRLQaetx10">
<img class="lazy" data-src="https://i.ytimg.com/vi/lLRLQaetx10/hqdefault.jpg"></a>
<h2>Public Goods Game with ConG</h2><a href="https://www.youtube.com/watch?v=TDC1SdlCnn0">
<img class="lazy" data-src="https://i.ytimg.com/vi/TDC1SdlCnn0/hqdefault.jpg"></a>
<h2>Prisonerss Dilemma Game in ConG Software</h2><a href="https://www.youtube.com/watch?v=Lt3FYoIOKyg">
<img class="lazy" data-src="https://i.ytimg.com/vi/Lt3FYoIOKyg/hqdefault.jpg"></a>
<h2>Hawk Dove with ConG - Economics Experiment Software</h2><a href="https://www.youtube.com/watch?v=4iJpRgidPPk">
<img class="lazy" data-src="https://i.ytimg.com/vi/4iJpRgidPPk/hqdefault.jpg"></a>
<h2>Creating Functions with R Software - example: the statistical mode</h2><a href="https://www.youtube.com/watch?v=rQEQAZ2Xzno">
<img class="lazy" data-src="https://i.ytimg.com/vi/rQEQAZ2Xzno/hqdefault.jpg"></a>
<h2>Calculating Mode with R Software (More on Rss Summary Stats)</h2><a href="https://www.youtube.com/watch?v=YvdYwC2YgeI">
<img class="lazy" data-src="https://i.ytimg.com/vi/YvdYwC2YgeI/hqdefault.jpg"></a>
<h2>Export Data with R (csv, tab-delineated and space separated examples)</h2><a href="https://www.youtube.com/watch?v=Md1hSm7kvy8">
<img class="lazy" data-src="https://i.ytimg.com/vi/Md1hSm7kvy8/hqdefault.jpg"></a>
<h2>Summary Stats with R Software</h2><a href="https://www.youtube.com/watch?v=J90kt3vGpFA">
<img class="lazy" data-src="https://i.ytimg.com/vi/J90kt3vGpFA/hqdefault.jpg"></a>
<h2>Creating a Histogram in R Software (the hist() function)</h2><a href="https://www.youtube.com/watch?v=wGW9M93YswY">
<img class="lazy" data-src="https://i.ytimg.com/vi/wGW9M93YswY/hqdefault.jpg"></a>
<h2>Loading Packages &amp; Working With Libraries - R Software</h2><a href="https://www.youtube.com/watch?v=hHqNxR1lUjY">
<img class="lazy" data-src="https://i.ytimg.com/vi/hHqNxR1lUjY/hqdefault.jpg"></a>
<h2>Summary Statistics In R Software (Pt. 1 of 3)</h2><a href="https://www.youtube.com/watch?v=O03ysvNRlxo">
<img class="lazy" data-src="https://i.ytimg.com/vi/O03ysvNRlxo/hqdefault.jpg"></a>
<h2>An Introduction to R - A Brief Tutorial for R {Software for Statistical Analysis}</h2><a href="https://www.youtube.com/watch?v=LjuXiBjxryQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/LjuXiBjxryQ/hqdefault.jpg"></a>
<h2>Loading Data Into R Software - (read.table, Data/CSV Import Tutorial)</h2><a href="https://www.youtube.com/watch?v=VLtazaiYo-c">
<img class="lazy" data-src="https://i.ytimg.com/vi/VLtazaiYo-c/hqdefault.jpg"></a>
<h2>How to use SQL within R and some performance comparisons | R Programming</h2><a href="https://www.youtube.com/watch?v=l2dKAq6ujnY">
<img class="lazy" data-src="https://i.ytimg.com/vi/l2dKAq6ujnY/hqdefault.jpg"></a>
<h2>Python vs R: some performance comparisons | R Programming</h2><a href="https://www.youtube.com/watch?v=FSBx9lpz0fs">
<img class="lazy" data-src="https://i.ytimg.com/vi/FSBx9lpz0fs/hqdefault.jpg"></a>
<h2>Can Julia really make your R code faster?! | R Programming</h2><a href="https://www.youtube.com/watch?v=1dvnJdK9nCQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/1dvnJdK9nCQ/hqdefault.jpg"></a>
<h2>Make your R code 18,878 times faster! (Abridged) | R Programming</h2><a href="https://www.youtube.com/watch?v=dhfM4xeHVHI">
<img class="lazy" data-src="https://i.ytimg.com/vi/dhfM4xeHVHI/hqdefault.jpg"></a>
<h2>Make your R code 18,878 times faster! (Unabridged) | R Programming</h2><a href="https://www.youtube.com/watch?v=78icyDMZJyQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/78icyDMZJyQ/hqdefault.jpg"></a>
<h2>Welcome to Dynamic Data Script!</h2><a href="https://www.youtube.com/watch?v=Hdz3R0tUqHs">
<img class="lazy" data-src="https://i.ytimg.com/vi/Hdz3R0tUqHs/hqdefault.jpg"></a>
<h2>dplyr intro | Data manipulation in R</h2><a href="https://www.youtube.com/watch?v=rm0BQSWoJlc">
<img class="lazy" data-src="https://i.ytimg.com/vi/rm0BQSWoJlc/hqdefault.jpg"></a>
<h2>dplyr | My classic workflow | Data Science | R Programming</h2><a href="https://www.youtube.com/watch?v=27zCOiIWwhE">
<img class="lazy" data-src="https://i.ytimg.com/vi/27zCOiIWwhE/hqdefault.jpg"></a>
<h2>dplyr::arrange() | How to use dplyr arrange function | R Programming</h2><a href="https://www.youtube.com/watch?v=QjFG1rGOPHk">
<img class="lazy" data-src="https://i.ytimg.com/vi/QjFG1rGOPHk/hqdefault.jpg"></a>
<h2>dplyr::summarize() | How to use dplyr summarise function | R Programming</h2><a href="https://www.youtube.com/watch?v=8oY1SIA92JQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/8oY1SIA92JQ/hqdefault.jpg"></a>
<h2>dplyr::mutate() | How to use dplyr mutate function | R Programming</h2><a href="https://www.youtube.com/watch?v=Y0zE9AWBVfg">
<img class="lazy" data-src="https://i.ytimg.com/vi/Y0zE9AWBVfg/hqdefault.jpg"></a>
<h2>dplyr::group_by() | How to use dplyr group by function | R Programming</h2><a href="https://www.youtube.com/watch?v=EUlEQiy3LBA">
<img class="lazy" data-src="https://i.ytimg.com/vi/EUlEQiy3LBA/hqdefault.jpg"></a>
<h2>dplyr::filter() | How to use dplyr filter function | R Programming</h2><a href="https://www.youtube.com/watch?v=5sGtqnz7Hss">
<img class="lazy" data-src="https://i.ytimg.com/vi/5sGtqnz7Hss/hqdefault.jpg"></a>
<h2>dplyr::select() | How to use dplyr select function | R Programming</h2><a href="https://www.youtube.com/watch?v=B5K5cQsHs8U">
<img class="lazy" data-src="https://i.ytimg.com/vi/B5K5cQsHs8U/hqdefault.jpg"></a>
<h2>R Programming for Beginners (2020) | Complete Tutorial | R &amp; RStudio</h2><a href="https://www.youtube.com/watch?v=BvKETZ6kr9Q">
<img class="lazy" data-src="https://i.ytimg.com/vi/BvKETZ6kr9Q/hqdefault.jpg"></a>
<h2>The paste() Function in R</h2><a href="https://www.youtube.com/watch?v=_mNnbWGAroU">
<img class="lazy" data-src="https://i.ytimg.com/vi/_mNnbWGAroU/hqdefault.jpg"></a>
<h2>The aggregate() Function in R</h2><a href="https://www.youtube.com/watch?v=zmiC7X9fUmo">
<img class="lazy" data-src="https://i.ytimg.com/vi/zmiC7X9fUmo/hqdefault.jpg"></a>
<h2>Apply Family of Functions in R Part 1: apply()</h2><a href="https://www.youtube.com/watch?v=f0U74ZvLfQo">
<img class="lazy" data-src="https://i.ytimg.com/vi/f0U74ZvLfQo/hqdefault.jpg"></a>
<h2>Making Functions in R</h2><a href="https://www.youtube.com/watch?v=i2VH5jIL76Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/i2VH5jIL76Y/hqdefault.jpg"></a>
<h2>While Loops in R</h2><a href="https://www.youtube.com/watch?v=SJVrHumq0zc">
<img class="lazy" data-src="https://i.ytimg.com/vi/SJVrHumq0zc/hqdefault.jpg"></a>
<h2>For Loops in R</h2><a href="https://www.youtube.com/watch?v=h987LWDvqlQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/h987LWDvqlQ/hqdefault.jpg"></a>
<h2>Conditional Statements in R</h2><a href="https://www.youtube.com/watch?v=cvlej_eKbmE">
<img class="lazy" data-src="https://i.ytimg.com/vi/cvlej_eKbmE/hqdefault.jpg"></a>
<h2>Apply Family of Functions in R Part 2: lapply() and sapply()</h2><a href="https://www.youtube.com/watch?v=ejVWRKidi9M">
<img class="lazy" data-src="https://i.ytimg.com/vi/ejVWRKidi9M/hqdefault.jpg"></a>
<h2>Apply Family of Functions  in R Part 3: tapply()</h2><a href="https://www.youtube.com/watch?v=HmBPDTtb6Bg">
<img class="lazy" data-src="https://i.ytimg.com/vi/HmBPDTtb6Bg/hqdefault.jpg"></a>
<h2>Inset graphs within ggplots</h2><a href="https://www.youtube.com/watch?v=Dkew_9bSCwE">
<img class="lazy" data-src="https://i.ytimg.com/vi/Dkew_9bSCwE/hqdefault.jpg"></a>
<h2>histograms in Rss ggplot</h2><a href="https://www.youtube.com/watch?v=HbzM53riug0">
<img class="lazy" data-src="https://i.ytimg.com/vi/HbzM53riug0/hqdefault.jpg"></a>
<h2>ggplot scatterplots in R</h2><a href="https://www.youtube.com/watch?v=oVShyv8i3EI">
<img class="lazy" data-src="https://i.ytimg.com/vi/oVShyv8i3EI/hqdefault.jpg"></a>
<h2>ggplot2 boxplots in R</h2><a href="https://www.youtube.com/watch?v=j5b7NNRycxs">
<img class="lazy" data-src="https://i.ytimg.com/vi/j5b7NNRycxs/hqdefault.jpg"></a>
<h2>Removing NAs in R dataframes</h2><a href="https://www.youtube.com/watch?v=wu9LNdPS0Ro">
<img class="lazy" data-src="https://i.ytimg.com/vi/wu9LNdPS0Ro/hqdefault.jpg"></a>
<h2>Selecting and removing columns from R dataframes</h2><a href="https://www.youtube.com/watch?v=XpgIPfHhsqU">
<img class="lazy" data-src="https://i.ytimg.com/vi/XpgIPfHhsqU/hqdefault.jpg"></a>
<h2>Naming and renaming columns in R dataframes</h2><a href="https://www.youtube.com/watch?v=Okc0IL5uTnA">
<img class="lazy" data-src="https://i.ytimg.com/vi/Okc0IL5uTnA/hqdefault.jpg"></a>
<h2>Selecting and removing rows in R dataframes</h2><a href="https://www.youtube.com/watch?v=KXSPxjjS8Fc">
<img class="lazy" data-src="https://i.ytimg.com/vi/KXSPxjjS8Fc/hqdefault.jpg"></a>
<h2>Exploring data in R</h2><a href="https://www.youtube.com/watch?v=mgslMeYwNmM">
<img class="lazy" data-src="https://i.ytimg.com/vi/mgslMeYwNmM/hqdefault.jpg"></a>
<h2>Combining data in R</h2><a href="https://www.youtube.com/watch?v=v5SIMV6Fi04">
<img class="lazy" data-src="https://i.ytimg.com/vi/v5SIMV6Fi04/hqdefault.jpg"></a>
<h2>Course Review | Linear Regression, Logistic Regression, Poisson Regression, Survival Analysis</h2><a href="https://www.youtube.com/watch?v=5rOUGoNWw0Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/5rOUGoNWw0Y/hqdefault.jpg"></a>
<h2>Survival Analysis Part 12 | Checking Cox PH Model Assumptions in R with RStudio</h2><a href="https://www.youtube.com/watch?v=M1QQ8QQn8Zs">
<img class="lazy" data-src="https://i.ytimg.com/vi/M1QQ8QQn8Zs/hqdefault.jpg"></a>
<h2>Survival Analysis Part 11 | Cox Proportional Hazards Model in R with RStudio</h2><a href="https://www.youtube.com/watch?v=TrS2M5imOt8">
<img class="lazy" data-src="https://i.ytimg.com/vi/TrS2M5imOt8/hqdefault.jpg"></a>
<h2>Survival Analysis Part 10 | Model Assumptions for Cox Proportional Hazards Model</h2><a href="https://www.youtube.com/watch?v=QAgtZKpKj9M">
<img class="lazy" data-src="https://i.ytimg.com/vi/QAgtZKpKj9M/hqdefault.jpg"></a>
<h2>Survival Analysis Part 9 | Cox Proportional Hazards Model</h2><a href="https://www.youtube.com/watch?v=aETMUW_TWV0">
<img class="lazy" data-src="https://i.ytimg.com/vi/aETMUW_TWV0/hqdefault.jpg"></a>
<h2>A COVID-19 Special: Social Distancing and Bending the Curve with R</h2><a href="https://www.youtube.com/watch?v=TIsXHxLNw6k">
<img class="lazy" data-src="https://i.ytimg.com/vi/TIsXHxLNw6k/hqdefault.jpg"></a>
<h2>Survival Analysis Part 8 | Kaplan Meier vs Exponential vs Cox Proportional Hazards (How The Differ)</h2><a href="https://www.youtube.com/watch?v=KDpAtrqS39w">
<img class="lazy" data-src="https://i.ytimg.com/vi/KDpAtrqS39w/hqdefault.jpg"></a>
<h2>Survival Analysis Part 7 | Exponential Model (Intro to Regression Models for Survival)</h2><a href="https://www.youtube.com/watch?v=T_goHnU8Eu4">
<img class="lazy" data-src="https://i.ytimg.com/vi/T_goHnU8Eu4/hqdefault.jpg"></a>
<h2>Survival Analysis Part 5 | Kaplan Meier Model in R with RStudio</h2><a href="https://www.youtube.com/watch?v=6_AF9mMuk9E">
<img class="lazy" data-src="https://i.ytimg.com/vi/6_AF9mMuk9E/hqdefault.jpg"></a>
<h2>Survival Analysis Part 4 | Kaplan Meier Model</h2><a href="https://www.youtube.com/watch?v=VJPPeUpyC6c">
<img class="lazy" data-src="https://i.ytimg.com/vi/VJPPeUpyC6c/hqdefault.jpg"></a>
<h2>Survival Analysis Part 3 | Kaplan Meier vs. Exponential vs. Cox Proportional Hazards (Pros &amp; Cons)</h2><a href="https://www.youtube.com/watch?v=K7bmmbD7KIg">
<img class="lazy" data-src="https://i.ytimg.com/vi/K7bmmbD7KIg/hqdefault.jpg"></a>
<h2>Survival Analysis Part 2 | Survival Function, Hazard, &amp; Hazard Ratio</h2><a href="https://www.youtube.com/watch?v=MdmWdIV5k-I">
<img class="lazy" data-src="https://i.ytimg.com/vi/MdmWdIV5k-I/hqdefault.jpg"></a>
<h2>Survival Analysis Part 1 | What is Censoring?</h2><a href="https://www.youtube.com/watch?v=vX3l36ptrTU">
<img class="lazy" data-src="https://i.ytimg.com/vi/vX3l36ptrTU/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #9</h2><a href="https://www.youtube.com/watch?v=mxNlTHBpJ3o">
<img class="lazy" data-src="https://i.ytimg.com/vi/mxNlTHBpJ3o/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #8</h2><a href="https://www.youtube.com/watch?v=SViNAyZTOA8">
<img class="lazy" data-src="https://i.ytimg.com/vi/SViNAyZTOA8/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #7</h2><a href="https://www.youtube.com/watch?v=NQB4W71WDh4">
<img class="lazy" data-src="https://i.ytimg.com/vi/NQB4W71WDh4/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #6</h2><a href="https://www.youtube.com/watch?v=ZWR6Bns90nQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/ZWR6Bns90nQ/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #5</h2><a href="https://www.youtube.com/watch?v=zFKOILNnN3w">
<img class="lazy" data-src="https://i.ytimg.com/vi/zFKOILNnN3w/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #4</h2><a href="https://www.youtube.com/watch?v=ewtqpg3ZbvE">
<img class="lazy" data-src="https://i.ytimg.com/vi/ewtqpg3ZbvE/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #3</h2><a href="https://www.youtube.com/watch?v=Sth3mdWk1IU">
<img class="lazy" data-src="https://i.ytimg.com/vi/Sth3mdWk1IU/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #2</h2><a href="https://www.youtube.com/watch?v=CpUAXsPAk30">
<img class="lazy" data-src="https://i.ytimg.com/vi/CpUAXsPAk30/hqdefault.jpg"></a>
<h2>MHA Meeting #2 Video #1</h2><a href="https://www.youtube.com/watch?v=IdjtA3ZgAks">
<img class="lazy" data-src="https://i.ytimg.com/vi/IdjtA3ZgAks/hqdefault.jpg"></a>
<h2>Poisson Regression: Overdispersion causes and Solutions</h2><a href="https://www.youtube.com/watch?v=0W5QF_OnR7w">
<img class="lazy" data-src="https://i.ytimg.com/vi/0W5QF_OnR7w/hqdefault.jpg"></a>
<h2>Poisson Regression: Zero Inflation (Excessive Zeros)</h2><a href="https://www.youtube.com/watch?v=eIY--zc5f24">
<img class="lazy" data-src="https://i.ytimg.com/vi/eIY--zc5f24/hqdefault.jpg"></a>
<h2>Poisson Regression Review</h2><a href="https://www.youtube.com/watch?v=A8H6gc9Eq0w">
<img class="lazy" data-src="https://i.ytimg.com/vi/A8H6gc9Eq0w/hqdefault.jpg"></a>
<h2>The Monty Hall Problem in Statistics | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=0aVZVRxMCzE">
<img class="lazy" data-src="https://i.ytimg.com/vi/0aVZVRxMCzE/hqdefault.jpg"></a>
<h2>Measures of Spread &amp; Variability: Range, Variance, SD, etc| Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=C7AoEjLyW78">
<img class="lazy" data-src="https://i.ytimg.com/vi/C7AoEjLyW78/hqdefault.jpg"></a>
<h2>Percentiles, Quantiles and Quartiles in Statistics | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Ky7QeVgv-BA">
<img class="lazy" data-src="https://i.ytimg.com/vi/Ky7QeVgv-BA/hqdefault.jpg"></a>
<h2>Plots for Two Variables | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=sbsY6neZ07g">
<img class="lazy" data-src="https://i.ytimg.com/vi/sbsY6neZ07g/hqdefault.jpg"></a>
<h2>Study Designs (Cross-sectional, Case-control, Cohort) | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=gXwI9W5wqjc">
<img class="lazy" data-src="https://i.ytimg.com/vi/gXwI9W5wqjc/hqdefault.jpg"></a>
<h2>Statistics Terminology and Definitions| Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=-JXDd52XsQE">
<img class="lazy" data-src="https://i.ytimg.com/vi/-JXDd52XsQE/hqdefault.jpg"></a>
<h2>Describing Distributions: Center, Spread &amp; Shape | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=2Y2l9QJCe6M">
<img class="lazy" data-src="https://i.ytimg.com/vi/2Y2l9QJCe6M/hqdefault.jpg"></a>
<h2>Mean, Median and Mode in Statistics | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=2pyq0TJmpzs">
<img class="lazy" data-src="https://i.ytimg.com/vi/2pyq0TJmpzs/hqdefault.jpg"></a>
<h2>Boxplots in Statistics | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=9AKLd5FHzfI">
<img class="lazy" data-src="https://i.ytimg.com/vi/9AKLd5FHzfI/hqdefault.jpg"></a>
<h2>Histograms and Density Plots for Numeric Variables | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=p-VOptZ0E5Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/p-VOptZ0E5Y/hqdefault.jpg"></a>
<h2>Bar Chart, Pie Chart, Frequency Tables | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Rx8wSEDq5Hs">
<img class="lazy" data-src="https://i.ytimg.com/vi/Rx8wSEDq5Hs/hqdefault.jpg"></a>
<h2>Variables and Types of Variables | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ZxV-kf0yBss">
<img class="lazy" data-src="https://i.ytimg.com/vi/ZxV-kf0yBss/hqdefault.jpg"></a>
<h2>Outtakes! MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=zrdujGQtp8Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/zrdujGQtp8Y/hqdefault.jpg"></a>
<h2>Permutation Hypothesis Test in R with Examples | R Tutorial 4.6 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=xRzEWLfEEIA">
<img class="lazy" data-src="https://i.ytimg.com/vi/xRzEWLfEEIA/hqdefault.jpg"></a>
<h2>Permutation Hypothesis Testing with Example | Statistics Tutorial # 37 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=rJ3AZCQuiLw">
<img class="lazy" data-src="https://i.ytimg.com/vi/rJ3AZCQuiLw/hqdefault.jpg"></a>
<h2>Course Review: Review of Regression Models | Statistics for Applied Epidemiology | Tutorial 12</h2><a href="https://www.youtube.com/watch?v=E5o1qz6nn7s">
<img class="lazy" data-src="https://i.ytimg.com/vi/E5o1qz6nn7s/hqdefault.jpg"></a>
<h2>Survival Analysis | Statistics for Applied Epidemiology | Tutorial 11</h2><a href="https://www.youtube.com/watch?v=sJPti8Yh4k4">
<img class="lazy" data-src="https://i.ytimg.com/vi/sJPti8Yh4k4/hqdefault.jpg"></a>
<h2>Poisson Regression Part II | Statistics for Applied Epidemiology | Tutorial 10</h2><a href="https://www.youtube.com/watch?v=URvhjZxyPhM">
<img class="lazy" data-src="https://i.ytimg.com/vi/URvhjZxyPhM/hqdefault.jpg"></a>
<h2>Poisson Regression Part I | Statistics for Applied Epidemiology | Tutorial 9</h2><a href="https://www.youtube.com/watch?v=0XfXHYDYoBA">
<img class="lazy" data-src="https://i.ytimg.com/vi/0XfXHYDYoBA/hqdefault.jpg"></a>
<h2>Logistic Regression Part III | Statistics for Applied Epidemiology | Tutorial 8</h2><a href="https://www.youtube.com/watch?v=oVeMquBjNYs">
<img class="lazy" data-src="https://i.ytimg.com/vi/oVeMquBjNYs/hqdefault.jpg"></a>
<h2>Bootstrap Confidence Interval with R | R Video Tutorial 4.5 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Om5TMGj9td4">
<img class="lazy" data-src="https://i.ytimg.com/vi/Om5TMGj9td4/hqdefault.jpg"></a>
<h2>Linear Regression Assignment Review | Statistics for Applied Epidemiology | Tutorial 7</h2><a href="https://www.youtube.com/watch?v=bsGz0Judj4Q">
<img class="lazy" data-src="https://i.ytimg.com/vi/bsGz0Judj4Q/hqdefault.jpg"></a>
<h2>Logistic Regression II | Statistics for Applied Epidemiology | Tutorial 6</h2><a href="https://www.youtube.com/watch?v=uK7n9A48YzQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/uK7n9A48YzQ/hqdefault.jpg"></a>
<h2>Logistic Regression I | Statistics for Applied Epidemiology | Tutorial 5</h2><a href="https://www.youtube.com/watch?v=tgVlcVSPNrA">
<img class="lazy" data-src="https://i.ytimg.com/vi/tgVlcVSPNrA/hqdefault.jpg"></a>
<h2>Multiple Linear Regression III | Statistics for Applied Epidemiology | Tutorial 4</h2><a href="https://www.youtube.com/watch?v=g0UKccXfW6I">
<img class="lazy" data-src="https://i.ytimg.com/vi/g0UKccXfW6I/hqdefault.jpg"></a>
<h2>Multiple Linear Regression II | Statistics for Applied Epidemiology | Tutorial 3</h2><a href="https://www.youtube.com/watch?v=YQkoVDPKtTQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/YQkoVDPKtTQ/hqdefault.jpg"></a>
<h2>Bootstrap Confidence Interval with Examples | Statistics Tutorial #36 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=-YgeLJRZQYY">
<img class="lazy" data-src="https://i.ytimg.com/vi/-YgeLJRZQYY/hqdefault.jpg"></a>
<h2>Multiple Linear Regression | Statistics for Applied Epidemiology | Tutorial 2</h2><a href="https://www.youtube.com/watch?v=Nwdp3wVxEBM">
<img class="lazy" data-src="https://i.ytimg.com/vi/Nwdp3wVxEBM/hqdefault.jpg"></a>
<h2>Simple Linear Regression | Statistics for Applied Epidemiology | Tutorial 1</h2><a href="https://www.youtube.com/watch?v=vt_akgeMd6g">
<img class="lazy" data-src="https://i.ytimg.com/vi/vt_akgeMd6g/hqdefault.jpg"></a>
<h2>Samples from a Normal Distribution | Statistics Tutorial #4 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=y0Vwi7O5l6k">
<img class="lazy" data-src="https://i.ytimg.com/vi/y0Vwi7O5l6k/hqdefault.jpg"></a>
<h2>Bootstrap Hypothesis Testing in R with Example | R Video Tutorial 4.4 | MarinStatsLecutres</h2><a href="https://www.youtube.com/watch?v=Zet-qmEEfCU">
<img class="lazy" data-src="https://i.ytimg.com/vi/Zet-qmEEfCU/hqdefault.jpg"></a>
<h2>Bootstrap Hypothesis Testing in Statistics with Example |Statistics Tutorial #35 |MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=9STZ7MxkNVg">
<img class="lazy" data-src="https://i.ytimg.com/vi/9STZ7MxkNVg/hqdefault.jpg"></a>
<h2>Statistics for Health Research Tutorial 12: Course Review and Exam Preparation</h2><a href="https://www.youtube.com/watch?v=OV19OmxCKkQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/OV19OmxCKkQ/hqdefault.jpg"></a>
<h2>Statistics for Health Research Tutorial 11: Linear Regression</h2><a href="https://www.youtube.com/watch?v=k30BDpGBHxo">
<img class="lazy" data-src="https://i.ytimg.com/vi/k30BDpGBHxo/hqdefault.jpg"></a>
<h2>Importing/Reading Excel data into R using RStudio (readxl) | R Tutorial 1.5b | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=JYVWufSQ4OI">
<img class="lazy" data-src="https://i.ytimg.com/vi/JYVWufSQ4OI/hqdefault.jpg"></a>
<h2>Statistics for Health Research Tutorial 10: Correlation, Simple Linear Regression</h2><a href="https://www.youtube.com/watch?v=upUFvcsnvS4">
<img class="lazy" data-src="https://i.ytimg.com/vi/upUFvcsnvS4/hqdefault.jpg"></a>
<h2>Statistics for Health Research Tutorial 9: Chi Square, Risk Difference, NNT, Risk Ratio, Odds Ratio</h2><a href="https://www.youtube.com/watch?v=3IYxWOSqj0M">
<img class="lazy" data-src="https://i.ytimg.com/vi/3IYxWOSqj0M/hqdefault.jpg"></a>
<h2>Statistics for Health Research Tutorial 8: ANOVA and Bonferroni Correction</h2><a href="https://www.youtube.com/watch?v=pDbXxC7O7O4">
<img class="lazy" data-src="https://i.ytimg.com/vi/pDbXxC7O7O4/hqdefault.jpg"></a>
<h2>R Squared or Coefficient of Determination | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=GI8ohuIGjJA">
<img class="lazy" data-src="https://i.ytimg.com/vi/GI8ohuIGjJA/hqdefault.jpg"></a>
<h2>Nonlinearity in Linear Regression | Statistics Tutorial #33 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=tOzwEv0PoZk">
<img class="lazy" data-src="https://i.ytimg.com/vi/tOzwEv0PoZk/hqdefault.jpg"></a>
<h2>Statistics for Health Research Tutorial 7: Analysis of Variance, The Test, and Assumptions</h2><a href="https://www.youtube.com/watch?v=CE-00a8lTGI">
<img class="lazy" data-src="https://i.ytimg.com/vi/CE-00a8lTGI/hqdefault.jpg"></a>
<h2>Simple Linear Regression Concept | Statistics Tutorial #32 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=vblX9JVpHE8">
<img class="lazy" data-src="https://i.ytimg.com/vi/vblX9JVpHE8/hqdefault.jpg"></a>
<h2>Case-Control Study and Odds Ratio | Statistics Tutorial #31| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=NqsSun9HZfI">
<img class="lazy" data-src="https://i.ytimg.com/vi/NqsSun9HZfI/hqdefault.jpg"></a>
<h2>Odds Ratio, Relative Risk, Risk Difference | Statistics Tutorial #30| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=JmuciUfCJ_w">
<img class="lazy" data-src="https://i.ytimg.com/vi/JmuciUfCJ_w/hqdefault.jpg"></a>
<h2>Chi Square Test of Independence | Statistics Tutorial #29| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=pfc9MUz03XA">
<img class="lazy" data-src="https://i.ytimg.com/vi/pfc9MUz03XA/hqdefault.jpg"></a>
<h2>Paired t Test, Two Sample t Test, Rank Sum Test &amp; more | Tutorial 6 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ciwlWyUe6sk">
<img class="lazy" data-src="https://i.ytimg.com/vi/ciwlWyUe6sk/hqdefault.jpg"></a>
<h2>Intro to Statistics: Bivariate, Parametric vs Non Parametric Tests | Tutorial 5 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=dyGjxBi4-qo">
<img class="lazy" data-src="https://i.ytimg.com/vi/dyGjxBi4-qo/hqdefault.jpg"></a>
<h2>ANOVA Part IV: Bonferroni Correction | Statistics Tutorial #28 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=pscJPuCwUG0">
<img class="lazy" data-src="https://i.ytimg.com/vi/pscJPuCwUG0/hqdefault.jpg"></a>
<h2>ANOVA Part III: F Statistic and P Value | Statistics Tutorial #27 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=k-xZzEYL8oc">
<img class="lazy" data-src="https://i.ytimg.com/vi/k-xZzEYL8oc/hqdefault.jpg"></a>
<h2>ANOVA (Analysis of Variance) and Sum of Squares | Statistics Tutorial #26 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=-AeU4y2vkIs">
<img class="lazy" data-src="https://i.ytimg.com/vi/-AeU4y2vkIs/hqdefault.jpg"></a>
<h2>One Way ANOVA (Analysis of Variance): Introduction | Statistics Tutorial #25 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=_VFLX7xJuqk">
<img class="lazy" data-src="https://i.ytimg.com/vi/_VFLX7xJuqk/hqdefault.jpg"></a>
<h2>Two Sample t-Test:Equal vs Unequal Variance Assumption| Statistics Tutorial #24| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ikS7itcmWZM">
<img class="lazy" data-src="https://i.ytimg.com/vi/ikS7itcmWZM/hqdefault.jpg"></a>
<h2>Two Sample t-test for Independent Groups | Statistics Tutorial #23| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=mBiVCrW2vSU">
<img class="lazy" data-src="https://i.ytimg.com/vi/mBiVCrW2vSU/hqdefault.jpg"></a>
<h2>Intro to Statistics: Hypothesis Testing, Types of Errors, Power | Tutorial 4 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=xbvaj7f6nFE">
<img class="lazy" data-src="https://i.ytimg.com/vi/xbvaj7f6nFE/hqdefault.jpg"></a>
<h2>Intro to Statistics: Confidence Interval &amp; Margin of Error | Tutorial 3 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=vBdCgit8uaU">
<img class="lazy" data-src="https://i.ytimg.com/vi/vBdCgit8uaU/hqdefault.jpg"></a>
<h2>Wilcoxon Signed Rank Test | Statistics Tutorial #22 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=v4ZHlTbTOK8">
<img class="lazy" data-src="https://i.ytimg.com/vi/v4ZHlTbTOK8/hqdefault.jpg"></a>
<h2>Paired t Test | Statistics Tutorial #21| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Q0V7WpzICI8">
<img class="lazy" data-src="https://i.ytimg.com/vi/Q0V7WpzICI8/hqdefault.jpg"></a>
<h2>Bivariate Analysis for Categorical &amp; Numerical | Statistics Tutorial #20 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=tnYcNyJB5FQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/tnYcNyJB5FQ/hqdefault.jpg"></a>
<h2>Statistical Literacy for Medical Students (UBC MEDD 419)| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=xNqEzV1CtlQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/xNqEzV1CtlQ/hqdefault.jpg"></a>
<h2>Bivariate Analysis Meaning | Statistics Tutorial #19 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=_m8v77qbkBA">
<img class="lazy" data-src="https://i.ytimg.com/vi/_m8v77qbkBA/hqdefault.jpg"></a>
<h2>Power Calculations in Hypothesis Testing | Statistics Tutorial #17 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ryWoLqe0sd8">
<img class="lazy" data-src="https://i.ytimg.com/vi/ryWoLqe0sd8/hqdefault.jpg"></a>
<h2>Intro to Statistics: Normal Distribution &amp; Central Limit Theorem | Tutorial 2 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=521VS6Pgx2E">
<img class="lazy" data-src="https://i.ytimg.com/vi/521VS6Pgx2E/hqdefault.jpg"></a>
<h2>Intro to Statistics: Plots, Screening Tests, Normal Distribution | Tutorial 1|  MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=EzNnJ9nfc50">
<img class="lazy" data-src="https://i.ytimg.com/vi/EzNnJ9nfc50/hqdefault.jpg"></a>
<h2>Errors and Power in Hypothesis Testing | Statistics Tutorial #16 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=OYbc3uKpGmg">
<img class="lazy" data-src="https://i.ytimg.com/vi/OYbc3uKpGmg/hqdefault.jpg"></a>
<h2>Bootstrapping and Resampling in Statistics with Example| Statistics Tutorial #12 |MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=O_Fj4q8lgmc">
<img class="lazy" data-src="https://i.ytimg.com/vi/O_Fj4q8lgmc/hqdefault.jpg"></a>
<h2>Margin of Error &amp; Sample Size for Confidence Interval | Statistics Tutorial #11| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=d96hI8vQRvs">
<img class="lazy" data-src="https://i.ytimg.com/vi/d96hI8vQRvs/hqdefault.jpg"></a>
<h2>Statistical Inference Definition with Example | Statistics Tutorial #18 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=wyu7uUbVYYM">
<img class="lazy" data-src="https://i.ytimg.com/vi/wyu7uUbVYYM/hqdefault.jpg"></a>
<h2>Hypothesis Test vs. Confidence Interval | Statistics Tutorial #15 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=J-yMiTaai4c">
<img class="lazy" data-src="https://i.ytimg.com/vi/J-yMiTaai4c/hqdefault.jpg"></a>
<h2>Hypothesis Testing: One Sided vs Two Sided Alternative | Statistics Tutorial #14 |MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Fsa-5_XdIMs">
<img class="lazy" data-src="https://i.ytimg.com/vi/Fsa-5_XdIMs/hqdefault.jpg"></a>
<h2>Hypothesis Testing: Calculations and Interpretations| Statistics Tutorial #13 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=zH_3FBKuQHA">
<img class="lazy" data-src="https://i.ytimg.com/vi/zH_3FBKuQHA/hqdefault.jpg"></a>
<h2>Standard Error of the Mean: Concept and Formula | Statistics Tutorial #6 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=-Xz89dB_Hco">
<img class="lazy" data-src="https://i.ytimg.com/vi/-Xz89dB_Hco/hqdefault.jpg"></a>
<h2>Statistics Video Tutorials at a Glance | Best Statistics Tutorials | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=3fYlGZWBdM4">
<img class="lazy" data-src="https://i.ytimg.com/vi/3fYlGZWBdM4/hqdefault.jpg"></a>
<h2>Statistics Course Overview | Best Statistics Course | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=AN3UkzE3HMg">
<img class="lazy" data-src="https://i.ytimg.com/vi/AN3UkzE3HMg/hqdefault.jpg"></a>
<h2>t-distribution in Statistics and Probability | Statistics Tutorial #9 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=sVkzPI1M7Ms">
<img class="lazy" data-src="https://i.ytimg.com/vi/sVkzPI1M7Ms/hqdefault.jpg"></a>
<h2>Confidence Interval for Mean with Example | Statistics Tutorial #10 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=nKQ2KcCCnb0">
<img class="lazy" data-src="https://i.ytimg.com/vi/nKQ2KcCCnb0/hqdefault.jpg"></a>
<h2>Hypothesis Testing Explained | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=6F6frEyMxuk">
<img class="lazy" data-src="https://i.ytimg.com/vi/6F6frEyMxuk/hqdefault.jpg"></a>
<h2>Confidence Interval Concept Explained  | Statistics Tutorial #7 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=dl0CXDsTYjk">
<img class="lazy" data-src="https://i.ytimg.com/vi/dl0CXDsTYjk/hqdefault.jpg"></a>
<h2>Central Limit Theorem &amp; Sampling Distribution Concepts | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=horm4zWU-vA">
<img class="lazy" data-src="https://i.ytimg.com/vi/horm4zWU-vA/hqdefault.jpg"></a>
<h2>Normal Distribution,  Z-Scores &amp; Empirical Rule | Statistics Tutorial #3 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=zUnC1CV4FAc">
<img class="lazy" data-src="https://i.ytimg.com/vi/zUnC1CV4FAc/hqdefault.jpg"></a>
<h2>Sample and Population in Statistics | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=DOnucdP7LNU">
<img class="lazy" data-src="https://i.ytimg.com/vi/DOnucdP7LNU/hqdefault.jpg"></a>
<h2>Standard Deviation &amp; Degrees of Freedom Explained | Statistics Tutorial | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=nlm9gfso4mw">
<img class="lazy" data-src="https://i.ytimg.com/vi/nlm9gfso4mw/hqdefault.jpg"></a>
<h2>tApply Function in R | R Tutorial 1.16 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=9ZWHfozPn6k">
<img class="lazy" data-src="https://i.ytimg.com/vi/9ZWHfozPn6k/hqdefault.jpg"></a>
<h2>Apply Function in R  | R Tutorial 1.15 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=7sJ8r6Lb7-o">
<img class="lazy" data-src="https://i.ytimg.com/vi/7sJ8r6Lb7-o/hqdefault.jpg"></a>
<h2>Export Data from R (csv , txt and other formats) | R Tutorial 1.6 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=WjpcbmcJjjM">
<img class="lazy" data-src="https://i.ytimg.com/vi/WjpcbmcJjjM/hqdefault.jpg"></a>
<h2>Starbucks and Statistics | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=WrnGRsECzT4">
<img class="lazy" data-src="https://i.ytimg.com/vi/WrnGRsECzT4/hqdefault.jpg"></a>
<h2>What is a Hypothesis Test and a P-Value? | Puppet Master of Statistics</h2><a href="https://www.youtube.com/watch?v=vwWEa8wU_6U">
<img class="lazy" data-src="https://i.ytimg.com/vi/vwWEa8wU_6U/hqdefault.jpg"></a>
<h2>What is a Confidence Interval? | Puppet Master of Statistics</h2><a href="https://www.youtube.com/watch?v=9jTJD5SLweY">
<img class="lazy" data-src="https://i.ytimg.com/vi/9jTJD5SLweY/hqdefault.jpg"></a>
<h2>What is a Sampling Distribution? | Puppet Master of Statistics</h2><a href="https://www.youtube.com/watch?v=olK80ngCbXc">
<img class="lazy" data-src="https://i.ytimg.com/vi/olK80ngCbXc/hqdefault.jpg"></a>
<h2>Hypothesis Testing: Errors and Power (one sample t test) I Statistics 101 #7 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ZXc9SRPDESc">
<img class="lazy" data-src="https://i.ytimg.com/vi/ZXc9SRPDESc/hqdefault.jpg"></a>
<h2>Hypothesis Testing: Conclusion (one sample t test) I Statistics 101 #6 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=R7y1dIRIqq8">
<img class="lazy" data-src="https://i.ytimg.com/vi/R7y1dIRIqq8/hqdefault.jpg"></a>
<h2>Hypothesis Testing: P Value (one sample t test) I Statistics 101 #5 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=WojcyhC7EVc">
<img class="lazy" data-src="https://i.ytimg.com/vi/WojcyhC7EVc/hqdefault.jpg"></a>
<h2>Hypothesis Testing: Critical values &amp; Rejection Regions I Statistics 101 #4 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=BdeuCflLPQI">
<img class="lazy" data-src="https://i.ytimg.com/vi/BdeuCflLPQI/hqdefault.jpg"></a>
<h2>Hypothesis Testing: Test Statistic (one sample t test) I Statistics 101 #3 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=fTYewURFLR0">
<img class="lazy" data-src="https://i.ytimg.com/vi/fTYewURFLR0/hqdefault.jpg"></a>
<h2>Hypothesis Testing: Null &amp; Alternative Hypothesis I Statistics 101 #2 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=L1GV6nLnbyE">
<img class="lazy" data-src="https://i.ytimg.com/vi/L1GV6nLnbyE/hqdefault.jpg"></a>
<h2>Hypothesis Testing: The Big Picture (One Sample t-test) I Statistics 101 #1 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=WVG-2xmzjyc">
<img class="lazy" data-src="https://i.ytimg.com/vi/WVG-2xmzjyc/hqdefault.jpg"></a>
<h2>What is RStudio and Why Should You Download It? | R Tutorial 1.1 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=riONFzJdXcs">
<img class="lazy" data-src="https://i.ytimg.com/vi/riONFzJdXcs/hqdefault.jpg"></a>
<h2>Polynomial Regression in R | R Tutorial 5.12 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ZYN0YD7UfK4">
<img class="lazy" data-src="https://i.ytimg.com/vi/ZYN0YD7UfK4/hqdefault.jpg"></a>
<h2>Install R and RStudio</h2><a href="https://www.youtube.com/watch?v=d-u_7vdag-0">
<img class="lazy" data-src="https://i.ytimg.com/vi/d-u_7vdag-0/hqdefault.jpg"></a>
<h2>Partial F-Test for Variable Selection in Linear Regression | R Tutorial 5.11| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=G_obrpV70QQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/G_obrpV70QQ/hqdefault.jpg"></a>
<h2>Sensitivity, Specificity, Positive and Negative Predictive Values | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=eeM7KPRNlSs">
<img class="lazy" data-src="https://i.ytimg.com/vi/eeM7KPRNlSs/hqdefault.jpg"></a>
<h2>Box Plots with Two Factors (Stratified Boxplots) in R | R Tutorial 2.3 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=s7ljwAzB5dQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/s7ljwAzB5dQ/hqdefault.jpg"></a>
<h2>Interpreting Interaction in Linear Regression with R | R Tutorial 5.10 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=vZUtDJbzFRQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/vZUtDJbzFRQ/hqdefault.jpg"></a>
<h2>Multiple Linear Regression with Interaction in R | R Tutorial 5.9 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=8YuuIsoYqsg">
<img class="lazy" data-src="https://i.ytimg.com/vi/8YuuIsoYqsg/hqdefault.jpg"></a>
<h2>Including Variables/ Factors in Regression with R, Part II | R Tutorial 5.8 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ZtBmMhGkxxA">
<img class="lazy" data-src="https://i.ytimg.com/vi/ZtBmMhGkxxA/hqdefault.jpg"></a>
<h2>Including Variables/ Factors in Regression with R, Part I | R Tutorial 5.7 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=KHTBwTBkCzg">
<img class="lazy" data-src="https://i.ytimg.com/vi/KHTBwTBkCzg/hqdefault.jpg"></a>
<h2>Changing Numeric Variable to Categorical in R | R Tutorial 5.4 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=EWs1Ordh8nI">
<img class="lazy" data-src="https://i.ytimg.com/vi/EWs1Ordh8nI/hqdefault.jpg"></a>
<h2>Add and Customize Legends to Plots in R | R Tutorial 2.11| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=mB3iSp9-OPc">
<img class="lazy" data-src="https://i.ytimg.com/vi/mB3iSp9-OPc/hqdefault.jpg"></a>
<h2>Add and Customize Text in Plots with R | R Tutorial 2.10 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Zx3Nspy8sws">
<img class="lazy" data-src="https://i.ytimg.com/vi/Zx3Nspy8sws/hqdefault.jpg"></a>
<h2>Change Reference (Baseline) Category in Regression with R | R Tutorial 5.6 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=XJw6xdBYG7c">
<img class="lazy" data-src="https://i.ytimg.com/vi/XJw6xdBYG7c/hqdefault.jpg"></a>
<h2>Dummy Variables or Indicator Variables in R | R Tutorial 5.5 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=2s8AwoKZ-UE">
<img class="lazy" data-src="https://i.ytimg.com/vi/2s8AwoKZ-UE/hqdefault.jpg"></a>
<h2>Valentiness Day Gift for Math/Stats Nerds [MarinStatsLectures]</h2><a href="https://www.youtube.com/watch?v=tceJhELBWow">
<img class="lazy" data-src="https://i.ytimg.com/vi/tceJhELBWow/hqdefault.jpg"></a>
<h2>Multiple Linear Regression in R | R Tutorial 5.3 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=q1RD5ECsSB0">
<img class="lazy" data-src="https://i.ytimg.com/vi/q1RD5ECsSB0/hqdefault.jpg"></a>
<h2>Checking Linear Regression Assumptions in R | R Tutorial 5.2 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=eTZ4VUZHzxw">
<img class="lazy" data-src="https://i.ytimg.com/vi/eTZ4VUZHzxw/hqdefault.jpg"></a>
<h2>MarinStatsLectures! About Us.</h2><a href="https://www.youtube.com/watch?v=ECQLuizC7mk">
<img class="lazy" data-src="https://i.ytimg.com/vi/ECQLuizC7mk/hqdefault.jpg"></a>
<h2>Sheldon Fail: Probability vs. Odds (Big Bang Theory)</h2><a href="https://www.youtube.com/watch?v=ec5CkOYnXfc">
<img class="lazy" data-src="https://i.ytimg.com/vi/ec5CkOYnXfc/hqdefault.jpg"></a>
<h2>Simple Linear Regression in R | R Tutorial 5.1 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=66z_MRwtFJM">
<img class="lazy" data-src="https://i.ytimg.com/vi/66z_MRwtFJM/hqdefault.jpg"></a>
<h2>Correlations and Covariance in R with Example  | R Tutorial 4.12 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=XaNKst8ODEQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/XaNKst8ODEQ/hqdefault.jpg"></a>
<h2>Odds Ratio, Relative Risk &amp; Risk Difference with R | R Tutorial 4.11| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=V_YNPQoAyCc">
<img class="lazy" data-src="https://i.ytimg.com/vi/V_YNPQoAyCc/hqdefault.jpg"></a>
<h2>Import Data, Copy Data from Excel to R CSV &amp; TXT Files | R Tutorial 1.5 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=qPk0YEKhqB8">
<img class="lazy" data-src="https://i.ytimg.com/vi/qPk0YEKhqB8/hqdefault.jpg"></a>
<h2>Chi-Square Test, Fisher¡¯s Exact Test, &amp; Cross Tabulations in R | R Tutorial 4.10| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=POiHEJqmiC0">
<img class="lazy" data-src="https://i.ytimg.com/vi/POiHEJqmiC0/hqdefault.jpg"></a>
<h2>ANOVA, ANOVA Multiple Comparisons &amp; Kruskal Wallis in R | R Tutorial 4.9 | MarinStatsLectures|</h2><a href="https://www.youtube.com/watch?v=lpdFr5SZR0Q">
<img class="lazy" data-src="https://i.ytimg.com/vi/lpdFr5SZR0Q/hqdefault.jpg"></a>
<h2>Wilcoxon Signed Rank Test in R with Example | R Tutorial 4.8 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=zM8OZUM5I4Y">
<img class="lazy" data-src="https://i.ytimg.com/vi/zM8OZUM5I4Y/hqdefault.jpg"></a>
<h2>Paired t-Test in R with Examples | R Tutorial 4.7 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=yD6aU0fY2lo">
<img class="lazy" data-src="https://i.ytimg.com/vi/yD6aU0fY2lo/hqdefault.jpg"></a>
<h2>Mann Whitney U / Wilcoxon Rank-Sum Test in R | R Tutorial 4.3 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=KroKhtCD9eE">
<img class="lazy" data-src="https://i.ytimg.com/vi/KroKhtCD9eE/hqdefault.jpg"></a>
<h2>Two-Sample t Test in R (Independent Groups) with Example | R Tutorial 4.2 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=RlhnNbPZC0A">
<img class="lazy" data-src="https://i.ytimg.com/vi/RlhnNbPZC0A/hqdefault.jpg"></a>
<h2>One-Sample t Test &amp; Confidence Interval in R with Example | R Tutorial 4.1| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=kvmSAXhX9Hs">
<img class="lazy" data-src="https://i.ytimg.com/vi/kvmSAXhX9Hs/hqdefault.jpg"></a>
<h2>t Distribution and t Scores in R | R Tutorial 3.4 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ETd-jPhI_tE">
<img class="lazy" data-src="https://i.ytimg.com/vi/ETd-jPhI_tE/hqdefault.jpg"></a>
<h2>Normal Distribution, Z Scores, and Normal Probabilities in R | R Tutorial 3.3| MarinStatslectures</h2><a href="https://www.youtube.com/watch?v=peEsXbdMY_4">
<img class="lazy" data-src="https://i.ytimg.com/vi/peEsXbdMY_4/hqdefault.jpg"></a>
<h2>Poisson Distribution in R | R Tutorial 3.2 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=778WK1Pf8eI">
<img class="lazy" data-src="https://i.ytimg.com/vi/778WK1Pf8eI/hqdefault.jpg"></a>
<h2>Binomial Distribution in R | R Tutorial 3.1| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=iG995W0XefU">
<img class="lazy" data-src="https://i.ytimg.com/vi/iG995W0XefU/hqdefault.jpg"></a>
<h2>Calculating Mean, Standard Deviation, Frequencies and More in R | R Tutorial 2.8| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=ACWuV16tdhY">
<img class="lazy" data-src="https://i.ytimg.com/vi/ACWuV16tdhY/hqdefault.jpg"></a>
<h2>How to Modify and Customize Plots in R | R Tutorial 2.9 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=lPOSwfxMd3c">
<img class="lazy" data-src="https://i.ytimg.com/vi/lPOSwfxMd3c/hqdefault.jpg"></a>
<h2>Scatterplots in R | R Tutorial 2.7 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=FEAS3akVxD8">
<img class="lazy" data-src="https://i.ytimg.com/vi/FEAS3akVxD8/hqdefault.jpg"></a>
<h2>Stacked and Grouped Bar Charts and Mosaic Plots in R |R Tutorial 2.6| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=rl1tB9p3FLg">
<img class="lazy" data-src="https://i.ytimg.com/vi/rl1tB9p3FLg/hqdefault.jpg"></a>
<h2>Stem and Leaf Plots in R  | R Tutorial 2.5 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=NpBRo0tRNeY">
<img class="lazy" data-src="https://i.ytimg.com/vi/NpBRo0tRNeY/hqdefault.jpg"></a>
<h2>Histograms in R | R Tutorial 2.4 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Hj1pgap4UOY">
<img class="lazy" data-src="https://i.ytimg.com/vi/Hj1pgap4UOY/hqdefault.jpg"></a>
<h2>Boxplots and Grouped Boxplots in R | R Tutorial 2.2 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=U64yNvlhv9I">
<img class="lazy" data-src="https://i.ytimg.com/vi/U64yNvlhv9I/hqdefault.jpg"></a>
<h2>Bar Charts and Pie Charts in R | R Tutorial 2.1 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=Eph_Y0BmHU0">
<img class="lazy" data-src="https://i.ytimg.com/vi/Eph_Y0BmHU0/hqdefault.jpg"></a>
<h2>Customizing The Look of R Studio | R Tutorial 1.14 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=5dNNcC-UBeA">
<img class="lazy" data-src="https://i.ytimg.com/vi/5dNNcC-UBeA/hqdefault.jpg"></a>
<h2>How to Install Packages in R | R Tutorial 1.13 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=3RWb5U3X-T8">
<img class="lazy" data-src="https://i.ytimg.com/vi/3RWb5U3X-T8/hqdefault.jpg"></a>
<h2>Writing Scripts in R | R Tutorial 1.12 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=qqz_D1vzS5M">
<img class="lazy" data-src="https://i.ytimg.com/vi/qqz_D1vzS5M/hqdefault.jpg"></a>
<h2>Setting Up Working Directory in R  | R Tutorial 1.11 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=OJ4WBjV5o1I">
<img class="lazy" data-src="https://i.ytimg.com/vi/OJ4WBjV5o1I/hqdefault.jpg"></a>
<h2>Logic Statements (TRUE/FALSE), cbind and rbind Functions in R | R Tutorial 1.10| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=NFaK1Qn4u3A">
<img class="lazy" data-src="https://i.ytimg.com/vi/NFaK1Qn4u3A/hqdefault.jpg"></a>
<h2>Subsetting (Sort/Select) Data in R with Square Brackets | R Tutorial 1.9| MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=jGf7WNh-LX8">
<img class="lazy" data-src="https://i.ytimg.com/vi/jGf7WNh-LX8/hqdefault.jpg"></a>
<h2>Working with Variables and Data in R | R Tutorial 1.8 | MarinStatslectures</h2><a href="https://www.youtube.com/watch?v=1BcGnHwUT6k">
<img class="lazy" data-src="https://i.ytimg.com/vi/1BcGnHwUT6k/hqdefault.jpg"></a>
<h2>Importing , Checking and Working with Data in R | R Tutorial 1.7 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=U4-RnTW5dfw">
<img class="lazy" data-src="https://i.ytimg.com/vi/U4-RnTW5dfw/hqdefault.jpg"></a>
<h2>Create and Work with Vectors and Matrices in R | R Tutorial 1.4 | MarinStatslectures</h2><a href="https://www.youtube.com/watch?v=2TcPAZOyV0U">
<img class="lazy" data-src="https://i.ytimg.com/vi/2TcPAZOyV0U/hqdefault.jpg"></a>
<h2>Getting started with R: Basic Arithmetic and Coding in R | R Tutorial 1.3 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=UYclmg1_KLk">
<img class="lazy" data-src="https://i.ytimg.com/vi/UYclmg1_KLk/hqdefault.jpg"></a>
<h2>Download and Install R and RStudio  | R Tutorial 1.2 | MarinStatsLectures</h2><a href="https://www.youtube.com/watch?v=cX532N_XLIs">
<img class="lazy" data-src="https://i.ytimg.com/vi/cX532N_XLIs/hqdefault.jpg"></a>
<h2>Using the lapply function in R</h2><a href="https://www.youtube.com/watch?v=GgdR2OV8r_M">
<img class="lazy" data-src="https://i.ytimg.com/vi/GgdR2OV8r_M/hqdefault.jpg"></a>
<h2>Using the apply function in R</h2><a href="https://www.youtube.com/watch?v=csLati8vpOo">
<img class="lazy" data-src="https://i.ytimg.com/vi/csLati8vpOo/hqdefault.jpg"></a>
<h2>Using tapply and split in R</h2><a href="https://www.youtube.com/watch?v=9yCLZnKkLBg">
<img class="lazy" data-src="https://i.ytimg.com/vi/9yCLZnKkLBg/hqdefault.jpg"></a>
<h2>The mapply function in R</h2><a href="https://www.youtube.com/watch?v=EziuPEgNqvU">
<img class="lazy" data-src="https://i.ytimg.com/vi/EziuPEgNqvU/hqdefault.jpg"></a>
<h2>Control Structures in R</h2><a href="https://www.youtube.com/watch?v=s_h9ruNwI_0">
<img class="lazy" data-src="https://i.ytimg.com/vi/s_h9ruNwI_0/hqdefault.jpg"></a>
<h2>Reading/Writing Data: Part 1</h2><a href="https://www.youtube.com/watch?v=aBzAels6jPk">
<img class="lazy" data-src="https://i.ytimg.com/vi/aBzAels6jPk/hqdefault.jpg"></a>
<h2>Reading/Writing Data: Part 2</h2><a href="https://www.youtube.com/watch?v=cUUqDWttMws">
<img class="lazy" data-src="https://i.ytimg.com/vi/cUUqDWttMws/hqdefault.jpg"></a>
<h2>Lecture 2c: Vectorized Operations</h2><a href="https://www.youtube.com/watch?v=Fm8SORJQjPY">
<img class="lazy" data-src="https://i.ytimg.com/vi/Fm8SORJQjPY/hqdefault.jpg"></a>
<h2>Lecture 2b: Subsetting</h2><a href="https://www.youtube.com/watch?v=hWbgqzsQJF0">
<img class="lazy" data-src="https://i.ytimg.com/vi/hWbgqzsQJF0/hqdefault.jpg"></a>
<h2>Lecture 2a: Data Types</h2><a href="https://www.youtube.com/watch?v=5AQM-yUX9zg">
<img class="lazy" data-src="https://i.ytimg.com/vi/5AQM-yUX9zg/hqdefault.jpg"></a>
<h2>Background and Overview</h2><a href="https://www.youtube.com/watch?v=kzxHxFHW6hs">
<img class="lazy" data-src="https://i.ytimg.com/vi/kzxHxFHW6hs/hqdefault.jpg"></a>
<h2>Simulation in R</h2><a href="https://www.youtube.com/watch?v=tvv4IA8PEzw">
<img class="lazy" data-src="https://i.ytimg.com/vi/tvv4IA8PEzw/hqdefault.jpg"></a>
<h2>PlottingBase</h2><a href="https://www.youtube.com/watch?v=R2Zh_kPxrmg">
<img class="lazy" data-src="https://i.ytimg.com/vi/R2Zh_kPxrmg/hqdefault.jpg"></a>
<h2>PlottingBaseDemo</h2><a href="https://www.youtube.com/watch?v=4KLfzsj-qkE">
<img class="lazy" data-src="https://i.ytimg.com/vi/4KLfzsj-qkE/hqdefault.jpg"></a>
<h2>PlottingLattice</h2><a href="https://www.youtube.com/watch?v=AhTjV9nAJv0">
<img class="lazy" data-src="https://i.ytimg.com/vi/AhTjV9nAJv0/hqdefault.jpg"></a>
<h2>PlottingMath</h2><a href="https://www.youtube.com/watch?v=gNT33XIgw6E">
<img class="lazy" data-src="https://i.ytimg.com/vi/gNT33XIgw6E/hqdefault.jpg"></a>
<h2>Manta Unleashed BigDataSG Meetup - Part 1 of 2</h2><a href="https://www.youtube.com/watch?v=5XwOzb3LcOA">
<img class="lazy" data-src="https://i.ytimg.com/vi/5XwOzb3LcOA/hqdefault.jpg"></a>
<h2>Manta Unleashed BigDataSG Meetup - Part 2 of 2</h2><a href="https://www.youtube.com/watch?v=W2kywGUuz3g">
<img class="lazy" data-src="https://i.ytimg.com/vi/W2kywGUuz3g/hqdefault.jpg"></a>
<h2>Manipulating Data in R</h2><a href="https://www.youtube.com/watch?v=Y05UK1phiTA">
<img class="lazy" data-src="https://i.ytimg.com/vi/Y05UK1phiTA/hqdefault.jpg"></a>
<h2>Simple Graphs in R</h2><a href="https://www.youtube.com/watch?v=S0uoef36iFU">
<img class="lazy" data-src="https://i.ytimg.com/vi/S0uoef36iFU/hqdefault.jpg"></a>
<h2>Open and Save in R</h2><a href="https://www.youtube.com/watch?v=owYP9OgmrJk">
<img class="lazy" data-src="https://i.ytimg.com/vi/owYP9OgmrJk/hqdefault.jpg"></a>
<h2>Setting Up R</h2><a href="https://www.youtube.com/watch?v=OhHmQmih9EM">
<img class="lazy" data-src="https://i.ytimg.com/vi/OhHmQmih9EM/hqdefault.jpg"></a>
<h2>The R Language</h2><a href="https://www.youtube.com/watch?v=Y7ZGgsMwfkk">
<img class="lazy" data-src="https://i.ytimg.com/vi/Y7ZGgsMwfkk/hqdefault.jpg"></a>
<h2>Cell</h2><a href="https://www.youtube.com/watch?v=4fhSoN3tuSA">
<img class="lazy" data-src="https://i.ytimg.com/vi/4fhSoN3tuSA/hqdefault.jpg"></a>
<h2>HitPipes - Flintstones Theme</h2><a href="https://www.youtube.com/watch?v=TyY9t9kVnno">
<img class="lazy" data-src="https://i.ytimg.com/vi/TyY9t9kVnno/hqdefault.jpg"></a>
<h2>HitPipes - My Sharona</h2><a href="https://www.youtube.com/watch?v=L03e2M9QTW4">
<img class="lazy" data-src="https://i.ytimg.com/vi/L03e2M9QTW4/hqdefault.jpg"></a>
<h2>HitPipes AC/DC Thunderstruck Intro</h2><a href="https://www.youtube.com/watch?v=eEnk8uXq5wQ">
<img class="lazy" data-src="https://i.ytimg.com/vi/eEnk8uXq5wQ/hqdefault.jpg"></a>
<h2>Stefanie finds Bioinformatics (1999) Flash Movie</h2><a href="https://www.youtube.com/watch?v=yzL1yJ8znz0">
<img class="lazy" data-src="https://i.ytimg.com/vi/yzL1yJ8znz0/hqdefault.jpg"></a>
<h2>Prototype PVC Instrument - HitPipes(tm)</h2><a href="https://www.youtube.com/watch?v=LdrNOB1dA28">
<img class="lazy" data-src="https://i.ytimg.com/vi/LdrNOB1dA28/hqdefault.jpg"></a>

<h2><span class="orange">Using rvest when login is required</span></h2>
The overall flow is to login, go to a web page collect information, add it a dataframe and then move to the next page.

library(rvest) 

#Address of the login webpage
login = "https://stackoverflow.com/users/login?ssrc=head&returnurl=http%3a%2f%2fstackoverflow.com%2f"

#create a web session with the desired login address

pgsession = html_session(login)
pgform = html_form(pgsession)[[2]]  #in this case the submit is the 2nd form
filled_form = set_values(pgform, email="*****", password="*****")
submit_form(pgsession, filled_form)

#pre allocate the final results dataframe.
results = data.frame()  

#loop through all of the pages with the desired info
for (i in 1:5)
{
  #base address of the pages to extract information from
  url = "http://stackoverflow.com/users/**********?tab=answers&sort=activity&page="
  url = paste0(url, i)
  page = jump_to(pgsession, url)

  #collect info on the question votes and question title
  summary = html_nodes(page, "div .answer-summary")
  question = matrix(html_text(html_nodes(summary, "div"), trim=TRUE), ncol=2, byrow = TRUE)

  #find date answered, hyperlink and whether it was accepted
  dateans = html_node(summary, "span") %>% html_attr("title")
  hyperlink = html_node(summary, "div a") %>% html_attr("href")
  accepted = html_node(summary, "div") %>% html_attr("class")

  #create temp results then bind to final results 
  rtemp = cbind(question, dateans, accepted, hyperlink)
  results = rbind(results, rtemp)
}

#Dataframe Clean-up
names(results) = c("Votes", "Answer", "Date", "Accepted", "HyperLink")
results$Votes = as.integer(as.character(results$Votes))
results$Accepted = ifelse(results$Accepted=="answer-votes default", 0, 1)

The loop in this case is limited to only 5 pages.
User specific values is ******.

<h2>unname</h2>
Remove the names or dimnames attribute of an R object.
unname(obj, force = FALSE)

<h2>convert date to a day of week</h2>
weekdays(as.Date("201022", format="%y%m%d"))

<h2>RGB to Hex converter</h2>
rgb(123,212,125, maxColorValue=255)


<h2>generate crayon color table</h2>
  # rgb(123,212,125, maxColorValue=255), edit final commands
sink("colorcmd.txt")
for(i in 0:7){
  for(j in 0:7){
    for(k in 0:7){
      colorCode = rgb(i*32,j*32,k*32, maxColorValue=256)
      colorName = gsub("#","c",colorCode)
      cat("\n",colorName, '= make_style\\("', colorCode, '"\\)', sep="")
      cat(';cat\\(',colorName,'\\(\"',colorCode,"\"\\),\"\\n\\\")", sep="")
    }
  }
  cat("\n")
}
sink()

<h2>filter out nodes with rvest</h2>
By using xml_remove(), you can literally remove those nodes

text = '<table>
    <tr class="alt">
        <td>1</td>
        <td>2</td>
        <td class="hidden">3</td>
   </tr>
   <tr class="tr0 close notule">
        <td colspan="9">4</td>
    </tr>
</table>'

pageSource = read_html(text)

#select nodes you want to remove
hidden_nodes = pageSource %>% html_nodes(".hidden")
close_nodes = pageSource %>% html_nodes(".tr0.close.notule")

#remove those nodes
xml_remove(hidden_nodes)
xml_remove(close_nodes)

pageSource %>% html_table()

Sample:
itemList = html_nodes(pagesource, className)

syz = itemList %>% html_nodes(".syz")
searchBox = itemList %>% html_nodes("#searchBox")
friendlink = itemList %>% html_nodes(".friendlink")
clear = itemList %>% html_nodes(".clear")
footer = itemList %>% html_nodes(".footer")

#remove those nodes
xml_remove(syz)
xml_remove(searchBox)
xml_remove(friendlink)
xml_remove(clear)
xml_remove(footer)

itemList = as.character(itemList)

<h2>Rvest Limitations and CasperJS for R</h2>
<a href="https://www.r-bloggers.com/2019/11/when-rvest-is-not-enough/" class="whitebut ">When rvest is not enough</a>

<h2>find repeated characters on same line</h2>
    for(i in 1:length(jsPrep)){
      MultiBracket = length(unlist(gregexpr("\\(", jsPrep[i])))
    }

<h2>Parallel Computing</h2>
<h3>Terminology</h3>
Let’s just nail down some terminology.

A <em>core</em> is a general term for either a single processor on your own computer (technically you only have one processor, but a modern processor like the i7 can have multiple cores - hence the term) or a single machine in a cluster network.

A <em>cluster</em> is a collection of objecting capable of hosting cores, either a network or just the collection of cores on your personal computer.

A <em>process</em> is a single running version of R (or more generally any program). 
Each core runs a single process.

<h3>The <code>parallel</code> package</h3>
There are a number of packages which can be used for parallel processing in R. 
Two of the earliest and strongest were <code>multicore</code> and <code>snow</code>. 
However, both were adopted in the base R installation and merged into the <code>parallel</code> package.

<code>library(parallel)</code>

You can easily check the number of cores you have access to with <code>detectCores</code>:

<code>detectCores()</code>

<code>## [1] 4</code>

The number of cores represented is not neccessarily correlated with the number of processors you actually have thanks to the concept of "logical CPUs". 
For the most part, you can use this number as accurate. 
Trying to use more cores than you have available won’t provide any benefit.

<h3>Methods of Paralleization</h3>
There are two main ways in which code can be parallelized, via <em>sockets</em> or via <em>forking</em>. 
These function slightly differently:

The <em>socket</em> approach launches a new version of R on each core. 
Technically this connection is done via networking (e.g.&nbsp;the same as if you connected to a remote server), but the connection is happening all on your own computer<a href="#fn3" id="fnref3"><sup>3</sup></a> I mention this because you may get a warning from your computer asking whether to allow R to accept incoming connections, you should allow it.

The <em>forking</em> approach copies the entire current version of R and moves it to a new core.

There are various pro’s and con’s to the two approaches:

Socket:

Pro: Works on any system (including Windows).

Pro: Each process on each node is unique so it can’t cross-contaminate.

Con: Each process is unique so it will be slower

Con: Things such as package loading need to be done in each process separately. 
Variables defined on your main version of R don’t exist on each core unless explicitly placed there.

Con: More complicated to implement.

Forking:

Con: Only works on POSIX systems (Mac, Linux, Unix, BSD) and not Windows.

Con: Because processes are duplicates, it can cause issues specifically with random number generation (which should usually be handled by <code>parallel</code> in the background) or when running in a GUI (such as RStudio). 
This doesn’t come up often, but if you get odd behavior, this may be the case.

Pro: Faster than sockets.

Pro: Because it copies the existing version of R, your entire workspace exists in each process.

Pro: Trivially easy to implement.

In general, I’d recommend using forking if you’re not on Windows.

<em>Note</em>: These notes were compiled on OS X.

<h3>Forking with <code>mclapply</code></h3>
The most straightforward way to enable parallel processing is by switching from using <code>lapply</code> to <code>mclapply</code>. 
(Note I’m using <code>system.time</code> instead of <code>profvis</code> here because I only care about running time, not profiling.)

<code>library(lme4)</code>

<code>## Loading required package: Matrix</code>

<code>f &lt;- function(i) {
  lmer(Petal.Width ~ . 
- Species + (1 | Species), data = iris)
}
 
system.time(save1 &lt;- lapply(1:100, f))</code>

<code>##    user  system elapsed 
##   2.048   0.019   2.084</code>

<code>system.time(save2 &lt;- mclapply(1:100, f))</code>

<code>##    user  system elapsed 
##   1.295   0.150   1.471</code>

If you were to run this code on Windows, <code>mclapply</code> would simply call <code>lapply</code>, so the code works but sees no speed gain.

<code>mclapply</code> takes an argument, <code>mc.cores</code>. 
By default, <code>mclapply</code> will use all cores available to it. 
If you don’t want to (either becaues you’re on a shared system or you just want to save processing power for other purposes) you can set this to a value lower than the number of cores you have. 
Setting it to 1 disables parallel processing, and setting it higher than the number of available cores has no effect.

<h3>Using sockets with <code>parLapply</code></h3>
As promised, the sockets approach to parallel processing is more complicated and a bit slower, but works on Windows systems. 
The general process we’ll follow is

<ol style="list-style-type: decimal">
Start a cluster with <mi>n</mi></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true">n</nobr><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math><script type="math/tex" id="MathJax-Element-3">n</script> nodes.

Execute any pre-processing code necessary in each node (e.g.&nbsp;loading a package)

Use <code>par*apply</code> as a replacement for <code>*apply</code>. 
Note that unlike <code>mcapply</code>, this is <em>not</em> a drop-in replacement.

Destroy the cluster (not necessary, but best practices).

</ol>

<h4>Starting a cluster</h4>
The function to start a cluster is <code>makeCluster</code> which takes in as an argument the number of cores:

<code>numCores &lt;- detectCores()
numCores</code>

<code>## [1] 4</code>

<code>cl &lt;- makeCluster(numCores)</code>

The function takes an argument <code>type</code> which can be either <code>PSOCK</code> (the socket version) or <code>FORK</code> (the fork version). 
Generally, <code>mclapply</code> should be used for the forking approach, so there’s no need to change this.

If you were running this on a network of multiple computers as opposed to on your local machine, there are additional argumnts you may wish to run, but generally the other defaults should be specific.

<h4>Pre-processing code</h4>
When using the socket approach to parallel processing, each process is started fresh, so things like loaded packages and any variables existing in your current session do not exist. 
We must instead move those into each process.

The most generic way to do this is the <code>clusterEvalQ</code> function, which takes a cluster and any expression, and executes the expression on each process.

<code>clusterEvalQ(cl, 2 + 2)</code>

<code>## [[1]]
## [1] 4
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 4
## 
## [[4]]
## [1] 4</code>

Note the lack of inheritance:

<code>x &lt;- 1
clusterEvalQ(cl, x)</code>

<code>## Error in checkForRemoteErrors(lapply(cl, recvResult)): 4 nodes produced errors; first error: object 'x' not found</code>

We could fix this by wrapping the assignment in a <code>clusterEvalQ</code> call:

<code>clusterEvalQ(cl, y &lt;- 1)</code>

<code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1
## 
## [[3]]
## [1] 1
## 
## [[4]]
## [1] 1</code>

<code>clusterEvalQ(cl, y)</code>

<code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1
## 
## [[3]]
## [1] 1
## 
## [[4]]
## [1] 1</code>

<code>y</code>

<code>## Error in eval(expr, envir, enclos): object 'y' not found</code>

However, now <code>y</code> doesn’t exist in the main process. 
We can instead use <code>clusterExport</code> to pass objects to the processes:

<code>clusterExport(cl, "x")
clusterEvalQ(cl, x)</code>

<code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1
## 
## [[3]]
## [1] 1
## 
## [[4]]
## [1] 1</code>

The second argument is a vector of strings naming the variables to pass.

Finally, we can use <code>clusterEvalQ</code> to load packages:

<code>clusterEvalQ(cl, {
  library(ggplot2)
  library(stringr)
})</code>

<code>## [[1]]
## [1] "stringr"   "ggplot2"   "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"     
## 
## [[2]]
## [1] "stringr"   "ggplot2"   "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"     
## 
## [[3]]
## [1] "stringr"   "ggplot2"   "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"     
## 
## [[4]]
## [1] "stringr"   "ggplot2"   "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"</code>

Note that this helpfully returns a list of the packages loaded in each process.

<h4>Using <code>par*apply</code></h4>
There are parallel versions of the three main <code>apply</code> statements: <code>parApply</code>, <code>parLapply</code> and <code>parSapply</code> for <code>apply</code>, <code>lapply</code> and <code>sapply</code> respectively. 
They take an additional argument for the cluster to operate on.

<code>parSapply(cl, Orange, mean, na.rm = TRUE)</code>

<code>##          Tree           age circumference 
##            NA      922.1429      115.8571</code>

All the general advice and rules about <code>par*apply</code> apply as with the normal <code>*apply</code> functions.

<h4>Close the cluster</h4>
<code>stopCluster(cl)</code>

This is not fully necessary, but is best practices. 
If not stopped, the processes continue to run in the background, consuming resources, and any new processes can be slowed or delayed. 
If you exit R, it should automatically close all processes also. 
This <em>does not</em> delete the <code>cl</code> object, just the cluster it refers to in the background.

Keep in mind that closing a cluster is equivalent to quitting R in each; anything saved there is lost and packages will need to be re-loaded.

<h4>Continuing the example</h4>
<code>cl &lt;- makeCluster(detectCores())
clusterEvalQ(cl, library(lme4))</code>

<code>## [[1]]
## [1] "lme4"      "Matrix"    "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"     
## 
## [[2]]
## [1] "lme4"      "Matrix"    "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"     
## 
## [[3]]
## [1] "lme4"      "Matrix"    "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"     
## 
## [[4]]
## [1] "lme4"      "Matrix"    "stats"     "graphics"  "grDevices" "utils"    
## [7] "datasets"  "methods"   "base"</code>

<code>system.time(save3 &lt;- parLapply(cl, 1:100, f))</code>

<code>##    user  system elapsed 
##   0.095   0.017   1.145</code>

<code>stopCluster(cl)</code>

Timing this is tricky - if we just time the <code>parLapply</code> call we’re not capturing the time to open and close the cluster, and if we time the whole thing, we’re including the call to lme4. 
To be completely fair, we need to include loading <code>lme4</code> in all three cases. 
I do this outside of this Markdown file to ensure no added complications. 
The three pieces of code were, with a complete restart of R after each:

<code>### lapply
library(parallel)
f &lt;- function(i) {
  lmer(Petal.Width ~ . 
- Species + (1 | Species), data = iris)
}

system.time({
  library(lme4)
  save1 &lt;- lapply(1:100, f)
})

### mclapply
library(parallel)
f &lt;- function(i) {
  lmer(Petal.Width ~ . 
- Species + (1 | Species), data = iris)
}

system.time({
  library(lme4)
  save2 &lt;- mclapply(1:100, f)
})

### mclapply
library(parallel)
f &lt;- function(i) {
  lmer(Petal.Width ~ . 
- Species + (1 | Species), data = iris)
}

system.time({
  cl &lt;- makeCluster(detectCores())
  clusterEvalQ(cl, library(lme4))
  save3 &lt;- parLapply(cl, 1:100, f)
  stopCluster(cl)
})</code>

<table>
<thead>
<tr>
<th align="center">lapply</th>
<th align="center">mclapply</th>
<th align="center">parLapply</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">4.237</td>
<td align="center">4.087</td>
<td align="center">6.954</td>
</tr>
</tbody>
</table>

This shows the additional overhead that can occur with the socket approach - it can definitely be faster, but in this case the overhead which is added slows it down. 
The individual running time of the single <code>parLapply</code> call is faster.

Also known as "embarrassingly parallel" though I don’t like that term.<a href="#fnref1">↩</a>

In this situation, we would actually run <em>slower</em> because of the overhead!<a href="#fnref2">↩</a>

The flexibility of this to work across computers is what allows massive servers made up of many computers to work in parallel.<a href="#fnref3">↩</a>

<h3>Loops and repetitive tasks using lapply</h3>
Let’s build a simple loop that uses sample with replacement to do a bootstrap analysis. 
In this case, we select <code>Sepal.Length</code> and <code>Species</code> from the <code>iris</code> dataset, subset it to 100 observations, and then iterate across 10,000 trials, each time resampling the observations with replacement. 
We then run a logistic regression fitting species as a function of length, and record the coefficients for each trial to be returned.

<code class="r">x &lt;- iris[which(iris[,5] != "setosa"), c(1,5)]
trials &lt;- 10000
res &lt;- data.frame()
system.time({
  trial &lt;- 1
  while(trial &lt;= trials) {
    ind &lt;- sample(100, 100, replace=TRUE)
    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))
    r &lt;- coefficients(result1)
    res &lt;- rbind(res, r)
    trial &lt;- trial + 1
  }
})</code>

<code>##    user  system elapsed 
##  20.031   0.458  21.220</code>

The issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use. 
In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task going to each processor. 
To do that, we need to convert our task to a function, and then use the <code>*apply()</code> family of R functions to apply that function to all of the members of a set. 
In R, using <code>apply</code> is often significantly faster than the equivalent code in a loop. 
Here’s the same code rewritten to use <code>lapply()</code>, which applies a function to each of the members of a list (in this case the trials we want to run):

<code class="r">x &lt;- iris[which(iris[,5] != "setosa"), c(1,5)]
trials &lt;- seq(1, 10000)
boot_fx &lt;- function(trial) {
  ind &lt;- sample(100, 100, replace=TRUE)
  result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))
  r &lt;- coefficients(result1)
  res &lt;- rbind(data.frame(), r)
}
system.time({
  results &lt;- lapply(trials, boot_fx)
})</code>

<code>##    user  system elapsed 
##  19.340   0.553  20.315</code>

<h3>Approaches to parallelization</h3>

When parallelizing jobs, one can:

<ul>
Use the multiple cores on a local computer through <code>mclapply</code></li>

Use multiple processors on local (and remote) machines using <code>makeCluster</code> and <code>clusterApply</code>

<ul>
In this approach, one has to manually copy data and code to each cluster member using <code>clusterExport</code></li>
This is extra work, but sometimes gaining access to a large cluster is worth it</li>
</ul></li>
</ul>

<h3>Parallelize using: mclapply</h3>

The <code>parallel</code> library can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel. 
This is done by using the <code>parallel::mclapply</code> function, which is analogous to <code>lapply</code>, but distributes the tasks to multiple processors. 
<code>mclapply</code> gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item).

<code class="r">library(parallel)
library(MASS)

starts &lt;- rep(100, 40)
fx &lt;- function(nstart) kmeans(Boston, 4, nstart=nstart)
numCores &lt;- detectCores()
numCores</code>

<code>## [1] 8</code>

<code class="r">system.time(
  results &lt;- lapply(starts, fx)
)</code>

<code>##    user  system elapsed 
##   1.346   0.024   1.372</code>

<code class="r">system.time(
  results &lt;- mclapply(starts, fx, mc.cores = numCores)
)</code>

<code>##    user  system elapsed 
##   0.801   0.178   0.367</code>

Now let’s demonstrate with our bootstrap example:

<code class="r">x &lt;- iris[which(iris[,5] != "setosa"), c(1,5)]
trials &lt;- seq(1, 10000)
boot_fx &lt;- function(trial) {
  ind &lt;- sample(100, 100, replace=TRUE)
  result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))
  r &lt;- coefficients(result1)
  res &lt;- rbind(data.frame(), r)
}
system.time({
  results &lt;- mclapply(trials, boot_fx, mc.cores = numCores)
})</code>

<code>##    user  system elapsed 
##  25.672   1.343   5.003</code>

<h3>Parallelize using: foreach and doParallel</h3>

The normal <code>for</code> loop in R looks like:

<code class="r">for (i in 1:3) {
  print(sqrt(i))
}</code>

<code>## [1] 1
## [1] 1.414214
## [1] 1.732051</code>

The <code>foreach</code> method is similar, but uses the sequential <code>%do%</code> operator to indicate an expression to run. 
Note the difference in the returned data structure.

<code class="r">library(foreach)
foreach (i=1:3) %do% {
  sqrt(i)
}</code>

<code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051</code>

In addition, <code>foreach</code> supports a parallelizable operator <code>%dopar%</code> from the <code>doParallel</code> package. 
This allows each iteration through the loop to use different cores or different machines in a cluster. 
Here, we demonstrate with using all the cores on the current machine:

<code class="r">library(foreach)
library(doParallel)</code>

<code>## Loading required package: iterators</code>

<code class="r">registerDoParallel(numCores)  # use multicore, set to the number of our cores
foreach (i=1:3) %dopar% {
  sqrt(i)
}</code>

<code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051</code>

<code class="r"># To simplify output, foreach has the .combine parameter that can simplify return values

# Return a vector
foreach (i=1:3, .combine=c) %dopar% {
  sqrt(i)
}</code>

<code>## [1] 1.000000 1.414214 1.732051</code>

<code class="r"># Return a data frame
foreach (i=1:3, .combine=rbind) %dopar% {
  sqrt(i)
}</code>

<code>##              [,1]
## result.1 1.000000
## result.2 1.414214
## result.3 1.732051</code>

The <a href="https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf">doParallel vignette</a> on CRAN shows a much more realistic example, where one can use `%dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined:

<code class="r"># Let's use the iris data set to do a parallel bootstrap
# From the doParallel vignette, but slightly modified
x &lt;- iris[which(iris[,5] != "setosa"), c(1,5)]
trials &lt;- 10000
system.time({
  r &lt;- foreach(icount(trials), .combine=rbind) %dopar% {
    ind &lt;- sample(100, 100, replace=TRUE)
    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))
    coefficients(result1)
  }
})</code>

<code>##    user  system elapsed 
##  24.117   1.303   4.944</code>

<code class="r"># And compare that to what it takes to do the same analysis in serial
system.time({
  r &lt;- foreach(icount(trials), .combine=rbind) %do% {
    ind &lt;- sample(100, 100, replace=TRUE)
    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))
    coefficients(result1)
  }
})</code>

<code>##    user  system elapsed 
##  19.445   0.571  20.302</code>

<code class="r"># When you're done, clean up the cluster
stopImplicitCluster()</code>

<h2>R run vbs</h2>
write the VBS as follows:

Dim Msg_Text
Msg_Text = WScript.Arguments(0)
MsgBox("Hello " & Msg_Text)

Create a system command in R like this:
system_command <- paste("WScript",
                        '"Msg_Script.vbs"',
                        '"World"',
                        sep = " ")

system(command = system_command, wait = TRUE)


If use named arguments instead:
Dim Msg_Text
Msg_Text = WScript.Arguments.Named.Item("Msg_Text")
MsgBox("Hello " & Msg_Text)

Then create a system command in R like this:
system_command <- paste("WScript",
                        '"Msg_Script.vbs"',
                        '/Msg_Text:"World"',
                        sep = " ")
system(command = system_command, wait = TRUE)

    system(paste("WScript", '"D:/Dropbox/STK/!!! STKMon !!!/playSound.vbs"', sep = " "))


<h2>colorspace: Manipulating and Assessing Colors and Palettes</h2>
https://cran.r-project.org/web/packages/colorspace/vignettes/colorspace.html
The <em>colorspace</em> package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualizations.

At the core of the package there are various utilities for computing with color spaces (as the name conveys). 
Thus, the package helps to map various three-dimensional representations of color to each other. 
A particularly important mapping is the one from the perceptually-based and device-independent color model HCL (Hue-Chroma-Luminance) to standard Red-Green-Blue (sRGB) which is the basis for color specifications in many systems based on the corresponding hex codes (e.g., in HTML but also in R). 
For completeness further standard color models are included as well in the package: <code>polarLUV()</code> (= HCL), <code>LUV()</code>, <code>polarLAB()</code>, <code>LAB()</code>, <code>XYZ()</code>, <code>RGB()</code>, <code>sRGB()</code>, <code>HLS()</code>, <code>HSV()</code>.

The HCL space (= polar coordinates in CIELUV) is particularly useful for specifying individual colors and color palettes as its three axes match those of the human visual system very well: Hue (= type of color, dominant wavelength), chroma (= colorfulness), luminance (= brightness).

The <em>colorspace</em> package provides three types of palettes based on the HCL model:

<em>Qualitative:</em> Designed for coding categorical information, i.e., where no particular ordering of categories is available and every color should receive the same perceptual weight. 
Function: <code>qualitative_hcl()</code>.

<em>Sequential:</em> Designed for coding ordered/numeric information, i.e., where colors go from high to low (or vice versa). 
Function: <code>sequential_hcl()</code>.

<em>Diverging:</em> Designed for coding ordered/numeric information around a central neutral value, i.e., where colors diverge from neutral to two extremes. 
Function: <code>diverging_hcl()</code>.

To aid choice and application of these palettes there are: scales for use with <em>ggplot2</em>; <em>shiny</em> (and <em>tcltk</em>) apps for interactive exploration; visualizations of palette properties; accompanying manipulation utilities (like desaturation, lighten/darken, and emulation of color vision deficiencies).

More detailed overviews and examples are provided in the articles:

<a href="http://colorspace.R-Forge.R-project.org/articles/color_spaces.html">Color Spaces: S4 Classes and Utilities</a>

<a href="http://colorspace.R-Forge.R-project.org/articles/hcl_palettes.html">HCL-Based Color Palettes</a>

<a href="http://colorspace.R-Forge.R-project.org/articles/ggplot2_color_scales.html">HCL-Based Color Scales for <em>ggplot2</em></a>

<a href="http://colorspace.R-Forge.R-project.org/articles/palette_visualization.html">Palette Visualization and Assessment</a>

<a href="http://colorspace.R-Forge.R-project.org/articles/hclwizard.html">Apps for Choosing Colors and Palettes Interactively</a>

<a href="http://colorspace.R-Forge.R-project.org/articles/color_vision_deficiency.html">Color Vision Deficiency Emulation</a>

<a href="http://colorspace.R-Forge.R-project.org/articles/manipulation_utilities.html">Color Manipulation and Utilities</a>

<a href="http://colorspace.R-Forge.R-project.org/articles/approximations.html">Approximating Palettes from Other Packages</a>

<a href="http://colorspace.R-Forge.R-project.org/articles/endrainbow.html">Somewhere over the Rainbow</a>

<h3>Installation</h3>
The stable release version of <em>colorspace</em> is hosted on the Comprehensive R Archive Network (CRAN) at <a href="https://CRAN.R-project.org/package=colorspace" class="uri">https://CRAN.R-project.org/package=colorspace</a> and can be installed via

<code>install.packages("colorspace")</code>

The development version of <em>colorspace</em> is hosted on R-Forge at <a href="https://R-Forge.R-project.org/projects/colorspace/" class="uri">https://R-Forge.R-project.org/projects/colorspace/</a> in a Subversion (SVN) repository. 
It can be installed via

<code>install.packages("colorspace", repos = "http://R-Forge.R-project.org")</code>

For Python users a beta re-implementation of the full <em>colorspace</em> package in Python 2/Python 3 is also available, see <a href="https://github.com/retostauffer/python-colorspace" class="uri">https://github.com/retostauffer/python-colorspace</a>.

<h3>Choosing HCL-based color palettes</h3>
The <em>colorspace</em> package ships with a wide range of predefined color palettes, specified through suitable trajectories in the HCL (hue-chroma-luminance) color space. 
A quick overview can be gained easily with the <code>hcl_palettes()</code> function:

<code>library("colorspace")
hcl_palettes(plot = TRUE)</code>

A suitable vector of colors can be easily computed by specifying the desired number of colors and the palette name (see the plot above), e.g.,

<code>q4 &lt;- qualitative_hcl(4, palette = "Dark 3")
q4</code>

<code>## [1] "#E16A86" "#909800" "#00AD9A" "#9183E6"</code>

The functions <code>sequential_hcl()</code>, and <code>diverging_hcl()</code> work analogously. 
Additionally, their hue/chroma/luminance parameters can be modified, thus allowing for easy customization of each palette. 
Moreover, the <code>choose_palette()</code>/<code>hclwizard()</code> app provide convenient user interfaces to perform palette customization interactively. 
Finally, even more flexible diverging HCL palettes are provided by <code>divergingx_hcl()</code>.

<h3>Usage with base graphics</h3>
The color vectors returned by the HCL palette functions can usually be passed directly to most base graphics function, typically through the <code>col</code> argument. 
Here, the <code>q4</code> vector created above is used in a time series display:

<code>plot(log(EuStockMarkets), plot.type = "single", col = q4, lwd = 2)
legend("topleft", colnames(EuStockMarkets), col = q4, lwd = 3, bty = "n")</code>

As another example for a sequential palette, we demonstrate how to create a spine plot displaying the proportion of Titanic passengers that survived per class. 
The <code>Purples 3</code> palette is used, which is quite similar to the <strong>ColorBrewer.org</strong> palette <code>Purples</code>. 
Here, only two colors are employed, yielding a dark purple and light gray.

<code>ttnc &lt;- margin.table(Titanic, c(1, 4))[, 2:1]
spineplot(ttnc, col = sequential_hcl(2, palette = "Purples 3"))</code>

<h3>Usage with <em>ggplot2</em></h3>
To provide access to the HCL color palettes from within <em>ggplot2</em> graphics suitable discrete and/or continuous <em>gglot2</em> color scales are provided. 
The scales are named via the scheme <code>scale_&lt;aesthetic&gt;_&lt;datatype&gt;_&lt;colorscale&gt;()</code>, where <code>&lt;aesthetic&gt;</code> is the name of the aesthetic (<code>fill</code>, <code>color</code>, <code>colour</code>), <code>&lt;datatype&gt;</code> is the type of the variable plotted (<code>discrete</code> or <code>continuous</code>) and <code>&lt;colorscale&gt;</code> sets the type of the color scale used (<code>qualitative</code>, <code>sequential</code>, <code>diverging</code>, <code>divergingx</code>).

To illustrate their usage two simple examples are shown using the qualitative <code>Dark 3</code> and sequential <code>Purples 3</code> palettes that were also employed above. 
For the first example, semi-transparent shaded densities of the sepal length from the iris data are shown, grouped by species.

<code>library("ggplot2")
ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_density(alpha = 0.6) +
  scale_fill_discrete_qualitative(palette = "Dark 3")</code>

And for the second example the sequential palette is used to code the cut levels in a scatter of price by carat in the diamonds data (or rather a small subsample thereof). 
The scale function first generates six colors but then drops the first color because the light gray is too light here. 
(Alternatively, the chroma and luminance parameters could also be tweaked.)

<code>dsamp &lt;- diamonds[1 + 1:1000 * 50, ]
ggplot(dsamp, aes(carat, price, color = cut)) + geom_point() +
  scale_color_discrete_sequential(palette = "Purples 3", nmax = 6, order = 2:6)</code>

<h3>Palette visualization and assessment</h3>
The <em>colorspace</em> package also provides a number of functions that aid visualization and assessment of its palettes.

<code>demoplot()</code> can display a palette (with arbitrary number of colors) in a range of typical and somewhat simplified statistical graphics.

<code>hclplot()</code> converts the colors of a palette to the corresponding hue/chroma/luminance coordinates and displays them in HCL space with one dimension collapsed. 
The collapsed dimension is the luminance for qualitative palettes and the hue for sequential/diverging palettes.

<code>specplot()</code> also converts the colors to hue/chroma/luminance coordinates but draws the resulting spectrum in a line plot.

For the qualitative <code>Dark 3</code> palette from above the following plots can be obtained.

<code>demoplot(q4, "bar")
hclplot(q4)
specplot(q4, type = "o")</code>

The bar plot is used as a typical application for a qualitative palette (in addition to the time series and density plots used above). 
The other two displays show that luminance is (almost) constant in the palette while the hue changes linearly along the color “wheel”. 
Ideally, chroma would have also been constant to completely balance the colors. 
However, at this luminance the maximum chroma differs across hues so that the palette is fixed up to use less chroma for the yellow and green elements.

Note also that in a bar plot areas are shaded (and not just points or lines) so that lighter colors would be preferable. 
In the density plot above this was achieved through semi-transparency. 
Alternatively, luminance could be increased as is done in the <code>"Pastel 1"</code> or <code>"Set 3"</code> palettes.

Subsequently, the same types of assessment are carried out for the sequential <code>"Purples 3"</code> palette as employed above.

<code>s9 &lt;- sequential_hcl(9, "Purples 3")
demoplot(s9, "heatmap")
hclplot(s9)
specplot(s9, type = "o")</code>

Here, a heatmap (based on the well-known Maunga Whau volcano data) is used as a typical application for a sequential palette. 
The elevation of the volcano is brought out clearly, using dark colors to give emphasis to higher elevations.

The other two displays show that hue is constant in the palette while luminance and chroma vary. 
Luminance increases monotonically from dark to light (as required for a proper sequential palette). 
Chroma is triangular-shaped which allows to better distinguish the middle colors in the palette when compared to a monotonic chroma trajectory.

<h2>figure margins too large</h2>
Every time you are creating plots you might get this error - "Error in plot.new() : figure margins too large".
To avoid such errors you can first check par("mar") output. You should be getting:
[1] 3.1 3.1 3.1 0.6
change to:
par(mar=c(1,1,1,1))

<h2>displayable colors from four planes of Lab space</h2>
ab = expand.grid(a = (-10:15)*15, b = (-15:10)*15)
require(graphics); require(stats) # for na.omit
par(mfrow = c(2, 2), mar = .1+c(3, 3, 3, .5), mgp = c(2,  .8,  0))

Lab = cbind(L = 20, ab)
srgb = convertColor(Lab, from = "Lab", to = "sRGB", clip = NA)
clipped = attr(na.omit(srgb), "na.action")
srgb[clipped, ] = 0
cols = rgb(srgb[, 1], srgb[, 2], srgb[, 3])
image((-10:15)*15, (-15:10)*15, matrix(1:(26*26), ncol = 26), col = cols,
  xlab = "a", ylab = "b", main = "Lab: L=20")

Lab = cbind(L = 40, ab)
srgb = convertColor(Lab, from = "Lab", to = "sRGB", clip = NA)
clipped = attr(na.omit(srgb), "na.action")
srgb[clipped, ] = 0
cols = rgb(srgb[, 1], srgb[, 2], srgb[, 3])
image((-10:15)*15, (-15:10)*15, matrix(1:(26*26), ncol = 26), col = cols,
  xlab = "a", ylab = "b", main = "Lab: L=40")

Lab = cbind(L = 60, ab)
srgb = convertColor(Lab, from = "Lab", to = "sRGB", clip = NA)
clipped = attr(na.omit(srgb), "na.action")
srgb[clipped, ] = 0
cols = rgb(srgb[, 1], srgb[, 2], srgb[, 3])
image((-10:15)*15, (-15:10)*15, matrix(1:(26*26), ncol = 26), col = cols,
  xlab = "a", ylab = "b", main = "Lab: L=60")

Lab = cbind(L = 80, ab)
srgb = convertColor(Lab, from = "Lab", to = "sRGB", clip = NA)
clipped = attr(na.omit(srgb), "na.action")
srgb[clipped, ] = 0
cols = rgb(srgb[, 1], srgb[, 2], srgb[, 3])
image((-10:15)*15, (-15:10)*15, matrix(1:(26*26), ncol = 26), col = cols,
  xlab = "a", ylab = "b", main = "Lab: L=80")

cols = t(col2rgb(palette())); rownames(cols) = palette(); cols
zapsmall(lab = convertColor(cols, from = "sRGB", to = "Lab", scale.in = 255))
stopifnot(all.equal(cols, # converting back.. getting the original:
   round(convertColor(lab, from = "Lab", to = "sRGB", scale.out = 255)),
                    check.attributes = FALSE))

<h2>R语言用数学生成美丽的图案</h2>
https://cran.r-project.org/web/packages/pacman/pacman.pdf

p_load
Load One or More Packages

This function is a wrapper for library and require.
It checks to see if a package is installed, if not it attempts to install the package from CRAN and/or any other repository in the pacman repository list.
Usage
p_load(..., char, install = TRUE, update = getOption("pac_update"),
character.only = FALSE)

一、ggplot2
ggplot2是R语言强大的可视化包，基于图像语法和分层架构以实现各种高质量的图形。

# ggplot2包安装和加载
library(pacman)
p_load(ggplot2)

二、在一个圆上画散点图
圆是一种美

ggplot2对数据有强大的表示能力，对应着各式各样的图形，可以从简单的散点图到复杂的小提琴图。
以geom_开头的函数族定义了要把数据以一种什么几何体来绘制。

我们先从半径为1的圆上绘制50个点开始。
即每个点(x,y)都对应在单位圆上。

2、半径为1的圆上绘制50个点
# 半径为1的圆上50个点
t = seq(0, 2*pi, length.out = 50)x = sin(t)y = cos(t)df = class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqb4dgEPO7Y8Lumic2h3X95bibeYcE8N2ICox6l54pvFWhu4ed2Qtm090Vg/640">

三、螺旋式排列
黄金角的美

植物的叶子呈螺旋状排列。
螺旋线是一条曲线，它从原点开始，随着它绕其旋转而远离该点。
在上面的图中，我们所有的点到原点的距离都是相同的。
将它们螺旋排列的一种简单方法是在x和y乘以一个因子。
我们使用黄金角：

<img class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqbS5e5f9ClQWQlLFzTDebw8NRmSqHibr80vZJNSeNoXLUPy6C8qicQ867w/640">


<img class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqbQ2vfrBLNLHOTTAbBRwEPJb0miaXOASkbyTpyyTlNZuUQnOQiaFTFtuxQ/640">

此数字的灵感来自黄金分割率，这是数学史上最著名的数字之一。
黄金分割率和黄金分割角都出现在自然界中意想不到的地方。
除了花瓣和植物叶子，您还会在种子头，松果，向日葵种子，贝壳，螺旋星系，飓风等中找到它们。

3、基于黄金角螺旋排列散点图
# 基于黄金角螺旋排列散点图
points = 500angle = pi * (3 - sqrt(5)) 
# 环境角计算公式
t = (1:points) * anglex = sin(t)y = cos(t)df = class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqbj5YibEGv2YRZKoKMzOFoD3pTeRDDJm9rxteDFvfODKt4s2zEE4eIPsw/640">

四、图像的修饰
精雕细琢

艺术的东西，总是一种恰到好处，不多不少。
使用ggplot2绘制的图形，除了把数据展示出一种美，也增加了一些其它组件，例如：
灰色的背景
水平和垂直的白线组成的网格线
轴的刻度
每个轴上都有一个标题
文本沿着轴方向做了标记
我们移除这些不必要的组件，同时对点的大小、颜色和透明做修饰和配置。

4、图像的修饰
# 图像的修饰
p = ggplot(df, aes(x*t, y*t))p + geom_point(size=8, alpha=0.5, color="darkgreen") +   theme(    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    title = element_blank(),
    text = element_blank(),
    panel.background = element_rect(fill = "white")  )


<img class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqb7TFTPhUAEoaYjSua8M44rP1iamgvEjse1Z5G7PDSicIp45A9VgibJBMoA/640">

五、蒲公英
迎风而飘

直到现在，所有的点都有相同的外观(大小，颜色，形状和alpha)。
有时，我们希望使点的外观依赖于数据集中的一个变量。
现在我们将设置大小变量。
我们还将改变点的形状。
虽然我们不能吹它，但最终的图像应该会让你想起蒲公英。

5、生成蒲公英
# 生成蒲公英
p = ggplot(df, aes(x*t, y*t))p + geom_point(aes(size = t), alpha = 0.5, shape = 8, color = "black") +  theme(    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    title = element_blank(),
    text = element_blank(),
    panel.background = element_rect(fill = "white"),
    legend.position = "none"  )


<img class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqbuCcicFvfuGicyDkVia3sZCuwUic9lTbHquWa93g8owAIxDB7bsr8dnjMibQ/640">

六、向日葵
向阳而生

植物不仅使用黄金角来布置叶子。
在葵花籽的排列中也满足这个规律。
 我们稍加修改，就可以绘制出向日葵，真奇妙。

6、生成向日葵
# 生成向日葵
p = ggplot(df, aes(x*t, y*t))p + geom_point(aes(size = t), alpha = 0.5, shape = 17, color = "yellow") +  theme(    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    title = element_blank(),
    text = element_blank(),
    panel.background = element_rect(fill = "darkmagenta"),
    legend.position = "none"  )


<img class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqbTxbxHATIG8SfTdibjY5cncgmmjG54fzoeImxBYaewGI2kDyOT2AfNnw/640">

七、角度变化
多姿多彩

通过角度的调整，可以生成多姿多彩的图案，感慨大自然的千变万化和奇妙无穷。
举一例如下。

7、角度变化后新图形
angle = 2.0points = 1000t = (1:points)*anglex = sin(t)y = cos(t)df = class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/pMPbyicMFiactibdfoUT7sXQsARXwtvoxqb8rjAdNDyGDiaciczFGHDpaNJnKbsq2WJH15jQibYcdJtsznBYRlaIo6Qg/640">

八、总结
充分发挥您的想象力

到目前为止，上面所展示的技术可以让我们根据自然的灵感创建无限数量的模式:唯一的限制是个人的想象力。
通过艺术的创造，美丽的欣赏，学习和使用ggplot2包，也是一件有趣的事情。

请发挥您的想象力，从各个方面做修改和创新，生成一幅幅美好的图案，以让人赏心悦目，其乐无穷。

附录：本文完整代码

# ggplot2包安装和加载
library(pacman)p_load(ggplot2)

# 半径为1的圆上50个点
t = seq(0, 2*pi, length.out = 50)x = sin(t)y = cos(t)df = data.frame(t, x, y)p = ggplot(df, aes(x, y))p + geom_point()

# 基于黄金角螺旋排列散点图
points = 500angle = pi * (3 - sqrt(5)) 

# 环境角计算公式
t = (1:points) * anglex = sin(t)y = cos(t)df = data.frame(t, x, y)p = ggplot(df, aes(x*t, y*t))p + geom_point()

# 图像的修饰
p = ggplot(df, aes(x*t, y*t))p + geom_point(size=8, alpha=0.5, color="darkgreen") +   theme(    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    title = element_blank(),
    text = element_blank(),
    panel.background = element_rect(fill = "white")  )

# 生成蒲公英
p = ggplot(df, aes(x*t, y*t))p + geom_point(aes(size = t), alpha = 0.5, shape = 8, color = "black") +  theme(    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    title = element_blank(),
    text = element_blank(),
    panel.background = element_rect(fill = "white"),
    legend.position = "none"  )

# 生成向日葵
p = ggplot(df, aes(x*t, y*t))p + geom_point(aes(size = t), alpha = 0.5, shape = 17, color = "yellow") +  theme(    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    title = element_blank(),
    text = element_blank(),
    panel.background = element_rect(fill = "darkmagenta"),
    legend.position = "none"  )angle = 2.0points = 1000t = (1:points)*anglex = sin(t)y = cos(t)df = data.frame(t, x, y)p = ggplot(df, aes(x*t, y*t))p + geom_point(aes(size = t), alpha = 0.5, shape = 17, color = "yellow") +  theme(    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    title = element_blank(),
    text = element_blank(),
    panel.background = element_rect(fill = "darkmagenta"),
    legend.position = "none"  )

参考资料
1、ggplot2包学习和使用
https://ggplot2.tidyverse.org/reference/
2、黄金角
https://en.wikipedia.org/wiki/Golden_angle

<h2>call python script from R with arguments</h2>
system('python scriptname')

To run the script asynchronously you can set the wait flag to false.

system('python test.py hello world', wait=FALSE)
https://stackoverflow.com/questions/41638558/how-to-call-python-script-from-r-with-arguments

<h2>plot with red tails</h2>
x = rnorm(1000)
hx = hist(x, breaks=10, plot=FALSE)
plot(hx, col=ifelse(abs(hx$breaks) < 1.669, 4, 2))

<h2>rvest: scraping the web using R</h2>
<h3>What can you do using rvest?</h3>
<li>Create an html document from a url, a file on disk or a string containing html with html().</li>
<li>Select parts of an html document using css selectors: html_nodes(). 
Learn more about it using vignette(“selectorgadget”) after installing and loading rvest in R. 
CSS selectors are used to select elements based on properties such as id, class, type, etc.

<li><a href="http://selectorgadget.com/">Selector Gadget website</a></li></li>
<li>Extract components with html_tag() (the name of the tag), html_text() (all text inside the tag), html_attr() (contents of a single attribute) and html_attrs() (all attributes). 
These are done after using html_nodes().

<li>HTML tags normally come in pairs like &lt;tagname&gt;content&lt;/tagname&gt;. 
In the examples we go through below, the content is usually contained between the
 
tags.</li></li>
<li>You can also use rvest with XML files: parse with xml(), then extract components using xml_node(), xml_attr(), xml_attrs(), xml_text() and xml_tag().</li>
<li>Parse tables into data frames with html_table().</li>
<li>Extract, modify and submit forms with html_form(), set_values() and submit_form().</li>
<li>Detect and repair encoding problems with guess_encoding() and repair_encoding(). 
Then pass the correct encoding into html() as an argument.</li>
<li>Navigate around a website as if you’re in a browser with html_session(), jump_to(), follow_link(), back(), forward(), submit_form() and so on. 
(This is still a work in progress).</li>
<li>The package also supports using magrittr for commands.</li>

Also have a look at the three links below for some more information:

<li><a href="https://github.com/hadley/rvest">rvest package on Github</a></li>
<li><a href="http://cran.r-project.org/web/packages/rvest/index.html">rvest documentation on CRAN</a></li>
<li><a href="http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/">rstudio blog on rvest</a></li>

</div>
<div id="starting-off-simple-scraping-the-lego-movie-on-imdb" class="section level2">
<h3>Starting off simple: Scraping The Lego Movie on imdb</h3>
<code class="r"><span class="pink">#install.packages("rvest")</span>

<span class="orange">library</span><span class="blue">(</span><span class="pink">rvest</span><span class="blue">)</span>

<span class="pink"># Store web url</span>
<span class="pink">lego_movie</span> <span class="operator">&lt;-</span> <span class="pink">html</span><span class="blue">(</span><span class="string">"http://www.imdb.com/title/tt1490017/"</span><span class="blue">)</span>

<span class="pink">#Scrape the website for the movie rating</span>
<span class="pink">rating</span> <span class="operator">&lt;-</span> <span class="pink">lego_movie</span> <span class="operator">%&gt;%</span> 
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"strong span"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">as.numeric</span><span class="blue">(</span><span class="blue">)</span>
<span class="pink">rating</span></code>
<code>## [1] 7.8</code>
<code class="r"><span class="pink"># Scrape the website for the cast</span>
<span class="pink">cast</span> <span class="operator">&lt;-</span> <span class="pink">lego_movie</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"#titleCast .itemprop span"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span>
<span class="pink">cast</span></code>
<code>##  [1] "Will Arnett"     "Elizabeth Banks" "Craig Berry"    
##  [4] "Alison Brie"     "David Burrows"   "Anthony Daniels"
##  [7] "Charlie Day"     "Amanda Farinos"  "Keith Ferguson" 
## [10] "Will Ferrell"    "Will Forte"      "Dave Franco"    
## [13] "Morgan Freeman"  "Todd Hansen"     "Jonah Hill"</code>
<code class="r"><span class="pink">#Scrape the website for the url of the movie poster</span>
<span class="pink">poster</span> <span class="operator">&lt;-</span> <span class="pink">lego_movie</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"#img_primary img"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_attr</span><span class="blue">(</span><span class="string">"src"</span><span class="blue">)</span>
<span class="pink">poster</span></code>
<code>## [1] "http://ia.media-imdb.com/images/M/MV5BMTg4MDk1ODExN15BMl5BanBnXkFtZTgwNzIyNjg3MDE@._V1_SX214_AL_.jpg"</code>

<code class="r"><span class="pink"># Extract the first review</span>
<span class="pink">review</span> <span class="operator">&lt;-</span> <span class="pink">lego_movie</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"#titleUserReviewsTeaser p"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span>
<span class="pink">review</span></code>
## [1] "The stand out feature of the Lego Movie for me would be the way the Lego Universe was created. 
The movie paid great attention to detail making everything appear as it would made from Lego, including the water and clouds, and the surfaces people walked on all had the circles sticking upwards a Lego piece would have. 
Combined with all the yellow faces, and Lego part during building, I was convinced action took place in the Lego Universe.A combination of adult and child friendly humour should entertain all, the movie has done well to ensure audiences of all ages are catered to. 
The voice cast were excellent, especially Liam Neeson's split personality police officer, making the 2 personalities sound distinctive, and giving his Bad Cop the usual Liam Neeson tough guy. 
The plot is about resisting an over-controlling ruler, highlighted by the name of the hero's \"resistance piece\". 
It is well thought through, well written, and revealing at the right times. 
Full of surprises, The Lego Movie won't let You see what's coming. 
Best animated film since Wreck it Ralph! Please let there be sequels."

</div>
<div id="scraping-indeed.com-for-jobs" class="section level2">
<h3>Scraping indeed.com for jobs</h3>
<code class="r"><span class="pink"># Submit the form on indeed.com for a job description and location using html_form() and set_values()</span>
<span class="pink">query</span> <span class="operator">=</span> <span class="string">"data science"</span>
<span class="pink">loc</span> <span class="operator">=</span> <span class="string">"New York"</span>
<span class="pink">session</span> <span class="operator">&lt;-</span> <span class="pink">html_session</span><span class="blue">(</span><span class="string">"http://www.indeed.com"</span><span class="blue">)</span>
<span class="pink">form</span> <span class="operator">&lt;-</span> <span class="pink">html_form</span><span class="blue">(</span><span class="pink">session</span><span class="blue">)</span><span class="blue">[</span><span class="blue">[</span><span class="number">1</span><span class="blue">]</span><span class="blue">]</span>
<span class="pink">form</span> <span class="operator">&lt;-</span> <span class="pink">set_values</span><span class="blue">(</span><span class="pink">form</span>, <span class="pink">q</span> <span class="operator">=</span> <span class="pink">query</span>, <span class="pink">l</span> <span class="operator">=</span> <span class="pink">loc</span><span class="blue">)</span>

<span class="pink"># The rvest submit_form function is still under construction and does not work for web sites which build URLs (i.e. 
GET requests. 
It does seem to work for POST requests). 
</span>
<span class="pink">#url &lt;- submit_form(session, indeed)</span>

<span class="pink"># Version 1 of our submit_form function</span>
<span class="pink">submit_form2</span> <span class="operator">&lt;-</span> <span class="orange">function</span><span class="blue">(</span><span class="pink">session</span>, <span class="pink">form</span><span class="blue">)</span><span class="blue">{</span>
  <span class="orange">library</span><span class="blue">(</span><span class="pink">XML</span><span class="blue">)</span>
  <span class="pink">url</span> <span class="operator">&lt;-</span> <span class="pink">XML</span><span class="operator">:</span><span class="operator">:</span><span class="pink">getRelativeURL</span><span class="blue">(</span><span class="pink">form</span><span class="operator">$</span><span class="pink">url</span>, <span class="pink">session</span><span class="operator">$</span><span class="pink">url</span><span class="blue">)</span>
  <span class="pink">url</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="pink">url</span>,<span class="string">'?'</span>,<span class="pink">sep</span><span class="operator">=</span><span class="string">''</span><span class="blue">)</span>
  <span class="pink">values</span> <span class="operator">&lt;-</span> <span class="pink">as.vector</span><span class="blue">(</span><span class="pink">rvest</span><span class="operator">:</span><span class="operator">:</span><span class="operator">:</span><span class="pink">submit_request</span><span class="blue">(</span><span class="pink">form</span><span class="blue">)</span><span class="operator">$</span><span class="pink">values</span><span class="blue">)</span>
  <span class="pink">att</span> <span class="operator">&lt;-</span> <span class="pink">names</span><span class="blue">(</span><span class="pink">values</span><span class="blue">)</span>
  <span class="orange">if</span> <span class="blue">(</span><span class="pink">tail</span><span class="blue">(</span><span class="pink">att</span>, <span class="pink">n</span><span class="operator">=</span><span class="number">1</span><span class="blue">)</span> <span class="operator">==</span> <span class="string">"NULL"</span><span class="blue">)</span><span class="blue">{</span>
    <span class="pink">values</span> <span class="operator">&lt;-</span> <span class="pink">values</span><span class="blue">[</span><span class="number">1</span><span class="operator">:</span><span class="pink">length</span><span class="blue">(</span><span class="pink">values</span><span class="blue">)</span><span class="operator">-</span><span class="number">1</span><span class="blue">]</span>
    <span class="pink">att</span> <span class="operator">&lt;-</span> <span class="pink">att</span><span class="blue">[</span><span class="number">1</span><span class="operator">:</span><span class="pink">length</span><span class="blue">(</span><span class="pink">att</span><span class="blue">)</span><span class="operator">-</span><span class="number">1</span><span class="blue">]</span>
  <span class="blue">}</span>
  <span class="pink">q</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="pink">att</span>,<span class="pink">values</span>,<span class="pink">sep</span><span class="operator">=</span><span class="string">'='</span><span class="blue">)</span>
  <span class="pink">q</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="pink">q</span>, <span class="pink">collapse</span> <span class="operator">=</span> <span class="string">'&amp;'</span><span class="blue">)</span>
  <span class="pink">q</span> <span class="operator">&lt;-</span> <span class="pink">gsub</span><span class="blue">(</span><span class="string">" "</span>, <span class="string">"+"</span>, <span class="pink">q</span><span class="blue">)</span>
  <span class="pink">url</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="pink">url</span>, <span class="pink">q</span>, <span class="pink">sep</span> <span class="operator">=</span> <span class="string">''</span><span class="blue">)</span>
  <span class="pink">html_session</span><span class="blue">(</span><span class="pink">url</span><span class="blue">)</span>
<span class="blue">}</span>

<span class="pink"># Version 2 of our submit_form function</span>
<span class="orange">library</span><span class="blue">(</span><span class="pink">httr</span><span class="blue">)</span>
<span class="pink"># Appends element of a list to another without changing variable type of x</span>
<span class="pink"># build_url function uses the httr package and requires a variable of the url class</span>
<span class="pink">appendList</span> <span class="operator">&lt;-</span> <span class="orange">function</span> <span class="blue">(</span><span class="pink">x</span>, <span class="pink">val</span><span class="blue">)</span>
<span class="blue">{</span>
  <span class="pink">stopifnot</span><span class="blue">(</span><span class="pink">is.list</span><span class="blue">(</span><span class="pink">x</span><span class="blue">)</span>, <span class="pink">is.list</span><span class="blue">(</span><span class="pink">val</span><span class="blue">)</span><span class="blue">)</span>
  <span class="pink">xnames</span> <span class="operator">&lt;-</span> <span class="pink">names</span><span class="blue">(</span><span class="pink">x</span><span class="blue">)</span>
  <span class="orange">for</span> <span class="blue">(</span><span class="pink">v</span> <span class="orange">in</span> <span class="pink">names</span><span class="blue">(</span><span class="pink">val</span><span class="blue">)</span><span class="blue">)</span> <span class="blue">{</span>
    <span class="pink">x</span><span class="blue">[</span><span class="blue">[</span><span class="pink">v</span><span class="blue">]</span><span class="blue">]</span> <span class="operator">&lt;-</span> <span class="orange">if</span> <span class="blue">(</span><span class="pink">v</span> <span class="operator">%in%</span> <span class="pink">xnames</span> <span class="operator">&amp;&amp;</span> <span class="pink">is.list</span><span class="blue">(</span><span class="pink">x</span><span class="blue">[</span><span class="blue">[</span><span class="pink">v</span><span class="blue">]</span><span class="blue">]</span><span class="blue">)</span> <span class="operator">&amp;&amp;</span> <span class="pink">is.list</span><span class="blue">(</span><span class="pink">val</span><span class="blue">[</span><span class="blue">[</span><span class="pink">v</span><span class="blue">]</span><span class="blue">]</span><span class="blue">)</span><span class="blue">)</span>
      <span class="pink">appendList</span><span class="blue">(</span><span class="pink">x</span><span class="blue">[</span><span class="blue">[</span><span class="pink">v</span><span class="blue">]</span><span class="blue">]</span>, <span class="pink">val</span><span class="blue">[</span><span class="blue">[</span><span class="pink">v</span><span class="blue">]</span><span class="blue">]</span><span class="blue">)</span>
    <span class="orange">else</span> <span class="pink">c</span><span class="blue">(</span><span class="pink">x</span><span class="blue">[</span><span class="blue">[</span><span class="pink">v</span><span class="blue">]</span><span class="blue">]</span>, <span class="pink">val</span><span class="blue">[</span><span class="blue">[</span><span class="pink">v</span><span class="blue">]</span><span class="blue">]</span><span class="blue">)</span>
  <span class="blue">}</span>
  <span class="pink">x</span>
<span class="blue">}</span>
 
<span class="pink"># Simulating submit_form for GET requests</span>
<span class="pink">submit_geturl</span> <span class="operator">&lt;-</span> <span class="orange">function</span> <span class="blue">(</span><span class="pink">session</span>, <span class="pink">form</span><span class="blue">)</span>
<span class="blue">{</span>
  <span class="pink">query</span> <span class="operator">&lt;-</span> <span class="pink">rvest</span><span class="operator">:</span><span class="operator">:</span><span class="operator">:</span><span class="pink">submit_request</span><span class="blue">(</span><span class="pink">form</span><span class="blue">)</span>
  <span class="pink">query</span><span class="operator">$</span><span class="pink">method</span> <span class="operator">&lt;-</span> <span class="literal">NULL</span>
  <span class="pink">query</span><span class="operator">$</span><span class="pink">encode</span> <span class="operator">&lt;-</span> <span class="literal">NULL</span>
  <span class="pink">query</span><span class="operator">$</span><span class="pink">url</span> <span class="operator">&lt;-</span> <span class="literal">NULL</span>
  <span class="pink">names</span><span class="blue">(</span><span class="pink">query</span><span class="blue">)</span> <span class="operator">&lt;-</span> <span class="string">"query"</span>
 
  <span class="pink">relativeurl</span> <span class="operator">&lt;-</span> <span class="pink">XML</span><span class="operator">:</span><span class="operator">:</span><span class="pink">getRelativeURL</span><span class="blue">(</span><span class="pink">form</span><span class="operator">$</span><span class="pink">url</span>, <span class="pink">session</span><span class="operator">$</span><span class="pink">url</span><span class="blue">)</span>
  <span class="pink">basepath</span> <span class="operator">&lt;-</span> <span class="pink">parse_url</span><span class="blue">(</span><span class="pink">relativeurl</span><span class="blue">)</span>
 
  <span class="pink">fullpath</span> <span class="operator">&lt;-</span> <span class="pink">appendList</span><span class="blue">(</span><span class="pink">basepath</span>,<span class="pink">query</span><span class="blue">)</span>
  <span class="pink">fullpath</span> <span class="operator">&lt;-</span> <span class="pink">build_url</span><span class="blue">(</span><span class="pink">fullpath</span><span class="blue">)</span>
  <span class="pink">fullpath</span>
<span class="blue">}</span>

<span class="pink"># Submit form and get new url</span>
<span class="pink">session1</span> <span class="operator">&lt;-</span> <span class="pink">submit_form2</span><span class="blue">(</span><span class="pink">session</span>, <span class="pink">form</span><span class="blue">)</span>

<span class="pink"># Get reviews of last company using follow_link()</span>
<span class="pink">session2</span> <span class="operator">&lt;-</span> <span class="pink">follow_link</span><span class="blue">(</span><span class="pink">session1</span>, <span class="pink">css</span> <span class="operator">=</span> <span class="string">"#more_9 li:nth-child(3) a"</span><span class="blue">)</span>
<span class="pink">reviews</span> <span class="operator">&lt;-</span> <span class="pink">session2</span> <span class="operator">%&gt;%</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">".description"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span>
<span class="pink">reviews</span></code>
<code>## [1] "Custody Client Services"                                       
## [2] "An exciting position on a trading floor"                       
## [3] "Great work environment"                                        
## [4] "A company that helps its employees to advance career."         
## [5] "Decent Company to work for while you still have the job there."</code>
<code class="r"><span class="pink"># Get average salary for each job listing based on title and location</span>
<span class="pink">salary_links</span> <span class="operator">&lt;-</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="pink">session1</span>, <span class="pink">css</span> <span class="operator">=</span> <span class="string">"#resultsCol li:nth-child(2) a"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_attr</span><span class="blue">(</span><span class="string">"href"</span><span class="blue">)</span>
<span class="pink">salary_links</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="pink">session</span><span class="operator">$</span><span class="pink">url</span>, <span class="pink">salary_links</span>, <span class="pink">sep</span><span class="operator">=</span><span class="string">''</span><span class="blue">)</span>
<span class="pink">salaries</span> <span class="operator">&lt;-</span> <span class="pink">lapply</span><span class="blue">(</span><span class="pink">salary_links</span>, . 
<span class="operator">%&gt;%</span> <span class="pink">html</span><span class="blue">(</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"#salary_display_table .salary"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span><span class="blue">)</span>
<span class="pink">salary</span> <span class="operator">&lt;-</span> <span class="pink">unlist</span><span class="blue">(</span><span class="pink">salaries</span><span class="blue">)</span>

<span class="pink"># Store web url</span>
<span class="pink">data_sci_indeed</span> <span class="operator">&lt;-</span> <span class="pink">session1</span>

<span class="pink"># Get job titles</span>
<span class="pink">job_title</span> <span class="operator">&lt;-</span> <span class="pink">data_sci_indeed</span> <span class="operator">%&gt;%</span> 
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"[itemprop=title]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span>

<span class="pink"># Get companies</span>
<span class="pink">company</span> <span class="operator">&lt;-</span> <span class="pink">data_sci_indeed</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"[itemprop=hiringOrganization]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span>

<span class="pink"># Get locations</span>
<span class="pink">location</span> <span class="operator">&lt;-</span> <span class="pink">data_sci_indeed</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"[itemprop=addressLocality]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span>

<span class="pink"># Get descriptions</span>
<span class="pink">description</span> <span class="operator">&lt;-</span> <span class="pink">data_sci_indeed</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"[itemprop=description]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_text</span><span class="blue">(</span><span class="blue">)</span>

<span class="pink"># Get the links</span>
<span class="pink">link</span> <span class="operator">&lt;-</span> <span class="pink">data_sci_indeed</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">"[itemprop=title]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span>
  <span class="pink">html_attr</span><span class="blue">(</span><span class="string">"href"</span><span class="blue">)</span>
<span class="pink">link</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="string">'[Link](https://www.indeed.com'</span>, <span class="pink">link</span>, <span class="pink">sep</span><span class="operator">=</span><span class="string">''</span><span class="blue">)</span>
<span class="pink">link</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="pink">link</span>, <span class="string">')'</span>, <span class="pink">sep</span><span class="operator">=</span><span class="string">''</span><span class="blue">)</span>

<span class="pink">indeed_jobs</span> <span class="operator">&lt;-</span> <span class="pink">data.frame</span><span class="blue">(</span><span class="pink">job_title</span>,<span class="pink">company</span>,<span class="pink">location</span>,<span class="pink">description</span>,<span class="pink">salary</span>,<span class="pink">link</span><span class="blue">)</span>

<span class="orange">library</span><span class="blue">(</span><span class="pink">knitr</span><span class="blue">)</span>
<span class="pink">kable</span><span class="blue">(</span><span class="pink">indeed_jobs</span>, <span class="pink">format</span> <span class="operator">=</span> <span class="string">"html"</span><span class="blue">)</span></code>
<table>
 <thead>
  <tr>
   <th style="text-align:left;"> 
job_title
</th>
   <th style="text-align:left;"> 
company
</th>
   <th style="text-align:left;"> 
location
</th>
   <th style="text-align:left;"> 
description
</th>
   <th style="text-align:left;"> 
salary
</th>
   <th style="text-align:left;"> 
link
</th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> 
Data Scientist
</td>
   <td style="text-align:left;"> 
Career Path Group
</td>
   <td style="text-align:left;"> 
New York, NY 10018 (Clinton area)
</td>
   <td style="text-align:left;"> 
Or higher in Computer Science or related field. 
Design, develop, and optimize our data and analytics system….
</td>
   <td style="text-align:left;"> 
$109,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=a2a76f399d9da571">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Data Scientist or Statistician
</td>
   <td style="text-align:left;"> 
Humana
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
Experience with unstructured data analysis. 
Humana is seeking an experienced statistician with demonstrated health and wellness data analysis expertise to join…
</td>
   <td style="text-align:left;"> 
$60,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=25cf421c9eeba56d">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Analyst
</td>
   <td style="text-align:left;"> 
1010data
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
Data providers can also use 1010data to share and monetize their data. 
1010data is the leading provider of Big Data Discovery and data sharing solutions….
</td>
   <td style="text-align:left;"> 
$81,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=2cea6a727056a108">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Data Scientist &amp; Visualization Engineer
</td>
   <td style="text-align:left;"> 
Enstoa
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
2+ years professional experience analyzing complex data sets, modeling, machine learning, and/or large-scale data mining….
</td>
   <td style="text-align:left;"> 
$210,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=a3f522a4a2d9c317">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Data Scientist - Intelligent Solutions
</td>
   <td style="text-align:left;"> 
JPMorgan Chase
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
Experience managing and growing a data science team. 
Data Scientist - Intelligent Solutions. 
Analyze communications data and Utilize statistical natural…
</td>
   <td style="text-align:left;"> 
$109,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=79b3ac9ca7865c94">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Analytics Program Lead
</td>
   <td style="text-align:left;"> 
AIG
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
Lead the analytical team for Data Solutions. 
Graduate degree from a renowned institution in any advanced quantitative modeling oriented discipline including but…
</td>
   <td style="text-align:left;"> 
$126,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=8baf5b98905c68f6">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Data Engineer
</td>
   <td style="text-align:left;"> 
Standard Analytics
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
Code experience in a production environment (familiar with data structures, parallelism, and concurrency). 
We aim to organize the world’s scientific information…
</td>
   <td style="text-align:left;"> 
$122,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=af931bd8281d19e7">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Summer Intern - Network Science and Big Data Analytics
</td>
   <td style="text-align:left;"> 
IBM
</td>
   <td style="text-align:left;"> 
Yorktown Heights, NY
</td>
   <td style="text-align:left;"> 
The Network Science and Big Data Analytics department at the IBM T. 
Our lab has access to large computing resources and data….
</td>
   <td style="text-align:left;"> 
$36,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=d20c6a677eda7671">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Data Scientist
</td>
   <td style="text-align:left;"> 
The Nielsen Company
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
As a Data Scientist in the Data Integration group, you will be involved in the process of integrating data to enable analyses of patterns and relationships…
</td>
   <td style="text-align:left;"> 
$109,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=8bfb40468f28c10c">Link</a>
</td>
  </tr>
  <tr>
   <td style="text-align:left;"> 
Data Analyst, IM Data Science
</td>
   <td style="text-align:left;"> 
BNY Mellon
</td>
   <td style="text-align:left;"> 
New York, NY
</td>
   <td style="text-align:left;"> 
The Data Analyst will support a wide variety of projects and initiatives of the Data Science Group, including the creation of back-end data management tools,…
</td>
   <td style="text-align:left;"> 
$84,000
</td>
   <td style="text-align:left;"> 
<a href="https://www.indeed.com/rc/clk?jk=779dc28f2010c4c6">Link</a>
</td>
  </tr>
</tbody>
</table>

Some more on CSS and HTML:

<li><a href="http://code.tutsplus.com/tutorials/the-30-css-selectors-you-must-memorize--net-16048">Useful CSS Rules</a></li>
<li><a href="http://www.w3.org/TR/microdata/#names:-the-itemprop-attribute">HTML5 microdata itemprop property</a></li>

</div>
<div id="more-examples-with-linkedin" class="section level2">
<h3>More examples with LinkedIn</h3>
<code class="r"><span class="pink"># Attempt to crawl LinkedIn, requires useragent to access Linkedin Sites</span>
<span class="pink">uastring</span> <span class="operator">&lt;-</span> <span class="string">"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36"</span>
<span class="pink">session</span> <span class="operator">&lt;-</span> <span class="pink">html_session</span><span class="blue">(</span><span class="string">"https://www.linkedin.com/job/"</span>, <span class="pink">user_agent</span><span class="blue">(</span><span class="pink">uastring</span><span class="blue">)</span><span class="blue">)</span>
<span class="pink">form</span> <span class="operator">&lt;-</span> <span class="pink">html_form</span><span class="blue">(</span><span class="pink">session</span><span class="blue">)</span><span class="blue">[</span><span class="blue">[</span><span class="number">1</span><span class="blue">]</span><span class="blue">]</span>
<span class="pink">form</span> <span class="operator">&lt;-</span> <span class="pink">set_values</span><span class="blue">(</span><span class="pink">form</span>, <span class="pink">keywords</span> <span class="operator">=</span> <span class="string">"Data Science"</span>, <span class="pink">location</span><span class="operator">=</span><span class="string">"New York"</span><span class="blue">)</span>
 
<span class="pink">new_url</span> <span class="operator">&lt;-</span> <span class="pink">submit_geturl</span><span class="blue">(</span><span class="pink">session</span>,<span class="pink">form</span><span class="blue">)</span>
<span class="pink">new_session</span> <span class="operator">&lt;-</span> <span class="pink">html_session</span><span class="blue">(</span><span class="pink">new_url</span>, <span class="pink">user_agent</span><span class="blue">(</span><span class="pink">uastring</span><span class="blue">)</span><span class="blue">)</span>
<span class="pink">jobtitle</span> <span class="operator">&lt;-</span> <span class="pink">new_session</span> <span class="operator">%&gt;%</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">".job [itemprop=title]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_text</span>
<span class="pink">company</span> <span class="operator">&lt;-</span> <span class="pink">new_session</span> <span class="operator">%&gt;%</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">".job [itemprop=name]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_text</span>
<span class="pink">location</span> <span class="operator">&lt;-</span> <span class="pink">new_session</span> <span class="operator">%&gt;%</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">".job [itemprop=addressLocality]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_text</span>
<span class="pink">description</span> <span class="operator">&lt;-</span> <span class="pink">new_session</span> <span class="operator">%&gt;%</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">".job [itemprop=description]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_text</span>
<span class="pink">url</span> <span class="operator">&lt;-</span> <span class="pink">new_session</span> <span class="operator">%&gt;%</span> <span class="pink">html_nodes</span><span class="blue">(</span><span class="string">".job [itemprop=title]"</span><span class="blue">)</span> <span class="operator">%&gt;%</span> <span class="pink">html_attr</span><span class="blue">(</span><span class="string">"href"</span><span class="blue">)</span>
<span class="pink">url</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="pink">url</span>, <span class="string">')'</span>, <span class="pink">sep</span><span class="operator">=</span><span class="string">''</span><span class="blue">)</span>
<span class="pink">url</span> <span class="operator">&lt;-</span> <span class="pink">paste</span><span class="blue">(</span><span class="string">'[Link]('</span>, <span class="pink">url</span>, <span class="pink">sep</span><span class="operator">=</span><span class="string">''</span><span class="blue">)</span>
<span class="pink">df</span> <span class="operator">&lt;-</span> <span class="pink">data.frame</span><span class="blue">(</span><span class="pink">jobtitle</span>, <span class="pink">company</span>, <span class="pink">location</span>, <span class="pink">url</span><span class="blue">)</span>

<span class="pink">df</span> <span class="operator">%&gt;%</span> <span class="pink">kable</span></code>
<table class="table table-condensed">
<thead>
<tr class="header">
<th align="left">jobtitle</th>
<th align="left">company</th>
<th align="left">location</th>
<th align="left">url</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Data Science Lead: Metis</td>
<td align="left">Kaplan</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/51429397?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Data Science Lead: Metis</td>
<td align="left">Kaplan Test Prep</td>
<td align="left">New York, NY</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/38695388?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Think Big Senior Data Scientist</td>
<td align="left">Think Big, A Teradata Company</td>
<td align="left">US-NY-New York</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/32056808?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Think Big Principal Data Scientist</td>
<td align="left">Think Big, A Teradata Company</td>
<td align="left">US-NY-New York</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/32057641?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Data Scientist - Professional Services Consultant (East …</td>
<td align="left">MapR Technologies</td>
<td align="left">Greater New York City Area</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/35995187?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Think Big Senior Data Scientist</td>
<td align="left">Teradata</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/51068145?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Think Big Principal Data Scientist</td>
<td align="left">Teradata</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/51068162?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Sr. 
Software Engineer - Data Science - HookLogic</td>
<td align="left">HookLogic, Inc.</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/49975389?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Think Big Data Scientist</td>
<td align="left">Think Big, A Teradata Company</td>
<td align="left">US-NY-New York</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/32057645?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Director of Data Science Programs</td>
<td align="left">DataKind</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/35605329?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Lead Data Scientist - VP - Intelligent Solutions</td>
<td align="left">JPMorgan Chase &amp; Co.</td>
<td align="left">US-NY-New York</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/40828814?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Senior Data Scientist for US Quantitative Fund, NYC</td>
<td align="left">GQR Global Markets</td>
<td align="left">Greater New York City Area</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/38602287?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Google Cloud Solutions Practice, Google Data Solution …</td>
<td align="left">PricewaterhouseCoopers</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/53729956?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Senior Data Scientist</td>
<td align="left">Dun and Bradstreet</td>
<td align="left">Short Hills, NJ, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/53796892?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Senior data scientist</td>
<td align="left">Mezzobit</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/38019145?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Think Big Data Scientist</td>
<td align="left">Teradata</td>
<td align="left">New York City, NY, US</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/51066835?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Data Scientist - Intelligent Solutions</td>
<td align="left">JPMorgan Chase &amp; Co.</td>
<td align="left">US-NY-New York</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/40855034?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Technical Trainer EMEA</td>
<td align="left">Datameer</td>
<td align="left">New York</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/41600114?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Elementary School Science Teacher</td>
<td align="left">Success Academy Charter Schools</td>
<td align="left">Greater New York City Area</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/38634099?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Middle School Science Teacher</td>
<td align="left">Success Academy Charter Schools</td>
<td align="left">Greater New York City Area</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/38633208?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Data Scientist (various levels)</td>
<td align="left">Burtch Works</td>
<td align="left">Greater New York City Area</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/41672840?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Sr. 
Data Scientist – Big Data, Online Advertising, Search</td>
<td align="left">Magnetic</td>
<td align="left">New York, NY</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/33977941?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Sr. 
Big Data Engineer FlexGraph</td>
<td align="left">ADP</td>
<td align="left">New York, NY</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/18708583?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="even">
<td align="left">Data Science Lead Instructor - Data Science, Teaching</td>
<td align="left">CyberCoders</td>
<td align="left">New York City, NY</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/40896475?trk=jserp_job_details_text">Link</a></td>
</tr>
<tr class="odd">
<td align="left">Director, Data Consulting</td>
<td align="left">Havas Media</td>
<td align="left">Greater New York City Area</td>
<td align="left"><a href="https://www.linkedin.com/jobs2/view/41671585?trk=jserp_job_details_text">Link</a></td>
</tr>
</tbody>
</table>
</div>
<div id="attemping-to-scrape-columbia-lionshare" class="section level2">
<h3>Attemping to scrape Columbia LionShare</h3>
<code class="r"><span class="pink"># Attempt to crawl Columbia Lionshare for jobs</span>
<span class="pink">session</span> <span class="operator">&lt;-</span> <span class="pink">html_session</span><span class="blue">(</span><span class="string">"http://www.careereducation.columbia.edu/lionshare"</span><span class="blue">)</span>
<span class="pink">form</span> <span class="operator">&lt;-</span> <span class="pink">html_form</span><span class="blue">(</span><span class="pink">session</span><span class="blue">)</span><span class="blue">[</span><span class="blue">[</span><span class="number">1</span><span class="blue">]</span><span class="blue">]</span>
<span class="pink">form</span> <span class="operator">&lt;-</span> <span class="pink">set_values</span><span class="blue">(</span><span class="pink">form</span>, <span class="pink">username</span> <span class="operator">=</span> <span class="string">"uni"</span><span class="blue">)</span>
<span class="pink">#Below code commented out in Markdown</span>

<span class="pink">#pw &lt;- .rs.askForPassword("Password?")</span>
<span class="pink">#form &lt;- set_values(form, password = pw)</span>
<span class="pink">#rm(pw)</span>
<span class="pink">#session2 &lt;- submit_form(session, form)</span>
<span class="pink">#session2 &lt;- follow_link(session2, "Job")</span>
<span class="pink">#form2 &lt;- html_form(session2)[[1]]</span>
<span class="pink">#form2 &lt;- set_values(form2, PositionTypes = 7, Keyword = "Data")</span>
<span class="pink">#session3 &lt;- submit_form(session2, form2)</span>

<span class="pink"># Unable to scrape because the table containing the job data uses javascript and doesn't load soon enough for rvest to collect information</span></code>
There isn't any equivalent to checking if the document finishes loading before scraping the data. 
The general recommendation appears to be using something entirely different such as Selenium to scrape web data.

<li><a href="http://www.seleniumhq.org/">Selenium, automating web browsers</a></li>

If you are webscraping with Python chances are that you have already tried urllib, httplib, requests, etc. 
These are excellent libraries, but some websites don’t like to be webscraped. 
In these cases you may need to disguise your webscraping bot as a human being. 
Selenium is just the tool for that. 
Selenium is a webdriver: it takes control of your browser, which then does all the work. 
Hence what the website “sees” is Chrome or Firefox or IE; it does not see Python or Selenium. 
That makes it a lot harder for the website to tell your bot from a human being.

<li><a href="http://thiagomarzagao.com/2013/11/12/webscraping-with-selenium-part-1/">Selenium tutorial</a></li>


<h2>4 Types of Classification Tasks in Machine Learning</h2>
https://machinelearningmastery.com/types-of-classification-in-machine-learning/
<h3>Tutorial Overview</h3>
This tutorial is divided into five parts; they are:

<ol><li>Classification Predictive Modeling</li><li>Binary Classification</li><li>Multi-Class Classification</li><li>Multi-Label Classification</li><li>Imbalanced Classification</li>
</ol>
<h3>Classification Predictive Modeling</h3>
There are many different types of classification algorithms for modeling classification predictive modeling problems.

There is no good theory on how to map algorithms onto problem types; instead, it is generally recommended that a practitioner use controlled experiments and discover which algorithm and algorithm configuration results in the best performance for a given classification task.

Classification predictive modeling algorithms are evaluated based on their results. Classification accuracy is a popular metric used to evaluate the performance of a model based on the predicted class labels. <a href="https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/">Classification accuracy is not perfect</a> but is a good starting point for many classification tasks.

Instead of class labels, some tasks may require the prediction of a <a href="https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/">probability of class membership</a> for each example. This provides additional uncertainty in the prediction that an application or user can then interpret. A popular diagnostic for evaluating predicted probabilities is the <a href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/">ROC Curve</a>.

There are perhaps four main types of classification tasks that you may encounter; they are:

<ul><li>Binary Classification</li><li>Multi-Class Classification</li><li>Multi-Label Classification</li><li>Imbalanced Classification</li>
</ul>
Let's take a closer look at each in turn.

<h3>Binary Classification</h3>
The class for the normal state is assigned the class label 0 and the class with the abnormal state is assigned the class label 1.

It is common to model a binary classification task with a model that predicts a <a href="https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/">Bernoulli probability distribution</a> for each example.

The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome as either a 0 or 1. For classification, this means that the model predicts a probability of an example belonging to class 1, or the abnormal state.

Popular algorithms that can be used for binary classification include:

<ul><li>Logistic Regression</li><li>k-Nearest Neighbors</li><li>Decision Trees</li><li>Support Vector Machine</li><li>Naive Bayes</li>
</ul>
Some algorithms are specifically designed for binary classification and do not natively support more than two classes; examples include Logistic Regression and Support Vector Machines.

Next, let's take a closer look at a dataset to develop an intuition for binary classification problems.

We can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">make_blobs() function</a> to generate a synthetic binary classification dataset.

The example below generates a dataset with 1,000 examples that belong to one of two classes, each with two input features.

# example of binary classification task
from numpy import where
from collections import Counter
from sklearn.datasets import make_blobs
from matplotlib import pyplot
# define dataset
X, y = make_blobs(n_samples=1000, centers=2, random_state=1)
# summarize dataset shape
print(X.shape, y.shape)
# summarize observations by class label
counter = Counter(y)
print(counter)
# summarize first few examples
for i in range(10):
	print(X[i], y[i])
# plot the dataset and color the by class label
for label, _ in counter.items():
	row_ix = where(y == label)[0]
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))
pyplot.legend()
pyplot.show()

Running the example first summarizes the created dataset showing the 1,000 examples divided into input (<em>X</em>) and output (<em>y</em>) elements.

The distribution of the class labels is then summarized, showing that instances belong to either class 0 or class 1 and that there are 500 examples in each class.

Next, the first 10 examples in the dataset are summarized, showing the input values are numeric and the target values are integers that represent the class membership.

(1000, 2) (1000,)

Counter({0: 500, 1: 500})

[-3.05837272  4.48825769] 0
[-8.60973869 -3.72714879] 1
[1.37129721 5.23107449] 0
[-9.33917563 -2.9544469 ] 1
[-11.57178593  -3.85275513] 1
[-11.42257341  -4.85679127] 1
[-10.44518578  -3.76476563] 1
[-10.44603561  -3.26065964] 1
[-0.61947075  3.48804983] 0
[-10.91115591  -4.5772537 ] 1

Finally, a scatter plot is created for the input variables in the dataset and the points are colored based on their class value.

We can see two distinct clusters that we might expect would be easy to discriminate.


<img class="lazy" data-src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2020/01/Scatter-Plot-of-Binary-Classification-Dataset.png">
Scatter Plot of Binary Classification Dataset

<h3>Multi-Class Classification</h3>
<a href="https://en.wikipedia.org/wiki/Multiclass_classification">Multi-class classification</a> refers to those classification tasks that have more than two class labels.

Examples include:

<ul><li>Face classification.</li><li>Plant species classification.</li><li>Optical character recognition.</li>
</ul>
Unlike binary classification, multi-class classification does not have the notion of normal and abnormal outcomes. Instead, examples are classified as belonging to one among a range of known classes.

The number of class labels may be very large on some problems. For example, a model may predict a photo as belonging to one among thousands or tens of thousands of faces in a face recognition system.

Problems that involve predicting a sequence of words, such as text translation models, may also be considered a special type of multi-class classification. Each word in the sequence of words to be predicted involves a multi-class classification where the size of the vocabulary defines the number of possible classes that may be predicted and could be tens or hundreds of thousands of words in size.

It is common to model a multi-class classification task with a model that predicts a <a href="https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/">Multinoulli probability distribution</a> for each example.

The Multinoulli distribution is a discrete probability distribution that covers a case where an event will have a categorical outcome, e.g. <em>K</em> in {1, 2, 3, &#8230;, <em>K</em>}. For classification, this means that the model predicts the probability of an example belonging to each class label.

Many algorithms used for binary classification can be used for multi-class classification.

Popular algorithms that can be used for multi-class classification include:

<ul><li>k-Nearest Neighbors.</li><li>Decision Trees.</li><li>Naive Bayes.</li><li>Random Forest.</li><li>Gradient Boosting.</li>
</ul>
Algorithms that are designed for binary classification can be adapted for use for multi-class problems.

This involves using a strategy of fitting multiple binary classification models for each class vs. all other classes (called one-vs-rest) or one model for each pair of classes (called one-vs-one).

<ul><li><strong>One-vs-Rest</strong>: Fit one binary classification model for each class vs. all other classes.</li><li><strong>One-vs-One</strong>: Fit one binary classification model for each pair of classes.</li>
</ul>
Binary classification algorithms that can use these strategies for multi-class classification include:

<ul><li>Logistic Regression.</li><li>Support Vector Machine.</li>
</ul>
Next, let's take a closer look at a dataset to develop an intuition for multi-class classification problems.

We can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">make_blobs() function</a> to generate a synthetic multi-class classification dataset.

The example below generates a dataset with 1,000 examples that belong to one of three classes, each with two input features.

# example of multi-class classification task
from numpy import where
from collections import Counter
from sklearn.datasets import make_blobs
from matplotlib import pyplot
# define dataset
X, y = make_blobs(n_samples=1000, centers=3, random_state=1)
# summarize dataset shape
print(X.shape, y.shape)
# summarize observations by class label
counter = Counter(y)
print(counter)
# summarize first few examples
for i in range(10):
	print(X[i], y[i])
# plot the dataset and color the by class label
for label, _ in counter.items():
	row_ix = where(y == label)[0]
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))
pyplot.legend()
pyplot.show()

Running the example first summarizes the created dataset showing the 1,000 examples divided into input (<em>X</em>) and output (<em>y</em>) elements.

The distribution of the class labels is then summarized, showing that instances belong to class 0, class 1, or class 2 and that there are approximately 333 examples in each class.

Next, the first 10 examples in the dataset are summarized showing the input values are numeric and the target values are integers that represent the class membership.

(1000, 2) (1000,)

Counter({0: 334, 1: 333, 2: 333})

[-3.05837272  4.48825769] 0
[-8.60973869 -3.72714879] 1
[1.37129721 5.23107449] 0
[-9.33917563 -2.9544469 ] 1
[-8.63895561 -8.05263469] 2
[-8.48974309 -9.05667083] 2
[-7.51235546 -7.96464519] 2
[-7.51320529 -7.46053919] 2
[-0.61947075  3.48804983] 0
[-10.91115591  -4.5772537 ] 1

Finally, a scatter plot is created for the input variables in the dataset and the points are colored based on their class value.

We can see three distinct clusters that we might expect would be easy to discriminate.


<img class="lazy" data-src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2020/01/Scatter-Plot-of-Multi-Class-Classification-Dataset.png">
Scatter Plot of Multi-Class Classification Dataset

<h3>Multi-Label Classification</h3>
<a href="https://en.wikipedia.org/wiki/Multi-label_classification">Multi-label classification</a> refers to those classification tasks that have two or more class labels, where one or more class labels may be predicted for each example.

Consider the example of <a href="https://machinelearningmastery.com/object-recognition-with-deep-learning/">photo classification</a>, where a given photo may have multiple objects in the scene and a model may predict the presence of multiple known objects in the photo, such as &#8220;<em>bicycle</em>,&#8221; &#8220;<em>apple</em>,&#8221; &#8220;<em>person</em>,&#8221; etc.

This is unlike binary classification and multi-class classification, where a single class label is predicted for each example.

It is common to model multi-label classification tasks with a model that predicts multiple outputs, with each output taking predicted as a Bernoulli probability distribution. This is essentially a model that makes multiple binary classification predictions for each example.

Classification algorithms used for binary or multi-class classification cannot be used directly for multi-label classification. Specialized versions of standard classification algorithms can be used, so-called multi-label versions of the algorithms, including:

<ul><li>Multi-label Decision Trees</li><li>Multi-label Random Forests</li><li>Multi-label Gradient Boosting</li>
</ul>
Another approach is to use a separate classification algorithm to predict the labels for each class.

Next, let's take a closer look at a dataset to develop an intuition for multi-label classification problems.

We can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html">make_multilabel_classification() function</a> to generate a synthetic multi-label classification dataset.

The example below generates a dataset with 1,000 examples, each with two input features. There are three classes, each of which may take on one of two labels (0 or 1).

# example of a multi-label classification task
from sklearn.datasets import make_multilabel_classification
# define dataset
X, y = make_multilabel_classification(n_samples=1000, n_features=2, n_classes=3, n_labels=2, random_state=1)
# summarize dataset shape
print(X.shape, y.shape)
# summarize first few examples
for i in range(10):
	print(X[i], y[i])

Running the example first summarizes the created dataset showing the 1,000 examples divided into input (<em>X</em>) and output (<em>y</em>) elements.

Next, the first 10 examples in the dataset are summarized showing the input values are numeric and the target values are integers that represent the class label membership.

(1000, 2) (1000, 3)

[18. 35.] [1 1 1]
[22. 33.] [1 1 1]
[26. 36.] [1 1 1]
[24. 28.] [1 1 0]
[23. 27.] [1 1 0]
[15. 31.] [0 1 0]
[20. 37.] [0 1 0]
[18. 31.] [1 1 1]
[29. 27.] [1 0 0]
[29. 28.] [1 1 0]


<h3>Imbalanced Classification</h3>
<a href="https://machinelearningmastery.com/what-is-imbalanced-classification/">Imbalanced classification</a> refers to classification tasks where the number of examples in each class is unequally distributed.

Typically, imbalanced classification tasks are binary classification tasks where the majority of examples in the training dataset belong to the normal class and a minority of examples belong to the abnormal class.

Examples include:

<ul><li>Fraud detection.</li><li>Outlier detection.</li><li>Medical diagnostic tests.</li>
</ul>
These problems are modeled as binary classification tasks, although may require specialized techniques.

Specialized techniques may be used to change the composition of samples in the training dataset by undersampling the majority class or oversampling the minority class.

Examples include:

<ul><li><a href="https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/">Random Undersampling</a>.</li><li><a href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/">SMOTE Oversampling</a>.</li>
</ul>
Specialized modeling algorithms may be used that pay more attention to the minority class when fitting the model on the training dataset, such as cost-sensitive machine learning algorithms.

Examples include:

<ul><li><a href="https://machinelearningmastery.com/cost-sensitive-logistic-regression/">Cost-sensitive Logistic Regression</a>.</li><li>Cost-sensitive Decision Trees.</li><li>Cost-sensitive Support Vector Machines.</li>
</ul>
Finally, alternative performance metrics may be required as reporting the classification accuracy may be misleading.

Examples include:

<ul><li>Precision.</li><li>Recall.</li><li>F-Measure.</li>
</ul>
Next, let's take a closer look at a dataset to develop an intuition for imbalanced classification problems.

We can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html">make_classification() function</a> to generate a synthetic imbalanced binary classification dataset.

The example below generates a dataset with 1,000 examples that belong to one of two classes, each with two input features.

# example of an imbalanced binary classification task
from numpy import where
from collections import Counter
from sklearn.datasets import make_classification
from matplotlib import pyplot
# define dataset
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_classes=2, n_clusters_per_class=1, weights=[0.99,0.01], random_state=1)
# summarize dataset shape
print(X.shape, y.shape)
# summarize observations by class label
counter = Counter(y)
print(counter)
# summarize first few examples
for i in range(10):
	print(X[i], y[i])
# plot the dataset and color the by class label
for label, _ in counter.items():
	row_ix = where(y == label)[0]
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))
pyplot.legend()
pyplot.show()

Running the example first summarizes the created dataset showing the 1,000 examples divided into input (<em>X</em>) and output (<em>y</em>) elements.

The distribution of the class labels is then summarized, showing the severe class imbalance with about 980 examples belonging to class 0 and about 20 examples belonging to class 1.

Next, the first 10 examples in the dataset are summarized showing the input values are numeric and the target values are integers that represent the class membership. In this case, we can see that most examples belong to class 0, as we expect.

(1000, 2) (1000,)

Counter({0: 983, 1: 17})

[0.86924745 1.18613612] 0
[1.55110839 1.81032905] 0
[1.29361936 1.01094607] 0
[1.11988947 1.63251786] 0
[1.04235568 1.12152929] 0
[1.18114858 0.92397607] 0
[1.1365562  1.17652556] 0
[0.46291729 0.72924998] 0
[0.18315826 1.07141766] 0
[0.32411648 0.53515376] 0

Finally, a scatter plot is created for the input variables in the dataset and the points are colored based on their class value.

We can see one main cluster for examples that belong to class 0 and a few scattered examples that belong to class 1. The intuition is that datasets with this property of imbalanced class labels are more challenging to model.


<img class="lazy" data-src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2020/01/Scatter-Plot-of-Imbalanced-Binary-Classification-Dataset.png">
Scatter Plot of Imbalanced Binary Classification Dataset

<h3>Further Reading</h3>
This section provides more resources on the topic if you are looking to go deeper.

<ul><li><a href="https://en.wikipedia.org/wiki/Statistical_classification">Statistical classification, Wikipedia</a>.</li><li><a href="https://en.wikipedia.org/wiki/Binary_classification">Binary classification, Wikipedia</a>.</li><li><a href="https://en.wikipedia.org/wiki/Multiclass_classification">Multiclass classification, Wikipedia</a>.</li><li><a href="https://en.wikipedia.org/wiki/Multi-label_classification">Multi-label classification, Wikipedia</a>.</li><li><a href="https://scikit-learn.org/stable/modules/multiclass.html">Multiclass and multilabel algorithms, scikit-learn API</a>.</li>
</ul>
<h3>Summary</h3>
In this tutorial, you discovered different types of classification predictive modeling in machine learning.

Specifically, you learned:

<ul><li>Classification predictive modeling involves assigning a class label to input examples.</li><li>Binary classification refers to predicting one of two classes and multi-class classification involves predicting one of more than two classes.</li><li>Multi-label classification involves predicting one or more classes for each example and imbalanced classification refers to classification tasks where the distribution of examples across the classes is not equal.</li>
</ul>

<h2>to remove empty list items</h2>
emptyItems = numeric()
for(i in 1:length(alist)){
  if( length(alist[[i]]) == 0){
    emptyItems = c(emptyItems, i)
  }
}
alist = alist[-emptyItems]

to append list item:
alist = append("asdf", alist)

<h2>to reverse a matrix</h2>
b <- apply(a, 2, rev)

<h2>data.table cbind two tables</h2>
c = 1:6
d = seq(-2,18,by=4)
(dt = data.table(c,d))

e = 12:17
f = seq(22,50,by=5)
g = data.table(e,f)

# using base R
(dt = cbind(dt,g))

<h2>Multivariate Analysis</h2>
<a href="http://www.sthda.com/english/articles/32-r-graphics-essentials/130-plot-multivariate-continuous-data/" class="whitebut ">Plot Multivariate Continuous Data</a>
<a href="https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/src/multivariateanalysis.html" class="whitebut ">Multivariate Analysis</a>

# use 2D Scatter plot
# ggplot Change colors by groups
library(ggplot2)
ggplot(mtcars, aes(x=wt, y=mpg, color=cyl+1)) + geom_point()
ggplot(iris[,1:3], aes(iris[,1], iris[,2], color=iris[,3])) + geom_point()

# use 3D scatterplot3d
library(scatterplot3d)
scatterplot3d(
  iris[,1:3], pch = 19, color = "steelblue",
   grid = TRUE, box = FALSE,
   mar = c(3, 3, 0.5, 3)        
  )

<h2>count occurences in list</h2>
srcList = c("a","c")

FLSZList = c("a","b","c")
FLSZ5List = c("a","e","f", "a")
X3List = c("k","e","g", "a")

totalList = list(FLSZList, FLSZ5List, X3List)

# for(i in srcList){
#   cat("element ",i, "\n")
#   totalCount = 0
#   for(j in 1:length(totalList)){
#       cat(totalList[[j]],": ")
#       if(length(grep(i, totalList[[j]]))>0){
#         totalCount = totalCount + 1
#       }
#       cat("after ",totalCount, " ")
#   }
#   cat(totalCount, "\n")
# }

  for(item in srcList){
    totalCount = 0
    for(j in 1:length(totalList)){
        if(length(grep(item, totalList[[j]]))>0){
          totalCount = totalCount + 1
        }
    }
    if(totalCount>1) cat(item, " ")
  }

<h2>svgViewR Plotting 3D points</h2>
<a href="https://aaronolsen.github.io/tutorials/3d_visualization/plot_points.html" class="whitebut ">3D points</a>

<h2>Rstudio Keyboard Shortcuts</h2>
<h3>Console</h3>

Move cursor to Console
Ctrl+2

Clear console
Ctrl+L

Move cursor to beginning of line
Home

Move cursor to end of line
End

Navigate command history
Up/Down

Popup command history
Ctrl+Up

Interrupt currently executing command
Esc

Change working directory
Ctrl+Shift+H

<h3>Source</h3>

Go to File/Function
Ctrl+. [period]

Move cursor to Source Editor
Ctrl+1

Toggle document outline
Ctrl+Shift+O

Toggle Visual Editor
Ctrl+Shift+F4

New document (except on Chrome/Windows)
Ctrl+Shift+N

New document (Chrome only)
Ctrl+Alt+Shift+N

Open document
Ctrl+O

Save active document
Ctrl+S

Save all documents
Ctrl+Alt+S

Close active document (except on Chrome)
Ctrl+W

Close active document (Chrome only)
Ctrl+Alt+W

Close all open documents
Ctrl+Shift+W

Close other documents
Ctrl+Shift+Alt+W

Preview HTML (Markdown and HTML)
Ctrl+Shift+K

Knit Document (knitr)
Ctrl+Shift+K

Compile Notebook
Ctrl+Shift+K

Compile PDF (TeX and Sweave)
Ctrl+Shift+K

Insert chunk (Sweave and Knitr)
Ctrl+Alt+I

Insert code section
Ctrl+Shift+R

Run current line/selection
Ctrl+Enter

Run current line/selection (retain cursor position)
Alt+Enter

Re-run previous region
Ctrl+Alt+P

Run current document
Ctrl+Alt+R

Run from document beginning to current line
Ctrl+Alt+B

Run from current line to document end
Ctrl+Alt+E

Run the current function definition
Ctrl+Alt+F

Run the current code section
Ctrl+Alt+T

Run previous Sweave/Rmd code
Ctrl+Shift+Alt+P

Run the current Sweave/Rmd chunk
Ctrl+Alt+C

Run the next Sweave/Rmd chunk
Ctrl+Alt+N

Source a file
Ctrl+Alt+G

Source the current document
Ctrl+Shift+S

Source the current document (with echo)
Ctrl+Shift+Enter

Send current line/selection to terminal
Ctrl+Alt+Enter

Fold Selected
Alt+L

Unfold Selected
Shift+Alt+L

Fold All
Alt+O

Unfold All
Shift+Alt+O

Go to line
Shift+Alt+G

Jump to
Shift+Alt+J

Expand selection
Ctrl+Shift+Up

Shrink selection
Ctrl+Shift+Down

Next section
Ctrl+PgDn

Previous section
Ctrl+PgUp

Split into lines
Ctrl+Alt+A

Edit lines from start
Ctrl+Alt+Shift+A

Switch to tab
Ctrl+Shift+. [period]

Previous tab
Ctrl+F11

Previous tab (desktop)
Ctrl+Shift+Tab

Next tab
Ctrl+F12

Next tab (desktop)
Ctrl+Tab

First tab
Ctrl+Shift+F11

Last tab
Ctrl+Shift+F12

Navigate back
Ctrl+F9

Navigate forward
Ctrl+F10

Extract function from selection
Ctrl+Alt+X

Extract variable from selection
Ctrl+Alt+V

Reindent lines
Ctrl+I

Comment/uncomment current line/selection
Ctrl+Shift+C

Reflow Comment
Ctrl+Shift+/

Reformat Selection
Ctrl+Shift+A

Show Diagnostics
Ctrl+Shift+Alt+D

Transpose Letters
&nbsp;No shortcut

Move Lines Up/Down
Alt+Up/Down

Copy Lines Up/Down
Shift+Alt+Up/Down

Jump to Matching Brace/Paren
Ctrl+P

Expand to Matching Brace/Paren
Ctrl+Shift+Alt+E

Add Cursor Above Current Cursor
Ctrl+Alt+Up

Add Cursor Below Current Cursor
Ctrl+Alt+Down

Move Active Cursor Up
Ctrl+Alt+Shift+Up

Move Active Cursor Down
Ctrl+Alt+Shift+Down

Find and Replace
Ctrl+F

Find Next
Win: F3, Linux: Ctrl+G

Find Previous
Win: Shift+F3, Linux: Ctrl+Shift+G

Use Selection for Find
Ctrl+F3

Replace and Find
Ctrl+Shift+J

Find in Files
Ctrl+Shift+F

Check Spelling
F7

Rename Symbol in Scope
Ctrl+Alt+Shift+M

Insert Roxygen Skeleton
Ctrl+Alt+Shift+R

<h3>Editing (Console and Source)</h3>

Undo
Ctrl+Z

Redo
Ctrl+Shift+Z

Cut
Ctrl+X

Copy
Ctrl+C

Paste
Ctrl+V

Select All
Ctrl+A

Jump to Word
Ctrl+Left/Right

Jump to Start/End
Ctrl+Home/End or Ctrl+Up/Down

Delete Line
Ctrl+D

Select
Shift+[Arrow]

Select Word
Ctrl+Shift+Left/Right

Select to Line Start
Alt+Shift+Left

Select to Line End
Alt+Shift+Right

Select Page Up/Down
Shift+PageUp/PageDown

Select to Start/End
Ctrl+Shift+Home/End or Shift+Alt+Up/Down

Delete Word Left
Ctrl+Backspace

Delete Word Right
No shortcut

Delete to Line End
No shortcut

Delete to Line Start
No shortcut
Indent
Tab (at beginning of line)

Outdent
Shift+Tab

Yank line up to cursor
Ctrl+U

Yank line after cursor
Ctrl+K

Insert currently yanked text
Ctrl+Y

Insert assignment operator
Alt+-

Insert pipe operator
Ctrl+Shift+M

Show help for function at cursor
F1

Show source code for function at cursor
F2

Find usages for symbol at cursor (C++)
Ctrl+Alt+U

<h3>Completions (Console and Source)</h3>

Attempt completion
Tab or Ctrl+Space

Navigate candidates
Up/Down

Accept selected candidate
Enter, Tab, or Right

Dismiss completion popup
Esc

<h3>Views</h3>

Move focus to Source Editor
Ctrl+1

Zoom Source Editor
Ctrl+Shift+1

Add Source Column
Ctrl+F7

Move focus to Console
Ctrl+2

Zoom Console
Ctrl+Shift+2

Move focus to Help
Ctrl+3

Zoom Help
Ctrl+Shift+3

Move focus to Terminal
Alt+Shift+M

Show History
Ctrl+4

Zoom History
Ctrl+Shift+4

Show Files
Ctrl+5

Zoom Files
Ctrl+Shift+5

Show Plots
Ctrl+6

Zoom Plots
Ctrl+Shift+6

Show Packages
Ctrl+7

Zoom Packages
Ctrl+Shift+7

Show Environment
Ctrl+8

Zoom Environment
Ctrl+Shift+8

Show Viewer
Ctrl+9

Zoom Viewer
Ctrl+Shift+9

Show Git/SVN
Ctrl+F1

Zoom Git/SVN
Ctrl+Shift+F1

Show Build
Ctrl+F2

Zoom Build
Ctrl+Shift+F2

Show Connections
Ctrl+F5

Zoom Connections
Ctrl+Shift+F5

Show Find in Files Results
Ctrl+F6

Zoom Tutorial
Ctrl+Shift+F6

Sync Editor &amp; PDF Preview
Ctrl+F8

Global Options
No shortcut

Project Options
No shortcut

<h3>Help</h3>

Show Keyboard Shortcut Reference
Alt+Shift+K

Search R Help
Ctrl+Alt+F1

Find in Help Topic
Ctrl+F

Previous Help Topic
Shift+Alt+F2

Next Help Topic
Shift+Alt+F3

Show Command Palette
Ctrl+Shift+P, Ctrl+Alt+Shift+P (Firefox)

<h3>Build</h3>

Build and Reload
Ctrl+Shift+B

Load All (devtools)
Ctrl+Shift+L

Test Package (Desktop)
Ctrl+Shift+T

Test Package (Web)
Ctrl+Alt+F7

Check Package
Ctrl+Shift+E

Document Package
Ctrl+Shift+D

<h3>Debug</h3>

Toggle Breakpoint
Shift+F9

Execute Next Line
F10

Step Into Function
Shift+F4

Finish Function/Loop
Shift+F7

Continue
Shift+F5

Stop Debugging
Shift+F8

<h3>Plots</h3>

Previous plot
Ctrl+Alt+F11

Next plot
Ctrl+Alt+F12

<h3>Git/SVN</h3>

Diff active source document
Ctrl+Alt+D

Commit changes
Ctrl+Alt+M

Scroll diff view
Ctrl+Up/Down

Stage/Unstage (Git)
Spacebar

Stage/Unstage and move to next (Git)
Enter

<h3>Session</h3>

Quit Session (desktop only)
Ctrl+Q

Restart R Session
Ctrl+Shift+F10

<h3>Terminal</h3>

New Terminal
Alt+Shift+R

Move Focus to Terminal
Alt+Shift+M

Previous Terminal
Alt+Shift+F11

Next Terminal
Alt+Shift+F12

<h3>Main Menu (Server)</h3>

File Menu
Alt+Shift+F

Edit Menu
Alt+Shift+E

Code Menu
Alt+Shift+C

View Menu
Alt+Shift+V

Plots Menu
Alt+Shift+P

Session Menu
Alt+Shift+S

Build Menu
Alt+Shift+B

Debug Menu
Alt+Shift+U

Profile Menu
Alt+Shift+I

Tools Menu
Alt+Shift+T

Help Menu
Alt+Shift+H

<h3>Accessibility</h3>

Toggle Screen Reader Support
Alt+Shift+/

Toggle Tab Key Always Moves Focus
Ctrl+Alt+Shift+T

Speak Text Editor Location
Ctrl+Alt+Shift+B

Focus Main Toolbar
Alt+Shift+Y

Focus Console Output
Alt+Shift+2

Focus Next Pane
F6

Focus Previous Pane
Shift+F6

<h2>opencv Face recognition</h2>
<a href="https://cran.r-project.org/web/packages/opencv/opencv.pdf" class="whitebut ">opencv.pdf</a>
install.packages("opencv")

Basic stuff:

Face recognition:

unconf <- ocv_read('https://jeroen.github.io/images/unconf18.jpg')
faces <- ocv_face(unconf)
ocv_write(faces, 'faces.jpg')

Or get the face location data:

facemask <- ocv_facemask(unconf)
attr(facemask, 'faces')

Live Webcam Examples

Live face detection:

library(opencv)
ocv_video(ocv_face)

Edge detection:

library(opencv)
ocv_video(ocv_edges)

Combine with Graphics
Replaces the background with a plot:

library(opencv)
library(ggplot2)

# get webcam size
test <- ocv_picture()
bitmap <- ocv_bitmap(test)
width <- dim(bitmap)[2]
height <- dim(bitmap)[3]

png('bg.png', width = width, height = height)
par(ask=FALSE)
print(ggplot2::qplot(speed, dist, data = cars, geom = c("smooth", "point")))
dev.off()
bg <- ocv_read('bg.png')
unlink('pg.png')
ocv_video(function(input){
  mask <- ocv_mog2(input)
  return(ocv_copyto(input, bg, mask))
})

Put your face in the plot:

# Overlay face filter
ocv_video(function(input){
  mask <- ocv_facemask(input)
  ocv_copyto(input, bg, mask)
})

Live Face Survey
Go stand on the left if you're a tidier

library(opencv)

# get webcam size
test <- ocv_picture()
bitmap <- ocv_bitmap(test)
width <- dim(bitmap)[2]
height <- dim(bitmap)[3]

# generates the plot
makeplot <- function(x){
  png('bg.png', width = width, height = height, res = 96)
  on.exit(unlink('bg.png'))
  groups <- seq(0, width, length.out = 4)
  left <- rep("left", sum(x < groups[2]))
  middle <- rep("middle", sum(x >= groups[2] & x < groups[3]))
  right <- rep("right", sum(x >= groups[3]))
  f <- factor(c(left, middle, right), levels = c('left', 'middle', 'right'),
              labels = c("Tidy!", "Whatever Works", "Base!"))
  color = I(c("#F1BB7B", "#FD6467", "#5B1A18"))
  plot(f, ylim = c(0, 5),
       main = "Are you a tidyer or baser?", col = color)
  dev.off()
  ocv_read('bg.png')
}

# overlays faces on the plot
ocv_video(function(input){
  mask <- ocv_facemask(input)
  faces <- attr(mask, 'faces')
  bg <- makeplot(faces$x)
  return(ocv_copyto(input, bg, mask))
})

<h2>magick package: Advanced Image-Processing</h2>
<a href="magick.html" class="redbut gold goldbs whitets">magick</a>
<a href="https://cran.r-project.org/web/packages/magick/vignettes/intro.html" class="whitebut ">The magick package: Advanced Image-Processing in R</a>

<h2>googleLanguageR speech-to-text voice recognition</h2>
includes speech-to-text via the Google Cloud Speech API
<a href="https://cran.r-project.org/web/packages/googleLanguageR/googleLanguageR.pdf" class="whitebut ">googleLanguageR.pdf</a>

<h2>R set timeout</h2>
timeout: Set maximum request time.
getOption("timeout")
timeout(seconds)

<h2>file.path</h2>
file.path("f:", "git", "surveyor")
# "f:/git/surveyor"

setwd(file.path("F:", "git", "roxygen2"))

getwd()
# "F:/git/roxygen2"

<h2>Extract images from pdf</h2>
shell(shQuote("D:/XpdfReader-win64/xpdf-tools-win-4.03/bin64/pdfimages a.pdf -j"))

<h2>expand.grid</h2>
expand.grid(c(0,1), c(0,1), c(0,1))
expand.grid(c(0,1), c(0,1,2))


<h2>handling chinese characters</h2>
remember to load file with utf-8 encoding
historyList = readLines("D:/Dropbox/Public/LibDocs/ChineseMed/醫案.html", encoding="UTF-8")

filter and replace as usual
historyList = gsub("　"," ",historyList)

remember to set encoding when writing file
options("encoding" = "UTF-8")
setwd("C:/Users/User/Desktop")
sink("test.html")
cat(historyList,sep="\n")
sink()

<h2>retrieving own ip address</h2>
issue a system() ipconfig command to operating system:

x <- system("ipconfig", intern=TRUE)

to extract just the ip address:
z <- x[grep("IPv4", x)]
gsub(".*? ([[:digit:]])", "\\1", z)

<h2>retrieving mac address</h2>
mac_addr <- system("getmac", intern = TRUE)

in dos: getmac /V | findstr /V disconnected

<h2>pre loaded data</h2>
LibPath: "D:/R-3.4.3/library"
1	"AirPassengers
	"Monthly Airline Passenger Numbers 1949-1960"                    

2	"BJsales
	"Sales Data with Leading Indicator"                              

3	"BJsales.lead (BJsales)
	"Sales Data with Leading Indicator"                              

4	"BOD
	"Biochemical Oxygen Demand"                                      

5	"CO2
	"Carbon Dioxide Uptake in Grass Plants"                          

6	"ChickWeight
	"Weight versus age of chicks on different diets"                 

7	"DNase
	"Elisa assay of DNase"                                           

8	"EuStockMarkets
	"Daily Closing Prices of Major European Stock Indices, 1991-1998"

9	"Formaldehyde
	"Determination of Formaldehyde"                                  

10	"HairEyeColor
	"Hair and Eye Color of Statistics Students"                      

11	"Harman23.cor
	"Harman Example 2.3"                                             

12	"Harman74.cor
	"Harman Example 7.4"                                             

13	"Indometh
	"Pharmacokinetics of Indomethacin"                               

14	"InsectSprays
	"Effectiveness of Insect Sprays"                                 

15	"JohnsonJohnson
	"Quarterly Earnings per Johnson & Johnson Share"                 

16	"LakeHuron
	"Level of Lake Huron 1875-1972"                                  

17	"LifeCycleSavings
	"Intercountry Life-Cycle Savings Data"                           

18	"Loblolly
	"Growth of Loblolly pine trees"                                  

19	"Nile
	"Flow of the River Nile"                                         

20	"Orange
	"Growth of Orange Trees"                                         

21	"OrchardSprays
	"Potency of Orchard Sprays"                                      

22	"PlantGrowth
	"Results from an Experiment on Plant Growth"                     

23	"Puromycin
	"Reaction Velocity of an Enzymatic Reaction"                     

24	"Seatbelts
	"Road Casualties in Great Britain 1969-84"                       

25	"Theoph
	"Pharmacokinetics of Theophylline"                               

26	"Titanic
	"Survival of passengers on the Titanic"                          

27	"ToothGrowth
	"The Effect of Vitamin C on Tooth Growth in Guinea Pigs"         

28	"UCBAdmissions
	"Student Admissions at UC Berkeley"                              

29	"UKDriverDeaths
	"Road Casualties in Great Britain 1969-84"                       

30	"UKgas
	"UK Quarterly Gas Consumption"                                   

31	"USAccDeaths
	"Accidental Deaths in the US 1973-1978"                          

32	"USArrests
	"Violent Crime Rates by US State"                                

33	"USJudgeRatings
	"Lawyers' Ratings of State Judges in the US Superior Court"      

34	"USPersonalExpenditure
	"Personal Expenditure Data"                                      

35	"UScitiesD
	"Distances Between European Cities and Between US Cities"        

36	"VADeaths
	"Death Rates in Virginia (1940)"                                 

37	"WWWusage
	"Internet Usage per Minute"                                      

38	"WorldPhones
	"The World's Telephones"                                         

39	"ability.cov
	"Ability and Intelligence Tests"                                 

40	"airmiles
	"Passenger Miles on Commercial US Airlines, 1937-1960"           

41	"airquality
	"New York Air Quality Measurements"                              

42	"anscombe
	"Anscombe's Quartet of 'Identical' Simple Linear Regressions"    

43	"attenu
	"The Joyner-Boore Attenuation Data"                              

44	"attitude
	"The Chatterjee-Price Attitude Data"                             

45	"austres
	"Quarterly Time Series of the Number of Australian Residents"    

46	"beaver1 (beavers)
	"Body Temperature Series of Two Beavers"                         

47	"beaver2 (beavers)
	"Body Temperature Series of Two Beavers"                         

48	"cars
	"Speed and Stopping Distances of Cars"                           

49	"chickwts
	"Chicken Weights by Feed Type"                                   

50	"co2
	"Mauna Loa Atmospheric CO2 Concentration"                        

51	"crimtab
	"Student's 3000 Criminals Data"                                  

52	"discoveries
	"Yearly Numbers of Important Discoveries"                        

53	"esoph
	"Smoking, Alcohol and (O)esophageal Cancer"                      

54	"euro
	"Conversion Rates of Euro Currencies"                            

55	"euro.cross (euro)
	"Conversion Rates of Euro Currencies"                            

56	"eurodist
	"Distances Between European Cities and Between US Cities"        

57	"faithful
	"Old Faithful Geyser Data"                                       

58	"fdeaths (UKLungDeaths)" [58"Monthly Deaths from Lung Diseases in the UK"                    

59	"freeny
	"Freeny's Revenue Data"                                          

60	"freeny.x (freeny)
	"Freeny's Revenue Data"                                          

61	"freeny.y (freeny)
	"Freeny's Revenue Data"                                          

62	"infert
	"Infertility after Spontaneous and Induced Abortion"             

63	"iris
	"Edgar Anderson's Iris Data"                                     

64	"iris3
	"Edgar Anderson's Iris Data"                                     

65	"islands
	"Areas of the World's Major Landmasses"                          

66	"ldeaths (UKLungDeaths)" [66"Monthly Deaths from Lung Diseases in the UK"                    

67	"lh
	"Luteinizing Hormone in Blood Samples"                           

68	"longley
	"Longley's Economic Regression Data"                             

69	"lynx
	"Annual Canadian Lynx trappings 1821-1934"                       

70	"mdeaths (UKLungDeaths)" [70"Monthly Deaths from Lung Diseases in the UK"                    

71	"morley
	"Michelson Speed of Light Data"                                  

72	"mtcars
	"Motor Trend Car Road Tests"                                     

73	"nhtemp
	"Average Yearly Temperatures in New Haven"                       

74	"nottem
	"Average Monthly Temperatures at Nottingham, 1920-1939"          

75	"npk
	"Classical N, P, K Factorial Experiment"                         

76	"occupationalStatus
	"Occupational Status of Fathers and their Sons"                  

77	"precip
	"Annual Precipitation in US Cities"                              

78	"presidents
	"Quarterly Approval Ratings of US Presidents"                    

79	"pressure
	"Vapor Pressure of Mercury as a Function of Temperature"         

80	"quakes
	"Locations of Earthquakes off Fiji"                              

81	"randu
	"Random Numbers from Congruential Generator RANDU"               

82	"rivers
	"Lengths of Major North American Rivers"                         

83	"rock
	"Measurements on Petroleum Rock Samples"                         

84	"sleep
	"Student's Sleep Data"                                           

85	"stack.loss (stackloss)" [85"Brownlee's Stack Loss Plant Data"                               

86	"stack.x (stackloss)
	"Brownlee's Stack Loss Plant Data"                               

87	"stackloss
	"Brownlee's Stack Loss Plant Data"                               

88	"state.abb (state)
	"US State Facts and Figures"                                     

89	"state.area (state)
	"US State Facts and Figures"                                     

90	"state.center (state)
	"US State Facts and Figures"                                     

91	"state.division (state)" [91"US State Facts and Figures"                                     

92	"state.name (state)
	"US State Facts and Figures"                                     

93	"state.region (state)
	"US State Facts and Figures"                                     

94	"state.x77 (state)
	"US State Facts and Figures"                                     

95	"sunspot.month
	"Monthly Sunspot Data, from 1749 to \"Present\""                 

96	"sunspot.year
	"Yearly Sunspot Data, 1700-1988"                                 

97	"sunspots
	"Monthly Sunspot Numbers, 1749-1983"                             

98	"swiss
	"Swiss Fertility and Socioeconomic Indicators (1888) Data"       

99	"treering
	"Yearly Treering Data, -6000-1979"                               

100	"trees
	"Girth, Height and Volume for Black Cherry Trees"                

101	"uspop
	"Populations Recorded by the US Census"                          

102	"volcano
	"Topographic Information on Auckland's Maunga Whau Volcano"      

103	"warpbreaks
	"The Number of Breaks in Yarn during Weaving"                    

104	"women
	"Average Heights and Weights for American Women"                 


<h2>Check value in column is less than the median of that column</h2>

df <- data.frame(a = c(1:10), b = rnorm(10), c = rnorm(10))
sapply(df, function(x){ x >= median(x) })


<h2>Creating dummies for categorical variables</h2>
In situations where we have categorical variables (factors) but need to use them in analytical methods that require numbers (for example, <span class="orange">K nearest neighbors (KNN), Linear Regression</span>), we need to create dummy variables.

A <code>dummy variable</code> is a numeric interpretation of the category or level of the factor variable.
That is, it represents every group or level of the categorical variable as a single numeric entity.

Read the <code>data-conversion.csv</code> file and store it in the working directory of your R environment. Install the <code>dummies</code> package. Then read the data:
<code>
install.packages("dummies")
library(dummies)
students &lt;- read.csv("data-conversion.csv")</code>

Create dummies for all factors in the data frame:
<code>students.new &lt;- dummy.data.frame(students, sep = ".")
names(students.new)

[1] "Age"      "State.NJ" "State.NY" "State.TX" "State.VA"
[6] "Gender.F" "Gender.M" "Height"   "Income"</code>
The <code>students.new</code> data frame now contains all the original variables and the newly added dummy variables. The <code>dummy.data.frame()</code> function has created dummy variables for all four levels of the <code>State</code> and two levels of <code>Gender</code> factors. However, we will generally omit one of the dummy variables for <code>State</code> and one for <code>Gender</code> when we use machine-learning techniques.
We can use the optional argument <code>all = FALSE</code> to specify that the resulting data frame should contain only the generated dummy variables and none of the original variables.

<h3>How it works...</h3>
The <code>dummy.data.frame()</code> function creates dummies for all the factors in the data frame supplied. Internally, it uses another <code>dummy()</code> function which creates dummy variables for a single factor. The <code>dummy()</code> function creates one new variable for every level of the factor for which we are creating dummies. It appends the variable name with the factor level name to generate names for the dummy variables. We can use the <code>sep</code> argument to specify the character that separates them—an empty string is the default:

<code>dummy(students$State, sep = ".")

      State.NJ State.NY State.TX State.VA
 [1,]        1        0        0        0
 [2,]        0        1        0        0
 [3,]        1        0        0        0
 [4,]        0        0        0        1
 [5,]        0        1        0        0
 [6,]        0        0        1        0
 [7,]        1        0        0        0
 [8,]        0        0        0        1
 [9,]        0        0        1        0
[10,]        0        0        0        1</code>

<h3>There's more...</h3>
In situations where a data frame has several factors, and you plan on using only a subset of these, you will create dummies only for the chosen subset.
<h4>Choosing which variables to create dummies for</h4>
To create dummies only for one variable or a subset of variables, we can use the <code>names</code> argument to specify the column names of the variables we want dummies for:

<code>students.new1 &lt;- dummy.data.frame(students, names = c("State","Gender") , sep = ".")</code>

<h2>How to Create dummy variables in R</h2>
<h3>Why do we need dummy variables in R?</h3>
Let us first understand the concept of dummy variables. 
Consider a dataset that represents some categorical data values.

Handling such a huge number of categories and groups is a cumbersome task for the <a href="https://www.journaldev.com/48036/machine-learning-in-r-introduction">machine learning model</a>. 
Thus arises the need to treat categorical or level entries.

This is when the concept of dummy entries comes into picture.

A <code>dummy variable</code> is a numeric interpretation of the category or level of the factor variable. 
That is, it represents every group or level of the categorical variable as a single numeric entity.

For example, consider a data set that contains a variable &#8216;Poll' with values &#8216;Yes' and &#8216;No'. 
Now, in order to represent the two groups as numeric entries, we can create dummies of the same.

So, the transformed dataset would now have two more additional columns as &#8216;Poll.1' which would represent &#8216;yes' type values (would assign 1 to all the data rows that are associated with level yes) and &#8216;Poll.2' for &#8216;No' type values.

<h3>R fast.dummies library to create dummy variables</h3>
R provides us with <b>fast.dummies</b> library that contains of dummy_cols() function for the creation of dummy variables at ease.

With <code>dummy_cols()</code> function, one can select the variables for whom the dummies need to be created.

<b>Syntax:</b>

dummy_cols(data, select_columns = 'columns')</code>

<b>Example:</b>

In this example, we have made use of the Bank Load Defaulter dataset. 
You can find the dataset <a href="https://github.com/Safa1615/Dataset--loan/blob/main/bank-loan.csv">here</a>.

Further, we have made use of dummy_cols() function to create dummy variables for the column &#8216;ed'.

rm(list = ls())
 
#install.packages('fastDummies')
library('fastDummies')
dta = read.csv("bank-loan.csv",header=TRUE)
dim(dta)
dum &lt;- dummy_cols(dta, select_columns = 'ed')
dim(dum)

</code>

<b>Output:</b>

As witnessed below, the initial number of columns of the data set equals to 9. 
Post creation of dummy variables, the number of columns equals to 14.

All the 5 levels of the ed variable has been segregated as a separate column. 
Only those rows which belongs to a certain category are set as 1, rest all values are set to zero(0).

&gt; dim(dta)
&#91;1] 850   9

&gt; dim(dum)
&#91;1] 850  14
</code>


<img class="lazy" data-src="https://cdn.journaldev.com/wp-content/uploads/2021/02/Creation-of-dummies-using-fastDummies-library.png">

What if we need to create dummies for multiple variables in a single shot or at once?

Well, we can then create a list of all the variables for which we need dummies using <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/c" target="_blank" rel="noreferrer noopener">c() function</a> and pass them as arguments through select_columns.

<b>Example:</b>

rm(list = ls())
 
#install.packages('fastDummies')
library('fastDummies')
dta = read.csv("bank-loan.csv",header=TRUE)
dim(dta)
dum &lt;- dummy_cols(dta, select_columns = c('ed','default'))
dim(dum)

</code>

<b>Output:</b>

Here, we have created dummies for both &#8216;ed' and &#8216;default' data columns.

&gt; dim(dta)
&#91;1] 850   9
&gt; dum &lt;- dummy_cols(dta, select_columns = c('ed','default'))
&gt; dim(dum)
&#91;1] 850  17
</code>


<img class="lazy" data-src="https://cdn.journaldev.com/wp-content/uploads/2021/02/Creation-of-dummies-for-multiple-columns.png">

<h3>R dummies library to create dummy variables</h3>
R <b>dummies </b>library can also be used to create dummy data variables for the categorical data columns at ease.

For the same, we can make use of <code>dummy()</code> function that enables us to create dummy entries for selected columns.

<b>Example:</b>

In the below example, we have created dummy variables of the column &#8216;ed' using dummy() function.

rm(list = ls())

library('dummies')
dta = read.csv("bank-loan.csv",header=TRUE)
dim(dta)
dum &lt;- dummy(dta$ed)
dim(dum)

</code>

<b>Output:</b>

As seen below, all the levels have been segregated as a different column.

Also, only those data rows that match to the particular level is set to 1 in the column else it is represented as zero.

For example, if the data represents the level &#8216;ed1', then it is set to 1 else it is set to 0.

<img class="lazy" data-src="https://cdn.journaldev.com/wp-content/uploads/2021/02/Creation-of-dummies-using-dummies-library.png">

<h2>Build your own neural network classifier</h2>
<h3>​Introduction</h3>
Image classification is one important field in Computer Vision, not only because so many applications are associated with it, but also a lot of Computer Vision problems can be effectively reduced to image classification. 
The state of art tool in image classification is Convolutional Neural Network (CNN). 
In this article, I am going to write a simple Neural Network with 2 layers (fully connected). 
I will first train it to classify a set of 4-class 2D data and visualize the decision boundary. 
Second, I am going to train my NN with the famous MNIST data (you can download it here: <a href="https://www.kaggle.com/c/digit-recognizer/download/train.csv" rel="noopener nofollow">https://www.kaggle.com/c/digit-recognizer/download/train.csv</a>) and see its performance. 
The first part is inspired by CS 231n course offered by Stanford: <a href="http://cs231n.github.io/" rel="noopener nofollow">http://cs231n.github.io/</a>, which is taught in Python.

<h3>​Data set generation</h3>
First, let’s create a spiral dataset with 4 classes and 200 examples each.

library(ggplot2)
library(caret)
 
N <- 200 # number of points per class
D <- 2 # dimensionality
K <- 4 # number of classes
X <- data.frame() # data matrix (each row = single example)
y <- data.frame() # class labels
 
set.seed(308)
 
for (j in (1:K)){
  r <- seq(0.05,1,length.out = N) # radius
  t <- seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp <- data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp <- data.frame(matrix(j, N, 1))
  X <- rbind(X, Xtemp)
  y <- rbind(y, ytemp)
}
 
data <- cbind(X,y)
colnames(data) <- c(colnames(X), 'label')

<code>X</code>, <code>y</code> are 800 by 2 and 800 by 1 data frames respectively, and they are created in a way such that a linear classifier cannot separate them. 
Since the data is 2D, we can easily visualize it on a plot. 
They are roughly evenly spaced and indeed a line is not a good decision boundary.

x_min <- min(X[,1])-0.2; x_max <- max(X[,1])+0.2
y_min <- min(X[,2])-0.2; y_max <- max(X[,2])+0.2
 
# lets visualize the data:
ggplot(data) + geom_point(aes(x=x, y=y, color = as.character(label)), size = 2) + theme_bw(base_size = 15) +
  xlim(x_min, x_max) + ylim(y_min, y_max) +
  ggtitle('Spiral Data Visulization') +
  coord_fixed(ratio = 0.8) +
  theme(axis.ticks=element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        axis.text=element_blank(), axis.title=element_blank(), legend.position = 'none')


<img src="https://miro.medium.com/max/1650/0*0oU0nasKE2BDgb1d.png">

<h3>Neural network construction</h3>
Now, let’s construct a NN with 2 layers. 
But before that, we need to convert X into a matrix (for matrix operation later on). 
For labels in y, a new matrix Y (800 by 4) is created such that for each example (each row in Y), the entry with index==label is 1 (and 0 otherwise).

X <- as.matrix(X)
Y <- matrix(0, N*K, K)
 
for (i in 1:(N*K)){
  Y[i, y[i,]] <- 1
}

Next, let’s build a function <code>nnet</code> that takes two matrices <code>X</code> and <code>Y</code> and returns a list of 4 with <code>W</code>, <code>b</code> and <code>W2</code>, <code>b2</code> (weight and bias for each layer). 
I can specify <code>step_size</code> (learning rate) and regularization strength (<code>reg</code>, sometimes symbolized as λ ).<br> ​<br>For the choice of activation and loss (cost) function, ReLU and softmax are selected respectively. 
If you have taken the ML class by Andrew Ng (strongly recommended), sigmoid and logistic cost function are chosen in the course notes and assignment. 
They look slightly different, but can be implemented fairly easily just by modifying the following code. 
Also note that the implementation below uses vectorized operation that may seem hard to follow. 
If so, you can write down dimensions of each matrix and check multiplications and so on. 
By doing so, you also know what’s under the hood for a neural network.

# %*% dot product, * element wise product
nnet <- function(X, Y, step_size = 0.5, reg = 0.001, h = 10, niteration){
  # get dim of input
  N <- nrow(X) # number of examples
  K <- ncol(Y) # number of classes
  D <- ncol(X) # dimensionality
 
  # initialize parameters randomly
  W <- 0.01 * matrix(rnorm(D*h), nrow = D)
  b <- matrix(0, nrow = 1, ncol = h)
  W2 <- 0.01 * matrix(rnorm(h*K), nrow = h)
  b2 <- matrix(0, nrow = 1, ncol = K)
 
  # gradient descent loop to update weight and bias
  for (i in 0:niteration){
    # hidden layer, ReLU activation
    hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T))
    hidden_layer <- matrix(hidden_layer, nrow = N)
    # class score
    scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T)
 
    # compute and normalize class probabilities
    exp_scores <- exp(scores)
    probs <- exp_scores / rowSums(exp_scores)
 
    # compute the loss: sofmax and regularization
    corect_logprobs <- -log(probs)
    data_loss <- sum(corect_logprobs*Y)/N
    reg_loss <- 0.5*reg*sum(W*W) + 0.5*reg*sum(W2*W2)
    loss <- data_loss + reg_loss
    # check progress
    if (i%%1000 == 0 | i == niteration){
      print(paste("iteration", i,': loss', loss))}
 
    # compute the gradient on scores
    dscores <- probs-Y
    dscores <- dscores/N
 
    # backpropate the gradient to the parameters
    dW2 <- t(hidden_layer)%*%dscores
    db2 <- colSums(dscores)
    # next backprop into hidden layer
    dhidden <- dscores%*%t(W2)
    # backprop the ReLU non-linearity
    dhidden[hidden_layer <= 0] <- 0
    # finally into W,b
    dW <- t(X)%*%dhidden
    db <- colSums(dhidden)
 
    # add regularization gradient contribution
    dW2 <- dW2 + reg *W2
    dW <- dW + reg *W
 
    # update parameter 
    W <- W-step_size*dW
    b <- b-step_size*db
    W2 <- W2-step_size*dW2
    b2 <- b2-step_size*db2
  }
  return(list(W, b, W2, b2))
}

<h3>​Prediction function and model training</h3>
Next, create a prediction function, which takes <code>X</code> (same col as training <code>X</code> but may have different rows) and layer parameters as input. 
The output is the column index of max score in each row. 
In this example, the output is simply the label of each class. 
Now we can print out the training accuracy.

nnetPred <- function(X, para = list()){
  W <- para[[1]]
  b <- para[[2]]
  W2 <- para[[3]]
  b2 <- para[[4]]
 
  N <- nrow(X)
  hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T)) 
  hidden_layer <- matrix(hidden_layer, nrow = N)
  scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T) 
  predicted_class <- apply(scores, 1, which.max)
 
  return(predicted_class)  
}
 
nnet.model <- nnet(X, Y, step_size = 0.4,reg = 0.0002, h=50, niteration = 6000)
## [1] "iteration 0 : loss 1.38628868932674"
## [1] "iteration 1000 : loss 0.967921639616882"
## [1] "iteration 2000 : loss 0.448881467342854"
## [1] "iteration 3000 : loss 0.293036646147359"
## [1] "iteration 4000 : loss 0.244380009480792"
## [1] "iteration 5000 : loss 0.225211501612035"
## [1] "iteration 6000 : loss 0.218468573259166"
predicted_class <- nnetPred(X, nnet.model)
print(paste('training accuracy:',mean(predicted_class == (y))))
## [1] "training accuracy: 0.96375"

<h3>Decision boundary</h3>
Next, let’s plot the decision boundary. 
We can also use the caret package and train different classifiers with the data and visualize the decision boundaries. 
It is very interesting to see how different algorithms make decisions. 
This is going to be another post.

# plot the resulting classifier
hs <- 0.01
grid <- as.matrix(expand.grid(seq(x_min, x_max, by = hs), seq(y_min, y_max, by =hs)))
Z <- nnetPred(grid, nnet.model)
 
ggplot()+
  geom_tile(aes(x = grid[,1],y = grid[,2],fill=as.character(Z)), alpha = 0.3, show.legend = F)+ 
  geom_point(data = data, aes(x=x, y=y, color = as.character(label)), size = 2) + 
  theme_bw(base_size = 15) +
  ggtitle('Neural Network Decision Boundary') +
  coord_fixed(ratio = 0.8) + 
  theme(axis.ticks=element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        axis.text=element_blank(), axis.title=element_blank(), legend.position = 'none')


<img src="https://miro.medium.com/max/1650/0*cRBaC-lD_r5TrSlC.png">

<h3>​MNIST data and preprocessing</h3>
The famous MNIST (“Modified National Institute of Standards and Technology”) dataset is a classic within the Machine Learning community that has been extensively studied. 
It is a collection of handwritten digits that are decomposed into a csv file, with each row representing one example, and the column values are grey scale from 0–255 of each pixel. 
First, let’s display an image.


<img src="https://miro.medium.com/max/1650/0*f_Cenh62jDzZITzQ.png">

Now, let’s preprocess the data by removing near zero variance columns and scaling by <code>max(X)</code>. 
The data is also splitted into two for cross validation. 
Once again, we need to create a <code>Y</code> matrix with dimension <code>N</code> by <code>K</code>. 
This time the non-zero index in each row is offset by 1: label 0 will have entry 1 at index 1, label 1 will have entry 1 at index 2, and so on. 
In the end, we need to convert it back. 
(Another way is put 0 at index 10 and no offset for the rest labels.)

<h3>​Model training and CV accuracy</h3>
Now we can train the model with the training set. 
Note even after removing nzv columns, the data is still huge, so it may take a while for result to converge. 
Here I am only training the model for 3500 iterations. 
You can vary the iterations, learning rate and regularization strength and plot the learning curve for optimal fitting.​

<h3>Prediction of a random image</h3>
Finally, let’s randomly select an image and predict the label.​

<h3>​Conclusion</h3>
It is rare nowadays for us to write our own machine learning algorithm from ground up. 
There are tons of packages available and they most likely outperform this one. 
However, by doing so, I really gained a deep understanding how neural network works. 
And at the end of the day, seeing your own model produces a pretty good accuracy is a huge satisfaction.

<h2>securely encrypt a string</h2>
library(sodium)
passkey <- sha256(charToRaw("password123"))
plaintext <- "西兰花"
plaintext.raw <- serialize(plaintext, NULL)
ciphertext <- data_encrypt(plaintext.raw, key = passkey)
unserialize(data_decrypt(ciphertext, key = sha256(charToRaw("password123"))))

<h2>Exploratory Data Analysis</h2>
Topics: <i>Variation; Visualising distributions;
Typical values; Unusual values; Missing values;
Covariation; A categorical and continuous variable;
Two categorical variables; Two continuous variables;
Patterns and models; ggplot2 calls</i>

Exploratory data analysis, or EDA for short. 
EDA is an iterative cycle. 

You:
Generate questions about your data.
Search for answers by visualising, transforming, and modelling your data.
Use what you learn to refine your questions and/or generate new questions.

EDA is not a formal process with a strict set of rules. 
More than anything, EDA is a state of mind. 
During the initial phases of EDA you should feel free to investigate every idea that occurs to you. 
Some of these ideas will pan out, and some will be dead ends. 
As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others.

EDA is an important part of any data analysis, even if the questions are handed to you on a platter, because you always need to investigate the quality of your data. 
Data cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not. 
To do data cleaning, you’ll need to deploy all the tools of EDA: visualisation, transformation, and modelling.

<h3>Prerequisites</h3>
In this chapter we’ll combine what you’ve learned about dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions.

<code><a href="https://rdrr.io/r/base/library.html">library</a>(<a href="http://tidyverse.tidyverse.org">tidyverse</a>)</code>

<h3>Questions</h3>
“There are no routine statistical questions, only questionable statistical
routines.” — Sir David Cox
“Far better an approximate answer to the right question, which is often
vague, than an exact answer to the wrong question, which can always be made
precise.” — John Tukey

Your goal during EDA is to develop an understanding of your data. 
The easiest way to do this is to use questions as tools to guide your investigation. 
When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.

EDA is fundamentally a creative process. 
And like most creative processes, the key to asking <em>quality</em> questions is to generate a large <em>quantity</em> of questions. 
It is difficult to ask revealing questions at the start of your analysis because you do not know what insights are contained in your dataset. 
On the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery. 
You can quickly drill down into the most interesting parts of your data—and develop a set of thought-provoking questions—if you follow up each question with a new question based on what you find.

There is no rule about which questions you should ask to guide your research. 
However, two types of questions will always be useful for making discoveries within your data. 
You can loosely word these questions as:

What type of variation occurs within my variables?

What type of covariation occurs between my variables?

The rest of this chapter will look at these two questions. 
I’ll explain what variation and covariation are, and I’ll show you several ways to answer each question. 
To make the discussion easier, let’s define some terms:

A <strong>variable</strong> is a quantity, quality, or property that you can measure.

A <strong>value</strong> is the state of a variable when you measure it. 
The value of a
variable may change from measurement to measurement.

An <strong>observation</strong> is a set of measurements made under similar conditions
(you usually make all of the measurements in an observation at the same
time and on the same object). 
An observation will contain several values,
each associated with a different variable. 
I’ll sometimes refer to
an observation as a data point.

<strong>Tabular data</strong> is a set of values, each associated with a variable and an
observation. 
Tabular data is <em>tidy</em> if each value is placed in its own
“cell”, each variable in its own column, and each observation in its own
row.

So far, all of the data that you’ve seen has been tidy. 
In real-life, most data isn’t tidy, so we’ll come back to these ideas again in <a href="tidy-data.html#tidy-data-1">tidy data</a>.

<h3>Variation</h3>
<strong>Variation</strong> is the tendency of the values of a variable to change from measurement to measurement. 
You can see variation easily in real life; if you measure any continuous variable twice, you will get two different results. 
This is true even if you measure quantities that are constant, like the speed of light. 
Each of your measurements will include a small amount of error that varies from measurement to measurement. 
Categorical variables can also vary if you measure across different subjects (e.g. 
the eye colors of different people), or different times (e.g. 
the energy levels of an electron at different moments).
Every variable has its own pattern of variation, which can reveal interesting information. 
The best way to understand that pattern is to visualise the distribution of the variable’s values.

<h3>Visualising distributions</h3>
How you visualise the distribution of a variable will depend on whether the variable is categorical or continuous. 
A variable is <strong>categorical</strong> if it can only take one of a small set of values. 
In R, categorical variables are usually saved as factors or character vectors. 
To examine the distribution of a categorical variable, use a bar chart:

<code>ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/7f4cb432b8891f01b8097e31f286cb54d7473ced/02c81/eda_files/figure-html/unnamed-chunk-1-1.png">
The height of the bars displays how many observations occurred with each x value. 
You can compute these values manually with <code><a href="https://dplyr.tidyverse.org/reference/count.html">dplyr::count()</a></code>:

<code>diamonds %>% 
  count(cut)
#> # A tibble: 5 x 2
#>   cut           n
#>   &lt;ord>     &lt;int>
#> 1 Fair       1610
#> 2 Good       4906
#> 3 Very Good 12082
#> 4 Premium   13791
#> 5 Ideal     21551</code>
A variable is <strong>continuous</strong> if it can take any of an infinite set of ordered values. 
Numbers and date-times are two examples of continuous variables. 
To examine the distribution of a continuous variable, use a histogram:

<code>ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/832184b1db9d6bca2080fa526b7d791f77d37b8f/880fa/eda_files/figure-html/unnamed-chunk-3-1.png">
You can compute this by hand by combining <code><a href="https://dplyr.tidyverse.org/reference/count.html">dplyr::count()</a></code> and <code><a href="https://ggplot2.tidyverse.org/reference/cut_interval.html">ggplot2::cut_width()</a></code>:

<code>diamonds %>% 
  count(cut_width(carat, 0.5))
#> # A tibble: 11 x 2
#>   `cut_width(carat, 0.5)`     n
#>   &lt;fct>                   &lt;int>
#> 1 [-0.25,0.25]              785
#> 2 (0.25,0.75]             29498
#> 3 (0.75,1.25]             15977
#> 4 (1.25,1.75]              5313
#> 5 (1.75,2.25]              2002
#> 6 (2.25,2.75]               322
#> # … with 5 more rows</code>
A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. 
In the graph above, the tallest bar shows that almost 30,000 observations have a <code>carat</code> value between 0.25 and 0.75, which are the left and right edges of the bar.

You can set the width of the intervals in a histogram with the <code>binwidth</code> argument, which is measured in the units of the <code>x</code> variable. 
You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. 
For example, here is how the graph above looks when we zoom into just the diamonds with a size of less than three carats and choose a smaller binwidth.

<code>smaller &lt;- diamonds %>% 
  <a href="https://rdrr.io/r/stats/filter.html">filter</a>(carat &lt; 3)
  
ggplot(data = smaller, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.1)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/2f4885ec2541f7490d6504d1e4c2d2707b7cd8d5/b5df8/eda_files/figure-html/unnamed-chunk-5-1.png">
If you wish to overlay multiple histograms in the same plot, I recommend using <code>geom_freqpoly()</code> instead of <code>geom_histogram()</code>. 
<code>geom_freqpoly()</code> performs the same calculation as <code>geom_histogram()</code>, but instead of displaying the counts with bars, uses lines instead. 
It’s much easier to understand overlapping lines than bars.

<code>ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +
  geom_freqpoly(binwidth = 0.1)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/73b1aa01f56fdebb5829f8bb9efefd2d424165dd/0799c/eda_files/figure-html/unnamed-chunk-6-1.png">
There are a few challenges with this type of plot, which we will come back to in <a href="exploratory-data-analysis.html#cat-cont">visualising a categorical and a continuous variable</a>.

Now that you can visualise variation, what should you look for in your plots? And what type of follow-up questions should you ask? I’ve put together a list below of the most useful types of information that you will find in your graphs, along with some follow-up questions for each type of information. 
The key to asking good follow-up questions will be to rely on your curiosity (What do you want to learn more about?) as well as your skepticism (How could this be misleading?).

<h3>Typical values</h3>
In both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. 
Places that do not have bars reveal values that were not seen in your data. 
To turn this information into useful questions, look for anything unexpected:

Which values are the most common? Why?

Which values are rare? Why? Does that match your expectations?

Can you see any unusual patterns? What might explain them?

As an example, the histogram below suggests several interesting questions:

Why are there more diamonds at whole carats and common fractions of carats?

Why are there more diamonds slightly to the right of each peak than there
are slightly to the left of each peak?

Why are there no diamonds bigger than 3 carats?

<code>ggplot(data = smaller, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.01)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/d4853921e3a2dbbac40b7b4ff2f138f0766b1f82/c7f2a/eda_files/figure-html/unnamed-chunk-7-1.png">
# 
smaller <- diamonds %>% filter(carat < 3)

ggplot(data = smaller, mapping = aes(x = carat)) + geom_histogram(binwidth = 0.1)

# same result:
index = diamonds$carat < 3
smaller1 <- diamonds[index,]
barplot(table(smaller1$carat))
cut_interval(smaller1$carat, n = NULL, length = 0.01)
barplot(table(cut_interval(smaller1$carat, n = NULL, length = 0.02)))

geom_freqpoly() performs the same calculation as geom_histogram(), but uses lines instead. 

ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) + geom_freqpoly(binwidth = 0.1)

Clusters of similar values suggest that subgroups exist in your data. 
To understand the subgroups, ask:

How are the observations within each cluster similar to each other?

How are the observations in separate clusters different from each other?

How can you explain or describe the clusters?

Why might the appearance of clusters be misleading?

The histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. 
Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between.

<code>ggplot(data = faithful, mapping = aes(x = eruptions)) + 
  geom_histogram(binwidth = 0.25)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/aaa1365ef32c8038eab5a48634c2f78274ab1386/7b1be/eda_files/figure-html/unnamed-chunk-8-1.png">
Many of the questions above will prompt you to explore a relationship <em>between</em> variables, for example, to see if the values of one variable can explain the behavior of another variable. 
We’ll get to that shortly.

<h3>Unusual values</h3>
Outliers are observations that are unusual; data points that don’t seem to fit the pattern. 
Sometimes outliers are data entry errors; other times outliers suggest important new science. 
When you have a lot of data, outliers are sometimes difficult to see in a histogram. 
For example, take the distribution of the <code>y</code> variable from the diamonds dataset. 
The only evidence of outliers is the unusually wide limits on the x-axis.

<code>ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/e6b7f3c6684eff5146e447bc122ca4ed032aec87/08e60/eda_files/figure-html/unnamed-chunk-9-1.png">
There are so many observations in the common bins that the rare bins are so short that you can’t see them (although maybe if you stare intently at 0 you’ll spot something). 
To make it easy to see the unusual values, we need to zoom to small values of the y-axis with <code>coord_cartesian()</code>:

<code>ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = <a href="https://rdrr.io/r/base/c.html">c</a>(0, 50))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/0d4e86fd0fb0909eae2ea1df2e703feaa1732297/d1e5d/eda_files/figure-html/unnamed-chunk-10-1.png">
(<code>coord_cartesian()</code> also has an <code><a href="https://rdrr.io/r/graphics/plot.window.html">xlim()</a></code> argument for when you need to zoom into the x-axis. 
ggplot2 also has <code><a href="https://rdrr.io/r/graphics/plot.window.html">xlim()</a></code> and <code><a href="https://rdrr.io/r/graphics/plot.window.html">ylim()</a></code> functions that work slightly differently: they throw away the data outside the limits.)

This allows us to see that there are three unusual values: 0, ~30, and ~60. 
We pluck them out with dplyr:

<code>unusual &lt;- diamonds %>% 
  <a href="https://rdrr.io/r/stats/filter.html">filter</a>(y &lt; 3 | y > 20) %>% 
  select(price, x, y, z) %>%
  arrange(y)
unusual
#> # A tibble: 9 x 4
#>   price     x     y     z
#>   &lt;int> &lt;dbl> &lt;dbl> &lt;dbl>
#> 1  5139  0      0    0   
#> 2  6381  0      0    0   
#> 3 12800  0      0    0   
#> 4 15686  0      0    0   
#> 5 18034  0      0    0   
#> 6  2130  0      0    0   
#> 7  2130  0      0    0   
#> 8  2075  5.15  31.8  5.12
#> 9 12210  8.09  58.9  8.06</code>
The <code>y</code> variable measures one of the three dimensions of these diamonds, in mm. 
We know that diamonds can’t have a width of 0mm, so these values must be incorrect. 
We might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars!

It’s good practice to repeat your analysis with and without the outliers. 
If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. 
However, if they have a substantial effect on your results, you shouldn’t drop them without justification. 
You’ll need to figure out what caused them (e.g. 
a data entry error) and disclose that you removed them in your write-up.

<h3>Exercises</h3>

Explore the distribution of each of the <code>x</code>, <code>y</code>, and <code>z</code> variables
in <code>diamonds</code>. 
What do you learn? Think about a diamond and how you
might decide which dimension is the length, width, and depth.

Explore the distribution of <code>price</code>. 
Do you discover anything unusual
or surprising? (Hint: Carefully think about the <code>binwidth</code> and make sure
you try a wide range of values.)

How many diamonds are 0.99 carat? How many are 1 carat? What
do you think is the cause of the difference?

Compare and contrast <code>coord_cartesian()</code> vs <code><a href="https://rdrr.io/r/graphics/plot.window.html">xlim()</a></code> or <code><a href="https://rdrr.io/r/graphics/plot.window.html">ylim()</a></code> when
zooming in on a histogram. 
What happens if you leave <code>binwidth</code> unset?
What happens if you try and zoom so only half a bar shows?

<h3>Missing values</h3>
If you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.

Drop the entire row with the strange values:

<code>diamonds2 &lt;- diamonds %>% 
  <a href="https://rdrr.io/r/stats/filter.html">filter</a>(between(y, 3, 20))</code>
I don’t recommend this option because just because one measurement
is invalid, doesn’t mean all the measurements are. 
Additionally, if you
have low quality data, by time that you’ve applied this approach to every
variable you might find that you don’t have any data left!

Instead, I recommend replacing the unusual values with missing values.
The easiest way to do this is to use <code>mutate()</code> to replace the variable
with a modified copy. 
You can use the <code><a href="https://rdrr.io/r/base/ifelse.html">ifelse()</a></code> function to replace
unusual values with <code>NA</code>:

<code>diamonds2 &lt;- diamonds %>% 
  mutate(y = <a href="https://rdrr.io/r/base/ifelse.html">ifelse</a>(y &lt; 3 | y > 20, NA, y))</code>

<code><a href="https://rdrr.io/r/base/ifelse.html">ifelse()</a></code> has three arguments. 
The first argument <code>test</code> should be a logical vector. 
The result will contain the value of the second argument, <code>yes</code>, when <code>test</code> is <code>TRUE</code>, and the value of the third argument, <code>no</code>, when it is false. 
Alternatively to ifelse, use <code><a href="https://dplyr.tidyverse.org/reference/case_when.html">dplyr::case_when()</a></code>. 
<code>case_when()</code> is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables.

Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. 
It’s not obvious where you should plot missing values, so ggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed:

<code>ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point()
#> Warning: Removed 9 rows containing missing values (geom_point).</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/0567235adf6a020b121e8bbbb68aca6c3ec757e1/4bb0c/eda_files/figure-html/unnamed-chunk-16-1.png">
To suppress that warning, set <code>na.rm = TRUE</code>:

<code>ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point(na.rm = TRUE)</code>
Other times you want to understand what makes observations with missing values different to observations with recorded values. 
For example, in <code><a href="https://rdrr.io/pkg/nycflights13/man/flights.html">nycflights13::flights</a></code>, missing values in the <code>dep_time</code> variable indicate that the flight was cancelled. 
So you might want to compare the scheduled departure times for cancelled and non-cancelled times. 
You can do this by making a new variable with <code><a href="https://rdrr.io/r/base/NA.html">is.na()</a></code>.

<code>nycflights13::<a href="https://rdrr.io/pkg/nycflights13/man/flights.html">flights</a> %>% 
  mutate(
    cancelled = <a href="https://rdrr.io/r/base/NA.html">is.na</a>(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(sched_dep_time)) + 
    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/3b39886be0ed133839461341db7faeb7420c7942/2b3e3/eda_files/figure-html/unnamed-chunk-18-1.png">
However this plot isn’t great because there are many more non-cancelled flights than cancelled flights. 
In the next section we’ll explore some techniques for improving this comparison.

<h3>Exercises</h3>

What happens to missing values in a histogram? What happens to missing
values in a bar chart? Why is there a difference?

What does <code>na.rm = TRUE</code> do in <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code> and <code><a href="https://rdrr.io/r/base/sum.html">sum()</a></code>?

<h3>Covariation</h3>
If variation describes the behavior <em>within</em> a variable, covariation describes the behavior <em>between</em> variables. 
<strong>Covariation</strong> is the tendency for the values of two or more variables to vary together in a related way. 
The best way to spot covariation is to visualise the relationship between two or more variables. 
How you do that should again depend on the type of variables involved.

<h3>A categorical and continuous variable</h3>
It’s common to want to explore the distribution of a continuous variable broken down by a categorical variable, as in the previous frequency polygon. 
The default appearance of <code>geom_freqpoly()</code> is not that useful for that sort of comparison because the height is given by the count. 
That means if one of the groups is much smaller than the others, it’s hard to see the differences in shape. 
For example, let’s explore how the price of a diamond varies with its quality:

<code>ggplot(data = diamonds, mapping = aes(x = price)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/e1fa3645c501327ca1dd355d1a58e4b5dd5ae395/175af/eda_files/figure-html/unnamed-chunk-19-1.png">
It’s hard to see the difference in distribution because the overall counts differ so much:

<code>ggplot(diamonds) + 
  geom_bar(mapping = aes(x = cut))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/8eef08495f22577681edeb581899317bcae342de/0dc3d/eda_files/figure-html/unnamed-chunk-20-1.png">
To make the comparison easier we need to swap what is displayed on the y-axis. 
Instead of displaying count, we’ll display <strong>density</strong>, which is the count standardised so that the area under each frequency polygon is one.

<code>ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + 
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/f9865bcc8f3f3b4213c6356cf214f11be754e3c7/4a632/eda_files/figure-html/unnamed-chunk-21-1.png">
There’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! But maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot.

Another alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot. 
A <strong>boxplot</strong> is a type of visual shorthand for a distribution of values that is popular among statisticians. 
Each boxplot consists of:

A box that stretches from the 25th percentile of the distribution to the
75th percentile, a distance known as the interquartile range (IQR). 
In the
middle of the box is a line that displays the median, i.e. 
50th percentile,
of the distribution. 
These three lines give you a sense of the spread of the
distribution and whether or not the distribution is symmetric about the
median or skewed to one side.

Visual points that display observations that fall more than 1.5 times the
IQR from either edge of the box. 
These outlying points are unusual
so are plotted individually.

A line (or whisker) that extends from each end of the box and goes to the<br>
farthest non-outlier point in the distribution.


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/153b9af53b33918353fda9b691ded68cd7f62f51/5b616/images/eda-boxplot.png">
Let’s take a look at the distribution of price by cut using <code>geom_boxplot()</code>:

<code>ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_boxplot()</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/dd32748293488cb51f7ca92587c4c8745c5855c0/9dcb3/eda_files/figure-html/unnamed-chunk-23-1.png">
We see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot). 
It supports the counterintuitive finding that better quality diamonds are cheaper on average! In the exercises, you’ll be challenged to figure out why.

<code>cut</code> is an ordered factor: fair is worse than good, which is worse than very good and so on. 
Many categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display. 
One way to do that is with the <code><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder()</a></code> function.

For example, take the <code>class</code> variable in the <code>mpg</code> dataset. 
You might be interested to know how highway mileage varies across classes:

<code>ggplot(data = mpg, mapping = aes(x = class, y = hwy)) +
  geom_boxplot()</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/9d74715c7564aa5b82548d56f35a55ffa8f845ed/463c0/eda_files/figure-html/unnamed-chunk-24-1.png">
To make the trend easier to see, we can reorder <code>class</code> based on the median value of <code>hwy</code>:

<code>ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = <a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a>(class, hwy, FUN = median), y = hwy))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/ef621dd0d8904e2091d1ed7db8553d90f89f33a5/29adc/eda_files/figure-html/unnamed-chunk-25-1.png">
If you have long variable names, <code>geom_boxplot()</code> will work better if you flip it 90°. 
You can do that with <code>coord_flip()</code>.

<code>ggplot(data = mpg) +
  geom_boxplot(mapping = aes(x = <a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a>(class, hwy, FUN = median), y = hwy)) +
  coord_flip()</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/46d22e252c5cc27cebbd2568030ac57be1e53ef9/f34fd/eda_files/figure-html/unnamed-chunk-26-1.png">

<h4>Exercises</h4>

Use what you’ve learned to improve the visualisation of the departure times
of cancelled vs. 
non-cancelled flights.

What variable in the diamonds dataset is most important for predicting
the price of a diamond? How is that variable correlated with cut?
Why does the combination of those two relationships lead to lower quality
diamonds being more expensive?

Install the ggstance package, and create a horizontal boxplot.
How does this compare to using <code>coord_flip()</code>?

One problem with boxplots is that they were developed in an era of
much smaller datasets and tend to display a prohibitively large
number of “outlying values”. 
One approach to remedy this problem is
the letter value plot. 
Install the lvplot package, and try using
<code>geom_lv()</code> to display the distribution of price vs cut. 
What
do you learn? How do you interpret the plots?

Compare and contrast <code>geom_violin()</code> with a facetted <code>geom_histogram()</code>,
or a coloured <code>geom_freqpoly()</code>. 
What are the pros and cons of each
method?

If you have a small dataset, it’s sometimes useful to use <code>geom_jitter()</code>
to see the relationship between a continuous and categorical variable.
The ggbeeswarm package provides a number of methods similar to
<code>geom_jitter()</code>. 
List them and briefly describe what each one does.

<h3>Two categorical variables</h3>
To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination. 
One way to do that is to rely on the built-in <code>geom_count()</code>:

<code>ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/7b7010bda735b55f17db8d3afffacd63b833d147/12920/eda_files/figure-html/unnamed-chunk-27-1.png">
The size of each circle in the plot displays how many observations occurred at each combination of values. 
Covariation will appear as a strong correlation between specific x values and specific y values.

Another approach is to compute the count with dplyr:

<code>diamonds %>% 
  count(color, cut)
#> # A tibble: 35 x 3
#>   color cut           n
#>   &lt;ord> &lt;ord>     &lt;int>
#> 1 D     Fair        163
#> 2 D     Good        662
#> 3 D     Very Good  1513
#> 4 D     Premium    1603
#> 5 D     Ideal      2834
#> 6 E     Fair        224
#> # … with 29 more rows</code>
Then visualise with <code>geom_tile()</code> and the fill aesthetic:

<code>diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
    geom_tile(mapping = aes(fill = n))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/6076a5cc3814716d72982f5025c9a9fcfe271135/d09ef/eda_files/figure-html/unnamed-chunk-29-1.png">
If the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. 
For larger plots, you might want to try the d3heatmap or heatmaply packages, which create interactive plots.

<h4>Exercises</h4>

How could you rescale the count dataset above to more clearly show
the distribution of cut within colour, or colour within cut?

Use <code>geom_tile()</code> together with dplyr to explore how average flight
delays vary by destination and month of year. 
What makes the
plot difficult to read? How could you improve it?

Why is it slightly better to use <code>aes(x = color, y = cut)</code> rather
than <code>aes(x = cut, y = color)</code> in the example above?

<h3>Two continuous variables</h3>
You’ve already seen one great way to visualise the covariation between two continuous variables: draw a scatterplot with <code>geom_point()</code>. 
You can see covariation as a pattern in the points. 
For example, you can see an exponential relationship between the carat size and price of a diamond.

<code>ggplot(data = diamonds) +
  geom_point(mapping = aes(x = carat, y = price))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/fce2e7b428f14530f408483742ecd32043598796/21088/eda_files/figure-html/unnamed-chunk-30-1.png">
Scatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black (as above).
You’ve already seen one way to fix the problem: using the <code>alpha</code> aesthetic to add transparency.

<code>ggplot(data = diamonds) + 
  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/fec521e4f29a7d938da5f243cf3e1cd8d8a514ec/344ef/eda_files/figure-html/unnamed-chunk-31-1.png">
But using transparency can be challenging for very large datasets. 
Another solution is to use bin. 
Previously you used <code>geom_histogram()</code> and <code>geom_freqpoly()</code> to bin in one dimension. 
Now you’ll learn how to use <code>geom_bin2d()</code> and <code>geom_hex()</code> to bin in two dimensions.

<code>geom_bin2d()</code> and <code>geom_hex()</code> divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. 
<code>geom_bin2d()</code> creates rectangular bins. 
<code>geom_hex()</code> creates hexagonal bins. 
You will need to install the hexbin package to use <code>geom_hex()</code>.

<code>ggplot(data = smaller) +
  geom_bin2d(mapping = aes(x = carat, y = price))

# install.packages("hexbin")
ggplot(data = smaller) +
  geom_hex(mapping = aes(x = carat, y = price))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/bd8faa5a44198b42b39eb84a94a8aa9090723aec/9a45f/eda_files/figure-html/unnamed-chunk-32-1.png">


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/48c515b1c463019bf93e32b648beeea37d848911/dc464/eda_files/figure-html/unnamed-chunk-32-2.png">

Another option is to bin one continuous variable so it acts like a categorical variable. 
Then you can use one of the techniques for visualising the combination of a categorical and a continuous variable that you learned about. 
For example, you could bin <code>carat</code> and then for each group, display a boxplot:

<code>ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/6a7c21b0a75c6cd54a707e6be1190559e1780dfe/97fb8/eda_files/figure-html/unnamed-chunk-33-1.png">
<code>cut_width(x, width)</code>, as used above, divides <code>x</code> into bins of width <code>width</code>. 
By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summarises a different number of points. 
One way to show that is to make the width of the boxplot proportional to the number of points with <code>varwidth = TRUE</code>.

Another approach is to display approximately the same number of points in each bin. 
That’s the job of <code>cut_number()</code>:

<code>ggplot(data = smaller, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/85b4b0538dc988dcf6ad4fb5c64ef5a45d286ea5/82b02/eda_files/figure-html/unnamed-chunk-34-1.png">

<h4>Exercises</h4>

Instead of summarising the conditional distribution with a boxplot, you
could use a frequency polygon. 
What do you need to consider when using
<code>cut_width()</code> vs <code>cut_number()</code>? How does that impact a visualisation of
the 2d distribution of <code>carat</code> and <code>price</code>?

Visualise the distribution of carat, partitioned by price.

How does the price distribution of very large diamonds compare to small
diamonds? Is it as you expect, or does it surprise you?

Combine two of the techniques you’ve learned to visualise the
combined distribution of cut, carat, and price.

Two dimensional plots reveal outliers that are not visible in one
dimensional plots. 
For example, some points in the plot below have an
unusual combination of <code>x</code> and <code>y</code> values, which makes the points outliers
even though their <code>x</code> and <code>y</code> values appear normal when examined separately.

<code>ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = <a href="https://rdrr.io/r/base/c.html">c</a>(4, 11), ylim = <a href="https://rdrr.io/r/base/c.html">c</a>(4, 11))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/b75ede65f85da37195fc8d31cae5f70efcd5e0b0/b8a4a/eda_files/figure-html/unnamed-chunk-35-1.png">
Why is a scatterplot a better display than a binned plot for this case?

<h3>Patterns and models</h3>
Patterns in your data provide clues about relationships. 
If a systematic relationship exists between two variables it will appear as a pattern in the data. 
If you spot a pattern, ask yourself:

Could this pattern be due to coincidence (i.e. 
random chance)?

How can you describe the relationship implied by the pattern?

How strong is the relationship implied by the pattern?

What other variables might affect the relationship?

Does the relationship change if you look at individual subgroups of the data?

A scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. 
The scatterplot also displays the two clusters that we noticed above.

<code>ggplot(data = faithful) + 
  geom_point(mapping = aes(x = eruptions, y = waiting))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/7427e8aa85fc6150d830776fca343b9d39f239bc/35c23/eda_files/figure-html/unnamed-chunk-36-1.png">
Patterns provide one of the most useful tools for data scientists because they reveal covariation. 
If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. 
If two variables covary, you can use the values of one variable to make better predictions about the values of the second. 
If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.

Models are a tool for extracting patterns out of data. 
For example, consider the diamonds data. 
It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. 
It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. 
The following code fits a model that predicts <code>price</code> from <code>carat</code> and then computes the residuals (the difference between the predicted value and the actual value). 
The residuals give us a view of the price of the diamond, once the effect of carat has been removed.

<code><a href="https://rdrr.io/r/base/library.html">library</a>(<a href="https://modelr.tidyverse.org">modelr</a>)

mod &lt;- <a href="https://rdrr.io/r/stats/lm.html">lm</a>(<a href="https://rdrr.io/r/base/Log.html">log</a>(price) ~ <a href="https://rdrr.io/r/base/Log.html">log</a>(carat), data = diamonds)

diamonds2 &lt;- diamonds %>% 
  <a href="https://modelr.tidyverse.org/reference/add_residuals.html">add_residuals</a>(mod) %>% 
  mutate(resid = <a href="https://rdrr.io/r/base/Log.html">exp</a>(resid))

ggplot(data = diamonds2) + 
  geom_point(mapping = aes(x = carat, y = resid))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/0db11cf59fcd403141ac4a164f4163141c9ecec0/0d9cf/eda_files/figure-html/unnamed-chunk-37-1.png">
Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.

<code>ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))</code>


<img class="lazy" data-src="https://d33wubrfki0l68.cloudfront.net/7bd23f88dac60d143f81be465a23a687fadad9fb/de235/eda_files/figure-html/unnamed-chunk-38-1.png">
You’ll learn how models, and the modelr package, work in the final part of the book, <a href="model-intro.html#model-intro">model</a>. 
We’re saving modelling for later because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand.

<h3>ggplot2 calls</h3>
As we move on from these introductory chapters, we’ll transition to a more concise expression of ggplot2 code. 
So far we’ve been very explicit, which is helpful when you are learning:

<code>ggplot(data = faithful, mapping = aes(x = eruptions)) + 
  geom_freqpoly(binwidth = 0.25)</code>
Typically, the first one or two arguments to a function are so important that you should know them by heart. 
The first two arguments to <code>ggplot()</code> are <code>data</code> and <code>mapping</code>, and the first two arguments to <code>aes()</code> are <code>x</code> and <code>y</code>. 
In the remainder of the book, we won’t supply those names. 
That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what’s different between plots. 
That’s a really important programming concern that we’ll come back in <a href="functions.html#functions">functions</a>.

Rewriting the previous plot more concisely yields:

<code>ggplot(faithful, aes(eruptions)) + 
  geom_freqpoly(binwidth = 0.25)</code>
Sometimes we’ll turn the end of a pipeline of data transformation into a plot. 
Watch for the transition from <code><a href="https://modelr.tidyverse.org/reference/pipe.html">%>%</a></code> to <code><a href="https://rdrr.io/r/base/Arithmetic.html">+</a></code>. 
I wish this transition wasn’t necessary but unfortunately ggplot2 was created before the pipe was discovered.

<code>diamonds %>% 
  count(cut, clarity) %>% 
  ggplot(aes(clarity, cut, fill = n)) + 
    geom_tile()</code>

<h3>Learning more</h3>
If you want to learn more about the mechanics of ggplot2, I’d highly recommend grabbing a copy of the ggplot2 book: <a href="https://amzn.com/331924275X">https://amzn.com/331924275X</a>. 
It’s been recently updated, so it includes dplyr and tidyr code, and has much more space to explore all the facets of visualisation. 
Unfortunately the book isn’t generally available for free, but if you have a connection to a university you can probably get an electronic version for free through SpringerLink.

Another useful resource is the <a href="https://amzn.com/1449316956"><em>R Graphics Cookbook</em></a> by Winston Chang. 
Much of the contents are available online at <a href="http://www.cookbook-r.com/Graphs/">http://www.cookbook-r.com/Graphs/</a>.

I also recommend <a href="https://amzn.com/1498715230"><em>Graphical Data Analysis with R</em></a>, by Antony Unwin. 
This is a book-length treatment similar to the material covered in this chapter, but has the space to go into much greater depth.

<h2>Tibbles</h2>
Tibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). 

Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects.

Here we will describe the <strong>tibble</strong> package, which provides opinionated data frames that make working in the tidyverse a little easier. 
In most places, I’ll use the term tibble and data frame interchangeably; when I want to draw particular attention to R’s built-in data frame, I’ll call them <code>data.frame</code>s.

If this chapter leaves you wanting to learn more about tibbles, you might enjoy <code>vignette("tibble")</code>.

<h3>Prerequisites</h3>
In this chapter we’ll explore the <strong>tibble</strong> package, part of the core tidyverse.

<code><a href="https://rdrr.io/r/base/library.html">library</a>(<a href="http://tidyverse.tidyverse.org">tidyverse</a>)</code>

<h3>Creating tibbles</h3>
Almost all of the functions that you’ll use in this book produce tibbles, as tibbles are one of the unifying features of the tidyverse. 
Most other R packages use regular data frames, so you might want to coerce a data frame to a tibble. 
You can do that with <code>as_tibble()</code>:

<code>as_tibble(iris)
#> # A tibble: 150 x 5
#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#>          &lt;dbl>       &lt;dbl>        &lt;dbl>       &lt;dbl> &lt;fct>  
#> 1          5.1         3.5          1.4         0.2 setosa 
#> 2          4.9         3            1.4         0.2 setosa 
#> 3          4.7         3.2          1.3         0.2 setosa 
#> 4          4.6         3.1          1.5         0.2 setosa 
#> 5          5           3.6          1.4         0.2 setosa 
#> 6          5.4         3.9          1.7         0.4 setosa 
#> # … with 144 more rows</code>

You can create a new tibble from individual vectors with <code>tibble()</code>. 
<code>tibble()</code> will automatically recycle inputs of length 1, and allows you to refer to variables that you just created, as shown below.

<code>tibble(
  x = 1:5, 
  y = 1, 
  z = x ^ 2 + y
)
#> # A tibble: 5 x 3
#>       x     y     z
#>   &lt;int> &lt;dbl> &lt;dbl>
#> 1     1     1     2
#> 2     2     1     5
#> 3     3     1    10
#> 4     4     1    17
#> 5     5     1    26</code>

If you’re already familiar with <code><a href="https://rdrr.io/r/base/data.frame.html">data.frame()</a></code>, note that <code>tibble()</code> does much less: it never changes the type of the inputs (e.g. 
it never converts strings to factors!), it never changes the names of variables, and it never creates row names.

It’s possible for a tibble to have column names that are not valid R variable names, aka <strong>non-syntactic</strong> names. 
For example, they might not start with a letter, or they might contain unusual characters like a space. 
To refer to these variables, you need to surround them with backticks, <code>`</code>:

<code>tb &lt;- tibble(
  `:)` = "smile", 
  ` ` = "space",
  `2000` = "number"
)
tb
#> # A tibble: 1 x 3
#>   `:)`  ` `   `2000`
#>   &lt;chr> &lt;chr> &lt;chr> 
#> 1 smile space number</code>

You’ll also need the backticks when working with these variables in other packages, like ggplot2, dplyr, and tidyr.

Another way to create a tibble is with <code>tribble()</code>, short for <strong>tr</strong>ansposed tibble. 
<code>tribble()</code> is customised for data entry in code: column headings are defined by formulas (i.e. 
they start with <code><a href="https://rdrr.io/r/base/tilde.html">~</a></code>), and entries are separated by commas. 
This makes it possible to lay out small amounts of data in easy to read form.

<code>tribble(
  ~x, ~y, ~z,
  #--|--|----
  "a", 2, 3.6,
  "b", 1, 8.5
)
#> # A tibble: 2 x 3
#>   x         y     z
#>   &lt;chr> &lt;dbl> &lt;dbl>
#> 1 a         2   3.6
#> 2 b         1   8.5</code>

I often add a comment (the line starting with <code>#</code>), to make it really clear where the header is.

<h3>Tibbles vs. 
data.frame</h3>
There are two main differences in the usage of a tibble vs. 
a classic <code>data.frame</code>: printing and subsetting.

<h3>Printing</h3>
Tibbles have a refined print method that shows only the first 10 rows, and all the columns that fit on screen. 
This makes it much easier to work with large data. 
In addition to its name, each column reports its type, a nice feature borrowed from <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code>:

<code>tibble(
  a = lubridate::<a href="http://lubridate.tidyverse.org/reference/now.html">now</a>() + <a href="https://rdrr.io/r/stats/Uniform.html">runif</a>(1e3) * 86400,
  b = lubridate::<a href="http://lubridate.tidyverse.org/reference/now.html">today</a>() + <a href="https://rdrr.io/r/stats/Uniform.html">runif</a>(1e3) * 30,
  c = 1:1e3,
  d = <a href="https://rdrr.io/r/stats/Uniform.html">runif</a>(1e3),
  e = <a href="https://rdrr.io/r/base/sample.html">sample</a>(letters, 1e3, replace = TRUE)
)
#> # A tibble: 1,000 x 5
#>   a                   b              c     d e    
#>   &lt;dttm>              &lt;date>     &lt;int> &lt;dbl> &lt;chr>
#> 1 2020-10-09 13:55:17 2020-10-16     1 0.368 n    
#> 2 2020-10-10 08:00:26 2020-10-21     2 0.612 l    
#> 3 2020-10-10 02:24:06 2020-10-31     3 0.415 p    
#> 4 2020-10-09 15:45:23 2020-10-30     4 0.212 m    
#> 5 2020-10-09 12:09:39 2020-10-27     5 0.733 i    
#> 6 2020-10-09 23:10:37 2020-10-23     6 0.460 n    
#> # … with 994 more rows</code>

Tibbles are designed so that you don’t accidentally overwhelm your console when you print large data frames. 
But sometimes you need more output than the default display. 
There are a few options that can help.

First, you can explicitly <code><a href="https://rdrr.io/r/base/print.html">print()</a></code> the data frame and control the number of rows (<code>n</code>) and the <code>width</code> of the display. 
<code>width = Inf</code> will display all columns:

<code>nycflights13::<a href="https://rdrr.io/pkg/nycflights13/man/flights.html">flights</a> %>% 
  <a href="https://rdrr.io/r/base/print.html">print</a>(n = 10, width = Inf)</code>

You can also control the default print behaviour by setting options:

<ul>
<code><a href="https://rdrr.io/r/base/options.html">options(tibble.print_max = n, tibble.print_min = m)</a></code>: if more than <code>n</code>
rows, print only <code>m</code> rows. 
Use <code><a href="https://rdrr.io/r/base/options.html">options(tibble.print_min = Inf)</a></code> to always show all rows.

Use <code><a href="https://rdrr.io/r/base/options.html">options(tibble.width = Inf)</a></code> to always print all columns, regardless of the width of the screen.

</ul>
You can see a complete list of options by looking at the package help with <code>package?tibble</code>.

A final option is to use RStudio’s built-in data viewer to get a scrollable view of the complete dataset. 
This is also often useful at the end of a long chain of manipulations.

<code>nycflights13::<a href="https://rdrr.io/pkg/nycflights13/man/flights.html">flights</a> %>% 
  <a href="https://rdrr.io/r/utils/View.html">View</a>()</code>

<h3>Subsetting</h3>
So far all the tools you’ve learned have worked with complete data frames. 
If you want to pull out a single variable, you need some new tools, <code><a href="https://rdrr.io/r/base/Extract.html">$</a></code> and <code><a href="https://rdrr.io/r/base/Extract.html">[[</a></code>. 
<code><a href="https://rdrr.io/r/base/Extract.html">[[</a></code> can extract by name or position; <code><a href="https://rdrr.io/r/base/Extract.html">$</a></code> only extracts by name but is a little less typing.

<code>df &lt;- tibble(
  x = <a href="https://rdrr.io/r/stats/Uniform.html">runif</a>(5),
  y = <a href="https://rdrr.io/r/stats/Normal.html">rnorm</a>(5)
)

# Extract by name
df$x
#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161
df[["x"]]
#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161

# Extract by position
df[[1]]
#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161</code>

To use these in a pipe, you’ll need to use the special placeholder <code>.</code>:

<code>df %>% .$x
#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161
df %>% .[["x"]]
#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161</code>

Compared to a <code>data.frame</code>, tibbles are more strict: they never do partial matching, and they will generate a warning if the column you are trying to access does not exist.

<h3>Interacting with older code</h3>
Some older functions don’t work with tibbles. 
If you encounter one of these functions, use <code><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame()</a></code> to turn a tibble back to a <code>data.frame</code>:

<code><a href="https://rdrr.io/r/base/class.html">class</a>(<a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a>(tb))
#> [1] "data.frame"</code>

The main reason that some older functions don’t work with tibble is the <code><a href="https://rdrr.io/r/base/Extract.html">[</a></code> function. 
We don’t use <code><a href="https://rdrr.io/r/base/Extract.html">[</a></code> much in this book because <code><a href="https://dplyr.tidyverse.org/reference/filter.html">dplyr::filter()</a></code> and <code><a href="https://dplyr.tidyverse.org/reference/select.html">dplyr::select()</a></code> allow you to solve the same problems with clearer code (but you will learn a little about it in <a href="vectors.html#vector-subsetting">vector subsetting</a>). 
With base R data frames, <code><a href="https://rdrr.io/r/base/Extract.html">[</a></code> sometimes returns a data frame, and sometimes returns a vector. 
With tibbles, <code><a href="https://rdrr.io/r/base/Extract.html">[</a></code> always returns another tibble.

<h3>Exercises</h3>
<ol style="list-style-type: decimal">
How can you tell if an object is a tibble? (Hint: try printing <code>mtcars</code>,
which is a regular data frame).

Compare and contrast the following operations on a <code>data.frame</code> and equivalent tibble. 
What is different? Why might the default data frame behaviours cause you frustration?

<code>df &lt;- <a href="https://rdrr.io/r/base/data.frame.html">data.frame</a>(abc = 1, xyz = "a")
df$x
df[, "xyz"]
df[, <a href="https://rdrr.io/r/base/c.html">c</a>("abc", "xyz")]</code>

If you have the name of a variable stored in an object, e.g. 
<code>var &lt;- "mpg"</code>,
how can you extract the reference variable from a tibble?

Practice referring to non-syntactic names in the following data frame by:

<ol style="list-style-type: decimal">
Extracting the variable called <code>1</code>.

Plotting a scatterplot of <code>1</code> vs <code>2</code>.

Creating a new column called <code>3</code> which is <code>2</code> divided by <code>1</code>.

Renaming the columns to <code>one</code>, <code>two</code> and <code>three</code>.

<code>annoying &lt;- tibble(
  `1` = 1:10,
  `2` = `1` * 2 + <a href="https://rdrr.io/r/stats/Normal.html">rnorm</a>(<a href="https://rdrr.io/r/base/length.html">length</a>(`1`))
)</code>

What does <code><a href="https://tibble.tidyverse.org/reference/enframe.html">tibble::enframe()</a></code> do? When might you use it?

What option controls how many additional column names are printed at the footer of a tibble?

<h2>differences in histograms</h2>
ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut))

barplot(table(diamonds$cut))
barplot(prop.table(table(diamonds$cut)))

hist(table(diamonds$cut))
hist(prop.table(table(diamonds$cut)))

<h2>Count frequency</h2>
diamonds %>%  count(cut)
table(diamonds$cut)
tabulate(diamonds$cut)

library("plyr")
count(diamonds, 'cut')

<h2>generate sine form values</h2>
0-2pi, steps 15 degree

sinvalue = numeric()
cosvalue = numeric()
sinvalueAdd45 = numeric()
cosvalueAdd45 = numeric()
elements = seq(0, 4*pi, 15*pi/180)
for (i in elements) sinvalue = c(sinvalue, sin(i))
for (i in elements) sinvalueAdd45 = c(sinvalueAdd45, sin(i+45*pi/180))
for (i in elements) cosvalue = c(cosvalue, cos(i))
for (i in elements) cosvalueAdd45 = c(cosvalueAdd45, cos(i+45*pi/180))
sinvalue = round(sinvalue, 4)
sinvalueAdd45 = round(sinvalueAdd45, 4)
cosvalue = round(cosvalue, 4)
cosvalueAdd45 = round(cosvalueAdd45, 4)
cat(sinvalue, ",", sinvalueAdd45, ",", cosvalue, ",", cosvalueAdd45)

<h2>datatble filter and grouping</h2>
create a data set with four variables
dt <- data.table(
  grp = factor(sample(1L:3L, 1e6, replace = TRUE)),
  x = rnorm(1e6),
  y = rnorm(1e6),
  z = sample(c(1:10, NA), 1e6, replace = TRUE)
)

dt[x < -.5, x_cat := "low"]
dt[x >= -.5 & x < .5, x_cat := "moderate"]
dt[x >= .5, x_cat := "high"]
This filters by the condition and then assigns values to x_cat either low, moderate, or high. 


<h2>Syntax of ifelse() function</h2>
ifelse(test_expression, x, y)

a = c(5,7,2,9)
ifelse(a %% 2 == 0,"even","odd")
[1] "odd"  "odd"  "even" "odd" 

<h2>select a subset</h2>
m <- matrix(1:20, ncol = 4) 
colnames(m) <- letters[1:4]

choose = c(16, 17, 18)
subset(m, m[,4] == choose)

<h2>subset of data table</h2>
aTable <- data.table(a=sample(c('a', 'b', 'c'), 10, replace=TRUE),
                 b=sample(c('a', 'b', 'c'), 10, replace=TRUE),
                 c=sample(10), key=c('a', 'b'))
chosen = c("a","c")
subset(aTable, b %in% chosen)

<h2>read ctv file</h2>
options("encoding" = "native.enc")
#options("encoding" = "UTF-8")
Sys.setlocale(category = 'LC_ALL', 'Chinese')	# this must be added to script to show chinese

dirStr = "D:/yhzq/T0002/export"
setwd(dirStr)
datafildname = "a210507.txt"
datafile = read.csv(datafildname, encoding="UTF-8", header=F, sep="\t")
head(datafile)

colnames(datafile) <- c("stkCode", "chiName", "U/D","amt","price")
datafile = datafile[,c("stkCode", "chiName", "U/D","amt","price")]

<h2>to grep multiple string variables</h2>
strs = c("whether in the any", "of the strings", "in the pattern", "in the")
toMatch = c("whether", "in", "pattern")
# use of the paste | method
haveMatch = grep(paste(toMatch,collapse="|"), atr)
matches <- unique(haveMatch)

<h2>tab width</h2>
The tab width is saved within a user-preference file located at:

%LOCALAPPDATA%\RStudio-Desktop\monitored\user-settings\user-settings
If RStudio is unable to read / write that file for some reason, then it will default back to using 2 spaces for the tab width.

It might be worth trying to reset your RStudio's state: https://support.rstudio.com/hc/en-us/articles/200534577-Resetting-RStudio-Desktop-s-State

<h2>table and count function comparison</h2>
The table() function gives the counts of a categorical variable, but the output is not a data frame – it’s a table, and it’s not easily accessible like a data frame. You can convert this to a data frame, but the result does not retain the variable name in the corresponding column name.
With Complements to the “plyr” Package
count() to the Rescue!
https://www.r-bloggers.com/2015/02/how-to-get-the-frequency-table-of-a-categorical-variable-as-a-data-frame-in-r/

w = table(mtcars$gear)
class(w)
[1] "table"

<h2>to chop large blocks into small blocks</h2>
# length of out = 99550
# chop into 20 blocks
for(i in 0:18){
     block = out[(i*5000+1):((i+1)*5000)]
	sink(paste0(theOutName,i,".html"))
	cat(htmlHeader, sep="\n")
	cat(block, sep="\n")
	cat(htmlTail, sep="\n")
	sink()
}
# this is final block
     block = out[95001:length(out)]
	sink(paste0(theOutName,"20.html"))
	cat(htmlHeader, sep="\n")
	cat(block, sep="\n")
	cat(htmlTail, sep="\n")
	sink()

# testing sequence
for(i in 0:18){
     cat(i*5000+1,(i+1)*5000, " ")
}

# testing sequence
for(i in 0:18){
     cat(i*5000+1,(i+1)*5000, " ")
     block = out[(i*5000+1):((i+1)*5000)]
     cat(length(block))
}

<h2>avoid script break by escape key</h2>
  longLine = readline()
  if((longLine == "as.raw(27)") | (longLine  ==  "")) {
    cat(yellow("\n\nScript Ended!\n\n"))
    break
  }

<h2>Packages For Natural Language Processing NLP</h2>
<h3>1 koRpus</h3>
It includes a diverse collection of functions for automatic language detection. 
It also includes indices of lexical diversity, such as type token ratio, MTLD, etc. 
koRpus' also provides a plugin for R GUI as well as IDE RKWard that assists in providing graphical dialogs for its basic features.&nbsp;
Know more <a href="https://cran.r-project.org/web/packages/koRpus/index.html" data-wpel-link="external" target="_blank">here</a>.
<h3>2 lsa</h3>
Latent Semantic Analysis or lsa is an R package that provides routines for performing a latent semantic analysis with R. 
The basic idea of this package is that text do have a higher-order or latent semantic structure which is obscured by word usage e.g. 
through the use of synonyms or polysemy.
Know more <a href="https://cran.r-project.org/web/packages/lsa/index.html" data-wpel-link="external" target="_blank">here</a>.
<h3>3 OpenNLP</h3>
OpenNLP provides an R interface to Apache OpenNLP, which is a collection of natural language processing tools written in Java. 
OpenNLP supports common <a href="https://analyticsindiamag.com/limits-of-transfer-learning-in-nlp/" data-wpel-link="internal">natural language processing</a> tasks such as tokenisation, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing and coreference resolution.
Know more <a href="https://cran.r-project.org/web/packages/openNLP/index.html" data-wpel-link="external" target="_blank">here</a>.
<h3>4 Quanteda</h3>
Quanteda is an R package for managing and analysing text. 
It is a fast, flexible, and comprehensive framework for quantitative text analysis in R. 
Quanteda provides functionality for corpus management, creating and manipulating tokens and ngrams, exploring keywords in context, forming and manipulating sparse matrices of documents by features and more.
Know more <a href="https://quanteda.io/" data-wpel-link="external" target="_blank">here</a>.
<h3>5 RWeka</h3>
RWeka is an interface to Weka, which is a collection of <a href="https://analyticsindiamag.com/name-language-prediction-using-recurrent-neural-network-in-pytorch/" data-wpel-link="internal">machine learning algorithms</a> for data mining tasks written in Java. 
It contains tools for data pre-processing, clustering, association rules, visualisation and more. 
This package contains an interface code, known as the Weka jar that resides in a separate package called &#8216;RWekajars'.
Know more <a href="https://cran.r-project.org/web/packages/RWeka/index.html" data-wpel-link="external" target="_blank">here</a>.
<h3>6 Spacyr</h3>
Spacyr is an R wrapper to the Python spaCy <a href="https://analyticsindiamag.com/hands-on-guide-to-sign-language-classification-using-cnn/" data-wpel-link="internal">NLP library</a>. 
The package is designed to provide easy access to the functionality of spaCy library in a simple format. 
One of the easiest methods to install spaCy and spacyr is through the spacyr function spacy_install().&nbsp;
Know more <a href="https://spacyr.quanteda.io/" data-wpel-link="external" target="_blank">here</a>.
<h3>7 Stringr</h3>
Stringr is a consistent, simple and easy to use R package that provides consistent wrappers for the string package and therefore simplifies the manipulation of character strings in R. 
It includes a set of internally consistent tools for working with character strings, i.e. 
sequences of characters surrounded by quotation marks.&nbsp;&nbsp;
See Also
<a href="https://analyticsindiamag.com/datasaur/" data-wpel-link="internal"><img width="180" height="180" src="https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-20x20.png" class="attachment-theissue-thumbnail-x2 size-theissue-thumbnail-x2 thb-lazyload lazyload wp-post-image" alt="" loading="lazy" sizes="(max-width: 180px) 100vw, 180px" data-src="https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-180x180.png" data-sizes="auto" data-srcset="https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-180x180.png 180w, https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-150x150.png 150w, https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-90x90.png 90w, https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-20x19.png 20w, https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-50x50.png 50w, https://analyticsindiamag.com/wp-content/uploads/2020/11/datasaur-96x96.png 96w" /></a>
<h6><a href="https://analyticsindiamag.com/datasaur/" title="Comprehensive Guide to Datasaur &#8211; The Text Data Annotator Tool" data-wpel-link="internal">Comprehensive Guide to Datasaur &#8211; The Text Data Annotator Tool</a></h6> 
Know more <a href="https://cran.r-project.org/web/packages/stringr/index.html" data-wpel-link="external" target="_blank">here</a>.
<h3>8 Text2vec&nbsp;</h3>
Text2vec is an R package which provides an efficient framework with a concise API for text analysis and <a href="https://analyticsindiamag.com/computer-vision-text-caption-google/" data-wpel-link="internal">natural language processing (NLP)</a>. 
Some of its important features include allowing users to easily solve complex tasks, maximise efficiency per single thread, transparently scale to multiple threads on multicore machines, use streams and iterators, among others.
Know more <a href="https://github.com/dselivanov/text2vec" data-wpel-link="external" target="_blank">here</a>.
<h3>9 TM</h3>
TM or Text Mining Package is a framework for text mining applications within R. 
The package provides a set of predefined sources, such as DirSource, DataframeSource, etc. 
which handle a directory, a vector interpreting each component as a document, or data frame like structures (such as CSV files), and more.
Know more <a href="https://cran.r-project.org/web/packages/tm/index.html" data-wpel-link="external" target="_blank">here</a>.
<h3>10 Wordcloud</h3>
Wordcloud is an R package that creates pretty word clouds, visualises differences and similarity between documents, and avoids overplotting in scatter plots with text. 
The word cloud is a commonly used plot to visualise a speech or set of documents in a clear way.&nbsp;
Know more <a href="https://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf" data-wpel-link="external" target="_blank">here</a>.

<h2>Text Analysis Packages</h2>
A Complete Overview of the Most Useful Packages in R Data Scientists Should Know About for Text Analysis
<h3>1. The All-Encompassing: Quanteda</h3>install.packages("quanteda")
library(quanteda)
This ranges from the basics in natural language processing — lexical diversity, text-preprocessing, constructing a corpus, token objects, document-feature matrix) — to more advanced statistical analysis such as wordscores or wordfish, document classification (e.g. Naive Bayes) and topic modelling.
A useful tutorial of the package is the one developed by Kohei Watanabe and Stefan Müller (<a href="https://tutorials.quanteda.io">link</a>).

<h3>2. The Transformer: Text2vec</h3>install.packages("text2vec")
library(text2vec)
This package allows you to construct a document-term matrix (dtm) or term co-occurence matrix (tcm) from documents. 
As such, you vectorize text by creating a map from words or n-grams to a vector space. 
Based on this, you can then fit a model to that dtm or tcm. 
This ranges from topic modelling (LDA, LSA), word embeddings (GloVe), collocations, similarity searches and more.
You can find a useful tutorial of the package <a href="http://text2vec.org/index.html">here</a>.

<h3>3. The Adapter: Tidytext</h3>install.packages("tidytext")
library(tidytext)
One of its benefits is that it works very well in tandem with other tidy tools in R such as dplyr or tidyr. 
You can find a useful tutorial of the package <a href="https://www.tidytextmining.com">here</a>.

<h3>4. The Matcher: Stringr</h3>install.packages("stringr")
library(stringr)
When it comes to text analysis, stringr is a particularly handy package to work with regular expressions as it provides a few useful pattern matching functions. 
Other functions include character manipulation (manipulating individual characters within the strings in character vectors) and whitespace tools (add, remove, manipulate whitespace).
The CRAN — R project has a useful tutorial on the package (<a href="https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html">link</a>).

<h3>5. The Show-Off: Spacyr</h3>install.packages("spacyr")
library(spacyr)spacy_install()spacy_initialize()
Most of you may know the spaCy package in Python. 
Well, spacyr provides a convenient wrapper of that package in R, making it easy to access the powerful functionality of spaCy in a simple format. 
To access these Python functionalities, spacyr opens a connection by being initialized within your R session.
This package is essential for more advanced natural language processing models — e.g. preparing text for deep learning — and other useful functionalities such as speech tagging, tokenization, parsing etc. 
In addition, it also works well in combination with the quanteda and tidytext packages.
You can find a useful tutorial to the package <a href="https://spacyr.quanteda.io/articles/using_spacyr.html">here</a>.

<h2>Text mine using NLP techniques</h2>
To investigate a collection of text documents (corpus) and find the words (entities) that represent the collection of words in this corpus. 
<h3>Operation Buggy</h3>
Say you are a tester and you are called in to help a DevOps team with their issue management system. 
But the only thing you have been given are text documents made by the testers, which are exports of JIRA issues they reported. 
They are large documents, and no one (including you) has time to manually read through them.

As a data scientist and QA expert, it’s your job to make sense of the data in the text documents. 
What parts of the system were tested, and which system components had the most found issues? This is where Natural Language Processing (NLP) can enter to tackle the problem, and R, the statistical computing environment with different R packages, can be used to perform NLP methods on your data. 
(Some packages include: <a href="https://www.rdocumentation.org/packages/tm/versions/0.7-5">tm</a>, <a href="https://cran.r-project.org/web/packages/textreuse/index.html">test reuse</a>, <a href="https://opennlp.apache.org/">openNLP</a>, etc.) The choice of package depends on what you want to analyze with your data.

In this example, the immediate objective is to turn a large library of text into actionable data to:




Find the issues with the highest risks (not the most buggy components of the system, because this component can also contain a lot of trivial issues).
Fix the component of the system with the most issues.



To tackle the problem, we need statistics. 
By using the statistical programming language R, we can make statistical algorithms to find the most buggy component of the system under test.

<h3>Retrieval of the data</h3>
First, we have to retrieve and preprocess the files to enable the search for the most buggy component. 
what R packages do we actually need?

These are mentioned in Table 1, including their functions.

<img class="lazy" data-src="https://sweetcode.io/wp-content/uploads/2018/08/Screen-Shot-2018-08-21-at-8.15.39-AM.png">

Table 1: R packages used

The functions of these R packages will be explained when the R packages are addressed.
Before you start to build the algorithm in R, you first have to install and load the libraries of the R packages.

After installation, every R script first starts with addressing the R libraries as shown below.

library(tm)
library(SnowballC)
library(topicmodels)

You can start with retrieving the dataset (or corpus for NLP).

For this experiment, we saved three text files with bug reports from three testers in a separate directory, also being our working directory (use setwd(“directory”) to set the working directory).

#set working directory (modify path as needed)
setwd(directory)

You can load the files from this directory in the corpus:

#load files into corpus
#get listing of .txt files in directory
filenames &lt;- list.files(getwd(),pattern="*.txt")  #getwd() represents working directory

Read the files into a character vector, which is a basic data structure and can be read by R.

#read files into a character vector
files &lt;- lapply(filenames,readLines)

We now have to create a corpus from the vector.

#create corpus from vector
articles.corpus &lt;- Corpus(VectorSource(files))

<h3>Preprocessing the data</h3>
Next, we need to preprocess the text to convert it into a format that can be processed for extracting information. 
An essential aspect involves the reduction of the size of the feature space before analyzing the text, i.e. 
normalization. 
(Several preprocessing methods are available, such as case-folding, stop word removal, stemming, lemmatization, contraction simplification etc.) What preprocessing method is necessary depends on the data we retrieve, and the kind of analysis to be performed.

Here,we use case-folding and stemming.

Case-folding to match all possible instances of a word (Auto and auto, for instance).

Stemming is the process of reducing the modified or derived words to their root form.
This way, we also match the resulting root forms.

# make each letter lowercase
articles.corpus &lt;- tm_map(articles.corpus, tolower)
#stemming
articles.corpus &lt;- tm_map(articles.corpus, stemDocument);

<h3>Create the DTM</h3>
The next step is to create a document-term matrix (DTM). 
This is critical, because to interpret and analyze the text files, they must ultimately be converted into a document-term matrix.
The DTM holds the number of term occurrences per document. 
The rows in a DTM represent the documents, and each term in a document is represented as a column. 
We’ll also remove the low-frequency words (or sparse terms) after converting the corpus into the DTM.

<h3>articleDtm &lt;- DocumentTermMatrix(articles.corpus, control = list(minWordLength = 3));
articleDtm2 &lt;- removeSparseTerms(articleDtm, sparse=0.98)</h3>
<h3>Topic modeling</h3>
We are now ready to find the words in the corpus that represent the collection of words used in the corpus: the essentials.

This is also called topic modeling.
The topic modeling technique we will use here is latent Dirichlet allocation (LDA). 
The purpose of <a href="https://www.linkedin.com/pulse/lda-explanation-gaurhari-dass">LDA</a> is to learn the representation of a fixed number of topics, and given this number of topics, learn the topic distribution that each document in a collection of documents has.

Explaining LDA goes far beyond the scope of this article. 
For now, just follow the code as written below.

#LDA
k = 5;
SEED = 1234;
article.lda &lt;- LDA(articleDtm2, k, method="Gibbs", control=list(seed = SEED))
lda.topics &lt;- as.matrix(topics(article.lda))
lda.topics
lda.terms &lt;- terms(article.lda)

If you now run the full code in R as explained above, you will calculate the essentials, the words in the corpus that represent the collection of words used in the corpus.

For this experiment, the results were:

&gt; lda.terms
    Topic 1     Topic 2     Topic 3     Topic 4     Topic 5 
     "theo" "customers"    "angela"       "crm"      "paul"

Topics 1 and 3 can be explained: theo and angela are testers.

Topic 5 is also easily explained: paul is a fixer.

Topic 4, crm, is the system under test, so it’s not surprising it shows up as a term in the LDA, because it is mentioned in every issue by every tester.

Now, we still have topic 2: customers.

Customers is a component of the system under test: crm.

Customers is most mentioned as a component in the issues found by all the testers involved.

Finally, we have found our most buggy component.

<h3>Wrap-up</h3>
This article described a method we can use to investigate a collection of text documents (corpus) and find the words that represent the collection of words in this corpus. 
For this article’s example, R (together with NLP techniques) was used to find the component of the system under test with the most issues found.

<h3>R code</h3>
library(tm)
library(SnowballC)
library(topicmodels)

# TEXT RETRIEVAL

#set working directory (modify path as needed)
ld

#load files into corpus
#get listing of .txt files in directory
filenames &lt;- list.files(getwd(),pattern="*.txt")
#read files into a character vector
files &lt;- lapply(filenames,readLines)
#create corpus from vector
articles.corpus &lt;- Corpus(VectorSource(files))

# TEXT PROCESSING

# make each letter lowercase
articles.corpus &lt;- tm_map(articles.corpus, tolower)

# stemming
articles.corpus &lt;- tm_map(articles.corpus, stemDocument);

# Ceate the Document Term Matrix (DTM)
articleDtm &lt;- DocumentTermMatrix(articles.corpus, control = list(minWordLength = 3));
articleDtm2 &lt;- removeSparseTerms(articleDtm, sparse=0.98)

# TOP MODELING

k = 5;
SEED = 1234;
article.lda &lt;- LDA(articleDtm2, k, method="Gibbs", control=list(seed = SEED))
lda.topics &lt;- as.matrix(topics(article.lda))
lda.topics
lda.terms &lt;- terms(article.lda)
lda.terms

<h2>Text Mining and Sentiment Analysis</h2>
In the third article of this series, Sanil Mhatre demonstrates how to perform a sentiment analysis using R including generating a word cloud, word associations, sentiment scores, and emotion classification. 
<a href="https://www.red-gate.com/simple-talk/sql/bi/text-mining-and-sentiment-analysis-introduction/">Text Mining and Sentiment Analysis: Introduction</a>
<a href="https://www.red-gate.com/simple-talk/sql/bi/text-mining-and-sentiment-analysis-power-bi-visualizations">Text Mining and Sentiment Analysis: Power BI Visualizations</a>
<a href="https://www.red-gate.com/simple-talk/sql/bi/text-mining-and-sentiment-analysis-with-r/">Text Mining and Sentiment Analysis: Analysis with R</a>
This is the third article of the "Text Mining and Sentiment Analysis" Series. 
The first article introduced Azure Cognitive Services and demonstrated the setup and use of Text Analytics APIs for extracting key Phrases & Sentiment Scores from text data. 
The second article demonstrated Power BI visualizations for analyzing Key Phrases & Sentiment Scores and interpreting them to gain insights. 
This article explores R for text mining and sentiment analysis. 
I will demonstrate several common text analytics techniques and visualizations in R.
Note: This article assumes basic familiarity with R and RStudio. 
Please jump to the References section for more information on installing R and RStudio. 
The Demo data raw text file and R script are available for download from my GitHub repository; please find the link in the References section.
R is a language and environment for statistical computing and graphics. 
It provides a wide variety of statistical and graphical techniques and is highly extensible. 
R is available as free software. 
It's easy to learn
and use and can produce well designed publication-quality plots. 
For the demos in this article, I am using R version 3.5.3 (2019-03-11), RStudio Version 1.1.456
The input file for this article has only one column, the "Raw text" of survey responses and is a text file.
A sample of the first few rows are shown in Notepad++ (showing all characters) in Figure 1.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-computer-description-automatica.png">
The demo R script and demo input text file are available on my GitHub repo (please find the link in the References section).
R has a rich set of packages for Natural Language Processing (NLP) and generating plots. 
The foundational steps involve loading the text file into an R Corpus, then cleaning and stemming the data before performing analysis. 
I will demonstrate these steps and analysis like Word Frequency, Word Cloud, Word Association, Sentiment Scores and Emotion Classification using various plots and charts.
<h3>Installing and loading R packages</h3>
The following packages are used in the examples in this article:
<strong>tm</strong> for text mining operations like removing numbers, special characters, punctuations and stop words (Stop words in any language are the most commonly occurring words that have very little value for NLP and should be filtered out. 
Examples of stop words in English are "the", "is", "are".)
<strong>snowballc</strong> for stemming, which is the process of reducing words to their base or root form. 
For example, a stemming algorithm would reduce the words "fishing", "fished" and "fisher" to the stem "fish".
<strong>wordcloud</strong> for generating the word cloud plot.
<strong>RColorBrewer</strong> for color palettes used in various plots
<strong>syuzhet</strong> for sentiment scores and emotion classification
<strong>ggplot2</strong> for plotting graphs
Open RStudio and create a new R Script. 
Use the following code to install and load these packages.
# Install
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
install.packages("syuzhet") # for sentiment analysis
install.packages("ggplot2") # for plotting graphs
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
<h3>Reading file data into R</h3>
The R base function <code>read.table()</code> is generally used to read a file in table format and imports data as a data frame. 
Several variants of this function are available, for importing different file formats;
<strong>read.csv() is</strong> used for reading comma-separated value (csv) files, where a comma "," is used a field separator
<strong>read.delim()</strong> is used for reading tab-separated values (.txt) files
The input file has multiple lines of text and no columns/fields (data is not tabular), so you will use the <code>readLines</code> function. 
This function takes a file (or URL) as input and returns a vector containing as many elements as the number of lines in the file. 
The <code>readLines</code> function simply extracts the text from its input source and returns each line as a character string. 
The <code>n=</code> argument is useful to read a limited number (subset) of lines from the input source (Its default value is -1, which reads all lines from the input source). 
When using the filename in this function's argument, R assumes the file is in your current working directory (you can use the <code>getwd()</code> function in R console to find your current working directory). 
You can also choose the input file interactively, using the <code>file.choose()</code> function within the argument. 
The next step is to load that Vector as a Corpus. 
In R, a Corpus is a collection of text document(s) to apply text mining or NLP routines on. 
Details of using the <code>readLines</code> function are sourced from: <a href="https://www.stat.berkeley.edu/~spector/s133/Read.html">https://www.stat.berkeley.edu/~spector/s133/Read.html</a> .
In your R script, add the following code to load the data into a corpus.
# Read the text file from local machine , choose file interactively
text &lt;- readLines(file.choose())
# Load the data as a corpus
TextDoc &lt;- Corpus(VectorSource(text))
Upon running this, you will be prompted to select the input file. 
Navigate to your file and click <em>Open </em>as shown in Figure 2.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-computer-description-automatica-1.png">
<h3>Cleaning up Text Data</h3>
Cleaning the text data starts with making transformations like removing special characters from the text. 
This is done using the <code>tm_map()</code> function to replace special characters like <code>/</code>, <code>@</code> and <code>|</code> with a space. 
The next step is to remove the unnecessary whitespace and convert the text to lower case.
Then remove the <em>stopwords</em>. 
They are the most commonly occurring words in a language and have very little value in terms of gaining useful information. 
They should be removed before performing further analysis. 
Examples of stopwords in English are "the, is, at, on<em>"</em>. 
There is no single universal list of stop words used by all NLP tools. 
<code>stopwords</code> in the <code>tm_map()</code> function supports several languages like English, French, German, Italian, and Spanish. 
Please note the language names are case sensitive. 
I will also demonstrate how to add your own list of stopwords, which is useful in this Team Health example for removing non-default stop words like "team", "company", "health". 
Next, remove numbers and punctuation.
The last step is text stemming. 
It is the process of reducing the word to its root form. 
The stemming process simplifies the word to its common origin. 
For example, the stemming process reduces the words "fishing", "fished" and "fisher" to its stem "fish". 
Please note stemming uses the <em>SnowballC</em> package. 
(You may want to skip the text stemming step if your users indicate a preference to see the original "unstemmed" words in the word cloud plot)
In your R script, add the following code to transform and run to clean-up the text data.
#Replacing "/", "@" and "|" with space
toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc &lt;- tm_map(TextDoc, toSpace, "/")
TextDoc &lt;- tm_map(TextDoc, toSpace, "@")
TextDoc &lt;- tm_map(TextDoc, toSpace, "\\|")
# Convert the text to lower case
TextDoc &lt;- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc &lt;- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc &lt;- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
TextDoc &lt;- tm_map(TextDoc, removeWords, c("s", "company", "team")) 
# Remove punctuations
TextDoc &lt;- tm_map(TextDoc, removePunctuation)
# Eliminate extra white spaces
TextDoc &lt;- tm_map(TextDoc, stripWhitespace)
# Text stemming - which reduces words to their root form
TextDoc &lt;- tm_map(TextDoc, stemDocument)
 
<h3>Building the term document matrix</h3>
After cleaning the text data, the next step is to count the occurrence of each word, to identify popular or trending topics. 
Using the function <code>TermDocumentMatrix()</code> from the text mining package, you can build a Document Matrix &ndash; a table containing the frequency of words.
In your R script, add the following code and run it to see the top 5 most frequently found words in your text.
# Build a term-document matrix
TextDoc_dtm &lt;- TermDocumentMatrix(TextDoc)
dtm_m &lt;- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v &lt;- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d &lt;- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
The following table of word frequency is the expected output of the <code>head</code> command on RStudio Console.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/word-image-1.png">
Plotting the top 5 most frequent words using a bar chart is a good basic way to visualize this word frequent data. 
In your R script, add the following code and run it to generate a bar chart, which will display in the <em>Plots</em> sections of RStudio.
# Plot the most frequent words
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
The plot can be seen in Figure 3.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati.png">
One could interpret the following from this bar chart:
The most frequently occurring word is "good". 
Also notice that negative words like "not" don't feature in the bar chart, which indicates there are no negative prefixes to change the context or meaning of the word "good" ( In short, this indicates most responses don't mention negative phrases like "not good").
"work", "health" and "feel" are the next three most frequently occurring words, which indicate that most people feel good about their work and their team's health.
Finally, the root "improv" for words like "improve", "improvement", "improving", etc. 
is also on the chart, and you need further analysis to infer if its context is positive or negative
<h3>Generate the Word Cloud</h3>
A word cloud is one of the most popular ways to visualize and analyze qualitative data. 
It's an image composed of keywords found within a body of text, where the size of each word indicates its frequency in that body of text. 
Use the word frequency data frame (table) created previously to generate the word cloud. 
In your R script, add the following code and run it to generate the word cloud and display it in the <em>Plots</em> section of RStudio.
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
Below is a brief description of the arguments used in the word cloud function;
<strong>words</strong> &#8211; words to be plotted
<strong>freq</strong> &#8211; frequencies of words
<strong>min.freq</strong> &ndash; words whose frequency is at or above this threshold value is plotted (in this case, I have set it to 5)
<strong>max.words</strong> &ndash; the maximum number of words to display on the plot (in the code above, I have set it 100)
<strong>random.order</strong> &ndash; I have set it to FALSE, so the words are plotted in order of decreasing frequency
<strong>rot.per</strong> &ndash; the percentage of words that are displayed as vertical text (with 90-degree rotation). 
I have set it 0.40 (40 %), please feel free to adjust this setting to suit your preferences
<strong>colors</strong> &ndash; changes word colors going from lowest to highest frequencies
You can see the resulting word cloud in Figure 4.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-1.png">
The word cloud shows additional words that occur frequently and could be of interest for further analysis. 
Words like "need", "support", "issu" (root for "issue(s)", etc. 
could provide more context around the most frequently occurring words and help to gain a better understanding of the main themes.
<h3>Word Association</h3>
Correlation is a statistical technique that can demonstrate whether, and how strongly, pairs of variables are related. 
This technique can be used effectively to analyze which words occur most often in association with the most frequently occurring words in the survey responses, which helps to see the context around these words
In your R script, add the following code and run it.
# Find associations 
findAssocs(TextDoc_dtm, terms = c("good","work","health"), corlimit = 0.25)
You should see the results as shown in Figure 5.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-2.png">
This script shows which words are most frequently associated with the top three terms (<code>corlimit = 0.25</code> is the lower limit/threshold I have set. 
You can set it lower to see more words, or higher to see less). 
The output indicates that "integr" (which is the root for word "integrity") and "synergi" (which is the root for words "synergy", "synergies", etc.) and occur 28% of the time with the word "good". 
You can interpret this as the context around the most frequently occurring word ("good") is positive. 
Similarly, the root of the word "together" is highly correlated with the word "work". 
This indicates that most responses are saying that teams "work together" and can be interpreted in a positive context.
You can modify the above script to find terms associated with words that occur at least 50 times or more, instead of having to hard code the terms in your script.
# Find associations for words that occur at least 50 times
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 50), corlimit = 0.25)
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-3.png">
<h3>Sentiment Scores</h3>
Sentiments can be classified as positive, neutral or negative. 
They can also be represented on a numeric scale, to better express the degree of positive or negative strength of the sentiment contained in a body of text.
This example uses the Syuzhet package for generating sentiment scores, which has four sentiment dictionaries and offers a method for accessing the sentiment extraction tool developed in the NLP group at Stanford. 
The <code>get_sentiment</code> function accepts two arguments: a character vector (of sentences or words) and a method. 
The selected method determines which of the four available sentiment extraction methods will be used. 
The four methods are <code>syuzhet</code> (this is the default), <code>bing</code>, <code>afinn</code> and <code>nrc</code>. 
Each method uses a different scale and hence returns slightly different results. 
Please note the outcome of <code>nrc</code> method is more than just a numeric score, requires additional interpretations and is out of scope for this article. 
The descriptions of the <code>get_sentiment</code> function has been sourced from : <a href="https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?">https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?</a>
Add the following code to the R script and run it.
# regular sentiment score using get_sentiment() function and method of your choice
# please note that different methods may have different scales
syuzhet_vector &lt;- get_sentiment(text, method="syuzhet")
# see the first row of the vector
head(syuzhet_vector)
# see summary statistics of the vector
summary(syuzhet_vector)
Your results should look similar to Figure 7.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-4.png">
<strong>Figure 7. 
Syuzhet vector</strong>
An inspection of the Syuzhet vector shows the first element has the value of <em>2.60</em>. 
It means the sum of the sentiment scores of all meaningful words in the first response(line) in the text file, adds up to 2.60. 
The scale for sentiment scores using the <code>syuzhet</code> method is decimal and ranges from -1(indicating most negative) to +1(indicating most positive). 
Note that the summary statistics of the <code>suyzhet</code> vector show a median value of 1.6, which is above zero and can be interpreted as the overall average sentiment across all the responses is positive.
Next, run the same analysis for the remaining two methods and inspect their respective vectors. 
Add the following code to the R script and run it.
# bing
bing_vector &lt;- get_sentiment(text, method="bing")
head(bing_vector)
summary(bing_vector)
#affin
afinn_vector &lt;- get_sentiment(text, method="afinn")
head(afinn_vector)
summary(afinn_vector)
Your results should resemble Figure 8.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-5.png">
<strong>Figure 8. 
bing and afinn vectors</strong>
Please note the scale of sentiment scores generated by:
<strong>bing</strong> &ndash; binary scale with -1 indicating negative and +1 indicating positive sentiment
<strong>afinn</strong> &ndash; integer scale ranging from -5 to +5
The summary statistics of <code>bing</code> and <code>afinn</code> vectors also show that the <code>Median</code> value of Sentiment scores is above 0 and can be interpreted as the overall average sentiment across the all the responses is positive.
Because these different methods use different scales, it's better to convert their output to a common scale before comparing them. 
This basic scale conversion can be done easily using R's built-in <code>sign</code> function, which converts all positive number to 1, all negative numbers to -1 and all zeros remain 0.
Add the following code to your R script and run it.
#compare the first row of each vector using sign function
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector))
)
Figure 9 shows the results.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-6.png">
<strong> Figure 9. 
Normalize scale and compare three vectors</strong>
Note the first element of each row (vector) is <em>1</em>, indicating that all three methods have calculated a positive sentiment score, for the first response (line) in the text.
<h3>Emotion Classification</h3>
Emotion classification is built on the NRC Word-Emotion Association Lexicon (aka EmoLex). 
The definition of "NRC Emotion Lexicon", sourced from <a href="http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm">http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</a> is "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). 
The annotations were manually done by crowdsourcing."
To understand this, explore the <code>get_nrc_sentiments</code> function, which returns a data frame with each row representing a sentence from the original file. 
The data frame has ten columns (one column for each of the eight emotions, one column for positive sentiment valence and one for negative sentiment valence). 
The data in the columns (anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive) can be accessed individually or in sets. 
The definition of <code>get_nrc_sentiments</code> has been sourced from: <a href="https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?">https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?</a>
Add the following line to your R script and run it, to see the data frame generated from the previous execution of the <code>get_nrc_sentiment</code> function.
# run nrc sentiment analysis to return data frame with each row classified as one of the following
# emotions, rather than a score: 
# anger, anticipation, disgust, fear, joy, sadness, surprise, trust 
# It also counts the number of positive and negative emotions found in each row
d&lt;-get_nrc_sentiment(text)
# head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe
head (d,10)
The results should look like Figure 10.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-7.png">
<strong>Figure 10. 
Data frame returned by get_nrc_sentiment function</strong>
The output shows that the first line of text has;
Zero occurrences of words associated with emotions of anger, disgust, fear, sadness and surprise
One occurrence each of words associated with emotions of anticipation and joy
Two occurrences of words associated with emotions of trust
Total of one occurrence of words associated with negative emotions
Total of two occurrences of words associated with positive emotions
The next step is to create two plots charts to help visually analyze the emotions in this survey text. 
First, perform some data transformation and clean-up steps before plotting charts. 
The first plot shows the total number of instances of words in the text, associated with each of the eight emotions. 
Add the following code to your R script and run it.
#transpose
td&lt;-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new &lt;- data.frame(rowSums(td[2:253]))
#Transformation and cleaning
names(td_new)[1] &lt;- "count"
td_new &lt;- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) &lt;- NULL
td_new2&lt;-td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")
You can see the bar plot in Figure 11.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-8.png">
<strong>Figure 11. 
Bar Plot showing the count of words in the text, associated with each emotion</strong>
This bar chart demonstrates that words associated with the positive emotion of "trust" occurred about five hundred times in the text, whereas words associated with the negative emotion of "disgust" occurred less than 25 times. 
A deeper understanding of the overall emotions occurring in the survey response can be gained by comparing these number as a percentage of the total number of meaningful words. 
Add the following code to your R script and run it.
#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Text", xlab="Percentage"
)
The Emotions bar plot can be seen in figure 12.
<img class="lazy" data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-9.png">
<strong>Figure 12. 
Bar Plot showing the count of words associated with each sentiment expressed as a percentage </strong>
This bar plot allows for a quick and easy comparison of the proportion of words associated with each emotion in the text. 
The emotion "trust" has the longest bar and shows that words associated with this positive emotion constitute just over 35% of all the meaningful words in this text. 
On the other hand, the emotion of "disgust" has the shortest bar and shows that words associated with this negative emotion constitute less than 2% of all the meaningful words in this text. 
Overall, words associated with the positive emotions of "trust" and "joy" account for almost 60% of the meaningful words in the text, which can be interpreted as a good sign of team health.
<h3>Conclusion</h3>
This article demonstrated reading text data into R, data cleaning and transformations. 
It demonstrated how to create a word frequency table and plot a word cloud, to identify prominent themes occurring in the text. 
Word association analysis using correlation, helped gain context around the prominent themes. 
It explored four methods to generate sentiment scores, which proved useful in assigning a numeric value to strength (of positivity or negativity) of sentiments in the text and allowed interpreting that the average sentiment through the text is trending positive. 
Lastly, it demonstrated how to implement an emotion classification with NRC sentiment and created two plots to analyze and interpret emotions found in the text.

<h2>Algorithmic Trading</h2>
<a href="https://hackernoon.com/unsupervised-machine-learning-for-fun-profit-with-basket-clusters-17a1161e7aa1" class="whitebut ">Generating Alpha with Vectorspace AI NLP/NLU Correlation Matrix</a>
<a href="https://www.datacamp.com/community/tutorials/finance-python-trading" class="whitebut ">Python For Finance: Algorithmic Trading</a>
<a href="https://www.oreilly.com/content/algorithmic-trading-in-less-than-100-lines-of-python-code/" class="whitebut ">Algorithmic trading in less than 100 lines of Python code</a>
<a href="https://github.com/llSourcell/AI_in_Finance" class="whitebut ">AI in Finance</a>
<a href="https://marutitech.com/ways-ai-transforming-finance/" class="whitebut ">5 Ways AI is Transforming the Finance Industry</a>
<a href="https://www.investopedia.com/articles/active-trading/101014/basics-algorithmic-trading-concepts-and-examples.asp" class="whitebut ">Basics of Algorithmic Trading: Concepts and Examples</a>
<a href="https://www.investopedia.com/terms/a/algorithmictrading.asp" class="whitebut ">Algorithmic Trading</a>
<a href="https://corporatefinanceinstitute.com/resources/knowledge/trading-investing/algorithmic-trading/" class="whitebut ">What is Algorithmic Trading?</a>
<a href="https://www.trality.com/blog/algorithmic-trading" class="whitebut ">algorithmic-trading account</a>
<a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/" class="whitebut ">Common Machine Learning Algorithms</a>

<h2>Tools for AI with R</h2>

C50
C50 finds application in building decision tree algorithms.

Class
‘class’ contains the knn( ) function which provides the food for constructing the k-nearest neighbours algorithm- an easy machine learning algorithm.
The knn( ) function uses the Euclidean distance method to identify the k-nearest neighbours; k is a user-specified number.

e1071
Provides the function naiveBayes( ) based on the simple application of conditional probability.

Let us try to analyse a situation where retailers need to find out what is the probability of a customer to buy bread when he has already bought butter.

Such type of analysis requires conditional probability which can be made available using e1071 package which in turn helps in finding effective business solutions.

Gmodels
During statistical analysis, we may often want to compare relationship between two nominal variables.
To explain this, let’s consider 2 nominal variables, one being ‘Income groups’ (Levels=High, Medium, Low), and the other being ‘Highest level of Education’ (Levels= Undegraduation, Graduation, Post-Graduation).We might be interested to find out whether the Income has a significant relationship with the affordability of the level of education.
Such analysis can be done using CrossTable( ) function available in gmodels package, where the results are represented in a tabular format with rows indicating the levels of one variable and the columns indicating the levels of the other variable.

Kernlab
OCR reads various characters using key dimensions.
The typical machine has to be able to distinguish the letters accurately.
Image processing is perhaps one of the most difficult tasks involved considering the amount of noise present, the positioning and orientation and how the image gets captured.
Support Vector Machine(SVM) models finds extensive applications in pattern recognition fields as it is highly dexterous in learning the complex patterns efficiently.

Neuralnet
Artificial Neural Network Algorithms (ANN) often referred to as ‘deep learning’ can be practised through the ‘neuralnet’ package.
ANN builds a model based on the understanding of how the human brain works by establishing a relationship between the input and the output signals.

RODBC
If the data is stored in SQL databases (Oracle, MySQL) or ODBC(Open Database Connectivity) and needs to be converted into R data frame, then nothing can be as effective as RODBC package to import this data frame.

rpart
For building regression trees.
Regression is a concept which involves establish relationship between a single dependant variable and independent variable(s).Suppose, a product company needs to determine how it’s sales have been due to promotions on TV, Out of Home (OOH), Newspapers, Magazines etc.
The rpart package containing the rpart() function helps explain the variance in the dependant variable( eg.
sales) caused by the independent variables(TV ads, newspaper ads, magazines).

Tm
These days lots of statistical analysis requires thorough processing of text data, be it SMS’s or mails, which involves a lot of tedious efforts.
This kind of analysis might even require removing punctuation marks, numbers and certain unwanted words like ‘but’,’or’ etc.
depending upon the business requirement.
The tm package contains flexible functions like corpus( ) which can read from pdf’s and word documents, and convert the text data into R vector and tm_map() which helps in cleaning the text data( removing blanks, conversion from upper to lower and viceversa etc.), thereby making the data ready for analysis.

Wordcloud
The package ‘wordcloud’ helps to create a diagrammatic representation of words and a user can actually customize the ‘wordcloud’ such as place the high-frequency words closer together in the centre, arrange the words in a random fashion, specify the frequency of a particular word etc.
thereby etching a long lasting impression in anyone’s mind.
Data science is driving the AI market, with organizations looking to leverage AI capabilities for predictive modeling.
To leverage these capabilities, organizations need developers trained in developing Artificial intelligence applications using R.
Businesses all over the world are looking for smarter tools and applications that help them reduce efforts and maximize profits.

<h2>Packages for sending emails from R</h2>
Here are the R packages you can use for sending emails:
<a href="https://github.com/olafmersmann/sendmailR" target="_blank">sendmailR</a>	A portable solution for sending emails from R (contains a simple SMTP client)
<a href="https://cran.r-project.org/web/packages/mail/index.html" target="_blank">mail</a>	An easy to use package for sending emails from R
<a href="https://github.com/rpremraj/mailR" target="_blank">mailR</a>	A wrapper around Apache Commons Email for sending emails from R
<a href="https://github.com/rich-iannone/blastula" target="_blank">blastula</a>	A package for creating and sending HTML emails from R through an SMTP server or Mailgun API
<a href="https://cran.r-project.org/web/packages/blatr/index.html" target="_blank">blatr</a>	A wrapper around Blat – a Windows command line utility that sends emails via SMTP or posts to Usenet via NNTP
<a href="https://github.com/r-lib/gmailr" target="_blank">gmailR</a>	A package for sending emails via the Gmail’s RESTful API
<a href="https://cran.r-project.org/web/packages/IMmailgun/index.html" target="_blank">IMmailgun</a>	A package for sending emails via the Mailgun API
<a href="https://github.com/datawookie/emayili/" target="_blank">emayili</a>	A package for sending emails from R via an SMTP server
<a href="https://github.com/omegahat/RDCOMClient" target="_blank">RDCOMClient</a>	A Windows-specific package for sending emails in R from the Outlook app
<a href="https://github.com/ropenscilabs/ponyexpress" target="_blank">ponyexpress</a>	A package to automate email sending from R via Gmail (based on the gmailR package)
We won’t focus on all of them, but we will introduce the most common and convenient options.
<h3>Sending emails in R via SMTP</h3>
Whichever R package of the following you choose, keep in mind that you need to have an SMTP server to send emails. 
In our examples, we’ll be using Mailtrap, a service providing a fake SMTP server for testing. 
<h3>sendmailR </h3>
sendmailR can be used for sending all sorts of email notifications such as completed jobs and scheduled tasks. 
At the same time, you can distribute analytical results to stakeholders using this R package as well. 
sendmailR is mostly used for SMTP servers without authentication. 
That’s why we won’t use Mailtrap in the following examples. 
Let’s install the package first:
install.packages("sendmailR",repos="http://cran.r-project.org")
Next, we create a data structure called Server, which is a map with a single key value pair – key: smtpServer, value: smtp.example.io: 
Server&lt;-list(smtpServer= "smtp.example.io")
Now, let’s write a few R lines to send a simple email:
library(sendmailR)
from &lt;- sprintf("&lt;user@sender.com>","The Sender") # the sender’s name is an optional value
to &lt;- sprintf("&lt;user@recipient.com>")
subject &lt;- "Test email subject"
body &lt;- "Test email body"
sendmail(from,to,subject,body,control=list(smtpServer= "smtp.example.io"))
The following code sample is for sending an email to multiple recipients:
from &lt;- sprintf("&lt;user@sender.com>","The Sender")
to &lt;-sprintf(c("&lt;user@recipient.com>","&lt;user2@recipient.com>", "&lt;user3@recipient.com>")
subject &lt;- "Test email subject"
body &lt;- "Test email body"
sapply(to,function(x) sendmail(from,to=x,subject,body,control=list(smtpServer= "smtp.example.io"))
And now, let’s send an email with an attachment as well:
from &lt;- sprintf("&lt;user@sender.com>","The Sender")
to &lt;- sprintf("&lt;user@recipient.com>")
subject &lt;- "Test email subject"
body &lt;- "Test email body"
attachmentPath &lt;-"C:/.../Attachment.png"
attachmentObject &lt;-mime_part(x=attachmentPath,name=attachmentName)
bodyWithAttachment &lt;- list(body,attachmentObject)
sendmail(from,to,subject,bodyWithAttachment,control=list(smtpServer= "smtp.example.io"))
NB: To send emails with sendmailR, you may also need to configure your machine so it can send emails from your local host. 
We’ve covered this step in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="/blog/setup-smtp-server/" target="_blank">How To Set Up An SMTP Server</a>.
<h3>mailR</h3>
If you employ an authentication-based SMTP server, you’d better pick the mailR package. 
It’s a wrapper around Apache Commons Email, an email library built on top of the Java Mail API. 
Due to this, mailR has a dependency on the rJava package, a low-level interface to Java VM. 
This requires Java Runtime Environment to be installed. 
You can download it from <a href="https://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html" target="_blank">Oracle</a>. 
In case of problems with pointing to the right Java binary, refer to <a href="https://github.com/s-u/rJava/issues/151" target="_blank">this troubleshooting guide</a> on GitHub. 
In practice, this may cause a bit of trouble when deploying in some environments. 
Nevertheless, mailR is a rather popular solution to automate sending emails with the R that offers the following:
multiple recipients (Cc, Bcc, and ReplyTo)multiple attachments (both from the file system and URLs)HTML formatted emails 
Install the package:
install.packages("mailR",repos="http://cran.r-project.org")
Now, we can use the Mailtrap SMTP server that requires authentication to send an email:
library(mailR)
send.mail(from = "user@sender.com",
    to = "user@recipient.com",
    subject = "Test email subject",
    body = "Test emails body",
    smtp = list(host.name = "smtp.mailtrap.io", port = 25,
          user.name = "********",
          passwd = "******", ssl = TRUE),
    authenticate = TRUE,
    send = TRUE)
Insert your Mailtrap credentials (user.name and passwd) and pick any SMTP port of 25, 465, 587, 2525.
Here is how to send an email to multiple recipients:
library(mailR)
send.mail(from = "user@sender.com",
    to = c("Recipient 1 &lt;user1@recipient.com>", "Recipient 2 &lt;user@recipient.com>"),
    cc = c("CC Recipient &lt;cc.user@recipient.com>"),
    bcc = c("BCC Recipient &lt;bcc.user@recipient.com>"),
    replyTo = c("Reply to Recipient &lt;reply-to@recipient.com>"),
    subject = "Test email subject",
    body = "Test emails body",
    smtp = list(host.name = "smtp.mailtrap.io", port = 25,
          user.name = "********",
          passwd = "******", ssl = TRUE),
    authenticate = TRUE,
    send = TRUE)
Now, let’s add a few attachments to the email:
library(mailR)
send.mail(from = "user@sender.com",
    to = c("Recipient 1 &lt;user1@recipient.com>", "Recipient 2 &lt;user@recipient.com>"),
    cc = c("CC Recipient &lt;cc.user@recipient.com>"),
    bcc = c("BCC Recipient &lt;bcc.user@recipient.com>"),
    replyTo = c("Reply to Recipient &lt;reply-to@recipient.com>"),
    subject = "Test email subject",
    body = "Test emails body",
    smtp = list(host.name = "smtp.mailtrap.io", port = 25,
          user.name = "********",
          passwd = "******", ssl = TRUE),
    authenticate = TRUE,
    send = TRUE,
    attach.files = c("./attachment.png", "https://dl.dropboxusercontent.com/u/123456/Attachment.pdf"),
    file.names = c("Attachment.png", "Attachment.pdf"), #this is an optional parameter
    file.descriptions = c("Description for Attachment.png", "Description for Attachment.pdf")) #this is an optional parameter
Eventually, let’s send an HTML email from R:
library(mailR)
send.mail(from = "user@sender.com",
    to = "user@recipient.com",
    subject = "Test email subject",
    body = "&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>",
    smtp = list(host.name = "smtp.mailtrap.io", port = 25,
          user.name = "********",
          passwd = "******", ssl = TRUE),
    authenticate = TRUE,
    send = TRUE)
You can also point to an HTML template by specifying its location, as follows:
body = "./Template.html",
<h3>blastula</h3>
The blastula package allows you to craft and send responsive HTML emails in R programming. 
We’ll review how to send emails via the SMTP server, however, blastula also supports the Mailgun API. 
Install the package:
install.packages("blastula",repos="http://cran.r-project.org")
and load it:
library(blastula)
Compose an email using Markdown formatting. 
You can also employ the following string objects:
add_readable_time – creates a nicely formatted date/time string for the current time 
add_image – transforms an image to an HTML string object
For example,
date_time &lt;- add_readable_time() # => "Thursday, November 28, 2019 at 4:34 PM (CET)"
img_file_path &lt;- "./attachment.png" # => "&lt;img cid=\"mtwhxvdnojpr__attachment.png\" src=\"data:image/png;base64,iVBORw0KG...g==\" width=\"520\" alt=\"\"/>\n"
img_string &lt;- add_image(file = img_file_path)
When composing an email, you will need the c() function to combine the strings in the email body and footer. 
You can use three main arguments: body, header, and footer. 
If you have Markdown and HTML fragments in the email body, use the md() function. 
Here is what we’ve got:
library(blastula)
email &lt;-
compose_email(
  body = md(
    c("&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>",
img_string
      )
    ),
 footer = md(
    c(
      "Test email footer", date_time, "."
      )
  )
)
Preview the email using attach_connect_email(email = email)
Now, let’s send the email. 
This can be done with the smtp_send() function through one of the following ways:
<ol>Providing the SMTP credentials directly via the creds() helper: </ol>
smtp_send(
  email = email,
  from = "user@sender.com",
  to = "user@recipient.com",
  credentials = creds(
    host = "smtp.mailtrap.io",
    port = 25,
    user = "********"
  )
)
<ol start="2">Using a credentials key that you can generate with the create_smtp_creds_key() function: </ol>
create_smtp_creds_key(
  id = "mailtrap",
  host = "smtp.mailtrap.io",
  port = 25,
  user = "********"
)
smtp_send(
  email = email,
  from = "user@sender.com",
  to = "user@recipient.com",
  credentials = creds_key("mailtrap")
)
<ol start="3">Using a credentials file that you can generate with the create_smtp_creds_file() function:</ol>
create_smtp_creds_file(
  file = "mailtrap_file",
  host = "smtp.mailtrap.io",
  port = 25,
  user = "********"
)
smtp_send(
  email = email,
  from = "user@sender.com",
  to = "user@recipient.com",
  credentials = creds_file("mailtrap_file")
)
NB: There is no way to programmatically specify a password for authentication. 
The user will be prompted to provide one during code execution.
<h3>emayili </h3>
emayili is the last package on our list for sending emails in R via SMTP. 
The package works with all SMTP servers and has minimal dependencies. 
Install it from GitHub and let’s move on:
install.packages("remotes")
library(remotes)
remotes::install_github("datawookie/emayili")
Emayili has two classes at the core:
envelope – to create emails server – to communicate with the SMTP server
Let’s create an email first:
library(emayili)
email &lt;- envelope() %>%
  from("user@sender.com") %>%
  to("user@recipient.com") %>%
  subject("Test email subject") %>%
  body("Test email body")
Now, configure the SMTP server:
smtp &lt;- server(host = "smtp.mailtrap.io",
         port = 25,
         username = "********",
         password = "*********")
To send the email to multiple recipients, enhance your emails with Cc, Bcc, and Reply-To header fields as follows:
email &lt;- envelope() %>%
  from("user@sender.com") %>%
  to(c("Recipient 1 &lt;user1@recipient.com>", "Recipient 2 &lt;user@recipient.com>")) %>%
  cc("cc@recipient.com") %>%
  bcc("bcc@recipient.com") %>%
  reply("reply-to@recipient.com") %>%
  subject("Test email subject") %>%
  body("Test email body")
You can also use the attachment() method to add attachments to your email:
email &lt;- email %>% attachment(c("./attachment.png", "https://dl.dropboxusercontent.com/u/123456/Attachment.pdf"))
Eventually, you can send your email with:
smtp(email, verbose = TRUE)
<h3>Sending emails via Gmail API – gmailR</h3>
Today, Gmail is one of the most popular email services. 
It provides RESTful API for a bunch of functionalities, such as:
send/receive HTML emails with attachmentsCRUD (create, read, update, and delete) operations with messages, drafts, threads, and labels access control of your Gmail inboxand so on
For sending emails from R via Gmail API, you need two things: the gmailR package and the API access. 
Let’s start with the latest, which requires four steps to be done:
<ol>Create a project in the Google API ConsoleEnable Gmail APISet up credentials and authentication with OAuth 2.0Download a JSON file with your credentials</ol>
We’ve described all these steps in <a href="/blog/send-emails-with-gmail-api/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">How to send emails with Gmail API</a>, so feel free to reference this blog post. 
After you’ve accomplished the preparation stage, get back to gmailR. 
The package is available on CRAN, so you can install, as follows:
install.packages("gmailr", repos="http://cran.r-project.org")
and load in your R script:
library(gmailr)
Now, you can use your downloaded JSON credentials file. 
Employ the use_secret_file() function. 
For example, if your JSON file is named GmailCredentials.json, this will look, as follows:
use_secret_file("GmailCredentials.json")
After that, create a MIME email object:
email &lt;- gm_mime() %>%
  gm_to("user@recipient.com") %>%
  gm_from("user@sender.com") %>%
  gm_subject("Test email subject") %>%
  gm_text_body("Test email body")
To create an HTML email, use markup to shape your HTML string, for example:
email &lt;- gm_mime() %>%
  gm_to("user@recipient.com") %>%
  gm_from("user@sender.com") %>%
  gm_subject("Test email subject") %>%
  gm_html_body("&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>")
To add an attachment, you can:
use the gm_attach_file() function, if the attachment has not been loaded into R. 
You can specify the MIME type yourself using the type parameter or let it be automatically guessed by mime::guess_type
email &lt;- gm_mime() %>%
  gm_to("user@recipient.com") %>%
  gm_from("user@sender.com") %>%
  gm_subject("Test email subject") %>%
  gm_html_body("&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>") %>%
  gm_attach_file("Attachment.png")
use attach_part() to attach the binary data to your file:
email &lt;- gm_mime() %>%
  gm_to("user@recipient.com") %>%
  gm_from("user@sender.com") %>%
  gm_subject("Test email subject") %>%
  gm_html_body("&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>") %>%
  gm_attach_part(part = charToRaw("attach me!"), name = "please")
If you need to include an image into HTML, you can use the &lt;img src=”cid:xy”> tag to reference the image. 
First create a plot to send, and save it to AttachImage.png:
# 1. 
use built-in mtcars data set
my_data &lt;- mtcars
# 2. 
Open file for writing
png("AttachImage.png", width = 350, height = 350)
# 3. 
Create the plot
plot(x = my_data$wt, y = my_data$mpg,
  pch = 16, frame = FALSE,
  xlab = "wt", ylab = "mpg", col = "#2E9FDF")
# 4. 
Close the file
dev.off()
Now, create an HTML email that references the plot as foobar:
email &lt;- gm_mime() %>%
  gm_to("user@recipient.com") %>%
  gm_from("user@sender.com") %>%
  gm_subject("Test email subject") %>%
  gm_html_body(
    '&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>
    &lt;br>&lt;img src="cid:foobar">'
  ) %>%
  gm_attach_file("AttachImage.png", id = "foobar")
Eventually, you can send your email:
gm_send_message(email)
Enjoying this Post?Join Our NewsletterOnly the best content, delivered once a month. 
Unsubscribe anytime.</p><form class="hustle-layout-form" novalidate="novalidate"><label for="hustle-field-email-module-1" id="hustle-field-email-module-1-label" class="hustle-screen-reader">Your email address</label><input id="hustle-field-email-module-1" type="email" class="hustle-input " name="email" value="" aria-labelledby="hustle-field-email-module-1-label" data-validate="1" data-required-error="Your email is required." data-validation-error="Please enter a valid email.">Your Email<button class="hustle-button hustle-button-submit " aria-live="polite" data-loading-text="Form is being submitted, please wait a bit.">Submit</button><input type="hidden" name="hustle_module_id" value="1"><input type="hidden" name="post_id" value="2497"><input type="hidden" name="hustle_sub_type" value="shortcode"><label for="hustle-gdpr-module-1-1" class="hustle-checkbox hustle-gdpr "><input type="checkbox" name="gdpr" id="hustle-gdpr-module-1-1" data-required-error="Please accept the terms and try again.">I have read and agree with <a href="https://mailtrap.io/privacy/" target="_blank">Mailtrap's privacy policy</a>.</label></form>
<h3>Sending emails from Outlook – RDCOMClient</h3>
R has a package for sending emails from Microsoft Outlook as well. 
It’s called RDCOMClient and allows you to connect to DCOM architecture, which you can consider an API for communicating with Microsoft Office in Windows environments. 
Let’s explore how to connect R to the Outlook app installed on your Windows.
Install RDCOMClient via an option of your choice:
from CRAN:
install.packages("RDCOMClient")
via devtools:
devtools::install_github("omegahat/RDCOMClient")
from the Windows command line:
R CMD INSTALL RDCOMClient
Warning: if you receive a message like package ‘RDCOMClient’ is not available (for R version 3.5.1)” during the installation from CRAN, try to install RDCOMClient from the source repository:
install.packages("RDCOMClient", repos = "http://www.omegahat.net/R")
Load the package, open Outlook, and create a simple email:
library(RDCOMClient)
Outlook &lt;- COMCreate("Outlook.Application")
Email = Outlook$CreateItem(0)
Email[["to"]] = "user@recipient.com"
Email[["subject"]] = "Test email subject"
Email[["body"]] = "Test email body"
If you need to change the default From: field and send from a secondary mailbox, use:
Email[["SentOnBehalfOfName"]] = "user@sender.com"
Here is how you can specify multiple recipients, as well as Cc and Bcc headers:
Email[["to"]] = "user1@recipient.com, user2@recipient.com"
Email[["cc"]] = "cc.user@recipient.com"
Email[["bcc"]] = "bcc.user@recipient.com"
To create an HTML email, use [["htmlbody"]]. 
You can simply add your HTML in the R code as follows:
library(RDCOMClient)
Outlook &lt;- COMCreate("Outlook.Application")
Email = Outlook$CreateItem(0)
Email[["to"]] = "user@recipietn.com"
Email[["subject"]] = "Test email subject"
Email[["htmlbody"]] =
"&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>"
Let’s also add an attachment: 
library(RDCOMClient)
Outlook &lt;- COMCreate("Outlook.Application")
Email = Outlook$CreateItem(0)
Email[["to"]] = "user@recipient.com"
Email[["subject"]] = "Test email subject"
Email[["htmlbody"]] =
"&lt;html>Test &lt;strong>email&lt;/strong> body&lt;/html>"
Email[["attachments"]]$Add("C:/.../Attachment.png")
Now, you can send the email:
outMail$Send()
<h3>How to send bulk emails from R?</h3>
Let’s say your mail list includes many more than ten recipients and you need to send bulk emails from R. 
We’ll show you how this can be done via Web API (gmailR) and SMTP (mailR).
<h3>Bulk emails with gmailR</h3>
As an example, we’ll inform recipients of how much they won in the lottery. 
For this, we need:
an enabled API access on your Google account. an installed gmailr R package.a set of R packages for data iteration: readr, dplyr, and purrr (or plyr as an alternative).a file containing the variable bits (lottery wins), Variables.csv, with the following format:
lastname,firstname,win_amount,email_address
SMITH,JOHN,1234,johnsmith@winner.com
LOCKWOOD,JANE,1234,janelockwood24@example.com
Now, let’s go through the mail steps to create an R script for bulk emails.
Load the packages and files we need:
suppressPackageStartupMessages(library(gmailr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library(purrr))
library(readr) # => if you don’t have it, run: install.packages("readr", repos="http://cran.r-project.org")
my_dat &lt;- read_csv("Variables.csv") 
Create a data frame that will insert variables from the file into the email: 
this_hw &lt;- "Lottery Winners"
email_sender &lt;- 'Best Lottery Ever &lt;info@best-lottery-ever.com>'
optional_bcc &lt;- 'Anonymous &lt;bcc@example.com>'
body &lt;- "Hi, %s.
Your lottery win is %s.
Thanks for betting with us!
"
edat &lt;- my_dat %>%
    mutate(
        To = sprintf('%s &lt;%s>', firstname, email_address),
        Bcc = optional_bcc,
        From = email_sender,
        Subject = sprintf('Lottery win for %s', win_amount),
        body = sprintf(body, firstname, win_amount)) %>%
    select(To, Bcc, From, Subject, body)
write_csv(edat, "data-frame.csv")
The data frame will be saved to data-frame.csv. 
This will provide an easy-to-read record of the composed emails. 
Now, convert each row of the data frame into a MIME object using the gmailr::mime() function. 
After that, purrr::pmap() generates the list of MIME objects, one per row of the input data frame:
emails &lt;- edat %>%
  pmap(mime)
str(emails, max.level = 2, list.len = 2)
If you use plyr (install.packages("plyr")), you can do this, as follows:
emails &lt;- plyr::dlply(edat, ~ To, function(x) mime(
  To = x$To,
  Bcc = x$Bcc,
  From = x$From,
  Subject = x$Subject,
  body = x$body))
Specify your JSON credentials file:
use_secret_file("GmailCredentials.json")
And send emails with purrr::safely(). 
This will protect your bulk emails from failures in the middle:
safe_send_message &lt;- safely(send_message)
sent_mail &lt;- emails %>%
  map(safe_send_message)
saveRDS(sent_mail,
        paste(gsub("\\s+", "_", this_hw), "sent-emails.rds", sep = "_"))
List recipients with TRUE in case of errors:
errors &lt;- sent_mail %>%
  transpose() %>%
  .$error %>%
  map_lgl(Negate(is.null))
Take a look at the full code now:
suppressPackageStartupMessages(library(gmailr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library(purrr))
library(readr) # => if you don’t have it, run: install.packages("readr", repos="http://cran.r-project.org")
my_dat &lt;- read_csv("Variables.csv")
this_hw &lt;- "Lottery Winners"
email_sender &lt;- 'Best Lottery Ever &lt;info@best-lottery-ever.com>'
optional_bcc &lt;- 'Anonymous &lt;bcc@example.com>'
body &lt;- "Hi, %s.
Your lottery win is %s.
Thanks for betting with us!
"
edat &lt;- my_dat %>%
    mutate(
        To = sprintf('%s &lt;%s>', firstname, email_address),
        Bcc = optional_bcc,
        From = email_sender,
        Subject = sprintf('Lottery win for %s', win_amount),
        body = sprintf(body, firstname, win_amount)) %>%
    select(To, Bcc, From, Subject, body)
write_csv(edat, "data-frame.csv")
emails &lt;- edat %>%
  pmap(mime)
str(emails, max.level = 2, list.len = 2)
use_secret_file("GmailCredentials.json")
safe_send_message &lt;- safely(send_message)
sent_mail &lt;- emails %>%
  map(safe_send_message)
saveRDS(sent_mail,
        paste(gsub("\\s+", "_", this_hw), "sent-emails.rds", sep = "_"))
errors &lt;- sent_mail %>%
  transpose() %>%
  .$error %>%
  map_lgl(Negate(is.null))
<h3>Bulk emails with mailR</h3>
If you want to send bulk emails with SMTP, make sure to have an appropriate SMTP server and install the mailR package. 
Once again, we’ll need a .csv file that will contain the data frame you want to integrate into the email. 
The data should be separated by a special character such as a comma, a semicolon, or a tab9. 
For example:
lastname; firstname; win_amount; email_address
SMITH; JOHN; 1234; johnsmith@winner.com
LOCKWOOD; JANE; 1234; janelockwood24@example.com
What you need to do next:
Build the HTML email body for a given recipient using the message_text function:
message_text &lt;- function(x) sprintf('Hello %s %s!\nCongratulation to your win.\nYour prize is XXX.\nBet with the Best Lottery Ever!', x$firstname, x$lastname)
Load the package and read in the mail list:
library(mailR)
mail_list &lt;- read.csv2("Variables.csv",as.is=TRUE)
Values in the Variables.csv should be separated with a semicolon (;). 
You can configure settings to read the data frame using the read.table or read.csv functions.
Create a file to write the information of each individual row in the mail_list after each email is sent.
my_file &lt;- file("mail.out",open="w")
# … write data here
close(my_file)
Perform the batch emailing to all students in the mail list:
for (recipient in 1:nrow(mail_list)) {
  body &lt;- message_text(mail_list[recipient,])
  send.mail(from="info@best-lottery-ever.com",
    to=as.character(mail_list[recipient,]$email_address),
    subject="Lottery Winners",
    body=body,
    html=TRUE,
    authenticate=TRUE,
    smtp = list(host.name = "smtp.mailtrap.io",
    user.name = "*****", passwd = "*****", ssl = TRUE),
    encoding = "utf-8",send=TRUE)
  print(mail_list[recipient,])
  Sys.sleep(runif(n=1,min=3,max=6))
  #write each recipient to a file
  result_file &lt;- file("mail.out",open="a")
  writeLines(text=paste0("[",recipient,"] ",
    paste0(as.character(mail_list[recipient,]),collapse="\t")),
    sep="\n",con=result_file)
  close(result_file)
}
And here is the full code:
message_text &lt;- function(x) sprintf('Hello %s %s!\nCongratulation to your win.\nYour prize is XXX.\nBet with the Best Lottery Ever!', x$firstname, x$lastname)
library(mailR)
mail_list &lt;- read.csv2("Variables.csv",as.is=TRUE)
my_file &lt;- file("mail.out",open="w")
# … write data here
close(my_file)
for (recipient in 1:nrow(mail_list)) {
  body &lt;- message_text(mail_list[recipient,])
  send.mail(from="info@best-lottery-ever.com",
    to=as.character(mail_list[recipient,]$email_address),
    subject="Lottery Winners",
    body=body,
    html=TRUE,
    authenticate=TRUE,
    smtp = list(host.name = "smtp.mailtrap.io",
    user.name = "*****", passwd = "*****", ssl = TRUE),
    encoding = "utf-8",send=TRUE)
  print(mail_list[recipient,])
  Sys.sleep(runif(n=1,min=3,max=6))
  #write each recipient to a file
  result_file &lt;- file("mail.out",open="a")
  writeLines(text=paste0("[",recipient,"] ",
    paste0(as.character(mail_list[recipient,]),collapse="\t")),
    sep="\n",con=result_file)
  close(result_file)
}
<h3>How to test email sending in R with Mailtrap</h3>
If you choose to send emails from R via SMTP, then Mailtrap is what you need for testing. 
It’s a universal service with a fake SMTP server underneath. 
This means, your test emails are not actually being sent. 
They go from your app or any other mail client to the SMTP server and are trapped there. 
Thus, you protect your real recipients from an undesirable experience – they won’t receive any of your test emails. 
All the aforementioned examples with Mailtrap credentials work in this way. 
If you need to test anything else, just replace your SMTP credentials with those of Mailtrap and that’s it. 
For this, you need to <a aria-label=" (opens in a new tab)" rel="noreferrer noopener" href="/register/signup?ref=header" target="_blank">sign up</a> first using your email, GitHub or Google account. 
A FREE FOREVER plan is available! For more on the features and functions provided by Mailtrap, read the <a href="https://help.mailtrap.io/article/12-getting-started-guide">Getting Started Guide</a>.
<h3>To wrap up</h3>
We’ve listed a number of options for sending emails in R, so choose the one that best fits your requirements. 
For example, if you need to send hundreds (or even thousands) of emails daily, gmailR may be the best solution. 
On the other hand, sending via SMTP is a more common and reliable way and R provides a few packages for this. 
So, good luck with your choice!



<script type='text/javascript' src='readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});

//d3.selectAll("h2").style("color", function() {
//   return "hsl(" + Math.random() * 360 + ",80%,50%)";
//});

</script>
</body>
</html>
