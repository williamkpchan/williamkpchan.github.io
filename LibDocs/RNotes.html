<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width"/>
<link rel="stylesheet" href="..\maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .apply, div.title').click(function(){
    parent.history.back();
    return false;
    });
});
</script>
<style>
body{width: 80%;margin-left: 10%}
li{text-align: left; margin-left: 20%;}
h1, h2 {color: gold;}
</style>
</head><body>
<center><h1>R Notes</h1>
<ul></ul>

<li>data visualization (ggplot2)</li>
<li>data manipulation (dplyr, lubridate, tidyr, stringr, readr, & forcats)</li>
<li>data analysis (combine ggplot2, dplyr to explore data and find insights)</li>
</ul>
<div id="toc"></div></center>
<br>
<br>
<br>


<pre>
<h2># MLFundStat and Hangseng Fund Stat</h2>
#=================
MLFundStat.html
the computation is long, it is possible to cut time by adjusting the cutdate variable.
this should be modified to new version using r chart.

<h2># Start Of R</h2>
#=================
Sys.setlocale(category = 'LC_ALL', 'Chinese')

use the .Rprofile.site file to run R commands for all users when their R session starts.
D:\R-3.5.1\etc\Rprofile.site
See: Initialization at startup.

This command could be an environment set:
Sys.setenv(FAME="/opt/fame")

<a href="https://www.rdocumentation.org/packages/base/versions/3.5.0/topics/Startup">Start Of R Initialization</a>

<h2># Encoding Problems</h2>
#=================
# Encoding Problems
Sys.getlocale()
getOption("encoding")
options(encoding = "UTF-8")
Encoding(txtstring) <- "UTF-8"
Encoding(txtstring)
txtstring
Sys.setlocale
Sys.setlocale(category = 'LC_ALL', 'Chinese')
Sys.setlocale(category = "LC_ALL", locale = "chs") 
Sys.setlocale(category = "LC_ALL", locale = "cht") # fanti

iconvlist()
theHeader = "http://qt.gtimg.cn/r=2&q=r_hk"
onecode = "02009"
con = url(paste0(theHeader,onecode), encoding = "GB2312")
thepage=readLines(con)
close(con)
Info=unlist(strsplit(thepage,"~"))
codename=Info[2]
codename
Encoding(codename)

==================
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
readLines(filename, encoding="UTF-8")
readLines(con = stdin(), n = -1L, ok = TRUE, warn = TRUE, encoding = "unknown", skipNul = FALSE)

<span class="redword"># note! the chiname encoding is ok inside R, but will be wrong when write to file by local pc locale, to solve the problem, set Sys.setlocale(category = 'LC_ALL', 'Chinese') </span>

readLines(con <- file("Unicode.txt", encoding = "UCS-2LE"))
close(con)
unique(Encoding(A)) # will most likely be UTF-8


</pre>

<pre>

<h2># important to remove leading zeros</h2>
#=================
# important to remove leading zeros
substr(t,regexpr("[^0]",t),nchar(t))


</pre>


<pre>

<h2>Pop up message in windows 8.1</h2>
#=================
Pop up message in windows 8.1
c.bat:  start MessageBox.vbs "This will be shown in a popup."
MessageBox.vbs :
Set objArgs = WScript.Arguments
messageText = objArgs(0)
MsgBox messageText

# options("scipen"=999)
# format(xx, scientific=F)
# options("scipen"=100, "digits"=4)
# getOption("scipen")
# or as.integer(functionResult);

df <- data.frame(matrix(ncol = 10000, nrow = 0))
colnames(df) <- c("a", "b," "c")
rm(list=ls())
Extracting a Single, Simple Table
The first step is to load the ¡§XML¡¨ package, 
then use the htmlParse() function to read the html document into an R object, 
and readHTMLTable() to read the table(s) in the document. 
The length() function indicates there is a single table in the document, simplifying our work.

The plot3d() function in the rgl package
library(rgl)
open3d()
attach(mtcars)
plot3d(disp,wt,mpg, col = rainbow(10))


</pre>

<pre>

<h2>library(stringr)</h2>
#============
library(stringr)
library(htmltools)
library(threejs)
data(mtcars)
data <- mtcars[order(mtcars$cyl),]
uv <- tabulate(mtcars$cyl)
col <- c(rep("red",uv[4]),rep("yellow",uv[6]),rep("blue",uv[8]))
row.names(mtcars)
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")

</pre>

<pre>

<h2>scatterplot3js(data[,c(3,6,1)],</h2>
#============
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")
# Gumball machine
N <- 100
i <- sample(3, N, replace=TRUE)
x <- matrix(rnorm(N*3),ncol=3)
lab <- c("small", "bigger", "biggest")
scatterplot3js(x, color=rainbow(N), labels=lab[i],
               size=i, renderer="canvas")
# Example 1 from the scatterplot3d package (cf.)
z <- seq(-10, 10, 0.1)
x <- cos(z)
y <- sin(z)
scatterplot3js(x,y,z, color=rainbow(length(z)),
   labels=sprintf("x=%.2f, y=%.2f, z=%.2f", x, y, z))
# Interesting 100,000 point cloud example, should run this with WebGL!
N1 <- 10000
N2 <- 90000
x <- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
y <- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
z <- c(rnorm(N1, sd=0.5), rpois(N2, lambda=20)-20)
col <- c(rep("#ffff00",N1),rep("#0000ff",N2))
scatterplot3js(x,y,z, color=col, size=0.25)
cat("\014")	CLS Screen
#
match returns a vector of the positions
v1 <- c("a","b","c","d")
v2 <- c("g","x","d","e","f","a","c")
x <- match(v1,v2)
6 NA  7  3
v1 %in% v2
TRUE FALSE  TRUE  TRUE
x <- match(v1,v2,nomatch=-1)
6 -1  7  3
%in% returns a logical vector indicating if there is a match or not


</pre>

<pre>

<h2>this check whether an element is inside a group</h2>
#=============
this check whether an element is inside a group
v <- c('a','b','c','e')
'b' %in% v

</pre>

<pre>

<h2>31:37 %in% 0:36</h2>
#=============
31:37 %in% 0:36
#
dmInfo=data.matrix(Info)	# convert dataframe to matrix, but the row and column is exchanged
#
bob <- data.frame(lapply(bob, as.character), stringsAsFactors=FALSE)	#Change numeric to characters
#
write.csv(Info,quote=FALSE, row.names = FALSE)	# write csv is the proper way to write the datafile
#

attach an excel file in R:
1: Install packages XLConnect and foreign and run both libraries
2: abcd <- readWorksheet(loadWorkbook('file extension'),sheet=1)
#
allocate vector of size 1.7 Gb
Try memory.limit() for the current memory limit Use memory.limit (size=50000) to increase memory limit. Try using a cloud based environment, 
try using package slam
use factors 

Concatenate and Split Strings in R
==================================
use the paste() function to concatenate
strsplit() function to split
pangram <- "The quick brown fox jumps over the lazy dog"
strsplit(pangram, " ")
"The"  "quick" "brown" "fox"  "jumps" "over" "the"  "lazy" "dog"

the unique elements
unique() function
unique(tolower(words))
"the"  "quick" "brown" "fox"  "jumps" "over" "lazy" "dog"
# find duplicates
words = unlist(strsplit(pangram, " "))
words = tolower(words)
duplicated(words)
words[duplicated(words)]

R split Function
================
split() function divides the data in a vector. 
unsplit() funtion do the reverse.
split(x, f, drop = FALSE, ...)
split(x, f, drop = FALSE, ...) <- value
unsplit(value, f, drop = FALSE)
x: vector, data frame
f: indices
drop: discard non existing levels or not


</pre>

<pre>

<h2>x <- read.csv("anova.csv",header=T,sep=",")</h2>
#=============
x <- read.csv("anova.csv",header=T,sep=",")
Subtype,Gender,Expression
A,m,-0.54
A,m,-0.8
Split the "Expression" values into two groups based on "Gender" variable, 
"f" for female group, and 
"m" for male group:
>g <- split(x$Expression, x$Gender)
>g
$f
  [1] -0.66 -1.15 -0.30 -0.40 -0.24 -0.92  0.48 -1.68 -0.80 -0.55 -0.11 -1.26
$m
  [1] -0.54 -0.80 -1.03 -0.41 -1.31 -0.43  1.01  0.14  1.42 -0.16  0.15 -0.62

Calculate the length, mean value of each group:
sapply(g,length)
  f   m 
135 146 
sapply(g,mean)
         f          m 
-0.3946667 -0.2227397

You may use lapply, return is a list:
lapply(g,mean)
unsplit() function combines the groups:
unsplit(g,x$Gender)

Apply
=====
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=30, ncol=3)
apply(m, 1, mean)
a 1 in the second argument, giving the mean of each row. 
apply(m, 2, mean)giving the mean of each column. 
apply(m, 2, function(x) length(x[x<0]))	# count -ve values
apply(m, 2, function(x) is.matrix(x))
apply(m, 2, is.vector)
apply(m, 2, function(x) mean(x[x>0]))

subset()
apply()
sapply()
lapply()
tapply()
aggregate()
apply 	apply a function to the rows or columns of a matrix
M <- matrix(seq(1,16), 4, 4)
apply(M, 1, min)
lapply 	apply a function to each element of a list in turn and get a list back
x <- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
sapply 	apply a function to each element of a list in turn, but you want a vector back
x <- list(a = 1, b = 1:3, c = 10:100)
sapply(x, FUN = length)  
vapply 	squeeze some more speed out of sapply
x <- list(a = 1, b = 1:3, c = 10:100)
vapply(x, FUN = length, FUN.VALUE = 0L) 

mapply 	apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in sapply
#Sums the 1st elements, the 2nd elements, etc. 
mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15

Map 	A wrapper to mapply with SIMPLIFY = FALSE, so it is guaranteed to return a list
rapply	For when you want to apply a function to each element of a nested list structure, recursively
tapply	For when you want to apply a function to subsets of a vector and the subsets are defined by some other vector, usually a factor
lapply is a list apply which acts on a list or vector and returns a list.
sapply is a simple lapply (function defaults to returning a vector or matrix when possible)
vapply is a verified apply (allows the return object type to be prespecified)
rapply is a recursive apply for nested lists, i.e. lists within lists
tapply is a tagged apply where the tags identify the subsets
apply is generic: applies a function to a matrix's rows or columns
by	a "wrapper" for tapply. The power of by arises when we want to compute a task that tapply can't handle
aggregate can be seen as another a different way of use tapply if we use it in such a way


xx = c(1,3,5,7,9,8,6,4,2,1,5)
duplicated(xx)
xx[duplicated(xx)]

Accessing dataframe by names:
mtcars["mpg"]
QueueNo = 12
mtcars[QueueNo,"mpg"]

some functions to remember
charToRaw(key)
as.raw(key)

A motion chart is a dynamic chart to explore several indicators over time. 
subset(airquality, Temp > 80, select = c(Ozone, Temp))
subset(airquality, Day == 1, select = -Temp)
subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp > 80))
 ## sometimes requiring a logical 'subset' argument is a nuisance nm <- rownames(state.x77) start_with_M <- nm %in% grep("^M", nm, value = TRUE)
subset(state.x77, start_with_M, Illiteracy:Murder) # but in recent versions of R this can simply be
subset(state.x77, grepl("^M", nm), Illiteracy:Murder)

join 3 dataframes
library("plyr")
join() function
names(gdp)[3] <- "GDP"
names(life_expectancy)[3] = "LifeExpectancy"
names(population)[3] = "Population"
gdp_life_exp <- join(gdp, life_expectancy)
development <- join(gdp_life_exp, population)

subset() function
dev_2005 <- subset(development, Year == 2005)
dev_2005_big <- subset(dev_2005, GDP >= 30000)

development_motion <- subset(development_complete, Country %in% selection)
library(googleVis)
gvisMotionChart() function
motion_graph <- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year")
plot(motion_graph)
motion_graph <- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "GDP", yvar = "LifeExpectancy", sizevar = "Population")
development_motion$logGDP <- log(development_motion$GDP)
motion_graph <- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "logGDP", yvar = "LifeExpectancy", sizevar = "Population")

my_list[[1]] extracts the first element of the list my_list, and my_list[["name"]] extracts the element in my_list that is called name. 
If the list is nested you can travel down the heirarchy by recursive subsetting. 
mylist[[1]][["name"]] is the element called name inside the first element of my_list.
A data frame is just a special kind of list, so you can use double bracket subsetting on data frames too. 
my_df[[1]] will extract the first column of a data frame and my_df[["name"]] will extract the column named name from the data frame.
names() and str() is a great way to explore the structure of a list.

i in 1:ncol(df)
This is a pretty common model for a sequence: a sequence of consecutive integers designed to index over one dimension of our data.
What might surprise you is that this isn't the best way to generate such a sequence, especially when you are using for loops inside your own functions. Let's look at an example where df is an empty data frame:
df <- data.frame()
1:ncol(df)
for (i in 1:ncol(df)) {
  print(median(df[[i]]))
}
Our sequence is now the somewhat non-sensical: 1, 0. You might think you wouldn't be silly enough to use a for loop with an empty data frame, but once you start writing your own functions, there's no telling what the input will be.
A better method is to use the seq_along() function.
if you grow the for loop at each iteration (e.g. using c()), your for loop will be very slow.
A general way of creating an empty vector of given length is the vector() function. 
It has two arguments: the type of the vector ("logical", "integer", "double", "character", etc.) and the length of the vector.
Then, at each iteration of the loop you must store the output in the corresponding entry of the output vector, i.e. assign the result to output[[i]]. (You might ask why we are using double brackets here when output is a vector. It's primarily for generalizability: this subsetting will work whether output is a vector or a list.)

A time series can be thought of as a vector or matrix of numbers, 
along with some information about what times those numbers were recorded. This information is stored in a ts object in R.
read in some time series data from an xlsx file using read_excel(), 
a function from the readxl package, 
and store the data as a ts object.
Use the read_excel() function to read the data from "exercise1.xlsx" into mydata.
mydata <- read_excel("exercise1.xlsx")
Create a ts object called myts using the ts() function. 
myts <- ts(mydata[,2:4], start = c(1981, 1), frequency = 4)

The first step in any data analysis task is to plot the data. 
Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. 
The features that you see in the plots must then be incorporated into the forecasting methods that you use. 
Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.
You will use the autoplot() function to produce time plots of the data. 
In each plot, look out for outliers, seasonal patterns, and other interesting features.
Use which.max() to spot the outlier in the gold series. 

library("fpp2")
autoplot(a10)
ggseasonplot(a10)
An interesting variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal. 
ggseasonplot(a10, polar = TRUE)
beer <- window(a10, start=1992)
autoplot(beer)
ggseasonplot(beer)
Use the window() function to consider only the ausbeer data from 1992 and save this to beer. 
Set a keyword start to the appropriate year.

x <- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});

</pre>

<pre>

<h2>x <- c(sort(sample(1:20, 9)), NA)</h2>
#===================
x <- c(sort(sample(1:20, 9)), NA)
y <- c(sort(sample(3:23, 7)), NA)
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)

alist = readLines("alist.txt")
blist = readLines("blist.txt")
out = setdiff(blist, alist)

writeClipboard(out)

</pre>

<pre>

<h2># To skip 3rd iteration and go to next iteration</h2>
#===================
# To skip 3rd iteration and go to next iteration
for(n in 1:5) {
  if(n==3) next
  cat(n)
}

</pre>

<pre>

<h2>googleVis chart</h2>
#===================
googleVis chart
===============
library(googleVis)
Line chart
==========
df=data.frame(country=c("US", "GB", "BR"), 
              val1=c(10,13,14), val2=c(23,12,32))
Line <- gvisLineChart(df)
plot(Line)
Scatter chart
=======================
# example 1
dat <- data.frame(x=c(1,2,3,4,5), y1=c(0,3,7,5,2), y2=c(1,NA,0,3,2))
plot(gvisScatterChart(dat, options=list(lineWidth=2, pointSize=2, width=900, height=600)))

# example 2, women
Scatter <- gvisScatterChart(women, 
                            options=list(
                              legend="none",
                              lineWidth=1, pointSize=2,
                              title="Women", vAxis="{title:'weight (lbs)'}",
                              hAxis="{title:'height (in)'}", 
                              width=900, height=600))
plot(Scatter)
# example 3
ex3dat <- data.frame(x=c(1,2,3,4,5,6,7,8), y1=c(0,3,7,5,2,0,8,6), y2=c(1,NA,0,3,2,6,4,2))
ex3 <- gvisScatterChart(ex3dat, 
                            options=list(
                              legend="none",
                              lineWidth=1, pointSize=2,
                              title="ex3", vAxis="{title:'weight (lbs)'}",
                              hAxis="{title:'height (in)'}", 
                              width=900, height=600))
plot(ex3)
# Note: to plot timeline chart, arrange the time in x axis, beginning with -ve and the last is 1 to show the sequence

</pre>

<pre>

<h2>#cat append to a file, open file in "a" mode</h2>
#===================
#cat append to a file, open file in "a" mode
theLine = paste0("\n", Selection)
updateFile <- file(thetradDaysLst, open = "a")
cat(theLine, file = updateFile)
close(updateFile)

</pre>

<pre>

<h2>install.packages("readr")</h2>
#===================
install.packages("readr")
library(readr)

</pre>

<pre>

<h2>iconv(keyword, "unknown", "GB2312")</h2>
#===================
iconv(keyword, "unknown", "GB2312")


</pre>

<pre>

<h2>Grabbing HTML Tags</h2>
#==========
Grabbing HTML Tags

<TAG\b[^>]*>(.*?)</TAG> matches the opening and closing pair of a specific HTML tag. 

Anything between the tags is captured into the first backreference. 
The question mark in the regex makes the star lazy, to make sure it stops before the first closing tag rather than before the last, like a greedy star would do. 
This regex will not properly match tags nested inside themselves, like in <TAG>one<TAG>two</TAG>one</TAG>.

<([A-Z][A-Z0-9]*)\b[^>]*>(.*?)</\1> will match the opening and closing pair of any HTML tag. 
Be sure to turn off case sensitivity. 
The key in this solution is the use of the backreference \1 in the regex. 
Anything between the tags is captured into the second backreference. 
This solution will also not match tags nested in themselves


</pre>

<pre>

<h2>find the new item</h2>
#==========
find the new item

theList = c("00700","02318","02007")
newList=c("03333","01398","02007")

newList[!(newList %in% theList)]

</pre>

<pre>

<h2>formating numbers</h2>
#==========
formating numbers
a <- seq(1,101,25)
sprintf("%03d", a)


</pre>

<pre>

<h2>the match function:</h2>
#==========
the match function:
match(x, table, nomatch = NA_integer_, incomparables = NULL)
%in%
match returns a vector of the positions of (first) matches of its first argument in its second.

Corpus<- c('animalada', 'fe', 'fernandez', 'ladrillo')
Lexicon<- c('animal', 'animalada', 'fe', 'fernandez', 'ladr', 'ladrillo')
Lexicon %in% Corpus

Lexicon[Lexicon %in% Corpus]

</pre>

<pre>

<h2>Machine Learning:</h2>
#==========
Machine Learning:

The caret package

Caret contains wrapper functions that allow you to use the exact same functions for training and predicting with dozens of different algorithms. On top of that, it includes sophisticated built-in methods for evaluating the effectiveness of the predictions you get from the model. 

Use The Titanic dataset

Training a model
training a bunch of different decision trees and having them vote 
Random forests work pretty well in *lots* of different situations, so I often try them first.

Evaluating the model

Cross-validation is a way to evaluate the performance of a model without needing any other data than the training data. 

Making predictions on the test set

Improving the model



</pre>

<pre>

<h2>to handle error 404 when scraping: use tryCatch()</h2>
#==========
to handle error 404 when scraping: use tryCatch()

for (i in urls) {
    tmp <- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}

</pre>

<pre>

<h2>write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",</h2>
#==========
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
            eol = "\n", na = "NA", dec = ".", row.names = FALSE,
            col.names = FALSE, qmethod = c("escape", "double"),
            fileEncoding = "")

write.table(finalTableList, theOutputname, row.names=FALSE, col.names=FALSE, quote = FALSE, sep = "\t" )

</pre>


<pre>

<h2>Four normal distribution functions:</h2>
#==========
Four normal distribution functions:

<a href="https://www.r-bloggers.com/normal-distribution-functions/">Four normal distribution functions:</a>
<br>

RNORM	Generates random numbers from normal distribution	
rnorm(n, mean, sd)
rnorm(1000, 3, .25)	Generates 1000 numbers from a normal with mean 3 and sd=.25

DNORM	Probability Density Function(PDF)
dnorm(x, mean, sd)
dnorm(0, 0, .5)	Gives the density (height of the PDF) of the normal with mean=0 and sd=.5. 

PNORM	Cumulative Distribution Function
(CDF)	pnorm(q, mean, sd)
pnorm(1.96, 0, 1)	Gives the area under the standard normal curve to the left of 1.96, i.e. ~0.975

QNORM	Quantile Function – inverse of
pnorm	qnorm(p, mean, sd)
qnorm(0.975, 0, 1)	Gives the value at which the CDF of the standard normal is .975, i.e. ~1.96

Note that for all functions, leaving out the mean and standard deviation would result in default values of mean=0 and sd=1, a standard normal distribution.


</pre>

<pre>

<h2>pnorm students scoring higher than 84</h2>
#==========
pnorm students scoring higher than 84
> pnorm(84, mean=72, sd=15.2, lower.tail=FALSE) 
[1] 0.21492
Answer
The percentage of students scoring 84 or more in the college entrance exam is 21.5%.


</pre>

<pre>

<h2>plot a histogram of 1000 draws from a normal distribution with mean 10, standard deviation 2.</h2>
#==========
plot a histogram of 1000 draws from a normal distribution with mean 10, standard deviation 2.
set.seed(seed)
x = rnorm(1000, 10, 2)
plot(x)
hist(x)

Using a QQ plot. Assess the normality:
qqnorm(x)
qqline(x)

In statistics, a Q–Q (quantile-quantile) plot is a probability plot, 
which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.
First, the set of intervals for the quantiles is chosen. 
A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). 
Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.


</pre>

<pre>

<h2>format leading zeros</h2>
#==========
format leading zeros

formatC(1, width = 2, format = "d", flag = "0")


</pre>

<pre>

<h2>library(pdftools)</h2>
#==========
setwd("D:/Users/Lawht/Desktop")
library(pdftools)
txt <- pdf_text("a.pdf")
str(txt)	# 361 pages
writeClipboard(txt[1])

txt1 = gsub(".*ORIGINATOR", "", txt)
txt1 = gsub("          ", "", txt1)

list = c(13:16, 19:22, 25:28, 31:34, 37:42, 45:48, 52:58, 62:68, 71:75, 78:85, 88:95, 98:105, 108:115, 118:124, 127:133, 136:142, 145:156, 159:169, 173:202, 206:221, 225:240, 244:258, 261:274, 277:290, 294:298, 302:308, 312:318, 323:331, 334:345, 348:359)

txt1 = txt1[list]

writeClipboard(txt1)

pdf_info("a.pdf")
pdf_text("a.pdf")
pdf_fonts("a.pdf")
pdf_attachments("a.pdf")
pdf_toc("a.pdf")

toc = pdf_toc("a.pdf")
sink("test.txt")
print(toc)
sink()

</pre>

<pre>

<h2>library(pdftools)</h2>
#==========
library(pdftools)
txt <- pdf_text("a.pdf")
str(txt)
txtList = unlist(strsplit(txt, "\\s{2,}"))

writeClipboard(txtList)

</pre>


<pre>

<h2>The name of the site environment variable R_ENVIRON</h2>
#==========
The name of the site environment variable R_ENVIRON
"R_HOME/etc/Renviron.site"

the default is "R_HOME/etc/Rprofile.site"

Sys.getenv("R_USER").

Examples

## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, 
so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, 
digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", 
"onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First <- function() cat("\n   Welcome to R!\n\n")
.Last <- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, 
set a CRAN mirror
  old <- getOption("defaultPackages"); r <- getOption("repos")
  r["CRAN"] <- "http://my.local.cran"
  options(defaultPackages = c(old, 
"MASS"), 
repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols <- Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), 
"\n")
# coo\bardoh\exabc"def'



</pre>

<pre>

<h2>How to Convert Factor into Numerical?</h2>
#==========
How to Convert Factor into Numerical?

When you convert factors to numeric, 
first you should convert it into characters and then convert into numeric. 
as.numeric(as.character(X))

Df$column<-as.numeric(as.factor(df$column)

as.integer(as.factor(region))


</pre>

<pre>
<h2>options(error=recover)</h2>
#==========
options(error=recover)

recover {utils}
Browsing after an Error

This function allows the user to browse directly on any of the currently active function calls, and is suitable as an error option.
The expression options(error = recover) will make this the error option.

Usage
recover()

When called, recover prints the list of current calls, and prompts the user to select one of them.
The standard R browser is then invoked from the corresponding environment;
the user can type ordinary R language expressions to be evaluated in that environment.

Turning off the options() debugging mode in R
options(error=NULL)


</pre>


<h2>Extract hyperlink from Excel file in R</h2>
<pre>
#==========

library(XML)

# rename file to .zip
my.zip.file <- sub("xlsx", "zip", my.excel.file)
file.copy(from = my.excel.file, to = my.zip.file)

# unzip the file
unzip(my.zip.file)

# unzipping produces a bunch of files which we can read using the XML package
# assume sheet1 has our data
xml <- xmlParse("xl/worksheets/sheet1.xml")

# finally grab the hyperlinks
hyperlinks <- xpathApply(xml, "//x:hyperlink/@display", namespaces="x")


<span class="redword">To repair Hyperlink address corrupted:</span>
copy file to desk top and rename to zip file
open zip file and locate: <span class="redword">\xl\worksheets\_rels</span>
open the sheet1.xml.rels with editor
remove all text: D:\Users\Lawht\AppData\Roaming\Microsoft\Excel\

</pre>


<h2>Extract part of a string</h2>
<pre>
#==========
x <- c("75 to 79", "80 to 84", "85 to 89")
substr(x, start = 1, stop = 2)

</pre>
<br>
<h2>alter grades</h2>
<pre>
#==========
alter grades

locate the word
get the line location
alter the score table
#==========

locate the word
v <- c('a','b','c','e')
'b' %in% v ## returns TRUE
match('b',v) ## returns the first location of 'b', in this case: 2

subv <- c('a', 'f')
subv %in% v ## returns a vector TRUE FALSE
is.element(subv, v) ## returns a vector TRUE FALSE

which()
which('a' == v) #[1] 2 4 For finding all occurances as vector of indices

grep() returns a vector of integers, which indicate where matches are.
yo <- c("a", "a", "b", "b", "c", "c")
grep("b", yo) # [1] 3 4

Partial String Matching
pmatch("med", c("mean", "median", "mode")) # returns 2

</pre>
<br>
<br>
<h2>table, cut and barplot</h2>
<pre>
atab=c(1,2,3,2,1,2,3,4,5,4)
table(atab)
cut(atab, 2)
table( cut(atab, 2))
counts = table( cut(atab, 4))
barplot(counts, main="Qty", xlab="grade")

</pre>
<br>
<br>
<br>
<h2>non-paste answer to concatenate two strings</h2>
<pre>
capture.output(cat(counts, sep = ","))

</pre>
<br>
<h2>V8 is an R interface JavaScript engine. </h2>
<pre>
This package helps us execute javascript code in R

#Loading both the required libraries
library(rvest)
library(V8)

#URL with js-rendered content to be scraped
link <- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'

#Read the html page content and extract all javascript codes that are inside a list

emailjs <- read_html(link) %>% html_nodes('li') %>% html_nodes('script') %>% html_text()

# Create a new v8 context
ct <- v8()

#parse the html content from the js output and print it as text
read_html(ct$eval(gsub('document.write','',emailjs))) %>%  html_text()

info@brewhemia.co.uk

Thus we have used rvest to extract the javascript code snippet from the desired location (that is coded in place of email ID) and used V8 to execute the javascript snippet (with slight code formatting) and output the actual email (that is hidden behind the javascript code). 

</pre>
<br>
<br>
<h2>extract protected pdf document</h2>
<pre>
library(pdftools)
setwd("C:/Users/User/Desktop")
txt <- pdf_text("a.pdf")
str(txt)	# 361 pages
# copy page 1
writeClipboard(txt[1])
# copy page 2
writeClipboard(txt[2])
# copy page 3
writeClipboard(txt[3])

Convert unicode character to string format: remove "\u"

theStr = "\u9999\u6e2f\u98df\u54c1\u6295\u8d44"	#  "香港食品投资"

=============================

Sys.setlocale(category = 'LC_ALL', 'Chinese')

library(pdftools)
setwd("C:/Users/User/Desktop")
txt <- pdf_text("45.pdf")
str(txt)

chi1 = gsub('\\u' , '&#x', txt[1])
chi2 = gsub('\\u' , '&#x', txt[2])
chi3 = gsub('\\u' , '&#x', txt[3])
sink("aaa.txt")
cat(chi1)
cat(chi2)
cat(chi3)
sink()


</pre>
<br>
<br>
<h2>Writing an R package</h2>
<pre>
  <a href="https://support.rstudio.com/hc/en-us/articles/200486488-Developing-Packages-with-RStudio">Develop Packages with RStudio</a>
<br>
<a href="http://web.mit.edu/insong/www/pdf/rpackage_instructions.pdf">rpackage_instructions.pdf</a>
<br>
<a href="https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/">Writing an R package from scratch</a>
<a href="Writing an R package.html"><span class="goldb">Writing an R package</span></a> 
<br>

</pre>
<br>
<br>
<h2>table, cut and breaks</h2>
<pre>
table(cut(as.numeric(resultTable[,3]), 10))
cut(as.numeric(resultTable[,3]),10)
breaks = c(seq(lower, 0, by = 5), 0, seq(0, upper, by = 5))

tableA = c(1,3,5,7,9)
tableB = c(1,3,5,7,2,4,6,8)
tableA = c(tableA, tableB)
tableA = sort(tableA)

table(tableA)

table(cut(tableA, 3))

breaks = c(seq(1, 3, by = 1), 4, seq(5, 9, by = 2))
table(cut(tableA, breaks))

</pre>
<br>
<br>
<h2>List the Files in a Directory</h2>
<pre>
List the Files in a Directory/Folder
list.files()

list.dirs(R.home("doc"))
list.dirs()
</pre>
<br>
<br>
<h2>test url exist</h2>
<pre>
library(httr)
http_error(theUrl)

<a href="https://stackoverflow.com/questions/18407177/load-image-from-website">Load image from website</a>
download.file("url", destfile="tmp.png", mode="wb")

<a href="https://jangorecki.gitlab.io/data.table/library/RCurl/html/url.exists.html">url.exists {RCurl}	</a> return true of false
<a href="https://stackoverflow.com/questions/31420210/r-check-existence-of-url-problems-with-httrget-and-url-exists">With httr use url_success()</a>
<br>
</pre>
<br>
<br>
<h2>Passing arguments to R script</h2>
<pre>
<a href="Passing arguments to R script.html"><span class="goldb">Passing arguments to R script</span></a> 

Rscript --vanilla testargument.R iris.txt newname

</pre>
<br>
<br>
<h2>School Revision Papers</h2>
<pre>
http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2018/
http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2017/
https://curriculum.gov.mt/en/Examination-Papers/Pages/list_secondary_papers.aspx
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F1-2ndTest-Eng.pdf
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F2-2ndTest-Eng.pdf
http://www.sttss.edu.hk/parents_corner/pastpaper.php
</pre>
<br>
<br>
<h2>difference between 1L and 1</h2>
<pre>
L specifies an integer type, rather than a double, it uses only 4 bytes per element
the function as.integer is simplified yb  "L " suffix

> str(1)
 num 1

> str(1L)
 int 1
</pre>
<br>
<br>
<br>
<h2>Datatable</h2>
<pre>
<a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf">Datatable Cheat Sheet</a>
data.table dt subset rows using i, and manipulate columns with j, grouped according to by	dt[i, j, by]
Create a data.table	data.table(a = c(1, 2), b = c("a", "b"))
convert a data frame or a list to a data.table	setDT(df) or as.data.table(df)
Subset data.table rows using i	dt[1:2, ]
subset data.table rows based on values in one or more columns	dt[a > 5, ]
data.table Logical Operators To Use In i	>,<,<=,>=, |, !,&, is.na(),!is.na(), %in%, %like%,  %between%
data.table extract column(s) by number. Prefix column numbers with “-” to drop	dt[, c(2)]
data.table extract column(s) by name	dt[, .(b, c)]
create a data.table with new columns based on the summarized values of rows	dt[, .(x = sum(a))]
compute a data.table column based on an expression	dt[, c := 1 + 2]
compute a data.table column based on an expression but only for a subset of rows	dt[a == 1, c := 1 + 2]
compute a data.table multiple columns based on separate expressions	dt[, `:=`(c = 1 , d = 2)]
delete a data.table column	dt[, c := NULL]
convert the type of a data.table column using as.integer(), as.numeric(), as.character(), as.Date(), etc..	dt[, b := as.integer(b)]
group data.table rows by values in specified column(s)	dt[, j, by = .(a)]
group data.table and simultaneously sort rows according to values in specified column(s)	dt[, j, keyby = .(a)]
summarize data.table rows within groups	dt[, .(c = sum(b)), by = a]
create a new data.table column and compute rows within groups	dt[, c := sum(b), by = a]
extract first data.table row of groups	dt[, .SD[1], by = a]
extract last data.table row of groups	dt[, .SD[.N], by = a]
perform a sequence of data.table operations by chaining multiple “[]”	dt[…][…]
reorder a data.table according to specified columns	setorder(dt, a, -b), “-” for descending
data.table’s functions prefixed with “set” and the operator “:=”	work without “<-” to alter data without making copies in memory
df <- as.data.table(df)	setDT(df)
extract unique data.table rows based on columns specified in “by”. Leave out “by” to use all columns	unique(dt, by = c("a", "b"))
return the number of unique data.table rows based on columns specified in “by”	uniqueN(dt, by = c("a", "b"))
rename data.table column(s)	setnames(dt, c("a", "b"), c("x", "y"))
data.table Syntax	DT[ i , j , by], i refers to rows. j refers to columns. by refers to adding a group
data.table Syntax arguments	DT[ i , j , by], with, which, allow.cartesian, roll, rollends, .SD, .SDcols, on, mult, nomatch
data.table fread() function	to read data, mydata = fread("https://github.com/flights_2014.csv")
data.table select only 'origin' column returns a vector	dat1 = mydata[ , origin]
data.table select only 'origin' column returns a data.table	dat1 = mydata[ , .(origin)] or dat1 = mydata[, c("origin"), with=FALSE]
data.table select column	dat2 =mydata[, 2, with=FALSE]
data.table select column Multiple Columns	dat3 = mydata[, .(origin, year, month, hour)], dat4 = mydata[, c(2:4), with=FALSE]
data.table Dropping Column	adding ! sign, dat5 = mydata[, !c("origin"), with=FALSE]
data.table Dropping Multiple Columns	dat6 = mydata[, !c("origin", "year", "month"), with=FALSE]
data.table select variables that contain 'dep'	use %like% operator, dat7 = mydata[,names(mydata) %like% "dep", with=FALSE]
data.table Rename Variables	setnames(mydata, c("dest"), c("Destination"))
data.table  rename multiple variables	setnames(mydata, c("dest","origin"), c("Destination", "origin.of.flight"))
data.table find all the flights whose origin is 'JFK'	dat8 = mydata[origin == "JFK"]
data.table Filter Multiple Values	dat9 = mydata[origin %in% c("JFK", "LGA")]
data.table selects not equal to 'JFK' and 'LGA'	dat10 = mydata[!origin %in% c("JFK", "LGA")]
data.table Filter Multiple variables	dat11 = mydata[origin == "JFK" & carrier == "AA"]
data.table Indexing Set Key	tells system that data is sorted by the key column
data.table setting 'origin' as a key	setkey(mydata, origin), 'origin' key is turned on. data12 = mydata[c("JFK", "LGA")]
data.table Indexing Multiple Columns	setkey(mydata, origin, dest), key is turned on. mydata[.("JFK", "MIA")] # First key 'origin' matches “JFK” second key 'dest' matches “MIA”
data.table Indexing Multiple Columns equivalent	mydata[origin == "JFK" & dest == "MIA"]
data.table  identify the column(s) indexed by	key(mydata)
data.table sort data using setorder()	mydata01 = setorder(mydata, origin)
data.table sorting on descending order	mydata02 = setorder(mydata, -origin)
data.table Sorting Data based on multiple variables	mydata03 = setorder(mydata, origin, -carrier)
data.table Adding Columns (Calculation on rows)	use := operator, mydata[, dep_sch:=dep_time - dep_delay]
data.table Adding Multiple Columns	mydata002 = mydata[, c("dep_sch","arr_sch"):=list(dep_time - dep_delay, arr_time - arr_delay)]
data.table IF THEN ELSE Method I	mydata[, flag:= 1*(min < 50)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table IF THEN ELSE Method II	mydata[, flag:= ifelse(min < 50, 1,0)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table build a chain	DT[ ] [ ] [ ], mydata[, dep_sch:=dep_time - dep_delay][,.(dep_time,dep_delay,dep_sch)]
data.table Aggregate Columns mean	mydata[, .(mean = mean(arr_delay, na.rm = TRUE),
data.table Aggregate Columns median	median = median(arr_delay, na.rm = TRUE),
data.table Aggregate Columns min	min = min(arr_delay, na.rm = TRUE),
data.table Aggregate Columns max	max = max(arr_delay, na.rm = TRUE))]
data.table Summarize Multiple Columns	all the summary function in a bracket, mydata[, .(mean(arr_delay), mean(dep_delay))]
data.table .SD operator	implies 'Subset of Data'
data.table .SD and .SDcols operators	calculate summary statistics for a larger list of variables
data.table calculates mean of two variables	mydata[, lapply(.SD, mean), .SDcols = c("arr_delay", "dep_delay")]
data.table Summarize all numeric Columns	mydata[, lapply(.SD, mean)]
data.table Summarize with multiple statistics	mydata[, sapply(.SD, function(x) c(mean=mean(x), median=median(x)))]
data.table Summarize by group 'origin	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = origin]
data.table Summary by group useing keyby= operator	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), keyby = origin]
data.table Summarize multiple variables by group 'origin'	mydata[, .(mean(arr_delay, na.rm = TRUE), mean(dep_delay, na.rm = TRUE)), by = origin], or mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin]
data.table remove non-unique / duplicate cases with unique()	setkey(mydata, "carrier"), unique(mydata)
data.table remove duplicated	setkey(mydata, NULL), unique(mydata), Note : Setting key to NULL is not required if no key is already set.
data.table Extract values within a group	mydata[, .SD[1:2], by=carrier], selects first and second values from a categorical variable carrier.
data.table Select LAST value from a group	mydata[, .SD[.N], by=carrier]
data.table window function frank()	dt = mydata[, rank:=frank(-distance,ties.method = "min"), by=carrier], calculating rank of variable 'distance' by 'carrier'. 
data.table cumulative sum cumsum()	dat = mydata[, cum:=cumsum(distance), by=carrier]
data.table lag and lead with shift()	shift(variable_name, number_of_lags, type=c("lag", "lead")), DT <- data.table(A=1:5), DT[ , X := shift(A, 1, type="lag")], DT[ , Y := shift(A, 1, type="lead")]
data.table  %between% operator to define a range	DT = data.table(x=6:10), DT[x %between% c(7,9)]
data.table %like% to find all the values that matches a pattern	DT = data.table(Name=c("dep_time","dep_delay","arrival"), ID=c(2,3,4)), DT[Name %like% "dep"] 
data.table Inner Join	Sample Data: (dt1 <- data.table(A = letters[rep(1:3, 2)], X = 1:6, key = "A")), (dt2 <- data.table(A = letters[rep(2:4, 2)], Y = 6:1, key = "A")), merge(dt1, dt2, by="A")
data.table Left Join	merge(dt1, dt2, by="A", all.x = TRUE)
data.table Right Join	merge(dt1, dt2, by="A", all.y = TRUE)
data.table Full Join	merge(dt1, dt2, all=TRUE)
Convert a data.table to data.frame	setDF(mydata)
convert data frame to data table	setDT(), setDT(X, key = "A")
data.table Reshape Data	dcast.data.table() and melt.data.table()
data.table Calculate total number of rows by month and then sort on descending order	mydata[, .N, by = month] [order(-N)], The .N operator is used to find count.
data.table Find top 3 months with high mean arrival delay	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = month][order(-mean_arr_delay)][1:3]
data.table Find origin of flights having average total delay is greater than 20 minutes	mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin][(arr_delay + dep_delay) > 20]
data.table Extract average of arrival and departure delays for carrier == 'DL' by 'origin' and 'dest' variables	mydata[carrier == "DL", lapply(.SD, mean, na.rm = TRUE), by = .(origin, dest), .SDcols = c("arr_delay", "dep_delay")]
data.table Pull first value of 'air_time' by 'origin' and then sum the returned values when it is greater than 300	mydata[, .SD[1], .SDcols="air_time", by=origin][air_time > 300, sum(air_time)]

</pre>
<br>
<br>
<h2>R Web Scraping</h2>
<pre>
<a href="https://www.datacamp.com/community/tutorials/r-web-scraping-rvest">R Web Scraping Rvest</a>
<a href="http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl/">webscraping-using-readlines-and-rcurl</a>
<a href="https://www.rdocumentation.org/packages/XML/versions/3.98-1.16/topics/xmlTreeParse">xmlTreeParse, htmlTreeParse</a>
<a href="http://www.cse.chalmers.se/~chrdimi/downloads/web/getting_web_data_r4_parsing_xml_html.pdf">getting web data parsing xml html</a>
<a href="https://stackoverflow.com/questions/35479549/error-in-r-no-applicable-method-for-xpathapply">error in r no applicable method for xpathapply</a>
<a href="https://blog.rstudio.com/2015/04/21/xml2/">Parse and process XML (and HTML) with xml2</a>
==================
web_page <- readLines("http://www.interestingwebsite.com")
web_page <- read.csv("http://www.programmingr.com/jan09rlist.html")

    # General-purpose data wrangling
    library(tidyverse)  

    # Parsing of HTML/XML files  
    library(rvest)    

    # String manipulation
    library(stringr)   

    # Verbose regular expressions
    library(rebus)     

    # Eases DateTime manipulation
    library(lubridate)

==================
install.packages("RCurl", dependencies = TRUE)
library("RCurl")
library("XML")

past <- getURL("http://www.iciba.com/past", ssl.verifypeer = FALSE)	# getURL cannot work
webpage <- read_html("http://www.iciba.com/past")	# getURL cannot work

jan09_parsed <- htmlTreeParse(jan09)

==================
http://www.iciba.com/past
ul class="base-list switch_part" class

library('rvest')
library(tidyverse)
url <- 'http://www.iciba.com/past'
webpage <- readLines(url, warn=FALSE)
webpage <- read_html(webpage)
grappedData <- html_nodes(webpage,'.base-list switch_part')

parseData = htmlTreeParse(webpage)

rank_data <- html_text(grappedData)

html_node("#mw-content-text > div > table:nth-child(18)")
html_table()

the function htmlParse() which is equivalent to xmlParse(file, isHTML = TRUE)
output = htmlParse(webpage)
class(output)

To parse content into an R structure :
htmlTreeParse() which is equivalent to htmlParse(file, useInternalNodes = FALSE)
output = htmlTreeParse(webpage)
class(output)

htmlTreeParse(file) especially suited for parsing HTML content
returns class "XMLDocumentContent" (R data structure)
equivalent to
xmlParse(file, isHTML = TRUE, useInternalNodes = FALSE)
htmlParse(file, useInternalNodes = FALSE)

root =xmlRoot(output)
xmlChildren(output)
xmlChildren(xmlRoot(output))
XMLNodeList

Functions for a given node
Function Description
xmlName() name of the node
xmlSize() number of subnodes
xmlAttrs() named character vector of all attributes
xmlGetAttr() value of a single attribute
xmlValue() contents of a leaf node
xmlParent() name of parent node
xmlAncestors() name of ancestor nodes
getSibling() siblings to the right or to the left
xmlNamespace() the namespace (if there’s one)

to parse HTML tables using R
sched <- readHTMLTable(html, stringsAsFactors = FALSE)

The html.raw object is not immediately useful because it literally contains all of the raw HTML for the entire webpage. We can parse the raw code using the xpathApply function which parses HTML based on the path argument, which in this case specifies parsing of HTML using the paragraph tag.

html.raw<-htmlTreeParse('http://www.dnr.state.mn.us/lakefind/showreport.html?downum=27013300',
    useInternalNodes=T    )
html.parse<-xpathApply(html.raw, "//p", xmlValue)

# evaluate input and convert to text
txt <- htmlToText(url)

==================
url <- 'http://www.iciba.com/past'
webpage <- readLines(url, warn=FALSE)
scraping_wiki <- read_html(webpage)
scraping_wiki %>% html_nodes("h1") %>% html_text()

url <- 'testvibrate.html'
webpage <- readLines(url, warn=FALSE)
x <- read_xml(webpage)
xml_name(x)


</pre>
<br>
<br>
<h2>R scraping html text example</h2>
<pre>
<a href="http://bradleyboehmke.github.io/2015/12/scraping-html-text.html">scraping-html-text</a>
library(rvest)

scraping_wiki <- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scraping_wiki %>% html_nodes("h1")
scraping_wiki %>% html_nodes("h2")
scraping_wiki %>% html_nodes("h1") %>% html_text()
scraping_wiki %>% html_nodes("h2") %>% html_text()
p_nodes <- scraping_wiki %>% html_nodes("p")
length(p_nodes)
p_text <- scraping_wiki %>% html_nodes("p") %>% html_text()
p_text[1]
p_text[5]

ul_text <- scraping_wiki %>% html_nodes("ul") %>% html_text()
length(ul_text)
ul_text[1]
substr(ul_text[2], start = 1, stop = 200)

li_text <- scraping_wiki %>% html_nodes("li") %>% html_text()
length(li_text)
li_text[1:8]
li_text[104:136]

all_text <- scraping_wiki %>% html_nodes("div") %>%  html_text()

body_text <- scraping_wiki %>% html_nodes("#mw-content-text") %>%  html_text()

# read the first 207 characters
substr(body_text, start = 1, stop = 207)

# read the last 73 characters
substr(body_text, start = nchar(body_text)-73, stop = nchar(body_text))

# Scraping a specific heading
scraping_wiki %>% html_nodes("#Techniques") %>%  html_text()
## [1] "Techniques"

# Scraping a specific paragraph
scraping_wiki %>% html_nodes("#mw-content-text > p:nth-child(20)") %>%  html_text()

# Scraping a specific list
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

# Scraping a specific reference list item
scraping_wiki %>% html_nodes("#cite_note-22") %>%  html_text()

# Cleaning up
library(magrittr)
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text() %>%  strsplit(split = "\n") %>% unlist() %>% .[. != ""]


library(stringr)

# read the last 700 characters
substr(body_text, start = nchar(body_text)-700, stop = nchar(body_text))

# clean up text
body_text %>% 
str_replace_all(pattern = "\n", replacement = " ") %>% 
str_replace_all(pattern = "[\\^]", replacement = " ") %>% 
str_replace_all(pattern = "\"", replacement = " ") %>% 
str_replace_all(pattern = "\\s+", replacement = " ") %>% 
str_trim(side = "both") %>% 
substr(start = nchar(body_text)-700, stop = nchar(body_text))

</pre>
<br>
<br>
<h2>sort() rank() order()</h2>
<pre>
<strong>Rank</strong> references the position of the value in the sorted vector and is in the same order as the <strong>original </strong>sequence
<strong>Order</strong> returns the position of the original value and is in the order of <strong>sorted </strong>sequence
The graphic below helps tie together the values reported by rank and order with the positions from which they come.
<img src="https://cdn-images-1.medium.com/max/800/1*3KeaXU6luJDyoatWkEwdug.jpeg">
x = c(1, 8,9, 4)
sort(x)
1 4 8 9

# the original position in the sorted order
rank(x)
1 3 4 2

# the sorted position in the original position
order(x)
1 4 2 3
</pre>
<br>
<br>
<h2>Bioinformatics</h2>
<pre>
<a href="https://cran.r-project.org/doc/contrib/Krijnen-IntroBioInfStatistics.pdf">Bioinformatics using R</a>
<br>
<a href="https://www.bioconductor.org/">bioconductor</a>
<br>
<a href="https://www.r-exercises.com/product/introduction-to-bioconductor-annotation-and-analysis-of-genomes-and-genomic-assays/">Introduction to Bioconductor:Annotation and Analysis of Genomes and Genomics Assays</a>
</pre>
<br>
<h2>a list of dataframes, 3D data arrangement</h2>
<pre>
d1 <- data.frame(y1=c(1,2,3),y2=c(4,5,6))
d2 <- data.frame(y1=c(3,2,1),y2=c(6,5,4))
d3 <- data.frame(y1=c(7,8,9),y2=c(5,2,6))
mylist <- list(d1, d2, d3)
names(mylist) <- c("List1","List2","List3")

mylist[1]	# same as mylist$List1

mylist[[2]][1,2]	# access an element inside a dataframe
mylist[[2]][2,2]	# same as mylist$List2[2,2]

to concate another dataframe:

d4 <- data.frame(y1=c(2,5,8),y2=c(1,4,7))
mylist[[4]] <- d4

to create an empty list:
data <- list()
</pre>
<br>
<br>
<h2>format time string</h2>
<pre>
Sys.time()

sub(".* | .*", "", Sys.time())

format(Sys.time(), '%H:%M')

gsub(":", "", format(Sys.time(), '%H:%M'))

</pre>
<br>
<br>
<h2>access Components of a Data Frame</h2>
<a href="https://www.datamentor.io/r-programming/data-frame/">access Components of a Data Frame</a>
<br>

<p>Components of data frame can be accessed like a list or like a matrix.</p>
<h3>Accessing like a list</h3>
<p>We can use either <code>[</code>, <code>[[</code> or <code>$</code> operator to access columns of data frame.</p>
<pre><code>&gt; x["Name"]
Name
1 John
2 Dora
&gt; x$Name
[1] "John" "Dora"
&gt; x[["Name"]]
[1] "John" "Dora"
&gt; x[[3]]
[1] "John" "Dora"
</code></pre>
<p>Accessing with <code>[[</code> or <code>$</code> is similar. However, it differs for <code>[</code> in that, indexing with <code>[</code> <span class="redword">will return us a data frame</span> but the other two will <span class="redword">reduce it into a vector</span>.</p>

<h3>Accessing like a matrix</h3>
<p>Data frames can be accessed like a matrix by providing index for row and column.</p>
<p>To illustrate this, we use datasets already available in R. Datasets that are available can be listed with the command <code>library(help = "datasets")</code>.</p>
<p>We will use the <code>trees</code> dataset which contains <code>Girth</code>, <code>Height</code> and <code>Volume</code> for Black Cherry Trees.</p>
<p>A data frame can be examined using functions like <code>str()</code> and <code>head()</code>.</p>
<pre><code>&gt; str(trees)
'data.frame':   31 obs. of 3 variables:
$ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...
$ Height: num  70 65 63 72 81 83 66 75 80 75 ...
$ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...
&gt; head(trees,n=3)
Girth Height Volume
1   8.3     70   10.3
2   8.6     65   10.3
3   8.8     63   10.2
</code></pre>
<p>We can see that <code>trees</code> is a data frame with 31 rows and 3 columns. We also display the first 3 rows of the data frame.</p>
<p>Now we proceed to access the data frame like a matrix.</p>
<pre><code>&gt; trees[2:3,]    # select 2nd and 3rd row
Girth Height Volume
2   8.6     65   10.3
3   8.8     63   10.2
&gt; trees[trees$Height &gt; 82,]    # selects rows with Height greater than 82
Girth Height Volume
6   10.8     83   19.7
17  12.9     85   33.8
18  13.3     86   27.4
31  20.6     87   77.0
&gt; trees[10:12,2]
[1] 75 79 76
</code></pre>
<p>We can see in the last case that the returned type is a vector since we extracted data from a single column.</p>
<p>This behavior can be avoided by passing the argument <code>drop=FALSE</code> as follows.</p>
<pre><code>&gt; trees[10:12,2, drop = FALSE]
Height
10     75
11     79
12     76
</code>


# access first row by index, returns a data.frame
x[1,]

# access first row by "name", returns a data.frame
> x["1",]

# access first row returns a vector
use as.numeric
str(as.numeric(wAveTable["1",]))

unlist which keeps the names.
str(unlist(wAveTable["1",]))

use transpose and as.vector
str(as.vector(t(wAveTable["1",])[,1]))

use only as.vector cannot convert to vector
str(as.vector(wAveTable["1",]))

# convert dataframe to matrix
data.matrix(wAveTable)

</pre>
<br>
<br>
<h2>read.csv as character</h2>
<pre>
wAveTable = read.csv("wAveTable.txt", sep="\t", colClasses=c('character', 'character', 'character'))
</pre>
<br>
<br>
<h2>frequency manipulation</h2>
<pre>
grade = c("low", "high", "medium", "high", "low", "medium", "high")

# using factor to count the frequency
foodfac <- factor(grade)
summary(foodfac)
max(summary(foodfac))
min(summary(foodfac))
levels(foodfac)
nlevels(foodfac)
summary(levels(foodfac))

# use of table to count frequency:
table(grade)
sort(table(grade))

table(grade)[1]
max(table(grade))
summary(table(grade))

# this locate the max item:
table(grade)[which(table(grade) == max(table(grade)))]

# change to dataframe and find the max item:
theTable = as.data.frame(table(grade))
theTable[which(theTable$Freq == max(theTable$Freq)),]

# use of the count function in plyr:
library(plyr)
count(grade)
count(mtcars, 'gear')

# use of the which function:
which(letters == "g")
x <- c(1,5,8,4,6)
which(x == 5)
which(x != 5)

</pre>
<br>
<br><br><br><br>

<script>
  $(function() {
    var toc = $('#toc');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
