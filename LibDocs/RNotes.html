<base target="_blank"><html><head><title>R Notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@13.0.1/dist/lazyload.min.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script>
  var showTopicNumber = true;
</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
strong, h1, h2 {color: gold;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()">

<center><h1>R Notes</h1>
<li>data visualization (ggplot2)</li>
<li>data manipulation (dplyr, lubridate, tidyr, stringr, readr, & forcats)</li>
<li>data analysis (combine ggplot2, dplyr to explore data and find insights)</li>
<a href="Non-standard evaluation.html" class="whitebut ">Non-standard evaluation</a>
<a href="https://statisticsglobe.com" class="whitebut ">Statistics Globe</a>
<a href="http://www.datasciencemadesimple.com" class="whitebut ">DataScience Made Simple</a>

<br>

<a href="Libraries for Python & R.html">Libraries for Python & R</a>
<a href="sparklyr.html" class="redbut red blueblackgrad">sparklyr</a>
<br>
<div id="toc"></div></center>
<br>
<br>
<br>
<a href="Libraries for Python & R.html">Libraries for Python & R</a>


<pre>
<br>
<h2>free books for R</h2>
<a href="http://www.cookbook-r.com/" target="_blank">Cookbook for R</a>
<a href="RCookbook.html" target="_blank" class="orangesha">&diams;RCookbook</a>
<a href="https://bookdown.org" class="whitebut " target="_blank">bookdown R books</a>
<a href="https://bookdown.org/home/archive/" class="whitebut " target="_blank">bookdown all books</a>
<a href="https://bookdown.org/home/tags/r-programming/" class="whitebut " target="_blank">bookdown r-programming books</a>
<a href="https://bookdown.org/rdpeng/rprogdatascience/" class="whitebut " target="_blank">R Programming for Data Science</a>


<br>
<h2>Data Frame</h2>
<pre>
Data Frame is a list of vectors of equal length

to create a dataframe:
n = c(2,3,5)
s = c('a','b','c')
b = c(TRUE, FALSE, FALSE)
df = data.frame(n,s,b)

Components of dataframe:
header, column names, data row, name of the row cell
single square bracket "[]", comma

Functions:
nrow(), ncol(), head()

Inport Data:
read.table("mydata.txt")
read.csv("mydata.csv")

retrieve the column vector by the double square bracket or the "$" operator
mtcars[[9]]
mtcars[["am"]]
mtcars$am
mtcars[,"am"]


retrieve a column slice with the single square bracket "[]"
mtcars[1]
mtcars["mpg"]
mtcars[c("mpg", "hp")]

Data frame Row Slice
mtcars[24,]
mtcars[c(3,24),]
mtcars["camaro z28",]
mtcars[c("datsun 710","camaro z28"),]


</pre>
<h2># MLFundStat and Hangseng Fund Stat</h2>
#=================
MLFundStat.html
the computation is long, it is possible to cut time by adjusting the cutdate variable.
this should be modified to new version using r chart.

<h2># Start Of R</h2>
#=================
Sys.setlocale(category = 'LC_ALL', 'Chinese')

use the .Rprofile.site file to run R commands for all users when their R session starts.
D:\R-3.5.1\etc\Rprofile.site
See: Initialization at startup.

This command could be an environment set:
Sys.setenv(FAME="/opt/fame")

<a href="https://www.rdocumentation.org/packages/base/versions/3.5.0/topics/Startup">Start Of R Initialization</a>

<h2># Encoding Problems</h2>
<code>To write text UTF8 encoding on Windows</code>

The problem is due to using the default system coding / or using some system write functions; I do not know the specifics but the behaviour is actually known

To write text UTF8 encoding on Windows one has to use the <b class="gold embossts redbs borRad10">useBytes=T</b> options in functions like writeLines or readLines:

txt <- "在"
<code>writeLines(txt, "test.txt", useBytes=T)

readLines("test.txt", encoding="UTF-8")</code>
[1] "在"

<code>writeLines(wholePage, theFilename, useBytes=T)</code>

#=================
# Encoding Problems
Sys.getlocale()
getOption("encoding")
options(encoding = "UTF-8")
Encoding(txtstring) <- "UTF-8"
Encoding(txtstring)
txtstring
Sys.setlocale
Sys.setlocale(category = 'LC_ALL', 'Chinese')
Sys.setlocale(category = "LC_ALL", locale = "chs") 
Sys.setlocale(category = "LC_ALL", locale = "cht") # fanti

Note: 
default: options("encoding" = "native.enc")
statTxtFile = "test.txt"
write("建设银行", statTxtFile, append=TRUE)
result file is ansi

add:
options("encoding" = "UTF-8")
write("建设银行", statTxtFile, append=TRUE)
result file is utf-8

mytext <- "this is my text"
Encoding(mytext)

options(encoding = "UTF-8")
getOption("encoding")

options(encoding='native.enc')
getOption("encoding")


iconvlist()
theHeader = "http://qt.gtimg.cn/r=2&q=r_hk"
onecode = "02009"
con = url(paste0(theHeader,onecode), encoding = "GB2312")
thepage=readLines(con)
close(con)
Info=unlist(strsplit(thepage,"~"))
codename=Info[2]
codename
Encoding(codename)

==================
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
readLines(filename, encoding="UTF-8")
readLines(con = stdin(), n = -1L, ok = TRUE, warn = TRUE, encoding = "unknown", skipNul = FALSE)

<span class="redword"># note! the chiname encoding is ok inside R, but will be wrong when write to file by local pc locale, to solve the problem, set Sys.setlocale(category = 'LC_ALL', 'Chinese') </span>

readLines(con <- file("Unicode.txt", encoding = "UCS-2LE"))
close(con)
unique(Encoding(A)) # will most likely be UTF-8
==================
guess_encoding(pageHeader)
pageHeader = repair_encoding(pageHeader, from="utf-8")
pageHeader = repair_encoding(pageHeader, "UTF-8")

iconv(pageHeader, to="UTF-8")
Encoding(pageHeader) <- "UTF-8"

Sys.getlocale("LC_ALL")
https://rpubs.com/mauriciocramos/encoding
==================

Read text as UTF-8 encoding

the following reads in encoding twice and works but reasons unknown
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
[1] "Zürich"

==================
the page source claim to be using UTF-8 encoding:
meta http-equiv="Content-Type" content="text/html; charset=utf-8"

So, the question is, are they really using a different enough encoding, 
or can we just convert to utf-8, guessing that any errors will be negligible?

A quick and dirty approach just force utf-8 using iconv:

TV_Audio_Video <- read_html(iconv(page_source[[1]], to = "UTF-8"), encoding = "utf8")

In general, this is a bad idea - better to specify the encoding it's from.
In this case, maybe the error is theirs, so this quick and dirty approach might be ok.

</pre>

<pre>

<h2># important to remove leading zeros</h2>
#=================
# important to remove leading zeros
substr(t,regexpr("[^0]",t),nchar(t))


</pre>


<pre>

<h2>Pop up message in windows 8.1</h2>
#=================
Pop up message in windows 8.1
c.bat:  start MessageBox.vbs "This will be shown in a popup."

MessageBox.vbs :
Set objArgs = WScript.Arguments
messageText = objArgs(0)
MsgBox messageText

in fact, save a file named test.vbs with content:
MsgBox "some message"

double click the file will run directly


# options("scipen"=999)
# format(xx, scientific=F)
# options("scipen"=100, "digits"=4)
# getOption("scipen")
# or as.integer(functionResult);

df <- data.frame(matrix(ncol = 10000, nrow = 0))
colnames(df) <- c("a", "b," "c")
rm(list=ls())
Extracting a Single, Simple Table
The first step is to load the ¡§XML¡¨ package, 
then use the htmlParse() function to read the html document into an R object, 
and readHTMLTable() to read the table(s) in the document. 
The length() function indicates there is a single table in the document, simplifying our work.

The plot3d() function in the rgl package
library(rgl)
open3d()
attach(mtcars)
plot3d(disp,wt,mpg, col = rainbow(10))


</pre>

<pre>

<h2>library(stringr)</h2>
#============
library(stringr)
library(htmltools)
library(threejs)
data(mtcars)
data <- mtcars[order(mtcars$cyl),]
uv <- tabulate(mtcars$cyl)
col <- c(rep("red",uv[4]),rep("yellow",uv[6]),rep("blue",uv[8]))
row.names(mtcars)
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")

</pre>

<pre>

<h2>scatterplot3js(data[,c(3,6,1)],</h2>
#============
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")
# Gumball machine
N <- 100
i <- sample(3, N, replace=TRUE)
x <- matrix(rnorm(N*3),ncol=3)
lab <- c("small", "bigger", "biggest")
scatterplot3js(x, color=rainbow(N), labels=lab[i],
               size=i, renderer="canvas")
# Example 1 from the scatterplot3d package (cf.)
z <- seq(-10, 10, 0.1)
x <- cos(z)
y <- sin(z)
scatterplot3js(x,y,z, color=rainbow(length(z)),
   labels=sprintf("x=%.2f, y=%.2f, z=%.2f", x, y, z))
# Interesting 100,000 point cloud example, should run this with WebGL!
N1 <- 10000
N2 <- 90000
x <- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
y <- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
z <- c(rnorm(N1, sd=0.5), rpois(N2, lambda=20)-20)
col <- c(rep("#ffff00",N1),rep("#0000ff",N2))
scatterplot3js(x,y,z, color=col, size=0.25)
cat("\014")	CLS Screen
#
match returns a vector of the positions
v1 <- c("a","b","c","d")
v2 <- c("g","x","d","e","f","a","c")
x <- match(v1,v2)
6 NA  7  3
v1 %in% v2
TRUE FALSE  TRUE  TRUE
x <- match(v1,v2,nomatch=-1)
6 -1  7  3
%in% returns a logical vector indicating if there is a match or not


</pre>

<pre>

<h2>this check whether an element is inside a group</h2>
#=============
this check whether an element is inside a group
v <- c('a','b','c','e')
'b' %in% v

</pre>

<pre>

<h2>check vector includes in 31:37 %in% 0:36</h2>
#=============
31:37 %in% 0:36
#
dmInfo=data.matrix(Info)	# convert dataframe to matrix, but the row and column is exchanged
#
bob <- data.frame(lapply(bob, as.character), stringsAsFactors=FALSE)	#Change numeric to characters
#
write.csv(Info,quote=FALSE, row.names = FALSE)	# write csv is the proper way to write the datafile
#

attach an excel file in R:
1: Install packages XLConnect and foreign and run both libraries
2: abcd <- readWorksheet(loadWorkbook('file extension'),sheet=1)
#
allocate vector of size 1.7 Gb
Try memory.limit() for the current memory limit Use memory.limit (size=50000) to increase memory limit. Try using a cloud based environment, 
try using package slam
use factors 

Concatenate and Split Strings in R
==================================
use the paste() function to concatenate
strsplit() function to split
pangram <- "The quick brown fox jumps over the lazy dog"
strsplit(pangram, " ")
"The"  "quick" "brown" "fox"  "jumps" "over" "the"  "lazy" "dog"

the unique elements
unique() function
unique(tolower(words))
"the"  "quick" "brown" "fox"  "jumps" "over" "lazy" "dog"

# <span class="gold">find duplicates</span>
# the intersect function is used for different set, not in inside a vector
# instead, use the duplicated function will be OK.

words = unlist(strsplit(pangram, " "))
words = tolower(words)
duplicated(words)
words[duplicated(words)]

arr = sample(1:36,6,replace=TRUE)
cat(arr, "\n")
arr[duplicated(arr)]


R split Function
================
split() function divides the data in a vector. 
unsplit() funtion do the reverse.
split(x, f, drop = FALSE, ...)
split(x, f, drop = FALSE, ...) <- value
unsplit(value, f, drop = FALSE)
x: vector, data frame
f: indices
drop: discard non existing levels or not


</pre>

<pre>

<h2>x <- read.csv("anova.csv",header=T,sep=",")</h2>
#=============
x <- read.csv("anova.csv",header=T,sep=",")
Subtype,Gender,Expression
A,m,-0.54
A,m,-0.8
Split the "Expression" values into two groups based on "Gender" variable, 
"f" for female group, and 
"m" for male group:
>g <- split(x$Expression, x$Gender)
>g
$f
  [1] -0.66 -1.15 -0.30 -0.40 -0.24 -0.92  0.48 -1.68 -0.80 -0.55 -0.11 -1.26
$m
  [1] -0.54 -0.80 -1.03 -0.41 -1.31 -0.43  1.01  0.14  1.42 -0.16  0.15 -0.62

Calculate the length, mean value of each group:
sapply(g,length)
  f   m 
135 146 
sapply(g,mean)
         f          m 
-0.3946667 -0.2227397

You may use lapply, return is a list:
lapply(g,mean)
unsplit() function combines the groups:
unsplit(g,x$Gender)

<h2><span class="blink red">Apply</span></h2>
=====
m <- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=30, ncol=3)
apply(m, 1, mean)
a 1 in the second argument, giving the mean of each row. 
apply(m, 2, mean)giving the mean of each column. 
apply(m, 2, function(x) length(x[x<0]))	# count -ve values
apply(m, 2, function(x) is.matrix(x))
apply(m, 2, is.vector)
apply(m, 2, function(x) mean(x[x>0]))

#=========
ma <- matrix(c(1:4, 1, 6:8), nrow = 2)

apply(ma, 1, table)

apply(ma, 1, stats::quantile)
apply(ma, 2, mean)

apply(m, 2, function(x) length(x[x<0]))

sapply lapply rollapply
sapply(1:3, function(x) x^2)

lapply return a list:
lapply(1:3, function(x) x^2)
use unlist with lapply to get a vector

sapply(1:3, function(x, y) mean(y[,x]), y=m)

A<-matrix(1:9, 3,3)
B<-matrix(4:15, 4,3)
C<-matrix(8:10, 3,2)
MyList<-list(A,B,C)
Z=sapply(MyList,"[", 1,1 )

#==========
te=matrix(1:20,nrow=2)
sapply(te,mean)	# this is a vector, order arrange in matrix direction
matrix(sapply(te,mean),nrow=2)	# this is changed to matrix

subset()
apply()
sapply()
lapply()
tapply()
aggregate()
apply 	apply a function to the rows or columns of a matrix
M <- matrix(seq(1,16), 4, 4)
apply(M, 1, min)
lapply 	apply a function to each element of a list in turn and get a list back
x <- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
sapply 	apply a function to each element of a list in turn, but you want a vector back
x <- list(a = 1, b = 1:3, c = 10:100)
sapply(x, FUN = length)  
vapply 	squeeze some more speed out of sapply
x <- list(a = 1, b = 1:3, c = 10:100)
vapply(x, FUN = length, FUN.VALUE = 0L) 

mapply 	apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in sapply

Note: 
mApply(X, INDEX, FUN, …, simplify=TRUE, keepmatrix=FALSE)
from Hmisc package

is different from 
mapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)


Examples

#Sums the 1st elements, the 2nd elements, etc. 
mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15

mapply(rep, 1:4, 4:1)
mapply(rep, times = 1:4, x = 4:1)
mapply(rep, times = 1:4, MoreArgs = list(x = 42))
mapply(function(x, y) seq_len(x) + y,
       c(a =  1, b = 2, c = 3),  # names from first
       c(A = 10, B = 0, C = -10))
word <- function(C, k) paste(rep.int(C, k), collapse = "")
utils::str(mapply(word, LETTERS[1:6], 6:1, SIMPLIFY = FALSE))

mapply(function(x,y){x^y},x=c(2,3),y=c(3,4))
8 81

values1 <- list(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))
values2 <- list(a = c(10, 11, 12), b = c(13, 14, 15), c = c(16, 17, 18)) 
mapply(function(num1, num2) max(c(num1, num2)), values1, values2)
 a  b  c 
12 15 18 



Map 	A wrapper to mapply with SIMPLIFY = FALSE, so it is guaranteed to return a list
rapply	For when you want to apply a function to each element of a nested list structure, recursively
tapply	For when you want to apply a function to subsets of a vector and the subsets are defined by some other vector, usually a factor
lapply is a list apply which acts on a list or vector and returns a list.
sapply is a simple lapply (function defaults to returning a vector or matrix when possible)
vapply is a verified apply (allows the return object type to be prespecified)
rapply is a recursive apply for nested lists, i.e. lists within lists
tapply is a tagged apply where the tags identify the subsets
apply is generic: applies a function to a matrix's rows or columns
by	a "wrapper" for tapply. The power of by arises when we want to compute a task that tapply can't handle
aggregate can be seen as another a different way of use tapply if we use it in such a way


xx = c(1,3,5,7,9,8,6,4,2,1,5)
duplicated(xx)
xx[duplicated(xx)]

Accessing dataframe by names:
mtcars["mpg"]
QueueNo = 12
mtcars[QueueNo,"mpg"]

some functions to remember
charToRaw(key)
as.raw(key)

A motion chart is a dynamic chart to explore several indicators over time. 
subset(airquality, Temp > 80, select = c(Ozone, Temp))
subset(airquality, Day == 1, select = -Temp)
subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp > 80))
 ## sometimes requiring a logical 'subset' argument is a nuisance nm <- rownames(state.x77) start_with_M <- nm %in% grep("^M", nm, value = TRUE)
subset(state.x77, start_with_M, Illiteracy:Murder) # but in recent versions of R this can simply be
subset(state.x77, grepl("^M", nm), Illiteracy:Murder)

join 3 dataframes
library("plyr")
join() function
names(gdp)[3] <- "GDP"
names(life_expectancy)[3] = "LifeExpectancy"
names(population)[3] = "Population"
gdp_life_exp <- join(gdp, life_expectancy)
development <- join(gdp_life_exp, population)

subset() function
dev_2005 <- subset(development, Year == 2005)
dev_2005_big <- subset(dev_2005, GDP >= 30000)

development_motion <- subset(development_complete, Country %in% selection)
library(googleVis)
gvisMotionChart() function
motion_graph <- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year")
plot(motion_graph)
motion_graph <- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "GDP", yvar = "LifeExpectancy", sizevar = "Population")
development_motion$logGDP <- log(development_motion$GDP)
motion_graph <- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "logGDP", yvar = "LifeExpectancy", sizevar = "Population")

my_list[[1]] extracts the first element of the list my_list, and my_list[["name"]] extracts the element in my_list that is called name. 
If the list is nested you can travel down the heirarchy by recursive subsetting. 
mylist[[1]][["name"]] is the element called name inside the first element of my_list.
A data frame is just a special kind of list, so you can use double bracket subsetting on data frames too. 
my_df[[1]] will extract the first column of a data frame and my_df[["name"]] will extract the column named name from the data frame.
names() and str() is a great way to explore the structure of a list.

i in 1:ncol(df)
This is a pretty common model for a sequence: a sequence of consecutive integers designed to index over one dimension of our data.
What might surprise you is that this isn't the best way to generate such a sequence, especially when you are using for loops inside your own functions. Let's look at an example where df is an empty data frame:
df <- data.frame()
1:ncol(df)
for (i in 1:ncol(df)) {
  print(median(df[[i]]))
}
Our sequence is now the somewhat non-sensical: 1, 0. You might think you wouldn't be silly enough to use a for loop with an empty data frame, but once you start writing your own functions, there's no telling what the input will be.
A better method is to use the seq_along() function.
if you grow the for loop at each iteration (e.g. using c()), your for loop will be very slow.
A general way of creating an empty vector of given length is the vector() function. 
It has two arguments: the type of the vector ("logical", "integer", "double", "character", etc.) and the length of the vector.
Then, at each iteration of the loop you must store the output in the corresponding entry of the output vector, i.e. assign the result to output[[i]]. (You might ask why we are using double brackets here when output is a vector. It's primarily for generalizability: this subsetting will work whether output is a vector or a list.)

A time series can be thought of as a vector or matrix of numbers, 
along with some information about what times those numbers were recorded. This information is stored in a ts object in R.
read in some time series data from an xlsx file using read_excel(), 
a function from the readxl package, 
and store the data as a ts object.
Use the read_excel() function to read the data from "exercise1.xlsx" into mydata.
mydata <- read_excel("exercise1.xlsx")
Create a ts object called myts using the ts() function. 
myts <- ts(mydata[,2:4], start = c(1981, 1), frequency = 4)

The first step in any data analysis task is to plot the data. 
Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. 
The features that you see in the plots must then be incorporated into the forecasting methods that you use. 
Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.
You will use the autoplot() function to produce time plots of the data. 
In each plot, look out for outliers, seasonal patterns, and other interesting features.
Use which.max() to spot the outlier in the gold series. 

library("fpp2")
autoplot(a10)
ggseasonplot(a10)
An interesting variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal. 
ggseasonplot(a10, polar = TRUE)
beer <- window(a10, start=1992)
autoplot(beer)
ggseasonplot(beer)
Use the window() function to consider only the ausbeer data from 1992 and save this to beer. 
Set a keyword start to the appropriate year.

x <- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});

</pre>

<pre>

<h2>x <- c(sort(sample(1:20, 9)), NA)</h2>
#===================
x <- c(sort(sample(1:20, 9)), NA)
y <- c(sort(sample(3:23, 7)), NA)
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)

alist = readLines("alist.txt")
blist = readLines("blist.txt")
out = setdiff(blist, alist)

writeClipboard(out)

</pre>

<pre>

<h2># To skip 3rd iteration and go to next iteration</h2>
#===================
# To skip 3rd iteration and go to next iteration
for(n in 1:5) {
  if(n==3) next
  cat(n)
}

</pre>

<pre>

<h2>googleVis chart</h2>
#===================
googleVis chart
===============
library(googleVis)

Line chart
==========
df=data.frame(country=c("US", "GB", "BR"), 
              val1=c(10,13,14), val2=c(23,12,32))
Line <- gvisLineChart(df)
plot(Line)

Scatter chart
=======================
# example 1
dat <- data.frame(x=c(1,2,3,4,5), y1=c(0,3,7,5,2), y2=c(1,NA,0,3,2))
plot(gvisScatterChart(dat, options=list(lineWidth=2, pointSize=2, width=900, height=600)))

# example 2, women
Scatter <- gvisScatterChart(women, 
               options=list(
                 legend="none", lineWidth=1, pointSize=2,
                 title="Women", vAxis="{title:'weight (lbs)'}",
                 hAxis="{title:'height (in)'}", width=900, height=600)
           )
plot(Scatter)

# example 3
ex3dat <- data.frame(x=c(1,2,3,4,5,6,7,8), y1=c(0,3,7,5,2,0,8,6), y2=c(1,NA,0,3,2,6,4,2))
ex3 <- gvisScatterChart(ex3dat, 
           options=list(
             legend="none", lineWidth=1, pointSize=2,
             title="ex3", vAxis="{title:'weight (lbs)'}",
             hAxis="{title:'height (in)'}", width=900, height=600)
       )
plot(ex3)
# Note: to plot timeline chart, arrange the time in x axis, beginning with -ve and the last is 1 to show the sequence


<h2>cat to a file</h2>
cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "data.txt", sep = "\n")

<h3>cat append to a file, open file in "a" mode</h3>
#===================
textVector = c("First thing","Second thing","c")

catObj <- file("theappend.txt", open = "a")
cat(textVector, file = catObj, sep="\n")
close(catObj)


</pre>

<pre>

<h2>install.packages("readr")</h2>
#===================
install.packages("readr")
library(readr)

</pre>

<pre>

<h2>iconv(keyword, "unknown", "GB2312")</h2>
#===================
iconv(keyword, "unknown", "GB2312")


</pre>

<pre>

<h2>Grabbing HTML Tags</h2>
#==========
Grabbing HTML Tags

<TAG\b[^>]*>(.*?)</TAG> matches the opening and closing pair of a specific HTML tag. 

Anything between the tags is captured into the first backreference. 
The question mark in the regex makes the star lazy, to make sure it stops before the first closing tag rather than before the last, like a greedy star would do. 
This regex will not properly match tags nested inside themselves, like in <TAG>one<TAG>two</TAG>one</TAG>.

<([A-Z][A-Z0-9]*)\b[^>]*>(.*?)</\1> will match the opening and closing pair of any HTML tag. 
Be sure to turn off case sensitivity. 
The key in this solution is the use of the backreference \1 in the regex. 
Anything between the tags is captured into the second backreference. 
This solution will also not match tags nested in themselves


</pre>

<pre>

<h2>find the new item</h2>
#==========
find the new item

theList = c("00700","02318","02007")
newList=c("03333","01398","02007")

newList[!(newList %in% theList)]

</pre>

<pre>

<h2>formating numbers</h2>
#==========
formating numbers
a <- seq(1,101,25)
sprintf("%03d", a)

format(round(a, 2), nsmall = 2)

</pre>

<pre>

<h2>the match function:</h2>
#==========
the match function:
match(x, table, nomatch = NA_integer_, incomparables = NULL)
%in%
match returns a vector of the positions of (first) matches of its first argument in its second.

Corpus<- c('animalada', 'fe', 'fernandez', 'ladrillo')
Lexicon<- c('animal', 'animalada', 'fe', 'fernandez', 'ladr', 'ladrillo')
Lexicon %in% Corpus

Lexicon[Lexicon %in% Corpus]

</pre>

<pre>

<h2>Machine Learning:</h2>
<a href="https://machinelearningmastery.com/machine-learning-in-r-step-by-step/">machine-learning-in-r-step-by-step</a>
<br>
<a href="https://lgatto.github.io/IntroMachineLearningWithR/index.html">An Introduction to Machine Learning with R</a>
<br>
<a href="https://www.r-bloggers.com/image-recognition-tutorial-in-r-using-deep-convolutional-neural-networks-mxnet-package/">mxnet</a>
<br>
<a href="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/">image classification</a>
<br>
#==========
Machine Learning:

The caret package

Caret contains wrapper functions that allow you to use the exact same functions for training and predicting with dozens of different algorithms. On top of that, it includes sophisticated built-in methods for evaluating the effectiveness of the predictions you get from the model. 

Use The Titanic dataset

Training a model
training a bunch of different decision trees and having them vote 
Random forests work pretty well in *lots* of different situations, so I often try them first.

Evaluating the model

Cross-validation is a way to evaluate the performance of a model without needing any other data than the training data. 

Making predictions on the test set

Improving the model



</pre>

<pre>

<h2>to handle error 404 when scraping: use tryCatch()</h2>
#==========
to handle error 404 when scraping: use tryCatch()

for (i in urls) {
    tmp <- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}
#==========
try(readLines(url), silent = TRUE)

tryCatch(readLines(url), error = function (e) conditionMessage(e))


</pre>

<pre>

<h2>write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",</h2>
#==========
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
            eol = "\n", na = "NA", dec = ".", row.names = FALSE,
            col.names = FALSE, qmethod = c("escape", "double"),
            fileEncoding = "")

write.table(finalTableList, theOutputname, row.names=FALSE, col.names=FALSE, quote = FALSE, sep = "\t" )

</pre>


<pre>

<h2>Four normal distribution functions:</h2>
#==========
Four normal distribution functions:

<a href="https://www.r-bloggers.com/normal-distribution-functions/">Four normal distribution functions:</a>
<br>

RNORM	Generates random numbers from normal distribution	
rnorm(n, mean, sd)
rnorm(1000, 3, .25)	Generates 1000 numbers from a normal with mean 3 and sd=.25

DNORM	Probability Density Function(PDF)
dnorm(x, mean, sd)
dnorm(0, 0, .5)	Gives the density (height of the PDF) of the normal with mean=0 and sd=.5. 

PNORM	Cumulative Distribution Function
(CDF)	pnorm(q, mean, sd)
pnorm(1.96, 0, 1)	Gives the area under the standard normal curve to the left of 1.96, i.e. ~0.975

QNORM	Quantile Function – inverse of
pnorm	qnorm(p, mean, sd)
qnorm(0.975, 0, 1)	Gives the value at which the CDF of the standard normal is .975, i.e. ~1.96

Note that for all functions, leaving out the mean and standard deviation would result in default values of mean=0 and sd=1, a standard normal distribution.


</pre>

<pre>

<h2>pnorm students scoring higher than 84</h2>
#==========
pnorm students scoring higher than 84
> pnorm(84, mean=72, sd=15.2, lower.tail=FALSE) 
[1] 0.21492
Answer
The percentage of students scoring 84 or more in the college entrance exam is 21.5%.


</pre>

<pre>

<h2>plot a histogram of 1000 draws from a normal distribution with mean 10, standard deviation 2.</h2>
#==========
plot a histogram of 1000 draws from a normal distribution with mean 10, standard deviation 2.
set.seed(seed)
x = rnorm(1000, 10, 2)
plot(x)
hist(x)

Using a QQ plot. Assess the normality:
qqnorm(x)
qqline(x)

In statistics, a Q–Q (quantile-quantile) plot is a probability plot, 
which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.
First, the set of intervals for the quantiles is chosen. 
A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). 
Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.


</pre>

<pre>

<h2>format leading zeros</h2>
#==========
format leading zeros

formatC(1, width = 2, format = "d", flag = "0")


</pre>

<pre>

<h2>library(pdftools)</h2>
#==========
setwd("C:/Users/User/Desktop")
library(pdftools)
txt <- pdf_text("a.pdf")
str(txt)	# 361 pages
writeClipboard(txt[1])

txt1 = gsub(".*ORIGINATOR", "", txt)
txt1 = gsub("          ", "", txt1)

list = c(13:16, 19:22, 25:28, 31:34, 37:42, 45:48, 52:58, 62:68, 71:75, 78:85, 88:95, 98:105, 108:115, 118:124, 127:133, 136:142, 145:156, 159:169, 173:202, 206:221, 225:240, 244:258, 261:274, 277:290, 294:298, 302:308, 312:318, 323:331, 334:345, 348:359)

txt1 = txt1[list]

writeClipboard(txt1)

pdf_info("a.pdf")
pdf_text("a.pdf")
pdf_fonts("a.pdf")
pdf_attachments("a.pdf")
pdf_toc("a.pdf")

toc = pdf_toc("a.pdf")
sink("test.txt")
print(toc)
sink()

</pre>

<pre>

<h2>library(pdftools)</h2>
#==========
library(pdftools)
txt <- pdf_text("a.pdf")
str(txt)
txtList = unlist(strsplit(txt, "\\s{2,}"))

writeClipboard(txtList)

</pre>


<pre>

<h2>The name of the site environment variable R_ENVIRON</h2>
#==========
The name of the site environment variable R_ENVIRON
"R_HOME/etc/Renviron.site"

the default is "R_HOME/etc/Rprofile.site"

Sys.getenv("R_USER").

Examples

## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, 
so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, 
digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", 
"onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First <- function() cat("\n   Welcome to R!\n\n")
.Last <- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, 
set a CRAN mirror
  old <- getOption("defaultPackages"); r <- getOption("repos")
  r["CRAN"] <- "http://my.local.cran"
  options(defaultPackages = c(old, 
"MASS"), 
repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols <- Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), 
"\n")
# coo\bardoh\exabc"def'



</pre>

<pre>

<h2>How to Convert Factor into Numerical?</h2>
#==========
How to Convert Factor into Numerical?

When you convert factors to numeric, 
first you should convert it into characters and then convert into numeric. 
as.numeric(as.character(X))

Df$column<-as.numeric(as.factor(df$column)

as.integer(as.factor(region))


</pre>

<pre>
<h2>options(error=recover)</h2>
#==========
options(error=recover)

recover {utils}
Browsing after an Error

This function allows the user to browse directly on any of the currently active function calls, and is suitable as an error option.
The expression options(error = recover) will make this the error option.

Usage
recover()

When called, recover prints the list of current calls, and prompts the user to select one of them.
The standard R browser is then invoked from the corresponding environment;
the user can type ordinary R language expressions to be evaluated in that environment.

Turning off the options() debugging mode in R
options(error=NULL)


</pre>


<h2>Extract hyperlink from Excel file in R</h2>
<pre>
#==========

library(XML)

# rename file to .zip
my.zip.file <- sub("xlsx", "zip", my.excel.file)
file.copy(from = my.excel.file, to = my.zip.file)

# unzip the file
unzip(my.zip.file)

# unzipping produces a bunch of files which we can read using the XML package
# assume sheet1 has our data
xml <- xmlParse("xl/worksheets/sheet1.xml")

# finally grab the hyperlinks
hyperlinks <- xpathApply(xml, "//x:hyperlink/@display", namespaces="x")


<span class="redword">To repair Hyperlink address corrupted:</span>
copy file to desk top and rename to zip file
open zip file and locate: <span class="redword">\xl\worksheets\_rels</span>
open the sheet1.xml.rels with editor
remove all text: D:\Users\Lawht\AppData\Roaming\Microsoft\Excel\

</pre>


<h2>Extract part of a string</h2>
<pre>
#==========
x <- c("75 to 79", "80 to 84", "85 to 89")
substr(x, start = 1, stop = 2)

</pre>
<br>
<h2>alter grades</h2>
<pre>
#==========
alter grades

locate the word
get the line location
alter the score table
#==========

locate the word
v <- c('a','b','c','e')
'b' %in% v ## returns TRUE
match('b',v) ## returns the first location of 'b', in this case: 2

subv <- c('a', 'f')
subv %in% v ## returns a vector TRUE FALSE
is.element(subv, v) ## returns a vector TRUE FALSE

which()
which('a' == v) #[1] 2 4 For finding all occurances as vector of indices

grep() returns a vector of integers, which indicate where matches are.
yo <- c("a", "a", "b", "b", "c", "c")
grep("b", yo) # [1] 3 4

ROC<-"中華民國 – 維基百科，自由的百科全書"
grep("中華民國",ROC)

Partial String Matching
pmatch("med", c("mean", "median", "mode")) # returns 2

</pre>
<br>
<br>
<h2>table, cut and barplot</h2>
<pre>
atab=c(1,2,3,2,1,2,3,4,5,4)
table(atab)
cut(atab, 2)
table( cut(atab, 2))
counts = table( cut(atab, 4))
barplot(counts, main="Qty", xlab="grade")

</pre>
<br>
<br>
<br>
<h2>non-paste answer to concatenate two strings</h2>
<pre>
capture.output(cat(counts, sep = ","))

</pre>
<br>
<h2>V8 is an R interface JavaScript engine. </h2>
<pre>
This package helps us execute javascript code in R

#Loading both the required libraries
library(rvest)
library(V8)

#URL with js-rendered content to be scraped
link <- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'

#Read the html page content and extract all javascript codes that are inside a list

emailjs <- read_html(link) %>% html_nodes('li') %>% html_nodes('script') %>% html_text()

# Create a new v8 context
ct <- v8()

#parse the html content from the js output and print it as text
read_html(ct$eval(gsub('document.write','',emailjs))) %>%  html_text()

info@brewhemia.co.uk

Thus we have used rvest to extract the javascript code snippet from the desired location (that is coded in place of email ID) and used V8 to execute the javascript snippet (with slight code formatting) and output the actual email (that is hidden behind the javascript code). 

####################
Getting email address through rvest
You need a javascript engine here to process the js code.
R has got V8.

Modify your code after installing V8 package:
library(rvest)
library(V8)

link <- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'
page <- read_html(link)
name_html <- html_nodes(page,'.placeHeading')
business_adr <- html_text(adr_html)
tel_html <- html_nodes(page,'.value')
business_tel <- html_text(tel_html)
emailjs <- page %>% html_nodes('li') %>% html_nodes('script') %>% html_text()
ct <- v8()
read_html(ct$eval(gsub('document.write','',emailjs))) %>% html_text()

</pre>
<br>
<br>
<h2>extract protected pdf document</h2>
<pre>
library(pdftools)
setwd("C:/Users/User/Desktop")
txt <- pdf_text("a.pdf")
str(txt)	# 361 pages
# copy page 1
writeClipboard(txt[1])
# copy page 2
writeClipboard(txt[2])
# copy page 3
writeClipboard(txt[3])

Convert unicode character to string format: remove "\u"

theStr = "\u9999\u6e2f\u98df\u54c1\u6295\u8d44"	#  "香港食品投资"

=============================

Sys.setlocale(category = 'LC_ALL', 'Chinese')

library(pdftools)
setwd("C:/Users/User/Desktop")
txt <- pdf_text("45.pdf")
str(txt)

chi1 = gsub('\\u' , '&#x', txt[1])
chi2 = gsub('\\u' , '&#x', txt[2])
chi3 = gsub('\\u' , '&#x', txt[3])
sink("aaa.txt")
cat(chi1)
cat(chi2)
cat(chi3)
sink()


</pre>
<br>
<br>
<h2>Writing an R package</h2>
<pre>
  <a href="https://support.rstudio.com/hc/en-us/articles/200486488-Developing-Packages-with-RStudio">Develop Packages with RStudio</a>
<br>
<a href="http://web.mit.edu/insong/www/pdf/rpackage_instructions.pdf">rpackage_instructions.pdf</a>
<br>
<a href="https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/">Writing an R package from scratch</a>
<a href="Writing an R package.html"><span class="goldb">Writing an R package</span></a> 
<br>

</pre>
<br>
<br>
<h2>table, cut and breaks</h2>
<pre>
table(cut(as.numeric(resultTable[,3]), 10))
cut(as.numeric(resultTable[,3]),10)
breaks = c(seq(lower, 0, by = 5), 0, seq(0, upper, by = 5))

tableA = c(1,3,5,7,9)
tableB = c(1,3,5,7,2,4,6,8)
tableA = c(tableA, tableB)
tableA = sort(tableA)

table(tableA)

table(cut(tableA, 3))

breaks = c(seq(1, 3, by = 1), 4, seq(5, 9, by = 2))
table(cut(tableA, breaks))

</pre>
<br>
<br>
<h2>List the Files in a Directory</h2>
<pre>
List the Files in a Directory/Folder
list.files()

list.dirs(R.home("doc"))
list.dirs()
</pre>
<br>
<br>
<h2>test url exist</h2>
<pre>
library(httr)
http_error(theUrl)

<a href="https://stackoverflow.com/questions/18407177/load-image-from-website">Load image from website</a>
download.file("url", destfile="tmp.png", mode="wb")

<a href="https://jangorecki.gitlab.io/data.table/library/RCurl/html/url.exists.html">url.exists {RCurl}	</a> return true of false
<a href="https://stackoverflow.com/questions/31420210/r-check-existence-of-url-problems-with-httrget-and-url-exists">With httr use url_success()</a>
<br>
</pre>
<br>
<br>
<h2>Passing arguments to R script</h2>
<pre>
<a href="Passing arguments to R script.html"><span class="goldb">Passing arguments to R script</span></a> 

Rscript --vanilla testargument.R iris.txt newname

To avoid Rscript.exe loop forever for keyboard input:
use this:
cat("a string please: ");
a <- readLines("stdin",n=1);

</pre>
<br>
<br>
<h2>School Revision Papers</h2>
<pre>
http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2018/
http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2017/
https://curriculum.gov.mt/en/Examination-Papers/Pages/list_secondary_papers.aspx
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F1-2ndTest-Eng.pdf
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F2-2ndTest-Eng.pdf
http://www.sttss.edu.hk/parents_corner/pastpaper.php
</pre>
<br>
<br>
<h2>difference between 1L and 1</h2>
<pre>
L specifies an integer type, rather than a double, it uses only 4 bytes per element
the function as.integer is simplified yb  "L " suffix

> str(1)
 num 1

> str(1L)
 int 1
</pre>
<br>
<br>
<br>
<h2><span class="white goldbs">Datatable</span></h2>
<pre>
<a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf">Datatable Cheat Sheet</a>
library(data.table)

dt=data.table(read.table("wAveTable.txt", header=TRUE, colClasses=c('character', 'numeric', 'numeric')))
colnames(dt)
"Code"   "WAve5"  "WAve10"
dt[WAve5 > 5, ]
summary(dt[WAve5 = 5, ])
summary(dt[WAve5 %between% c(7,9), ])

data.table dt subset rows using i, and manipulate columns with j, grouped according to by	dt[i, j, by]
Create a data.table	data.table(a = c(1, 2), b = c("a", "b"))
convert a data frame or a list to a data.table	setDT(df) or as.data.table(df)
Subset data.table rows using i	dt[1:2, ]
subset data.table rows based on values in one or more columns	dt[a > 5, ]
data.table Logical Operators To Use In i	>,<,<=,>=, |, !,&, is.na(),!is.na(), %in%, %like%,  %between%
data.table extract column(s) by number. Prefix column numbers with “-” to drop	dt[, c(2)]
data.table extract column(s) by name	dt[, .(b, c)]
create a data.table with new columns based on the summarized values of rows	dt[, .(x = sum(a))]
compute a data.table column based on an expression	dt[, c := 1 + 2]
compute a data.table column based on an expression but only for a subset of rows	dt[a == 1, c := 1 + 2]
compute a data.table multiple columns based on separate expressions	dt[, `:=`(c = 1 , d = 2)]
delete a data.table column	dt[, c := NULL]
convert the type of a data.table column using as.integer(), as.numeric(), as.character(), as.Date(), etc..	dt[, b := as.integer(b)]
group data.table rows by values in specified column(s)	dt[, j, by = .(a)]
group data.table and simultaneously sort rows according to values in specified column(s)	dt[, j, keyby = .(a)]
summarize data.table rows within groups	dt[, .(c = sum(b)), by = a]
create a new data.table column and compute rows within groups	dt[, c := sum(b), by = a]
extract first data.table row of groups	dt[, .SD[1], by = a]
extract last data.table row of groups	dt[, .SD[.N], by = a]
perform a sequence of data.table operations by chaining multiple “[]”	dt[…][…]
reorder a data.table according to specified columns	setorder(dt, a, -b), “-” for descending
data.table’s functions prefixed with “set” and the operator “:=”	work without “<-” to alter data without making copies in memory
df <- as.data.table(df)	setDT(df)
extract unique data.table rows based on columns specified in “by”. Leave out “by” to use all columns	unique(dt, by = c("a", "b"))
return the number of unique data.table rows based on columns specified in “by”	uniqueN(dt, by = c("a", "b"))
rename data.table column(s)	setnames(dt, c("a", "b"), c("x", "y"))
data.table Syntax	DT[ i , j , by], i refers to rows. j refers to columns. by refers to adding a group
data.table Syntax arguments	DT[ i , j , by], with, which, allow.cartesian, roll, rollends, .SD, .SDcols, on, mult, nomatch
data.table fread() function	to read data, mydata = fread("https://github.com/flights_2014.csv")
data.table select only 'origin' column returns a vector	dat1 = mydata[ , origin]
data.table select only 'origin' column returns a data.table	dat1 = mydata[ , .(origin)] or dat1 = mydata[, c("origin"), with=FALSE]
data.table select column	dat2 =mydata[, 2, with=FALSE]
data.table select column Multiple Columns	dat3 = mydata[, .(origin, year, month, hour)], dat4 = mydata[, c(2:4), with=FALSE]
data.table Dropping Column	adding ! sign, dat5 = mydata[, !c("origin"), with=FALSE]
data.table Dropping Multiple Columns	dat6 = mydata[, !c("origin", "year", "month"), with=FALSE]
data.table select variables that contain 'dep'	use %like% operator, dat7 = mydata[,names(mydata) %like% "dep", with=FALSE]
data.table Rename Variables	setnames(mydata, c("dest"), c("Destination"))
data.table  rename multiple variables	setnames(mydata, c("dest","origin"), c("Destination", "origin.of.flight"))
data.table find all the flights whose origin is 'JFK'	dat8 = mydata[origin == "JFK"]
data.table Filter Multiple Values	dat9 = mydata[origin %in% c("JFK", "LGA")]
data.table selects not equal to 'JFK' and 'LGA'	dat10 = mydata[!origin %in% c("JFK", "LGA")]
data.table Filter Multiple variables	dat11 = mydata[origin == "JFK" & carrier == "AA"]
data.table Indexing Set Key	tells system that data is sorted by the key column
data.table setting 'origin' as a key	setkey(mydata, origin), 'origin' key is turned on. data12 = mydata[c("JFK", "LGA")]
data.table Indexing Multiple Columns	setkey(mydata, origin, dest), key is turned on. mydata[.("JFK", "MIA")] # First key 'origin' matches “JFK” second key 'dest' matches “MIA”
data.table Indexing Multiple Columns equivalent	mydata[origin == "JFK" & dest == "MIA"]
data.table  identify the column(s) indexed by	key(mydata)
data.table sort data using setorder()	mydata01 = setorder(mydata, origin)
data.table sorting on descending order	mydata02 = setorder(mydata, -origin)
data.table Sorting Data based on multiple variables	mydata03 = setorder(mydata, origin, -carrier)
data.table Adding Columns (Calculation on rows)	use := operator, mydata[, dep_sch:=dep_time - dep_delay]
data.table Adding Multiple Columns	mydata002 = mydata[, c("dep_sch","arr_sch"):=list(dep_time - dep_delay, arr_time - arr_delay)]
data.table IF THEN ELSE Method I	mydata[, flag:= 1*(min < 50)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table IF THEN ELSE Method II	mydata[, flag:= ifelse(min < 50, 1,0)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table build a chain	DT[ ] [ ] [ ], mydata[, dep_sch:=dep_time - dep_delay][,.(dep_time,dep_delay,dep_sch)]
data.table Aggregate Columns mean	mydata[, .(mean = mean(arr_delay, na.rm = TRUE),
data.table Aggregate Columns median	median = median(arr_delay, na.rm = TRUE),
data.table Aggregate Columns min	min = min(arr_delay, na.rm = TRUE),
data.table Aggregate Columns max	max = max(arr_delay, na.rm = TRUE))]
data.table Summarize Multiple Columns	all the summary function in a bracket, mydata[, .(mean(arr_delay), mean(dep_delay))]
data.table .SD operator	implies 'Subset of Data'
data.table .SD and .SDcols operators	calculate summary statistics for a larger list of variables
data.table calculates mean of two variables	mydata[, lapply(.SD, mean), .SDcols = c("arr_delay", "dep_delay")]
data.table Summarize all numeric Columns	mydata[, lapply(.SD, mean)]
data.table Summarize with multiple statistics	mydata[, sapply(.SD, function(x) c(mean=mean(x), median=median(x)))]
data.table Summarize by group 'origin	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = origin]
data.table Summary by group useing keyby= operator	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), keyby = origin]
data.table Summarize multiple variables by group 'origin'	mydata[, .(mean(arr_delay, na.rm = TRUE), mean(dep_delay, na.rm = TRUE)), by = origin], or mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin]
data.table remove non-unique / duplicate cases with unique()	setkey(mydata, "carrier"), unique(mydata)
data.table remove duplicated	setkey(mydata, NULL), unique(mydata), Note : Setting key to NULL is not required if no key is already set.
data.table Extract values within a group	mydata[, .SD[1:2], by=carrier], selects first and second values from a categorical variable carrier.
data.table Select LAST value from a group	mydata[, .SD[.N], by=carrier]
data.table window function frank()	dt = mydata[, rank:=frank(-distance,ties.method = "min"), by=carrier], calculating rank of variable 'distance' by 'carrier'. 
data.table cumulative sum cumsum()	dat = mydata[, cum:=cumsum(distance), by=carrier]
data.table lag and lead with shift()	shift(variable_name, number_of_lags, type=c("lag", "lead")), DT <- data.table(A=1:5), DT[ , X := shift(A, 1, type="lag")], DT[ , Y := shift(A, 1, type="lead")]
data.table  %between% operator to define a range	DT = data.table(x=6:10), DT[x %between% c(7,9)]
data.table %like% to find all the values that matches a pattern	DT = data.table(Name=c("dep_time","dep_delay","arrival"), ID=c(2,3,4)), DT[Name %like% "dep"] 
data.table Inner Join	Sample Data: (dt1 <- data.table(A = letters[rep(1:3, 2)], X = 1:6, key = "A")), (dt2 <- data.table(A = letters[rep(2:4, 2)], Y = 6:1, key = "A")), merge(dt1, dt2, by="A")
data.table Left Join	merge(dt1, dt2, by="A", all.x = TRUE)
data.table Right Join	merge(dt1, dt2, by="A", all.y = TRUE)
data.table Full Join	merge(dt1, dt2, all=TRUE)
Convert a data.table to data.frame	setDF(mydata)
convert data frame to data table	setDT(), setDT(X, key = "A")
data.table Reshape Data	dcast.data.table() and melt.data.table()
data.table Calculate total number of rows by month and then sort on descending order	mydata[, .N, by = month] [order(-N)], The .N operator is used to find count.
data.table Find top 3 months with high mean arrival delay	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = month][order(-mean_arr_delay)][1:3]
data.table Find origin of flights having average total delay is greater than 20 minutes	mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin][(arr_delay + dep_delay) > 20]
data.table Extract average of arrival and departure delays for carrier == 'DL' by 'origin' and 'dest' variables	mydata[carrier == "DL", lapply(.SD, mean, na.rm = TRUE), by = .(origin, dest), .SDcols = c("arr_delay", "dep_delay")]
data.table Pull first value of 'air_time' by 'origin' and then sum the returned values when it is greater than 300	mydata[, .SD[1], .SDcols="air_time", by=origin][air_time > 300, sum(air_time)]

</pre>
<br>
<br>
<h2><span class="gold bordred1 blink">R Web Scraping</span></h2>
<a href="https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples" class="whitebut red bluebs blueblackgrad whitets blinkNmove">get started xpath selectors</a>
<pre>
<a href="https://www.datacamp.com/community/tutorials/r-web-scraping-rvest">R Web Scraping Rvest</a>
<a href="http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl/">webscraping-using-readlines-and-rcurl</a>
<a href="https://www.rdocumentation.org/packages/XML/versions/3.98-1.16/topics/xmlTreeParse">xmlTreeParse, htmlTreeParse</a>
<a href="http://www.cse.chalmers.se/~chrdimi/downloads/web/getting_web_data_r4_parsing_xml_html.pdf">getting web data parsing xml html</a>
<a href="https://stackoverflow.com/questions/35479549/error-in-r-no-applicable-method-for-xpathapply">error in r no applicable method for xpathapply</a>
<a href="https://blog.rstudio.com/2015/04/21/xml2/">Parse and process XML (and HTML) with xml2</a>
==================
web_page <- readLines("http://www.interestingwebsite.com")
web_page <- read.csv("http://www.programmingr.com/jan09rlist.html")

    # General-purpose data wrangling
    library(tidyverse)  

    # Parsing of HTML/XML files  
    library(rvest)    

    # String manipulation
    library(stringr)   

    # Verbose regular expressions
    library(rebus)     

    # Eases DateTime manipulation
    library(lubridate)

==================
install.packages("RCurl", dependencies = TRUE)
library("RCurl")
library("XML")

past <- getURL("http://www.iciba.com/past", ssl.verifypeer = FALSE)	# getURL cannot work
webpage <- read_html("http://www.iciba.com/past")	# getURL cannot work

jan09_parsed <- htmlTreeParse(jan09)

==================
http://www.iciba.com/past
ul class="base-list switch_part" class

library('rvest')
library(tidyverse)
url <- 'http://www.iciba.com/past'
webpage <- readLines(url, warn=FALSE)
webpage <- read_html(webpage)
grappedData <- html_nodes(webpage,'.base-list switch_part')

parseData = htmlTreeParse(webpage)

rank_data <- html_text(grappedData)

html_node("#mw-content-text > div > table:nth-child(18)")
html_table()

the function htmlParse() which is equivalent to xmlParse(file, isHTML = TRUE)
output = htmlParse(webpage)
class(output)

To parse content into an R structure :
htmlTreeParse() which is equivalent to htmlParse(file, useInternalNodes = FALSE)
output = htmlTreeParse(webpage)
class(output)

htmlTreeParse(file) especially suited for parsing HTML content
returns class "XMLDocumentContent" (R data structure)
equivalent to
xmlParse(file, isHTML = TRUE, useInternalNodes = FALSE)
htmlParse(file, useInternalNodes = FALSE)

root =xmlRoot(output)
xmlChildren(output)
xmlChildren(xmlRoot(output))
XMLNodeList

Functions for a given node
Function Description
xmlName() name of the node
xmlSize() number of subnodes
xmlAttrs() named character vector of all attributes
xmlGetAttr() value of a single attribute
xmlValue() contents of a leaf node
xmlParent() name of parent node
xmlAncestors() name of ancestor nodes
getSibling() siblings to the right or to the left
xmlNamespace() the namespace (if there’s one)

to parse HTML tables using R
sched <- readHTMLTable(html, stringsAsFactors = FALSE)

The html.raw object is not immediately useful because it literally contains all of the raw HTML for the entire webpage. We can parse the raw code using the xpathApply function which parses HTML based on the path argument, which in this case specifies parsing of HTML using the paragraph tag.

html.raw<-htmlTreeParse('http://www.dnr.state.mn.us/lakefind/showreport.html?downum=27013300',
    useInternalNodes=T    )
html.parse<-xpathApply(html.raw, "//p", xmlValue)

# evaluate input and convert to text
txt <- htmlToText(url)

==================
url <- 'http://www.iciba.com/past'
webpage <- readLines(url, warn=FALSE)
scraping_wiki <- read_html(webpage)
scraping_wiki %>% html_nodes("h1") %>% html_text()

url <- 'testvibrate.html'
webpage <- readLines(url, warn=FALSE)
x <- read_xml(webpage)
xml_name(x)
===========

This cannot work in office
library(rvest)
Sys.setlocale(category = 'LC_ALL', 'Chinese')
webpage <- read_html("http://www.iciba.com/haunt")
ullist = webpage %>% html_nodes("ul")
content = ullist[2] %>% html_text()
content = gsub("n.| |\n|adj.|adv.|prep.|vt.|vi.|&","",content)
content = gsub("，|；"," ",content) %>%  strsplit(split = " ") %>% unlist() %>% sort() %>% unique()
paste0("past","\t",capture.output(cat(content)))

</pre>
<br>
<br>
<h2>R scraping html text example</h2>
<pre>
<a href="http://bradleyboehmke.github.io/2015/12/scraping-html-text.html">scraping-html-text</a>
library(rvest)

scraping_wiki <- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scraping_wiki %>% html_nodes("h1")
scraping_wiki %>% html_nodes("h2")
scraping_wiki %>% html_nodes("h1") %>% html_text()
scraping_wiki %>% html_nodes("h2") %>% html_text()
p_nodes <- scraping_wiki %>% html_nodes("p")
length(p_nodes)
p_text <- scraping_wiki %>% html_nodes("p") %>% html_text()
p_text[1]
p_text[5]

ul_text <- scraping_wiki %>% html_nodes("ul") %>% html_text()
length(ul_text)
ul_text[1]
substr(ul_text[2], start = 1, stop = 200)

li_text <- scraping_wiki %>% html_nodes("li") %>% html_text()
length(li_text)
li_text[1:8]
li_text[104:136]

all_text <- scraping_wiki %>% html_nodes("div") %>%  html_text()

body_text <- scraping_wiki %>% html_nodes("#mw-content-text") %>%  html_text()

# read the first 207 characters
substr(body_text, start = 1, stop = 207)

# read the last 73 characters
substr(body_text, start = nchar(body_text)-73, stop = nchar(body_text))

# Scraping a specific heading
scraping_wiki %>% html_nodes("#Techniques") %>%  html_text()
## [1] "Techniques"

# Scraping a specific paragraph
scraping_wiki %>% html_nodes("#mw-content-text > p:nth-child(20)") %>%  html_text()

# Scraping a specific list
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

# Scraping a specific reference list item
scraping_wiki %>% html_nodes("#cite_note-22") %>%  html_text()

# Cleaning up
library(magrittr)
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text() %>%  strsplit(split = "\n") %>% unlist() %>% .[. != ""]


library(stringr)

# read the last 700 characters
substr(body_text, start = nchar(body_text)-700, stop = nchar(body_text))

# clean up text
body_text %>% 
str_replace_all(pattern = "\n", replacement = " ") %>% 
str_replace_all(pattern = "[\\^]", replacement = " ") %>% 
str_replace_all(pattern = "\"", replacement = " ") %>% 
str_replace_all(pattern = "\\s+", replacement = " ") %>% 
str_trim(side = "both") %>% 
substr(start = nchar(body_text)-700, stop = nchar(body_text))

################
# rvest tutorials
https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/
https://blog.gtwang.org/r/rvest-web-scraping-with-r/
https://www.rdocumentation.org/packages/rvest/versions/0.3.4
https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/
https://lmyint.github.io/post/dnd-scraping-rvest-rselenium/

################
# parse guancha
library(rvest)
pageHeader="https://user.guancha.cn/main/content?id=181885"
pagesource <- read_html(pageHeader)

################
# parse RTHK and metroradio
library(rvest)
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource <- read_html(pageHeader)
className = ".ns2-title"
keywordList <- html_nodes(pagesource, className)
html_text(keywordList)

pageHeader = "http://www.metroradio.com.hk/MetroFinance/News/NewsLive.aspx"
pagesource <- read_html(pageHeader)
className = ".n13newslist"
keywordList <- html_nodes(pagesource, className)
className = "a"
keywordList <- html_nodes(keywordList, className)
html_text(keywordList)

################
# parse xhamster
library(rvest)
pageHeader = "https://xhamster.com/users/fredlake/photos"
pagesource <- read_html(pageHeader)
className = ".xh-paginator-button"
keywordList <- html_nodes(pagesource, className)
html_text(keywordList)
html_name(keywordList)
html_attrs(keywordList)

thelist = unlist(html_attrs(keywordList))

length(keywordList)
as.numeric(html_text(keywordList[length(keywordList)]))

pagesource %>% html_nodes(className) %>% html_text() %>% as.numeric()

for ( i in keywordList ) { 
 qlink <- html_nodes(s, ".gallery-thumb")
 cat("Title:", html_text(qlink), "\n")
 qviews <- html_nodes(s, "name")
 cat("Views:", html_text(qviews), "\n")
}
################
# parse text and href
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource <- read_html(pageHeader)

className = ".ns2-title"
keywordList <- html_nodes(pagesource, className)

className = "a"
a <- html_nodes(keywordList, className)

html_text(a)
html_attr(a, "href")

################
# extract huanqiu.com gallery

pageHeader = "https://china.huanqiu.com/gallery/9CaKrnQhXac"
pagesource <- read_html(pageHeader)
className = "article"
keywordList <- html_nodes(pagesource, className)

className = "img"
img <- html_nodes(keywordList, className)
html_attr(img, "src")
html_attr(img, "data-alt")

################
# html_nodes samples
html_nodes(".a1.b1")
html_nodes(".b1:not(.a1)")  # <i>Select class contains b1 not a1:</i>
html_nodes(".content__info__item__value")
html_nodes("[class='b1']")
html_nodes("center")
html_nodes("font")
html_nodes(ateam, "center")
html_nodes(ateam, "center font")
html_nodes(ateam, "center font b")
html_nodes("table") %>% .[[3]] %>% html_table()
html_nodes("td")
html_nodes() returns all nodes
html_nodes(pagesource, className)
html_nodes(pg, "div > input:first-of-type"), "value")
html_nodes(s, ".gallery-thumb")
html_nodes(s, "name")
html_nodes(xpath = '//*[@id="a"]')

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td <- ateam %>% html_nodes("center") %>% html_nodes("td")
td %>% html_nodes("font")

if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_node("font")
}

# To pick out an element at specified position, use magrittr::extract2
# which is an alias for [[
library(magrittr)
ateam %>% html_nodes("table") %>% extract2(1) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% `[[`(1) %>% html_nodes("img")

# Find all images contained in the first two tables
ateam %>% html_nodes("table") %>% `[`(1:2) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% extract(1:2) %>% html_nodes("img")

# XPath selectors ---------------------------------------------
# If you prefer, you can use xpath selectors instead of css: 
html_nodes(doc, xpath = "//table//td")).

# chaining with XPath is a little trickier - you may need to vary
# the prefix you're using - // always selects from the root node
# regardless of where you currently are in the doc
ateam %>% html_nodes(xpath = "//center//font//b") %>% html_nodes(xpath = "//b")


read_html()
html_node()	# to find the first node
html_nodes(doc, "table td")	# to find the all node
html_nodes(doc, xpath = "//table//td"))

html_name()	# the name of the tag
html_tag()	# Extract the tag names
html_text()	# Extract all text inside the tag 

html_attr()	Extract the a single attribute
html_attrs()	Extract all the attributes

# html_attrs(keywordList) this cannot use id, just list all details
# html_attr(keywordList, "id") this select the ids
# html_attr(keywordList, "href") this select the hrefs

html_nodes("#titleCast .itemprop span")
html_nodes("#img_primary img")
html_nodes("div.name > strong > a")
html_attr("href")

html_text(keywordList, trim = FALSE)
html_name(keywordList)
html_children(keywordList)
html_attrs(keywordList)
html_attr(keywordList, "[href]", default = NA_character_)

parse with xml()
then extract components using 
xml_node()
xml_attr()
xml_attrs()
xml_text() and xml_name()

Parse tables into data frames with 
html_table().

Extract, modify and submit forms with 
html_form()
set_values()
submit_form().

Detect and repair encoding problems with 
guess_encoding()	Detect text encoding
repair_encoding()	repair text encoding

Navigate around a website as if you’re in a browser with 
html_session()
jump_to()
follow_link()
back()
forward()

Extract, modify and submit forms with 
html_form(), 
set_values() 
and submit_form()


The toString() function collapse the list of strings into one.

html_node(":not(#commentblock)")	# exclude tags

######### demos #########
# Inspired by https://github.com/notesofdabbler
library(rvest)
library(tidyr)

page <- read_html("http://www.zillow.com/homes/for_sale/....")

houses <- page %>% html_nodes(".photo-cards li article")
z_id <- houses %>% html_attr("id")

address <- houses %>% html_node(".zsg-photo-card-address") %>% html_text()

price <- houses %>% html_node(".zsg-photo-card-price") %>% html_text() %>% readr::parse_number()

params <- houses %>% html_node(".zsg-photo-card-info") %>% html_text() %>% strsplit("\u00b7")

beds <- params %>% purrr::map_chr(1) %>% readr::parse_number()
baths <- params %>% purrr::map_chr(2) %>% readr::parse_number()
house_area <- params %>% purrr::map_chr(3) %>% readr::parse_number()

################
pagesource %>% html_nodes("table") %>% .[[3]] %>% html_table()

read_html(doc) %>% html_nodes(".b1:not(.a1)") # <i>Select class contains b1 not a1:</i>
# [1] <span class="b1"> text2 </span>

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")
# [1] <span class="b1"> text2 </span>

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
# [1] <span class="a1 b1"> text1 </span>

combine class and ID in CSS selector
div#content.sectionA  # <i>this is 'and' operation</i>

=====================
<i>select 2 classes in 1 tag</i>
Select class contains b1 not a1:
read_html(doc) %>% html_nodes(".b1:not(.a1)")

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
=====================
standard CSS selector specify either or both

html_nodes(".content__info__item__value, skill")  # <i>the comma is 'or' operation</i>
{xml_nodeset (4)}
[1] <span class="content__info__item__value duration">5h 59m 42s</span>
[2] <span class="content__info__item__value skill">Beginner + Intermediate</span>
[3] <span class="content__info__item__value released">September 26, 2013</span>
[4] <span class="content__info__item__value viewers">82,552</span>

# has both classes in_learning_page
html_nodes(".content__info__item__value.skill")   # <i>this is 'and' operation</i>
{xml_nodeset (1)}
[1] <span class="content__info__item__value skill">Beginner + Intermediate</span>


in_learning_page %>%
  html_nodes(".content__info__item__value") %>% 
  str_subset(., "viewers")

h <- read_html(text)

h %>% html_nodes(xpath = '//*[@id="a"]') %>% xml_attr("value")

html_attr(html_nodes(pg, "div > input:first-of-type"), "value")

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td <- ateam %>% html_nodes("center") %>% html_nodes("td")

# When applied to a list of nodes, html_nodes() returns all nodes,
# collapsing results into a new nodelist.
td %>% html_nodes("font")
# nodes, it returns a "missing" node
if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_nodes("font")

</pre>
<br>
<br>
<h2>sort() rank() order()</h2>
<pre>
<strong>Rank</strong> references the position of the value in the sorted vector and is in the same order as the <strong>original </strong>sequence
<strong>Order</strong> returns the position of the original value and is in the order of <strong>sorted </strong>sequence
The graphic below helps tie together the values reported by rank and order with the positions from which they come.
<img class="lazy" data-src="https://cdn-images-1.medium.com/max/800/1*3KeaXU6luJDyoatWkEwdug.jpeg">
x = c(1, 8,9, 4)
sort(x)
1 4 8 9

# the original position in the sorted order
rank(x)
1 3 4 2

# the sorted position in the original position
order(x)
1 4 2 3
</pre>
<br>
<br>
<h2>Bioinformatics</h2>
<pre>
<a href="https://cran.r-project.org/doc/contrib/Krijnen-IntroBioInfStatistics.pdf">Bioinformatics using R</a>
<br>
<a href="https://www.bioconductor.org/">bioconductor</a>
<br>
<a href="https://www.r-exercises.com/product/introduction-to-bioconductor-annotation-and-analysis-of-genomes-and-genomic-assays/">Introduction to Bioconductor:Annotation and Analysis of Genomes and Genomics Assays</a>
</pre>
<br>
<h2>a list of dataframes, 3D data arrangement</h2>
<pre>
d1 <- data.frame(y1=c(1,2,3),y2=c(4,5,6))
d2 <- data.frame(y1=c(3,2,1),y2=c(6,5,4))
d3 <- data.frame(y1=c(7,8,9),y2=c(5,2,6))
mylist <- list(d1, d2, d3)
names(mylist) <- c("List1","List2","List3")

mylist[1]	# same as mylist$List1

mylist[[2]][1,2]	# access an element inside a dataframe
mylist[[2]][2,2]	# same as mylist$List2[2,2]

to concate another dataframe:

d4 <- data.frame(y1=c(2,5,8),y2=c(1,4,7))
mylist[[4]] <- d4

to create an empty list:
data <- list()
</pre>
<br>
<br>
<h2>format time string</h2>
<pre>
Sys.time()

sub(".* | .*", "", Sys.time())

format(Sys.time(), '%H:%M')

gsub(":", "", format(Sys.time(), '%H:%M'))

</pre>
<br>
<h2>extract 5 digit from string</h2>
<pre>
activityListCode = str_replace(activityListCode, ".*\\b(\\d{5})\\b.*", "\\1")

</pre>
<br>
<h2>access Components of a Data Frame</h2>
<a href="https://www.datamentor.io/r-programming/data-frame/">access Components of a Data Frame</a>
<br>

<p>Components of data frame can be accessed like a list or like a matrix.</p>
<h3>Accessing like a list</h3>
<p>We can use either <code>[</code>, <code>[[</code> or <code>$</code> operator to access columns of data frame.</p>
<pre><code>&gt; x["Name"]
Name
1 John
2 Dora
&gt; x$Name
[1] "John" "Dora"
&gt; x[["Name"]]
[1] "John" "Dora"
&gt; x[[3]]
[1] "John" "Dora"
</code></pre>
<p>Accessing with <code>[[</code> or <code>$</code> is similar. However, it differs for <code>[</code> in that, indexing with <code>[</code> <span class="redword">will return us a data frame</span> but the other two will <span class="redword">reduce it into a vector</span>.</p>

<h3>Accessing like a matrix</h3>
<p>Data frames can be accessed like a matrix by providing index for row and column.</p>
<p>To illustrate this, we use datasets already available in R. Datasets that are available can be listed with the command <code>library(help = "datasets")</code>.</p>
<p>We will use the <code>trees</code> dataset which contains <code>Girth</code>, <code>Height</code> and <code>Volume</code> for Black Cherry Trees.</p>
<p>A data frame can be examined using functions like <code>str()</code> and <code>head()</code>.</p>
<pre><code>&gt; str(trees)
'data.frame':   31 obs. of 3 variables:
$ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...
$ Height: num  70 65 63 72 81 83 66 75 80 75 ...
$ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...
&gt; head(trees,n=3)
Girth Height Volume
1   8.3     70   10.3
2   8.6     65   10.3
3   8.8     63   10.2
</code></pre>
<p>We can see that <code>trees</code> is a data frame with 31 rows and 3 columns. We also display the first 3 rows of the data frame.</p>
<p>Now we proceed to access the data frame like a matrix.</p>
<pre><code>&gt; trees[2:3,]    # select 2nd and 3rd row
Girth Height Volume
2   8.6     65   10.3
3   8.8     63   10.2
&gt; trees[trees$Height &gt; 82,]    # selects rows with Height greater than 82
Girth Height Volume
6   10.8     83   19.7
17  12.9     85   33.8
18  13.3     86   27.4
31  20.6     87   77.0
&gt; trees[10:12,2]
[1] 75 79 76
</code></pre>
<p>We can see in the last case that the returned type is a vector since we extracted data from a single column.</p>
<p>This behavior can be avoided by passing the argument <code>drop=FALSE</code> as follows.</p>
<pre><code>&gt; trees[10:12,2, drop = FALSE]
Height
10     75
11     79
12     76
</code>


# access first row by index, returns a data.frame
x[1,]

# access first row by "name", returns a data.frame
> x["1",]

# access first row returns a vector
use as.numeric
str(as.numeric(wAveTable["1",]))

unlist which keeps the names.
str(unlist(wAveTable["1",]))

use transpose and as.vector
str(as.vector(t(wAveTable["1",])[,1]))

use only as.vector cannot convert to vector
str(as.vector(wAveTable["1",]))

# convert dataframe to matrix
data.matrix(wAveTable)

</pre>
<br>
<br>
<h2>read.csv as character</h2>
<pre>
wAveTable = read.csv("wAveTable.txt", sep="\t", colClasses=c('character', 'character', 'character'))
</pre>
<br>
<br>
<h2>frequency manipulation</h2>
<pre>
grade = c("low", "high", "medium", "high", "low", "medium", "high")

# using factor to count the frequency
foodfac <- factor(grade)
summary(foodfac)
max(summary(foodfac))
min(summary(foodfac))
levels(foodfac)
nlevels(foodfac)
summary(levels(foodfac))

# use of table to count frequency:
table(grade)
sort(table(grade))

table(grade)[1]
max(table(grade))
summary(table(grade))

# this locate the max item:
table(grade)[which(table(grade) == max(table(grade)))]

# change to dataframe and find the max item:
theTable = as.data.frame(table(grade))
theTable[which(theTable$Freq == max(theTable$Freq)),]

# use of the count function in plyr:
library(plyr)
count(grade)
count(mtcars, 'gear')

# use of the which function:
which(letters == "g")
x <- c(1,5,8,4,6)
which(x == 5)
which(x != 5)

</pre>
<br>
<br>
<h2>5 must have R programming tools</h2>
<pre>
<h4>1) RStudio</h4>
<h4>2) lintr</h4>
If you come from the world of Python, you’ve probably heard of 
<a href="https://stackoverflow.com/questions/8503559/what-is-linting" data-href="https://stackoverflow.com/questions/8503559/what-is-linting" rel="nofollow noopener" target="_blank">linting</a>. 
Essentially, linting 
<a href="https://en.wikipedia.org/wiki/Lint_%28software%29" data-href="https://en.wikipedia.org/wiki/Lint_%28software%29" rel="nofollow noopener" target="_blank">analyzes</a> your code for readability. 
It makes sure you don’t produce code that looks like this:
<pre># This is some bad R code
<br>if ( mean(x,na.rm=T)==1) { print(“This code is bad”); } # Still bad code because this line is SO long</pre>
There are 
<em>many</em> things wrong with this code. 
For starters, the code is too long. 
Nobody likes to read code with seemingly endless lines. 
There are also no spaces after the comma in the 
<code>mean()</code> function, or any spaces between the 
<code>==</code> operator. 
Oftentimes data science is done hastily, but linting your code is a good reminder for creating portable and understandable code. 
After all, if you can’t explain what you are doing or how you are doing it, your data science job is incomplete. 

<a href="https://cran.r-project.org/web/packages/lintr/index.html" data-href="https://cran.r-project.org/web/packages/lintr/index.html" rel="nofollow noopener" target="_blank">lintr</a> is an R package, growing in popularity, that allows you to lint your code. 
Once you install lintr, linting a file is as easy as 
<code>lint(&quot;filename.R&quot;)</code> .
<h4>3) Caret</h4>
<a href="http://topepo.github.io/caret/index.html" data-href="http://topepo.github.io/caret/index.html" rel="nofollow noopener" target="_blank">Caret</a>, which you can find on 
<a href="https://cran.r-project.org/web/packages/caret/caret.pdf" data-href="https://cran.r-project.org/web/packages/caret/caret.pdf" rel="nofollow noopener" target="_blank">CRAN</a>, is central to a data scientist’s toolbox in R. 
Caret allows one to quickly develop models, set cross-validation methods and analyze model performance all in one. 
Right out of the box, Caret abstracts the various interfaces to user-made algorithms and allows you to swiftly create models from averaged neural networks to boosted trees. 
It can even handle parallel processing. 
Some of the models caret includes are: AdaBoost, Decision Trees &amp; Random Forests, Neural Networks, Stochastic Gradient Boosting, nearest neighbors, support vector machines — among the most commonly used machine learning algorithms.
<h4>4) Tidyverse</h4>
You may not have heard of 
<code>tidyverse</code> as a whole, but chances are, you’ve used one of the packages in it. 
Tidyverse is a set of unified packages meant to make data science… 
<em>easyr</em> (classic R pun). 
These packages alleviate many of the problems a data scientist may run into when dealing with data, such as loading data into your workspace, manipulating data, tidying data or visualizing data. 
Undoubtedly, these packages make dealing with data in R more efficient.
It’s incredibly easy to get Tidyverse, you just run 
<code>install.packages(&quot;tidyverse&quot;)</code> and you get:
<a href="http://ggplot2.tidyverse.org/" data-href="http://ggplot2.tidyverse.org/" rel="nofollow noopener" target="_blank">ggplot2</a>: A popular R package for creating graphics
<a href="http://dplyr.tidyverse.org/" data-href="http://dplyr.tidyverse.org/" rel="nofollow noopener" target="_blank">dplyr</a>: A popular R package for efficiently manipulating data
tidyr: An R package for tidying up data sets
<a href="http://readr.tidyverse.org/" data-href="http://readr.tidyverse.org/" rel="nofollow noopener" target="_blank">readr</a>: An R package for reading in data
<a href="http://purrr.tidyverse.org/" data-href="http://purrr.tidyverse.org/" rel="nofollow noopener" target="_blank">purrr</a>: An R package which extends R’s functional programming toolkit
<a href="http://tibble.tidyverse.org/" data-href="http://tibble.tidyverse.org/" rel="nofollow noopener" target="_blank">tibble</a>: An R package which introduces the 
<em>tibble (tbl_df)</em>, an enhancement of the data frame
By and large, ggplot2 and dplyr are some of the most common packages in the R sphere today, and you’ll see countless posts on StackOverflow on how to use either package.

<em>(Fine Print: Keep in mind, you can’t just load everything with </em>
<code>
<em>library(tidyverse)</em></code>
<em> you must load each individually!)</em>
<h4>5) Jupyter Notebooks or R Notebooks</h4>
Data science 
<em>MUST</em> be transparent and reproducible. 
For this to happen, we have to see your code! The two most common ways to do this are through 
<a href="http://jupyter.org/" data-href="http://jupyter.org/" rel="nofollow noopener" target="_blank">Jupyter Notebooks</a> or 
<a href="http://rmarkdown.rstudio.com/r_notebooks.html" data-href="http://rmarkdown.rstudio.com/r_notebooks.html" rel="nofollow noopener" target="_blank">R Notebooks</a>.
Essentially, a notebook (of either kind) allows you to run R code block by block, and show output block my block. 
We can see on the left that we are summarizing the data, then checking the output. 
After, we plot the data, then view the plot. 
All of these actions take place within the notebook, and it makes analyzing both output and code a simultaneous process. 
This can help data scientists collaborate and ease the friction of having to open up someone’s code and understand what it does. 
Additionally, notebooks also make data science 
<em>reproducible</em>, which gives validity to whatever data science work you do!
<h4>Honorable Mention: Git</h4>
Last but not least, I want to mention Git. 
Git is a 
<em>version control</em> system. 
So why use it? Well, it’s in the name. 
Git allows you to keep versions of the code you are working on. 
It also allows multiple people to work on the same project and allows those changes to be attributed to certain contributors. 
You’ve probably heard of 
<a href="http://www.github.com" data-href="http://www.github.com" rel="nofollow noopener" target="_blank">Github</a>, undoubtedly one of the most popular git servers.
You can visit my website at 
<a href="http://www.peterxeno.com" data-href="http://www.peterxeno.com" rel="nofollow noopener" target="_blank">www.peterxeno.com</a> and my Github at 
<a href="https://github.com/peterxeno" data-href="https://github.com/peterxeno" rel="nofollow noopener" target="_blank">www.github.com/peterxeno</a>


<h2>R with Javascript</h2>
<a href="R and D3.html">R and D3</a>
<a href="https://www.opencpu.org/posts/js-release-0-1/">tools for working with JavaScript in R</a>
<a href="http://www.di.fc.ul.pt/~jpn/r/langs/javascript.html">R Connecting with Javascript</a>

<h2><span class="white bordgreen1">tryCatch</span></h2>
<a href="https://codeday.me/bug/20170502/13495.html">如何在R中写trycatch</a>
<a href="https://stackoverflow.com/questions/12193779/how-to-write-trycatch-in-r">write trycatch in R</a>
<a href="http://mazamascience.com/WorkingWithData/?p=912">error Handing with tryCatch()</a>

e.g.
readUrl = function(url) {
    out = tryCatch(
        {
            message("This is the 'try' part")
            readLines(con=url, warn=FALSE) 
        },
        error=function(cond) {
            message(paste("URL does not seem to exist:", url))
            message("Here's the original error message:")
            message(cond)
            # Choose a return value in case of error
            return(NA)
        },
        warning=function(cond) {
            message(paste("URL caused a warning:", url))
            message("Here's the original warning message:")
            message(cond)
            # Choose a return value in case of warning
            return(NULL)
        },
        finally={
        # Here goes everything that should be executed at the end,
        # regardless of success or error.
        # If you want more than one expression to be executed, then you 
        # need to wrap them in curly brackets ({...}); otherwise you could
        # just have written 'finally={expression}' 
            message(paste("Processed URL:", url))
            message("Some other message at the end")
        }
    )    
    return(out)
}

e.g.
x <- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});

e.g.
for (i in urls) {
    tmp <- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}

a retry function:
retry <- function(dothis, max = 10, init = 0){
	suppressWarnings( tryCatch({
		if(init<max) dothis}, 
			error = function(e){retry(dothis, max, init = init+1)}
		)
	)
}
dothis <- function(){do somthing}

<h2>Download Image</h2>
<a href="https://stackoverflow.com/questions/29110903/how-to-download-and-display-an-image-from-an-url-in-r">Download Image</a>
<br>
If I try your code it looks like the image is downloaded. However, when opened with windows image viewer it also says it is corrupt. The reason for this is that you don't have specified the mode in the download.file statement.

Try this:

download.file(y,'y.jpg', mode = 'wb')

download.file('http://78.media.tumblr.com/83a81c41926c1da585916a5c092b4789/tumblr_or0y0vdjOP1rttk8po1_1280.jpg','y.jpg', mode = 'wb')

To view the image in R, have a look at

library(jpeg)
jj <- readJPEG("y.jpg",native=TRUE)
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(jj,0,0,1,1)

<h2>Download Something</h2>
Download Something
Download Something
Download Something

<h2>testShiny</h2>
setwd("D:/KPC/testShiny")
runApp("D:/KPC/testShiny")

</pre>
<h2>Error in file(filename, "r", encoding = encoding)</h2>
The error indicate that either the file doesn't exist or the source() command an incorrect path. 

<h2>call a R program from another R program</h2>
source("program_B.R")

<h2>to view all the functions present in a package</h2>
<pre>
To list all objects in the package use ls
ls("package:Hmisc")
Note that the package must be attached.

To list all strings
lsf.str("package:dplyr")
lsf.str("package:Hmisc")

To see the list of currently loaded packages use
search()

Alternatively calling the help would also do, even if the package is not attached:
help(package = dplyr)
help(package = Hmisc)

Finally, use RStudio which provides an autocomplete function.
So, for instance, typing Hmisc:: in the console or while editing a file will result in a popup list of all dplyr functions/objects.
</pre>


<h2>cut2</h2>
<pre>
Function like cut but left endpoints are inclusive.

install.packages("Hmisc")
library(Hmisc)

alist = c(-15,18,2,5,4,-7,-5,-3,-1,0,2,1,5,4,6)
breaks = c(-5,-3,-1,0,1,3,5)
table(cut2(alist, breaks))
</pre>
<h2>Reference A Data Frame Column</h2>
<pre>
with the double square bracket "[[]]" operator.
LastDayTable[["Vol"]] 
or
LastDayTable$Vol
or
<span class="cyanword">LastDayTable[,"Vol"] </span>
</pre>
<h2>Writing data to a file</h2>
<h3 id="problem">Problem</h3>

<p>You want to write data to a file.</p>

<h3 id="solution">Solution</h3>

<h3 id="writing-to-a-delimited-text-file">Writing to a delimited text file</h3>

<p>The easiest way to do this is to use <code>write.csv()</code>. By default, <code>write.csv()</code> includes row names, but these are usually unnecessary and may cause confusion.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="c1"># A sample data frame
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.table</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="o">=</span><span class="s1">'
 subject sex size
       1   M    7
       2   F    NA
       3   F    9
       4   M   11
 '</span><span class="p">)</span><span class="w">


</span><span class="c1"># Write to a file, suppress row names
</span><span class="n">write.csv</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="s2">"data.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">row.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">

</span><span class="c1"># Same, except that instead of "NA", output blank cells
</span><span class="n">write.csv</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="s2">"data.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">row.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">na</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span><span class="w">

</span><span class="c1"># Use tabs, suppress row names and column names
</span><span class="n">write.table</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="s2">"data.csv"</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="o">=</span><span class="s2">"\t"</span><span class="p">,</span><span class="w"> </span><span class="n">row.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">col.names</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> 
</span></code></pre>
</div>

<h3 id="saving-in-r-data-format">Saving in R data format</h3>

<p><code>write.csv()</code> and <code>write.table()</code> are best for interoperability with other data analysis programs. They will not, however, preserve special attributes of the data structures, such as whether a column is a character type or factor, or the order of levels in factors. In order to do that, it should be written out in a special format for R.</p>

<p>Below are are three primary ways of doing this:</p>

<p>The first method is to output R source code which, when run, will re-create the object. This should work for most data objects, but it may not be able to faithfully re-create some more complicated data objects.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="c1"># Save in a text format that can be easily loaded in R
</span><span class="n">dump</span><span class="p">(</span><span class="s2">"data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"data.Rdmpd"</span><span class="p">)</span><span class="w">
</span><span class="c1"># Can save multiple objects:
</span><span class="n">dump</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"data1"</span><span class="p">),</span><span class="w"> </span><span class="s2">"data.Rdmpd"</span><span class="p">)</span><span class="w">

</span><span class="c1"># To load the data again: 
</span><span class="n">source</span><span class="p">(</span><span class="s2">"data.Rdmpd"</span><span class="p">)</span><span class="w">
</span><span class="c1"># When loaded, the original data names will automatically be used.
</span></code></pre>
</div>

<p>The next method is to write out individual data objects in RDS format. This format can be binary or ASCII. Binary is more compact, while ASCII will be more efficient with version control systems like Git.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="c1"># Save a single object in binary RDS format
</span><span class="n">saveRDS</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="s2">"data.rds"</span><span class="p">)</span><span class="w">
</span><span class="c1"># Or, using ASCII format
</span><span class="n">saveRDS</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="s2">"data.rds"</span><span class="p">,</span><span class="w"> </span><span class="n">ascii</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="c1"># To load the data again:
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="s2">"data.rds"</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<p>It’s also possible to save multiple objects into an single file, using the RData format.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="c1"># Saving multiple objects in binary RData format
</span><span class="n">save</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">file</span><span class="o">=</span><span class="s2">"data.RData"</span><span class="p">)</span><span class="w">
</span><span class="c1"># Or, using ASCII format
</span><span class="n">save</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">file</span><span class="o">=</span><span class="s2">"data.RData"</span><span class="p">,</span><span class="w"> </span><span class="n">ascii</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="c1"># Can save multiple objects
</span><span class="n">save</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">data1</span><span class="p">,</span><span class="w"> </span><span class="n">file</span><span class="o">=</span><span class="s2">"data.RData"</span><span class="p">)</span><span class="w">

</span><span class="c1"># To load the data again:
</span><span class="n">load</span><span class="p">(</span><span class="s2">"data.RData"</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<p>An important difference between <code>saveRDS()</code> and <code>save()</code> is that, with the former, when you <code>readRDS()</code> the data, you specify the name of the object, and with the latter, when you <code>load()</code> the data, the original object names are automatically used. Automatically using the original object names can sometimes simplify a workflow, but it can also be a drawback if the data object is meant to be distributed to others for use in a different environment.</p>

<h2>Debugging a script or function</h2>
    <h3>Problem</h3>

<p>You want to debug a script or function.</p>

<h3>Solution</h3>

<p>Insert this into your code at the place where you want to start debugging:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="nf">browser</span><span class="p">()</span><span class="w">
</span></code></pre>
</div>

<p>When the R interpreter reaches that line, it will pause your code and you will be able to look at and change variables.</p>

<p>In the browser, typing these letters will do things:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>c</td>
      <td>Continue</td>
    </tr>
    <tr>
      <td>n (or Return)</td>
      <td>Next step</td>
    </tr>
    <tr>
      <td>Q</td>
      <td>quit</td>
    </tr>
    <tr>
      <td>Ctrl-C</td>
      <td>go to top level</td>
    </tr>
  </tbody>
</table>

<p>When in the browser, you can see what variables are in the current scope.</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">ls</span><span class="p">()</span><span class="w">
</span></code></pre>
</div>

<p>To pause and start a browser for every line in your function:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">debug</span><span class="p">(</span><span class="n">myfunction</span><span class="p">)</span><span class="w">
</span><span class="n">myfunction</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<h3>Useful options</h3>

<p>By default, every time you press Enter at the browser prompt, it runs the next step. This is equivalent to pressing <code class="highlighter-rouge">n</code> and then Enter. This can be annoying. To disable it use:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">browserNLdisabled</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<p>To start debugging whenever an error is thrown, run this before your function which throws an error:</p>

<div class="language-r highlighter-rouge"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">error</span><span class="o">=</span><span class="n">recover</span><span class="p">)</span><span class="w">
</span></code></pre>
</div>

<p>If you want these options to be set every time you start R, you can put them in your <code class="highlighter-rouge">~/.Rprofile</code> file.</p>

<h2>data.table vs data.frame</h2>
<a href="data.table vs data.frame.html">data.table vs data.frame</a>
<br>
<a href="Introduction to data.table.html">Introduction to data.table</a>
<br>
<a href="http://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html">JOINing data in R using data.table</a>
<br>
<a href="Advanced tips and tricks with data.table.html"><span class="goldwhiteb">&diams;Advanced tips and tricks with data.table</span></a>
<br>
<pre>
X = data.table(a=1:5, b=6:10, c=c(5:1))

length(X[b %between% c(7,9)])
length(X[b %inrange% c(7,9)])

# inrange()
Y = data.table(a=c(8,3,10,7,-10), val=runif(5))
range = data.table(start = 1:5, end = 6:10)
Y[a %inrange% range]

https://stackoverflow.com/questions/16652533/insert-a-row-in-a-data-table
insert-a-row-in-a-data-table
dt1 <- list(1,4,7)
rbind(dt1, X)

dt1 <- data.table(1,4,7)
rbindlist(list(dt1, X))

===================
use data.frame
df <- data.frame( name=c("John", "Adam"), date=c(3, 5) )

Extract exact matches:

subset(df, date==3)
nrow(subset(df, date==3))

Extract matches in range:

subset(df, date>4 & date<6)

  name date
2 Adam    5

</pre>

<a href="Data.Table Tutorial.html"><span class="silverredb">&diams;Data.Table Tutorial</span></a>
<br>

<a href="http://brooksandrew.github.io/simpleblog/articles/advanced-data-table/">Advanced tips and tricks with data.table package</a>
<br>

<br>
<h2>DiagrammeR</h2>
<a href="http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html">DiagrammeR</a><br>
<a href="DiagrammeR Docs.html">DiagrammeR Docs</a>

<br>
<h2>ROC Curves</h2>
<a href="ROC Curves.html">ROC Curves</a>
<br>
<h2>capture.output to file</h2>
capture.output(options(), file="temp.txt")

<h2>writing functions</h2>
<pre>
a simple function:

square = function(x){x*x}

to square a vector:

x = c(1,3,5)
square(x)

to square a matrix:
x = cbind(c(1,3),c(5,7))
square(x)

to return a list of objects, use list():
square = function(x){return(list(x*x,x*x*x))}
square(x)

using debug() to debug:
debug(square)
square(x-a)

using print to debug in function:
square = function(x){
print(x)
print(x*x)
x*x
}

using stop() and stopifnot() to write your own error msg:
squareRoot = function(x){
	if(x&lt;0){
		stop("cannot use negative number!")
	}
	sqrt(x)
}
squareRoot(-1)

good function practices:
keep short function
write comments
try with examples
use debug and error msg
</pre>
<h2>For Loop in R with Examples</h2>
<a href="https://www.guru99.com/r-for-loop.html">For Loop in R with Examples</a>

<h2>case_when and switch</h2>
<pre>
switch("shape", "color" = "red", "shape" = "square", "length" = 5)

library(dplyr)
Length=3.5
mode <- case_when(
                (Length < 1) ~ "Walk",
                (1 <= Length & Length < 5) ~ "bike",
                (5 <= Length & Length < 10) ~ "drive",
                (Length >= 10) ~ "fly"
          )

<h2>Calling multiple external program from R</h2>

<a href="https://stackoverflow.com/questions/21966209/calling-external-program-from-r-with-multiple-commands-in-system">Calling multiple external program from R</a>
<br>
for(i in 1:10){
cmd=paste("export FOO=",i," ; echo \"$FOO\" ",sep='')
system(cmd)
}

<h2>rmItems</h2>
# rmItems(fmList, itemList) remove itemList from fmList
 rmItems <- function(fmList, itemList){
		commons = fmList[fmList %in% itemList]
		for(item in commons){fmList = fmList[-(which(fmList == item))]}
		return(fmList)
 }

# remove fraudSTK
 CodeTable = rmItems(CodeTable, fraudSTK)

milList8 = c("a","b","c","d")
milList20 = c("a","b","c","f")
setdiff(fmList,itemList)


<h2>R porjects</h2>
<a href="R porjects.html">R porjects</a>

<h2>call C from R</h2>
<a href="call C from R.html">call C from R</a>

<h2>Make R Beep</h2>

<a href="https://www.geoffchappell.com/studies/windows/win32/kernel32/api/index.htm">KERNEL32 Functions</a>
<a href="https://stackoverflow.com/questions/3365657/is-there-a-way-to-make-r-beep-play-a-sound-at-the-end-of-a-script">Make R Beep</a>

rundll32.exe Kernel32.dll,Beep 550,1000
rundll32.exe cmdext.dll,MessageBeepStub
rundll32 user32.dll,MessageBeep
BOOL Beep(
  DWORD dwFreq,
  DWORD dwDuration
);

C:\Windows\Media\Delta

install.packages("audio") 
library(audio)

# play(x, rate, ...)
# x <- audioSample(sin(1:8000/10), 8000)
# play(x)
# 10000 is the sound period, 10 is the freq code
play(sin(1:10000/3))
Sys.sleep(1)
play(sin(1:10000/4))
Sys.sleep(1)
play(sin(1:10000/5))
Sys.sleep(1)
play(sin(1:10000/6))
Sys.sleep(1)
play(sin(1:10000/7))
Sys.sleep(1)
play(sin(1:10000/8))
Sys.sleep(1)
play(sin(1:10000/9))
Sys.sleep(1)
play(sin(1:10000/10))
Sys.sleep(1)
play(sin(1:10000/20))
Sys.sleep(1)
play(sin(1:10000/30))
Sys.sleep(1)

<h2>read clipboard</h2>

simply use: readClipboard()

this gives too many columns:
read.table(file = "clipboard", sep = ",")

<h2>dplyr Data Manipulation</h2>
<a href="dplyr Data Manipulation.html">dplyr Data Manipulation</a>

<h2>Language Server Protocol</h2>
<a href="D:\R-3.4.3\bin\x64\R.exe">install.packages("languageserver") Language Server Protocol:</a>
<br>
Adding features like auto complete, go to definition, or documentation on hover for a programming language takes significant effort.

<h2>to run R by batch script</h2>
Rscript.exe  alert.r
Rscript.exe  something.r

Note: Rscript.exe cannot run with Chinese

calling chrome by batch script in sequence
can also call by R

<a href="https://kknews.cc/tech/92k2828.html">R与中文那些事 R script with Chinese</a>

<a href="https://stackoverflow.com/questions/31190468/integrating-r-and-its-graphics-with-existing-javascript-html-application"><strong>R and Javascript : Execution, Libraries, Integration</strong></a>
<br>
In today’s date, R is the megastar language for <a href="https://www.cuelogic.com/big-data-solution" data-href="https://www.cuelogic.com/big-data-solution" target="_blank">big data analytics</a>. 
In this article, I will talk about on coordination, visualization and execution of R and JavaScript. 
However, you may ask the question for what reason somebody might want to incorporate R into web applications?
There are quite a few reasons for this. 
When you add R to your solution, a vast opportunity of analytics opens up like statistics, predictive data modelling, forecasting, machine learning, visualization and much more.
R is developed by statisticians, scientists or professional analysts using the script but the reports and the results generated by them on the desktop can be easily emailed or presented in the form of presentation, but that is limiting the business use and other potential uses.
If R is incorporated with JavaScript, then web delivery can happen smoothly, and it can help in making efficient business decision making. 
Integrating R into web application naturally becomes quintessential.

<h2>Create Apps with Rt</h2>
<a href="https://www.r-bloggers.com/deploying-desktop-apps-with-r/">Create Apps with R</a>


<h2>Integrate R into JavaScript</h2>
There can be various ways through which you can integrate R with JavaScript. 
Here I am discussing the following methods that I prefer for Rand Javascript integration.
<strong>1. Deploy R open</strong>
Through Deploy R opens you can easily embed results of various R functions like- data and charts into any application. 
This specific structure is an open source server-based system planned especially for R, which makes it simple to call the R code at a real time.
The workflow for this is simple: first, the programmer develops R script which is then published on the Deploy R server. 
The published R script that can be executed from any standard application using DeployR API. 
Using client libraries JavaScript now can make calls to the server. 
The results returned by the call can be embedded into the displayed or processed according to the application.
<strong>2. Open CPU JavaScript API</strong>
This offers straightforward RPC and information input/Output through Ajax strategies that can be fused in JavaScript of your HTML page.

<h2>Visualization with R and JavaScript</h2>
You can make use of numerous JavaScript libraries that help in creating web functionality for dynamic data visualizations for R.
Here I will be elaborating some of those tools like D3, Highchart, and leaflet. 
You can quickly implement these tools in your R and program knowledge of JavaScript is not mandatory for this.
As I have already mentioned that R is an open source analytical software, it can create high dimensional data visualizations. 
Ggplot2 is a standout among the most downloaded bundle that has helped R to accomplish best quality level as a data visualization tool.
Javascript then again is a scripting dialect in which R can be consolidated to make data visualisation. 
Numerous javascript libraries can help in creating great intuitive plots, some of them are d3.Js, c3.js, vis.js, plotly.js, sigma.js, dygraphs.js.
HTM widgets act as a bridge between R and JavaScript. 
It is the principal support for building connectors between two languages. 
The flow of a program for HTM widgets r can be visualized as under:
• Information is perused into R
• Data is handled (and conceivably controlled) by R
• Data is changed over to JavaScript Object Notation (JSON) arrange
• Information is bound to JavaScript
• Information is prepared (and conceivably controlled) by JavaScript
• Information is mapped to plotting highlights and rendered
Now let us discuss some of the data visualization packages:
<strong>• r d3 package</strong>
Data-driven documents or d3 is one of the popular JavaScript visualization libraries. 
D3 can produce visualization for almost everything including choropleths, scatter plots, graphs, network visualizations and many more. 
Multiple R packages are using only D3 plotting methods. 
You can refer r d3 package tutorials to learn about this.
• <strong>ggplot2</strong> <br> <br> It is really very easy to create plots in R, but you may ask me whether it is same for creating custom plots, the answer is “yes”, and that is the primary motivation behind why ggplot came into existence. 
With ggplot, you can make complex multi-layered designs effectively.
Here you can start plotting with axes then add points and lines. 
But the only drawback that it has it is relatively slower than base R, and new developers might find it difficult to learn.
• <strong>Leaflet</strong>
The leaflet has found its profound use in GIS (mapping), this is an open source library. 
The R packages that backings this is composed and kept up by RStudio and ports. 
Using this developer can create pop up text, custom zoom levels, tiles, polygon, planning and many more.
The ggmap bundle of javaScript can be utilised for the estimation of the latitude and longitude.
• <strong>Lattice</strong>
Lattice helps in plotting visualized multivariate data. 
Here you can have tilled plots that help in comparing values or subgroups of a given variable. 
Here you will discover numerous lattice highlights has been acquired as utilizes grid package for its usage. 
The underlying logic used by lattice is very much similar to base R.
<strong>• visNetwork</strong>
For the graphical representation of nodes and edges, the visual network is referred. 
Vis.js is a standout amongst the most famous library among numerous that can do this sort of plotting. 
visNetwork is the related with R package for this.
Network plots ought to be finished remembering nodes and edges. 
For visNetwork, these two should be separated into two different data frames one for the nodes and the other
<strong>• Highcarter</strong>
This is another visualization tool which is very similar to D3. 
You can use this tool for a variety of plots like line, spline, arealinerange, column range, polar chart and many more. 
For the commercial use of Highcarter, you need to get a license while for the non-commercial you don’t need one.
Highcarter library can be accessed very easily using various chart () functions. 
Using this function, you can create a plot in a single task. 
This function is very much similar to qplot() of ggplot2 of D3. 
chart () can produce different types of scenarios depending on the data inputs and specifications.
<strong>• RColor Brewer</strong>
With this package, you can use color for your plots, graphs, and maps. 
This package works nicely with schemes.
<strong>• Plotly</strong>
It is a well distinguish podium for data visualization that works inordinately with R and Python notebook. 
It has similarity with the high career as both are known for interactive plotting. 
But here you get some extra as it offers something that most of the package don’t like contour plots, candlestick chart, and 3d charts.
• <strong>SunTrust</strong>
It is the way for representing data visualization as it nicely describes the sequence of events. 
The diagram that it produces speaks about itself. 
You don’t need an explanation for the chart as it is self-explanatory.
• <strong>RGL</strong>
For creating three-dimensional plots in R you should check out RGL. 
It has comparability with lattice, and on the off chance that you are an accomplished R developer you will think that its simple.
<strong>• Threejs</strong>
This is an R package and an HTML widget that helps in incorporating several data visualization from the JavaScript library.
Some of the visualization function three are as follows:
• Graphjs: this is used for implementing 3D interactive data visualization. 
This function accepts igraph as the first argument. 
This manages definition for nodes and edges.
• Scatterplot3js: this function is used for creating three dimensional scatter plot.
• Globejs: this function of JavaScript is used for plotting surface maps and data points on earth.
• <strong>Shiny</strong>
The most significant benefit of JavaScript visualization is it can be implanted voluntarily into the web application. 
They can be injected into several frameworks, one of such context of R development is shiny.
Shiny is created and maintained by R Studio. 
It is a <a href="https://www.cuelogic.com/custom-software-development" data-href="https://www.cuelogic.com/custom-software-development" target="_blank">software application development</a> instrument, to a great extent employed for making wise interfaces with R. 
R shiny tutorial will take in more about shiny.
Shiny is a podium for facilitating R web development.
Connecting R with javascript using libraries
Web scuffling has formed into an original piece of examination as through this movement you can pucker your required information. 
But the data should be extracted before any web developer start to insert javascript render content into the web page. 
To help in such situation R has an excellent package called V8 which acts as an interface to JavaScript. 
R v8 is the most generally utilized capacity utilized for interfacing r in javascript. 
You can undoubtedly implement JS code in R without parting the current session. 
The library function used for this is rvest().
To run the JavaScript in R, we need a context handler, within that context handler you can start programming. 
Then you can export the R data into JavaScript.
Some other JavaScript libraries that help in analytical programming such as Linear Regression, SVMs etc. 
are as follows:
• Brain.js()
• Mljs
• Webdnn
• Convnetjs

<h2>Conclusion:</h2>
R and Javascript can practically unlock innumerable possibility in Data Science and Analytics. 
Both technologies are working towards developing better integrations, knowledge repositories, libraries and use cases. 
It is a good time to use both of this together. 
The future looks bright.


<a href="https://hackernoon.com/r-and-javascript-execution-libraries-integration-40a30726f295">Integrating R and Javascript/HTML Application</a>
<br>
<h2>Rserve package</h2>
There is javascript implementation of Rserve client available rserve-js.
You can call R from javascript efficiently using Rserve package. 

<h2>FastRWeb</h2>
FastRWeb is an infrastructure that allows any webserver to use R scripts for generating content on the fly, such as web pages or graphics. 
URLs are mapped to scripts and can have optional arguments that are passed to the R function run from the script. 
For example http://my.server/cgi-bin/R/foo.png?n=100 would cause FastRWeb to look up a script foo.png.R, source it and call run(n="100"). 
So for example the script could be as simple as

run <- function(n=10, ...) {
   p <- WebPlot(800, 600)
   n <- as.integer(n)
   plot(rnorm(n), rnorm(n), col=2, pch=19)
   p
}
This can potentially then be called using JavaScript to dynamically load images and display them.

<a href="https://stackoverflow.com/questions/22179512/suggestions-needed-for-building-r-server-rest-apis-that-i-can-call-from-externa/29537593#29537593">building R server REST API's that I can call from external app</a>
<br>
<h2>httpuv</h2>
You can use httpuv to fire up a basic server then handle the GET/POST requests. The following isn't "REST" per se, but it should provide the basic framework:

library(httpuv)
library(RCurl)
library(httr)

app <- list(call=function(req) {

  query <- req$QUERY_STRING
  qs <- httr:::parse_query(gsub("^\\?", "", query))

  status <- 200L
  headers <- list('Content-Type' = 'text/html')

  if (!is.character(query) || identical(query, "")) {
    body <- "\r\n<html><body></body></html>"
  } else {
    body <- sprintf("\r\n<html><body>a=%s</body></html>", qs$a)
  }

  ret <- list(status=status,
              headers=headers,
              body=body)

  return(ret)

})

message("Starting server...")

server <- startServer("127.0.0.1", 8000, app=app)
on.exit(stopServer(server))

while(TRUE) {
  service()
  Sys.sleep(0.001)
}

stopServer(server)

<br>
<h2>Cucumber Selenium</h2>
<a href="https://www.guru99.com/using-cucumber-selenium.html">Cucumber Selenium</a>
<a href="https://www.youtube.com/watch?v=ZSfOEBh9BRM">Cucumber Selenium Tutorial</a>
<br>
<a href="https://blog.gtwang.org/r/rselenium-r-selenium-browser-web-scraping-tutorial/">RSelenium：R 使用 Selenium 操控瀏覽器下載網頁資料</a>
<br>

<h2>SQL databases and R</h2>
<a href="https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html">SQL databases and R</a>

<h2>SQLite</h2>
<a href="https://db.rstudio.com/databases/sqlite/">R SQLite</a>
<br>
<pre>
install.packages("RSQLite")

Or install the latest development version from GitHub with:
# install.packages("devtools")
devtools::install_github("rstats-db/RSQLite")

To install from GitHub, you’ll need a development environment.

Basic usage
library(DBI)
# Create an ephemeral in-memory RSQLite database
con <- dbConnect(RSQLite::SQLite(), ":memory:")

dbListTables(con)
## character(0)
dbWriteTable(con, "mtcars", mtcars)
dbListTables(con)
## [1] "mtcars"
dbListFields(con, "mtcars")
##  [1] "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"   "gear"
## [11] "carb"
dbReadTable(con, "mtcars")
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
...

# You can fetch all results:
res <- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
dbFetch(res)
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
...

dbClearResult(res)

# Or a chunk at a time
res <- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
while(!dbHasCompleted(res)){
  chunk <- dbFetch(res, n = 5)
  print(nrow(chunk))
}
## [1] 5
## [1] 5
## [1] 1
# Clear the result
dbClearResult(res)

# Disconnect from the database
dbDisconnect(con)
Acknowledgements
Many thanks to Doug Bates, Seth Falcon, Detlef Groth, Ronggui Huang, Kurt Hornik, Uwe Ligges, Charles Loboz, Duncan Murdoch, and Brian D. 
Ripley for comments, suggestions, bug reports, and/or patches.

</pre>
<br>

<h2>Invoking the Rstudio Viewer</h2>
viewer <- getOption("viewer")
viewer("<a href="https://www.rt.com/")">viewer("C:/Users/User/Desktop/Debugging with RStudio.html")</a>

<h2>to sum only elements greater than 5</h2>
a<-sample.int(10,20,replace=TRUE)
sum(a[a>5])

<h2>Customizing RStudio themes</h2>
<a href="https://www.r-bloggers.com/make-rstudio-look-the-way-you-want-because-beauty-matters/">Make RStudio Beauty</a>

D:\RStudio\www\rstudio\806BBC582D6B8DF91384AD7E3EFC9A52.cache.css

<a href="https://support.rstudio.com/hc/en-us/articles/200532307-Customizing-Fonts-and-Appearance">Customizing Fonts and Appearance</a>
<br>

<h2>table()</h2>
table()的输出可以看成是一个带名字的数字向量。
可以用names()和as.numeric()分别得到名称和频数：> 

x <- sample(c("a", "b", "c"), 100, replace=TRUE)
tablex = table(x)

names(tablex)
[1] "a" "b" "c"

> as.numeric(tablex)
[1] 42 25 33

可以直接把输出结果转化为数据框，as.data.frame()：> 
as.data.frame(tablex)
  x Freq
1 a   42
2 b   25
3 c   33

<h2>with(data, expr, …)</h2>
applys an expression to a dataset.
eg
with(BOD,{BOD$demand <- BOD$demand + 1; print(BOD$demand)})

<h2>R regular expression</h2>
<a href="https://blog.yjtseng.info/post/regexpr/">R regex</a>

<h2>R Operator Syntax</h2>
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Syntax.html">R Operator Syntax and Precedence</a>

:: :::	access variables in a namespace
$ @	component / slot extraction
[ [[	indexing
^	exponentiation (right to left)
- +	unary minus and plus
:	sequence operator
%any%	special operators (including %% and %/%)
* /	multiply, divide
+ -	(binary) add, subtract
< > <= >= == !=	ordering and comparison
!	negation
& &&	and
| ||	or
~	as in formulae
-> ->>	rightwards assignment
<- <<-	assignment (right to left)
=	assignment (right to left)
?	help (unary and binary)

exampleRPackage
The exampleRPackage can be installed from github:

# install.packages("devtools")
devtools::install_github("mvuorre/exampleRPackage")

The file you are reading now is the package’s README, which describes how to create R packages with functions, data, and appropriate documentation. 


A Simple Example of Using replyr::gapply
It’s a common situation to have data from multiple processes in a “long” data format. 
It’s also natural to split that data apart to analyze or transform it, per-process — and then to bring the results of that data processing together, for comparison. 
Such a work pattern is called “Split-Apply-Combine”. 
A simple example of one such implementation, replyr::gapply, from package, replyr.

K-means clustering
K-means is a clustering techniques that subdivide the data sets into a set of k groups, where k is the number of groups pre-specified by the analyst.

Determining the optimal number of clusters: use factoextra::fviz_nbclust()

</pre>
<h2>树状图</h2>
<a href="Dendrograms in R.html" target="_blank">Dendrograms in R</a>
<br>
<h2>shiny and rpanel - a quick comparison</h2>
<pre>
Shiny is a package from RStudio that lets you produce interactive web pages. 
You build a page with some control widgets and a handler that does something dependent on the value of those widgets. 
You can build your interface programmatically or create a boilerplate html page that gets filled in by control and output widgets.

A conceptually similar pattern is implemented by the rpanel package, but this uses the tcltk toolkit. 
A panel is created, control widgets added, and callbacks on the controls can run R code to, for example, update a plot.

qq plot example
Here's the rpanel version:

require(rpanel)
# box-cox transform
bc.fn <- function(y, lambda) {
    if (abs(lambda) < 0.001) 
        z <- log(y) else z <- (y^lambda - 1)/lambda
}

# qqplot of transformed data
qq.draw <- function(panel) {
    z <- bc.fn(panel$y, panel$lambda)
    qqnorm(z, main = paste("lambda =", round(panel$lambda, 2)))
    panel
}

# create a new panel with some initial data
panel <- rp.control(y = exp(rnorm(50)), lambda = 1)

# add a slider that calls qq.draw on change
rp.slider(panel, lambda, -2, 2, qq.draw)

Run these functions and you should see a slider and a graphics window. 
Move the slider to modify the plot.
Note that this might not work too well under RStudio because of the way the embedded RStudio graphics device captures output.


And here is the shiny version, which comes in two files living in their own folder.  ui.R and server.R

First qqplot/ui.R:

library(shiny)
# this defines our page layout
shinyUI(pageWithSidebar(
  headerPanel("qqplot example"),
  sidebarPanel(
  # a slider called 'lambda':
    sliderInput("lambda", "Lambda value", min = -2, max = 2, step=0.01, value = 0)
  ),
  mainPanel(
    # the main panel is the plotted output from qqplot:
    plotOutput("qqPlot")
  )
))

and qqplot/server.R:

library(shiny)
shinyServer(function(input, output) {
    # b-c transform
    bc.fn <- function(y, lambda) {
        if (abs(lambda) < 0.001) 
            z <- log(y) else z <- (y^lambda - 1)/lambda
    }
    # initial data
    y = exp(rnorm(50))
    # here's the qqplot method:
    output$qqPlot <- reactivePlot(function() {
        z <- bc.fn(y, input$lambda)
        qqnorm(z, main = paste("lambda =", round(input$lambda, 2)))
    })
})

With that done, launch the app with:

runApp("qqplot")
that should open up the page in your web browser. 
Hit break, stop, or control-C to quit.


Notes
The rpanel plot updates as you drag the slider, whereas shiny updates only when you let go of the slider.

I find that when I hit Control-C and break a running shiny app, then my tcltk windows go all unresponsive until I quit R and start again. 
Threading issues? This is on Linux. 
I've always had problems with tcltk widgets going unresponsive on me, or ending up unkillable.

The shiny UI looks, well, “shiny”, but the rpanel interface looks a bit old and not very exciting (if you can get excited by user interfaces).

Using the tkrplot package, you can build integrated rpanel packages with controls and plots in the same window. 
Without it, you are stuck with separate graphics and control windows.

Which should I use?
How do I know?! Shiny looks better, but I do like the update on drag of rpanel - it gives you much better feedback as you control the plot. 
Maybe this can be done in shiny with some additional work.

I don't really like the two-file method of shiny. 
Looking at the code I see the files just get sourced in, so conceivably it could be possible to run shiny apps just by specifying the shinyServer and shinyUI functions - but shiny monitors the server.R and ui.R file for changes and updates the application, which is quite nice.

So there's the basic existential dilemma. 
Choice. 
I can even throw some more things into the mix if you want - there' RServe, or RApache with gWidgetsWWW and probably many many more. 
I'm sure we can all agree that the days of needing Java and Apache Tomcat to deploy R applications to the web are now over (http://sysbio.mrc-bsu.cam.ac.uk/Rwui/tutorial/quick_tour.html).

I might try and implement some more of the rpanel examples in shiny shortly. 
Or why don't you have a go, and publish your works here?

<h2>R GUI 視窗程式設計</h2>
<a href="http://www.hmwu.idv.tw/web/R/F01-hmwu_R-GUI-Design.pdf">R GUI 視窗程式設計 tcltk/tcltk2, rpanel</a>
<a href="http://adrian.waddell.ch/EssentialSoftware/Rtcltk_geometry.pdf">Rtcltk_geometry</a>

<a href="http://rstudio-pubs-static.s3.amazonaws.com/2666_f0de0980ac9048d0a71d0f507cd83c3f.html">shiny and rpanel - a quick comparison</a>

<h3>rpanel sample</h3>
<a href="https://www.academia.edu/370260/rpanel_Simple_Interactive_Controls_for_R_Functions_Using_the_tcltk_Package_9999_">rpanel: Simple Interactive Controls for R Functions Using the tcltk Package </a>
<a href="https://www.academia.edu/attachments/1880059/download_file?st=MTU2NDAxOTg4MywxODIuMjM5LjExNS4zNg%3D%3D&s=swp-splash-header">download rpanel sample</a>
<a href="www.stats.gla.ac.uk/~adrian/rpanel">The `rpanel' package</a>
<a href="http://www.stats.gla.ac.uk/~adrian/rpanel/scripts/rpanel-paper-scripts.r">Simple Interactive Controls for R Functions scripts</a>

library(rpanel)
x11(width=4,height=4)
qq.draw <- function(panel)
 { z <- bc.fn(panel$y, panel$lambda)
   qqnorm(z, main = paste("lambda =",round(panel$lambda, 2)))
   panel
 }
 panel <- rp.control(y = exp(rnorm(50)), lambda = 1)
 rp.slider(panel, lambda, -2, 2, qq.draw,showvalue = TRUE)


<h3>create a  matrix</h3>
A = matrix( 
c(2, 4, 3, 1, 5, 7), # the data elements 
nrow=2,              # number of rows 
ncol=3,              # number of columns 
byrow = TRUE)        # fill matrix by rows 


<h2>cross tabulations</h2>
<a href="Contingency Table.html" class="bordred2 borRad10 green whitebs">Contingency Table</a>  <a href="Xtabs exercises.html" class="bordred2 borRad10 white whitebs">Xtabs exercises</a> 

a chart is different from a table
a chart is a graphic representation
a table is a numeric representation

frequaency table is a single row table

cross tabulations, 列联表, contingency tables, 又称交互分类表 按两个或更多变量分类时所列出的频数表。

R provides many methods for creating frequency and contingency tables. 

generate frequency tables using the table( ) function, table( ) function can also create cross tab, table( ) can also generate multidimensional tables based on 3 or more categorical variables.

generate tables of proportions using the prop.table( ) function
generate marginal frequencies using margin.table( )

# 2-Way Frequency Table using table() function
attach(mtcars)
mytable <- table(mtcars$gear,mtcars$cyl) # A will be rows, B will be columns 
mytable # print table 

# 2-Way Frequency Table using xtabs()
y = xtabs(~ cyl + gear, mtcars)	# xtabs gives row and col labels

margin.table(mytable, 1) # A frequencies (summed over B) 
margin.table(mytable, 2) # B frequencies (summed over A)

prop.table(mytable) # cell percentages
prop.table(mytable, 1) # row percentages 
prop.table(mytable, 2) # column percentages

# 3-Way Frequency Table 
mytable <- table(A, B, C) 
mytable <- table(mtcars$gear,mtcars$cyl,mtcars$mpg)
mytable

# 3-Way Frequency Table
mytable <- xtabs(~A+B+c, data=mydata)
mytable <- xtabs(~gear+cyl+mpg, mtcars)
summary(mytable) # chi-square test of indepedence

<a href="https://www.statmethods.net/stats/frequencies.html">Frequencies and Crosstabs</a>

<h2>parallel 平行計算</h2>
<a href="https://blog.gtwang.org/r/r-parallel-computing-module-tutorial/">R 的 parallel 平行計算套件使用教學與範例</a>
<br>
<a href="How-to go parallel in R.html">How-to go parallel in R</a>
<br>
<h2>edply</h2>
<a href="https://www.r-bloggers.com/edply-combining-plyr-and-expand-grid/">edply: combining plyr and expand.grid</a>

<h2>column merge two tables</h2>
lista = c(1:5)
listb = c(6:10)
listc = paste0(lista, "  ",listb)
lista
listb
listc

data from files:
lista = readLines("list1.txt")
listb = readLines("list2.txt")
listc = paste0(lista, "  ",listb)
sink("list3.txt")
cat(listc, sep="\n")
sink()

note: may use cbind in dataframe
lista = c(1:5)
listb = c(6:10)
listc = c(11:15)
MC = matrix()  # this is an empty matrix

MB = matrix( c(lista,listb,listc), nrow=5, ncol=3)  # a 3 column matrix
MC = cbind(MB[,1],MB[,3])   # now MC is a two column matrix

<h2>chop in blocks</h2>
groupPageNum = 7
theList = 1:78
if(length(theList)%%groupPageNum==0){
  pageNo = length(theList)%/%groupPageNum
}else{
  pageNo = length(theList)%/%groupPageNum +1
}
pageNo


for(page in 1:pageNo){
  if(length(theList) > groupPageNum){
	thepage= theList[1:groupPageNum]
	theList= theList[-(1:groupPageNum)]
     arrangePages(thepage)
	page = page + 1
  }else{
     arrangePages(theList)
  }
}


<h2>remove items</h2>

fmList=c('02917','01876','01960','03938','02951','02952','06820','06110','03601','01895')
itemList=c('02718','02696')

commons = fmList[fmList %in% itemList]
cat("\n\nnumber of Items to remove: ", length(commons), "\n")
for(item in commons){fmList = fmList[-(which(fmList == item))]}
fmList

<h2>extract chinanews images</h2>
http://www.chinanews.com/tp/hd2011/2019/10-20/909276.shtml
copy the thumb address and replace ending 320x300.jpg with 1000x2000

<h2>cut(x,breaks)</h2>
x = sort(rnorm(13,5,12))
x
cut(x,5)

<h2>R GUI: RGtk or Tcl/Tk, gWidgets</h2>
<a href="http://www.ggobi.org/rgtk2/">RGtk2</a>

<a href="https://www.r-bloggers.com/playing-with-guis-in-r-with-rgtk2/">Playing with GUIs in R with RGtk2</a>

<a href="https://www.r-bloggers.com/gui-building-in-r-gwidgets-vs-deducer/">GUI building in R: gWidgets vs Deducer</a>

require("RGtk2")

window <- gtkWindow()
window["title"] <- "Calculator"

frame <- gtkFrameNew("Calculate")
window$add(frame)

box1 <- gtkVBoxNew()
box1$setBorderWidth(30)
frame$add(box1)   #add box1 to the frame

box2 <- gtkHBoxNew(spacing= 10) #distance between elements
box2$setBorderWidth(24)

TextToCalculate<- gtkEntryNew() #text field with expresion to calculate
TextToCalculate$setWidthChars(25)
box1$packStart(TextToCalculate)

label = gtkLabelNewWithMnemonic("Result") #text label
box1$packStart(label)

result<- gtkEntryNew() #text field with result of our calculation
result$setWidthChars(25)
box1$packStart(result)

box2 <- gtkHBoxNew(spacing= 10) # distance between elements
box2$setBorderWidth(24)
box1$packStart(box2)

Calculate <- gtkButton("Calculate")
box2$packStart(Calculate,fill=F) #button which will start calculating

Sin <- gtkButton("Sin") #button to paste sin() to TextToCalculate
box2$packStart(Sin,fill=F)

Cos <- gtkButton("Cos") #button to paste cos() to TextToCalculate
box2$packStart(Cos,fill=F)

model<-rGtkDataFrame(c("double","integer"))
combobox <- gtkComboBox(model)
#combobox allowing to decide whether we want result as integer or double

crt <- gtkCellRendererText()
combobox$packStart(crt)
combobox$addAttribute(crt, "text", 0)

gtkComboBoxSetActive(combobox,0)
box2$packStart(combobox)

DoCalculation<-function(button)
{

  if ((TextToCalculate$getText())=="") return(invisible(NULL)) #if no text do nothing

   #display error if R fails at calculating
   tryCatch(
      if (gtkComboBoxGetActive(combobox)==0)
   result$setText((eval(parse(text=TextToCalculate$getText()))))
   else (result$setText(as.integer(eval(parse(text=TextToCalculate$getText()))))),
   error=function(e)
      {
      ErrorBox <- gtkDialogNewWithButtons("Error",window, "modal","gtk-ok", GtkResponseType["ok"])
      box1 <- gtkVBoxNew()
      box1$setBorderWidth(24)
      ErrorBox$getContentArea()$packStart(box1)

      box2 <- gtkHBoxNew()
      box1$packStart(box2)

      ErrorLabel <- gtkLabelNewWithMnemonic("There is something wrong with your text!")
      box2$packStart(ErrorLabel)
      response <- ErrorBox$run()


      if (response == GtkResponseType["ok"])
         ErrorBox$destroy()

      }
   )

}


  PasteSin<-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"sin()",sep=""))

}

PasteCos<-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"cos()",sep=""))

}

#however button variable was never used inside 
#functions, without it gSignalConnect would not work
gSignalConnect(Calculate, "clicked", DoCalculation)
gSignalConnect(Sin, "clicked", PasteSin)
gSignalConnect(Cos, "clicked", PasteCos)
Now it works like planned.



library(RGtk2)
createWindow <- function()
{
    window <- gtkWindow()

    label <- gtkLabel("Hello World")
    window$add(label)
}

createWindow()
gtk.main()

<h2>To keep the scripts and algorithm secret</h2>
by saving functions using save(). 
For example, here's a function f() you want to keep secret:

f <- function(x, y) { return(x + y)}

Save it :
save(f, file = 'C:\\Users\\Joyce\\Documents\\R\\Secret.rda')

And when you want to use the function:
load("C:\\Users\\Joyce\\Documents\\R\\Secret.rda")

Save all functions in separate files, 
put them in a folder and have one plain old .R script
loading them all in and executing whatever.
Zip the whole thing up and distribute it to whoever.
Maybe even compile it into a package.
Effectively the whole thing would be read-only then.

This solution isn't that great though.
You can still see the function in R by typing the name of the function
so it's not hidden in that sense.
But if you open the .rda files their contents are all garbled.
It all depends really on how experienced the recipients of your code are with R.

One form of having encrypted code is implemented in the petals function in the TeachingDemos package.

it would only take intermediate level programing skills to find the hidden code,
however it does take deliberate effort and the user would not be able to claim having seen the code by accident.
You would then need some type of license agreement in place to enforce any no peeking agreements.

Well you are going to need R installed on the deployment machine.

<h2>Test if characters are in a string</h2>
grepl("abc", "abcde")
note: RE will be applied, take care of the expression

<h2>get password</h2>
install.packages("getPass")
pass = getPass::getPass(msg = "PASSWORD: ", noblank = FALSE, forcemask = FALSE)

<h2>tryCatch loop</h2>
  retrieveData <- function(urlAddr){      
    retryCounter = 0
    while(retryCounter < 20) {
      cat("..",retryCounter," ") 
      retriveFile <- tryCatch(readLines(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter <- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

  retrieveData <- function(urlAddr){      
    retryCounter = 1
    while(retryCounter < 20) {
      cat("..try ",retryCounter," ") 
      retriveFile <- tryCatch(read_html(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter <- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

<h2>變異數分析, 方差分析 ANOVA </h2>
<a href="http://personality-project.org/r/r.guide/r.anova.html">r.anova</a>
<br>
<a href="https://alex59638.pixnet.net/blog/post/403137005-用r進行anova%28變方分析%29">ANOVA可分析多組間的差異 變異數分析 (ANOVA)</a>
<a href="http://programmermagazine.github.io/201310/htm/article3.html">主成分分析 Principle Component Analysis</a>
<br>
<h2>常用統計檢驗法簡介</h2>

T.test(又稱 T 檢定、T檢驗、t.test，以下簡稱T檢驗)
T檢驗主要用於檢定樣本的平均值，這是一項重點。

如果要看一個樣本的平均是否等於某值，要用 T 檢驗。

如果要看兩個樣本的平均是否相等，要用 T 檢驗。

T 檢驗分成三種類別
1.單樣本T檢驗(One smaple T test)
2.獨立雙樣本T檢驗
3.配對雙樣本T檢驗

要看 30 個男生的身高是否等於 180，用單樣本T檢驗。
[R語法:t.test(樣本,mu=平均)]

要看 A 班與 B 班男生身高是否相等，用獨立雙樣本T檢驗。
[R語法:t.test(A樣本,B樣本)]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A樣本,B樣本,paired=TURE)]

每一種 T 檢驗都還可以再分成雙尾與單尾檢驗。
[R語法:t.test(樣本,mu=平均,alternative= "two.sided")]

two.sided代表等於，就是雙尾的意思，也可以改成單尾的大於"greater"或是單尾的小於"less"。

重點只有"檢驗平均等於某值時"是雙尾，"檢驗平均小於某值時"是單尾，"檢驗平均大於某值時"是單尾。
請看到這裡後不要再講單尾或是雙尾了，一點意義也沒有，講大於等於小於就好了。
但Eecel沒有大於小於的選項，只有單尾雙尾，因此要自己判斷是大於還是小於(從樣本平均看即可)。
[Eecel語法:TTEST(A樣本,B樣本,2,2)]，2代表雙尾，改成1就變成單尾。

要看 30 個男生的身高是否大於 180，用單樣本T檢驗
[R語法:t.test(樣本,mu=180),alternative="greater"]

要看 A 班與 B 班男生身高差異是否小於 30，用獨立雙樣本T檢驗
[R語法:t.test(A,B,mu=30,alternative="less")]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A,B,mu=30,paired=T)]
[Eecel語法:TTEST(A樣本,B樣本,2,1)]

其中獨立雙樣本T檢驗(Welch Two smaple T test)還細分成變異數相等或變異數不相等兩種，這要看你母體與取樣的方法，如果不確定，嚴格一點是認為不相等的。

變異數相等
[R語法:t.test(A,B,mu=0,var.equal=T)]

變異數不相等
[R語法:t.test(A,B,mu=0,var.equal=F)]

<h3>卡方檢定 chi-square test(以下簡稱卡方檢定)</h3>
卡方檢驗用於確認樣本是否符合某種分配
骰子丟一百次，每面的機率是否為1/6</a>)，
或是兩個屬性之間是否有所關聯(男生是否比較容易選擇藍色商品)。

這其實是一樣的概念，假設兩個屬性之間無關，其分佈上應該會呈現隨機;
如果兩個屬性有關，例如男生喜歡藍色商品，在同樣的其況下，男生買藍色商品的次數會比男生買紅色商品的次數多，也就是不符合隨機的分配(理論上無關的話次數會一樣多)。

卡方檢定分成三種
1.適合度檢定（Goodness of fit test）
2.獨立性檢定（Test of independence）
3.同質性檢定 (Test of Homogeneity)

其實獨立性與同質性檢定是同一個東西，只是問法不一樣而已卡方適合度檢定用來檢驗樣本是否服從某種分佈，這種分佈你的心裡要有底，比方隨機(丟骰子各面是1/6)，孟德爾的紅花白花是3:1等等，如果你不知道要選擇哪種分佈，那就不能用卡方適合度檢定。

紅花969株，白花360株，檢驗是否符合孟德爾3:1的分佈，用卡方適合度檢定
chisq.test(c(969,360),p=c(0.75,0.25))
#次數表放第一個變數,p後面接機率，機率合要等於1[R語法:chisq.test(次數表,p=機率)]

骰子1000次，檢驗每面是否為1/6的分佈，用卡方適合度檢定
x=ceiling(runif(1000)*6)#丟1000次骰子, ceiling是無條件進位，讓數值落在1~6的整數
table(x)
#這是卡方檢定的重點，必須輸入統計次數，知道骰出1的有幾次，2的有幾次
chisq.test(table(x),p=c(1/6,1/6,1/6,1/6,1/6,1/6))
#次數表放第一個，p後面接分佈的機率，本次是6個1/6。

[R語法:chisq.test(次數表,p=機率)]

<h3>費雪精確性檢定 Fisher's exact test</h3>
類似卡方檢定的小樣本方式，通常用於樣本小於20的狀況，案例是猜八杯茶是先加奶還是先加茶。
fisher.test(table(real,guess))

<h2>變異數分析 ANOVA</h2>
兩組資料連續看是否有差異，用t.test，兩組以上則用ANOVA，其虛無假說H0:u1=u2=u3=...un。
若p值小於0.05，則認為並非所有的資料來自同一個母體。

若要知道到底是哪組資料不同，可使用 
pairwise.t.test(Y, B, p.adjust.method="none")
其中Y為資料列，B為組別列，並且不調整p值。
雙因子變異數分析
aov(cardspent~factor(region)*factor(gender)
使用*符號而不是+


<h2>Logistic Regression</h2>
Logistic regression, also called a logit model, is used to model dichotomous outcome variables. 

Logistic regression is a method for fitting a regression curve, y = f(x), when y is a categorical variable. 
The typical use of this model is predicting y given a set of predictors x. The predictors can be continuous, categorical or a mix of both.

一般的線性迴歸都是連續數值，例如身高或體重。
但有些情況下的應變數為類別，例如生還與否(1或0)，就可以採用Logistic Regression。

Logistic Regression有幾項要點，
1.他需要應變數為類別變項
2.他會給出一個式子，帶入自變數後(可為連續變項或類別變項)，會得出一個值
3.這個值稱為勝算比


以鐵達尼號乘客名單的資料作為範例分析
model1&lt;-glm data="titanic_passenger," family="binomial(link=" formula="survival~fare," logit="" na.action="na.exclude)&lt;/p"&gt;summary(model1)
其中fare 對 survival 的對數機率為 0.013108
勝算比為exp(0.013108)=1.013085
多一英鎊，多1%生還率。
參考資料
<a href="https://sites.google.com/site/rlearningsite/catagory/logit" target="_blank">Logistic迴歸模型</a>
<a href="http://xn--r-vc8at2mlrkqvkh65cu2ccyjyqb/" target="_blank">R语言逻辑回归分析</a>
<a href="https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/" target="_blank">How to perform a Logistic Regression in R</a><!---glm-->


<a href="http://ccckmit.wikidot.com/r:optimize">一維空間優化方法：optimize()</a>
<br>
<a href="http://ccckmit.wikidot.com/r:main">R 統計軟體 作者：陳鍾誠</a>
<br>
<a href="http://programmermagazine.github.io/201309/htm/article3.html">R 統計軟體(6) – 迴歸分析</a>
<br>

<h2>Advanced Statistics Tree-Based Models</h2>
<a href="https://www.datacamp.com/community/tutorials/decision-trees-R">Decision Trees in R</a>
<br>
<a href="https://www.guru99.com/r-decision-trees.html">Decision Tree in R with Example</a>
<br>
<a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a>
<br>
<a href="http://www.di.fc.ul.pt/~jpn/r/tree/tree.html">Classification & Regression Trees</a>
<br>
<a href="https://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html">Classification and Regression Trees with the rpart.plot package</a>
<br>

<h2>Grouping functions (tapply, by, aggregate) and the *apply family</h2>
R has many *apply functions.
Much of the functionality of the *apply family is covered by the extremely popular <code>plyr</code> package, the base functions remain useful and worth knowing.

<li><strong>apply</strong> - <em>When you want to apply a function to the rows or columns of a matrix (and higher-dimensional analogues); not generally advisable for data frames as it will coerce to a matrix first.</em>

<code># Two dimensional matrix
M &lt;- matrix(seq(1,16), 4, 4)

# apply min to rows
apply(M, 1, min)
[1] 1 2 3 4

# apply max to columns
apply(M, 2, max)
[1]  4  8 12 16

# 3 dimensional array
M &lt;- array( seq(32), dim = c(4,4,2))

# Apply sum across each M[*, , ] - i.e Sum across 2nd and 3rd dimension, look from top is an area
apply(M, 1, sum)
# Result is one-dimensional
[1] 120 128 136 144

# Apply sum across each M[*, *, ] - i.e Sum across 3rd dimension
apply(M, c(1,2), sum)
# Result is two-dimensional
     [,1] [,2] [,3] [,4]
[1,]   18   26   34   42
[2,]   20   28   36   44
[3,]   22   30   38   46
[4,]   24   32   40   48
</code>

If you want row/column means or sums for a 2D matrix, be sure to investigate the highly optimized, lightning-quick <code>colMeans</code>, <code>rowMeans</code>, <code>colSums</code>, <code>rowSums</code>.</li>
<li><strong>lapply</strong> - <em>When you want to apply a function to each element of a list in turn and get a list back.</em>

This is the workhorse of many of the other *apply functions. 
Peel back their code and you will often find <code>lapply</code> underneath.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
$a 
[1] 1
$b 
[1] 3
$c 
[1] 91
lapply(x, FUN = sum) 
$a 
[1] 1
$b 
[1] 6
$c 
[1] 5005</code></li>
<li><strong>sapply</strong> - <em>When you want to apply a function to each element of a list in turn, but you want a <strong>vector</strong> back, rather than a list.</em>

If you find yourself typing <code>unlist(lapply(...))</code>, stop and consider <code>sapply</code>.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Compare with above; a named vector, not a list 
sapply(x, FUN = length)  
a  b  c   
1  3 91

sapply(x, FUN = sum)   
a    b    c    
1    6 5005 
</code>

In more advanced uses of <code>sapply</code> it will attempt to coerce the result to a multi-dimensional array, if appropriate. 
For example, if our function returns vectors of the same length, <code>sapply</code> will use them as columns of a matrix:

<code>sapply(1:5,function(x) rnorm(3,x))
</code>

If our function returns a 2 dimensional matrix, <code>sapply</code> will do essentially the same thing, treating each returned matrix as a single long vector:

<code>sapply(1:5,function(x) matrix(x,2,2))</code>

Unless we specify <code>simplify = "array"</code>, in which case it will use the individual matrices to build a multi-dimensional array:

<code>sapply(1:5,function(x) matrix(x,2,2), simplify = "array")</code>

Each of these behaviors is of course contingent on our function returning vectors or matrices of the same length or dimension.</li>
<li><strong>vapply</strong> - <em>When you want to use <code>sapply</code> but perhaps need to squeeze some more speed out of your code.</em>

For <code>vapply</code>, you basically give R an example of what sort of thing your function will return, which can save some time coercing returned values to fit in a single atomic vector.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Note that since the advantage here is mainly speed, this
# example is only for illustration. 
We're telling R that
# everything returned by length() should be an integer of length 1. 

vapply(x, FUN = length, FUN.VALUE = 0L) 
a  b  c  
1  3 91
</code></li>
<li><strong>mapply</strong> - <em>For when you have several data structures (e.g. 
vectors, lists) and you want to apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in <code>sapply</code>.</em>

This is multivariate in the sense that your function must accept multiple arguments.

<code>#Sums the 1st elements, the 2nd elements, etc. 

mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15
#To do rep(1,4), rep(2,3), etc.
mapply(rep, 1:4, 4:1)   
[[1]]
[1] 1 1 1 1

[[2]]
[1] 2 2 2

[[3]]
[1] 3 3

[[4]]
[1] 4
</code></li>
<li><strong>Map</strong> - <em>A wrapper to <code>mapply</code> with <code>SIMPLIFY = FALSE</code>, so it is guaranteed to return a list.</em>

<code>Map(sum, 1:5, 1:5, 1:5)
[[1]]
[1] 3

[[2]]
[1] 6

[[3]]
[1] 9

[[4]]
[1] 12

[[5]]
[1] 15
</code></li>
<li><strong>rapply</strong> - <em>For when you want to apply a function to each element of a <strong>nested list</strong> structure, recursively.</em>

To give you some idea of how uncommon <code>rapply</code> is, I forgot about it when first posting this answer! Obviously, I'm sure many people use it, but YMMV. 
<code>rapply</code> is best illustrated with a user-defined function to apply:

<code># Append ! to string, otherwise increment
myFun &lt;- function(x){
    if(is.character(x)){
      return(paste(x,"!",sep=""))
    }
    else{
      return(x + 1)
    }
}

#A nested list structure
l &lt;- list(a = list(a1 = "Boo", b1 = 2, c1 = "Eeek"), 
          b = 3, c = "Yikes", 
          d = list(a2 = 1, b2 = list(a3 = "Hey", b3 = 5)))


# Result is named vector, coerced to character          
rapply(l, myFun)

# Result is a nested list like l, with values altered
rapply(l, myFun, how="replace")
</code></li>
<li><strong>tapply</strong> - <em>For when you want to apply a function to <strong>subsets</strong> of a vector and the subsets are defined by some other vector, usually a factor.</em>

The black sheep of the *apply family, of sorts. 
The help file's use of the phrase "ragged array" can be a bit <a href="https://stackoverflow.com/questions/6297201/explain-r-tapply-description/6297396#6297396">confusing</a>, but it is actually quite simple.

A vector:

<code>x &lt;- 1:20</code>

A factor (of the same length!) defining groups:

<code>y &lt;- factor(rep(letters[1:5], each = 4))</code>

Add up the values in <code>x</code> within each subgroup defined by <code>y</code>:

<code>tapply(x, y, sum)  
 a  b  c  d  e  
10 26 42 58 74 
</code>

More complex examples can be handled where the subgroups are defined by the unique combinations of a list of several factors. 
<code>tapply</code> is similar in spirit to the split-apply-combine functions that are common in R (<code>aggregate</code>, <code>by</code>, <code>ave</code>, <code>ddply</code>, etc.) Hence its black sheep status.</li>

<b>Slice vector</b>
We can use lapply() or sapply() interchangeable to slice a data frame. 
We create a function, below_average(), that takes a vector of numerical values and returns a vector that only contains the values that are strictly above the average. 

below_ave <- function(x) {  
    ave <- mean(x) 
    return(x[x > ave])
}

Compare both results with the identical() function.
dataf_s<- sapply(dataf, below_ave)
dataf_l<- lapply(dataf, below_ave)
identical(dataf_s, dataf_l)


<h2>Principal Component Methods</h2>
<a href="Principal Component Methods.html">Principal Component Methods</a>


<h2>NLP techniques</h2>
<a href="NLP techniques.html">NLP techniques</a>
<br>


<h2>RMySQL R connect to MySQL</h2>
root
asdf1234
SHOW DATABASES

# 1. Library
library(RMySQL)

# 2. Settings
db_user <- 'root'
db_password <- 'asdf1234'
db_name <- 'sampledb'
# db_table <- 'example'
db_table <- 'world'

db_host <- '127.0.0.1' # for local access
db_port <- 3306

# 3. Read data from db
mydb <-  dbConnect(MySQL(), user = db_user, password = db_password,
         dbname = db_name, host = db_host, port = db_port)
s <- paste0("select * from ", db_table)
rs <- dbSendQuery(mydb, s)
df <-  fetch(rs, n = -1)
on.exit(dbDisconnect(mydb))

<h2>convert R {xml_node} to plain text while preserving the tags</h2>
className = "#icnt"
keywordList <- html_nodes(pagesource, className)
as.character(keywordList)

<h2>convert R objects into a binary format</h2>
x <- list(1, 2, 3)
serialize(x, NULL)
The serialize() function is used to convert individual R objects into a binary format that can be communicated across an arbitrary connection. This may get sent to a file, but it could get sent over a network or other connection.

<h2>Convert an R Object to a Character String</h2>
x <- c("a", "b", "aaaaaaaaaaa")
toString(x)
toString(x, width = 8)


<h2>html_node, html_nodes</h2>
html_node retrieves the first element it encounter, 
while html_nodes returns each matching element in the page as a list.

use html_nodes instead of html_node.

The toString() function collapse the list of strings into one.

library(rvest)
pagesource <- read_html("url")

testpost <- pagesource %>% 
  html_nodes("#contentmiddle>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>Excluding Nodes in RVest</h2>
library(rvest)
pagesource <- read_html("url")

testpost <- pagesource %>% 
  html_nodes("#content>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>xml_remove()</h2>
By using xml_remove(), you can literally remove any nodes

text <- '
<table> <tr class="alt">
     <td>1</td>
     <td>2</td>
     <td class="hidden">3</td>
   </tr>
   <tr class="tr0 close notule">
     <td colspan="9">4</td> </tr>
</table>'

html_tree <- read_html(text)

#select nodes you want to remove
hidden_nodes <- html_tree %>% html_nodes(".hidden")
close_nodes <- html_tree %>% html_nodes(".tr0.close.notule")

#remove those nodes
xml_remove(hidden_nodes)
xml_remove(close_nodes)

html_tree %>% html_table()


<h2>view all xml_nodeset class object (output of rvest::html_nodes)</h2>
print.AsIs(keywordList)

<h2>Install package loaclly</h2>
# 安装export包
if(!require(export)){
install.packages('export')
require(export)
}

下载安装包文件
打开git bash，执行命令：
git clone https://github.com/tomwenseleers/export.git

BUILD 安装包文件
R CMD BUILD export

安装包压缩文件
R CMD INSTALL

测试export包是否可以使用
require(export)

<h2>e1071 package Support vector machine</h2>
<a href="e1071 package SVM.html" class="whitebut ">e1071 package SVM</a>

<h2>substitute()</h2>
a <- 1
b <- 2
substitute(a + b + z) ## a + b + z

<h2>When to use CPUs vs GPUs vs TPUs?</h2>
Behind every machine learning algorithm is hardware crunching away at multiple gigahertz. 
You may have noticed several processor options when setting up Kaggle notebooks, but which one is best for you? In this blog post, we compare the relative advantages and disadvantages of using CPUs (<a href="https://www.intel.com/content/www/us/en/products/processors/xeon.html" target="_blank">Intel Xeon</a>*) vs GPUs (<a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a>) vs TPUs (<a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a>) for training machine learning models that were written using <a href="https://keras.io/" target="_blank">tf.keras</a> (Figure 1**). 
We’re hoping this will help you make sense of the options and select the right choice for your project.

<img class="lazy" data-src="https://miro.medium.com/max/1466/1*suXcuHEe29aKLPrQnXGBrg.png">

How we prepared the test
In order to compare the performance of CPUs vs GPUs vs TPUs for accomplishing common data science tasks, we used the <a href="https://www.tensorflow.org/datasets/catalog/tf_flowers" target="_blank">tf_flowers dataset</a> to train a convolutional neural network, and then the exact same code was run three times using the three different backends (CPUs vs GPUs vs TPUs; GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM). 
The accompanying <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">tutorial notebook</a> demonstrates a few best practices for getting the best performance out of your TPU.
For example:

Using a dataset of sharded files (<a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">e.g., .TFRecord</a>)
Using the <a href="https://www.tensorflow.org/guide/data" target="_blank">tf.data</a> API to pass the training data to the TPU
Using large batch sizes (e.g. 
batch_size=128)

By adding these precursory steps to your workflow, it is possible to avoid a common I/O bottleneck that otherwise prevents the TPU from operating at its full potential. 
You can find additional tips for optimizing your code to run on TPUs by visiting the official <a href="https://www.kaggle.com/docs/tpu" target="_blank">Kaggle TPU documentation</a>.
How the hardware performed
The most notable difference between the three hardware types that we tested was the speed that it took to train a model using <a href="https://keras.io/" target="_blank">tf.keras</a>. 
The tf.keras library is one of the most popular machine learning frameworks because tf.keras makes it easy to quickly experiment with new ideas. 
If you spend less time writing code then you have more time to perform your calculations, and if you spend less time waiting for your code to run, then you have more time to evaluate new ideas (Figure 2). 
tf.keras and TPUs are a powerful combination when participating in <a href="https://kaggle.com/c/flower-classification-with-tpus" target="_blank">machine learning competitions</a>!

<img class="lazy" data-src="https://miro.medium.com/max/1438/1*bqmG-YzgJzVeLbQ5Ym1iFg.png">
For our first experiment, we used the same code (a modified version*** of the <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">official tutorial notebook</a>) for all three hardware types, which required using a very small batch size of 16 in order to avoid out-of-memory errors from the CPU and GPU. 
Under these conditions, we observed that TPUs were responsible for a ~100x speedup as compared to CPUs and a ~3.5x speedup as compared to GPUs when training an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model (Figure 3). 
Because TPUs operate more efficiently with large batch sizes, we also tried increasing the batch size to 128 and this resulted in an additional ~2x speedup for TPUs and out-of-memory errors for GPUs and CPUs. 
Under these conditions, the TPU was able to train an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model more than 7x as fast as the GPU from the previous experiment****.

<img class="lazy" data-src="https://miro.medium.com/max/1438/1*p2X9DQcq9K5Iu76Kk82vrg.png">
The observed speedups for model training varied according to the type of model, with Xception and Vgg16 performing better than ResNet50 (Figure 4). Model training was the only type of task where we observed the TPU to outperform the GPU by such a large margin. 
For example, we observed that in our hands the TPUs were ~3x faster than CPUs and ~3x slower than GPUs for performing a small number of predictions (TPUs perform exceptionally when making predictions in some situations such as when <a href="https://docs.google.com/presentation/d/1O49AkNyYV48n0X4nWr7KE-5aask88pz9gBSQ26ZG-5o/edit#slide=id.g50ce3d3866_0_1590" target="_blank">making predictions</a> on very large batches, which were not present in this experiment).

<img class="lazy" data-src="https://miro.medium.com/max/46/1*p7U2zlYn9O5Yvjluh2P-dg.png">
<img class="lazy" data-src="https://miro.medium.com/max/1466/1*p7U2zlYn9O5Yvjluh2P-dg.png">
To supplement these results, we note that <a href="https://arxiv.org/abs/1907.10701" target="_blank">Wang<em> et. 
al</em></a> have developed a rigorous benchmark called ParaDnn [1] that can be used to compare the performance of different hardware types for training machine learning models. 
By using this method Wang<em> et. 
al</em> were able to conclude that the performance benefit for parameterized models ranged from 1x to 10x, and the performance benefit for real models ranged from 3x to 6.8x when a TPU was used instead of a GPU (Figure 5). 
TPUs perform best when combined with sharded datasets, large batch sizes, and large models.

<img class="lazy" data-src="https://miro.medium.com/max/1466/1*QbP2CPDZH5BQWlnaTtW3oA.png">
Price considerations when training models
While our comparisons treated the hardware equally, there is a sizeable difference in pricing. TPUs are ~5x as expensive as GPUs (<a href="https://cloud.google.com/compute/gpus-pricing" target="_blank">$1.46/hr</a> for a <a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a> GPU vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$8.00/hr</a> for a <a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a> vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$4.50/hr</a> for the TPUv2 with “on-demand” access on <a href="https://cloud.google.com/pricing/" target="_blank">GCP</a> ). 
If you are trying to optimize for cost then it makes sense to use a TPU if it will train your model at least 5 times as fast as if you trained the same model using a GPU.
We consistently observed model training speedups on the order of ~5x when the data was stored in <a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">a sharded format</a> in a <a href="https://www.kaggle.com/paultimothymooney/how-to-move-data-from-kaggle-to-gcs-and-back" target="_blank">GCS bucket</a> then passed to the TPU in large batch sizes, and therefore we recommend TPUs to cost-conscious consumers that are familiar with the <a href="http://tf.data" target="_blank">tf.data</a> API.
Some machine learning practitioners prioritize the reduction of model training time as opposed to prioritizing the reduction of model training costs. 
For someone that just wants to train their model as fast as possible, the TPU is the best choice. 
If you spend less time training your model, then you have more time to iterate upon new ideas. 
But don’t take our word for it — you can evaluate the performance benefits of CPUs, GPUs, and TPUs by running your own code in a <a href="https://www.kaggle.com/docs/kernels#the-kernels-environment" target="_blank">Kaggle Notebook</a>, free-of-charge. 
Kaggle users are already having a lot of fun and success experimenting with TPUs and text data: check out <a href="https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127333" target="_blank">this forum post</a> that describes how TPUs were used to train a BERT transformer model to win $8,000 (2nd prize) in a recent <a href="https://www.kaggle.com/c/tensorflow2-question-answering" target="_blank">Kaggle competition</a>.
Which hardware option should you choose?
In summary, we recommend CPUs for their versatility and for their large memory capacity. 
GPUs are a great alternative to CPUs when you want to speed up a variety of data science workflows, and TPUs are best when you specifically want to train a machine learning model as fast as you possibly can.
You can get better results by optimizing your code for the specific hardware that you are using and we think it would be especially interesting to compare runtimes for code that has been optimized for a GPU to runtimes for code that has been optimized for a TPU. 
For example, it would be interesting to record the time that it takes to train a gradient-boosted model using a GPU-accelerated library such as <a href="https://rapids.ai/" target="_blank">RAPIDS.ai</a> and then to compare that to the time that it takes to train a deep learning model using a TPU-accelerated library such as <a href="https://keras.io/" target="_blank">tf.keras</a>.
What is the least amount of time that one can train an accurate machine learning model? How many different ideas can you evaluate in a single day? When used in combination with tf.keras, TPUs allow machine learning practitioners to spend less time writing code and less time waiting for their code to run — leaving more time to evaluate new ideas and improve one’s performance in <a href="http://kaggle.com/c/flower-classification-with-tpus" target="_blank">Kaggle Competitions</a>.

<h3>Footnotes</h3>* CPU types vary according to variability. 
In addition to the Intel Xeon CPUs, you can also get assigned to either Intel Skylake, Intel Broadwell, or Intel Haswell CPUs. 
GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM).
** Image for Figure 1 from <a href="https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference" target="_blank">https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference,</a> with permission.
*** The tutorial notebook was modified to keep the parameters (e.g. 
batch_size, learning_rate, etc) consistent between the three different backends.
**** CPU and GPU experiments used a batch size of 16 because it allowed the Kaggle notebooks to run from top to bottom without memory errors or 9-hr timeout errors. 
Only TPU-enabled notebooks were able to run successfully when the batch size was increased to 128.


<h2>Diff function – Difference between elements of vector</h2>
Differences between elements of a vector

diff(x, lag = 1, differences = 1)
x – numeric vector
lag-an integer indicating how many lags to use.
Difference- order of difference

# diff in r examples
> x=c(1,2,3,5,8,13,21)
> diff(x)
[1] 1 1 2 3 5 8

The diff function provides the option “lag”.
The default specification of this option is 1.

If we want to increase the size of the lag, we can specify the lag option within the diff command as follows:

x <- c(5, 2, 10, 1, 3)
diff(x, lag = 2)                # Apply diff with lag
# 5 -1 -7

Example of difference function in R with lag 1 and differences 2:

#difference function in R with lag=1 and differences=2

diff(c(2,3,5,18,4,6,4),lag=1,differences=2)
First it is differenced with lag=1 and the result is again differenced with lag=1
So the output will be
[1]   1  11  -27   16   -4

ie. get the lag difference result, and then redo the difference again on the result:
2,3,5,18,4,6,4
  1,2,13,-14,2,-2
    1,11,-27,16,-4

<h2>cut2 function</h2>
cut2(x, cuts, m, g, levels.mean, digits, minmax=TRUE, oneval=TRUE)
Cut a Numeric Variable into Intervals
but left endpoints are inclusive and labels are of the form [lower, upper), except that last interval is [lower,upper].

x <- runif(1000, 0, 100)
z <- cut2(x, c(10,20,30))
table(z)
table(cut2(x, g=10))      # quantile groups
table(cut2(x, m=50))      # group x into intevals with at least 50 obs.

<h2>To clear up the memory</h2>
rm(list = ls())
.rs.restartR() # this will restart

memory.size(max=T) # gives the amount of memory obtained by the OS
memory.size(max=F) # gives the amount of memory being used
m = matrix(runif(10e7), 10000, 1000)
memory.size(max=F)

To clear up the memory
gc()
memory.size(max=F)
# still some memory being used

<h2>remove XML nodes</h2>
<a href="https://cran.r-project.org/web/packages/xml2/vignettes/modification.html" class="whitebut ">Node Modification</a>
<a href="https://cran.r-project.org/web/packages/XML/XML.pdf" class="whitebut ">Package XML</a>

#find parent nodes
parent<- review %>% html_nodes("blockquote")

#find children nodes to exclude
toremove<-parent %>% html_node("div.bbcode_container")

#remove nodes
xml_remove(toremove)

The xml_remove() can be used to remove a node (and it’s children) from a tree. 

library(XML)
r <- xmlRoot(doc)
removeNodes(r[names(r) == "location"])

<h2>Comment out block of code</h2>

if(FALSE) {
  all your code
}


<h2>Reading XML data</h2>
Data in XML format are rarely organized in a way that would allow the xmlToDataFrame function to work. 
You're better off extracting everything in lists and then binding the lists together in a data frame:

require(XML)
data <- xmlParse("http://forecast.weather.gov/MapClick.php?lat=29.803&lon=-82.411&FcstType=digitalDWML")

xml_data <- xmlToList(data)

<code>&gt; install.packages("XML")</code>
<code>&gt; library(XML)
text = paste0("&lt;bookstore>&lt;book>","&lt;title>Everyday Italian&lt;/title>","&lt;author>Giada De Laurentiis&lt;/author>","&lt;year>2005&lt;/year>","&lt;/book>&lt;/bookstore>")
</code>
Parse the XML file
<code>xmldoc <- xmlParse(text)
rootNode <- xmlRoot(xmldoc)
rootNode[1]

xmlToDataFrame(nodes = getNodeSet(xmldoc, "//title"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//author"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//book"))

newdf = xmlToDataFrame(getNodeSet(xmldoc, "//book"))
newdf = xmlToDataFrame(getNodeSet(xmldoc, "//title"))
</code>

Extract XML data:

<code>&gt; data &lt;- xmlSApply(rootNode,function(x) xmlSApply(x, xmlValue))</code>

text = paste0("&lt;CD>","&lt;TITLE>Empire Burlesque&lt;/TITLE>","&lt;ARTIST>Bob Dylan&lt;/ARTIST>","&lt;COUNTRY>USA&lt;/COUNTRY>","&lt;COMPANY>Columbia&lt;/COMPANY>","&lt;PRICE>10.90&lt;/PRICE>","&lt;YEAR>1985&lt;/YEAR>","&lt;/CD>")
<code>xmldoc <- xmlParse(text)
rootNode <- xmlRoot(xmldoc)
rootNode[1]</code>

Convert the extracted data into a data frame:

<code>&gt; cd.catalog &lt;- data.frame(t(data),row.names=NULL)</code>

Verify the results

The <code>xmlParse</code> function returns an object of the <code>XMLInternalDocument</code> class, which is a C-level internal data structure.
The <code>xmlRoot()</code> function gets access to the root node and its elements. 
We check the first element of the root node:

<code>&gt; rootNode[1]

$CD
&lt;CD&gt;
  &lt;TITLE&gt;Empire Burlesque&lt;/TITLE&gt;
  &lt;ARTIST&gt;Bob Dylan&lt;/ARTIST&gt;
  &lt;COUNTRY&gt;USA&lt;/COUNTRY&gt;
  &lt;COMPANY&gt;Columbia&lt;/COMPANY&gt;
  &lt;PRICE&gt;10.90&lt;/PRICE&gt;
  &lt;YEAR&gt;1985&lt;/YEAR&gt;
&lt;/CD&gt;
attr(,"class")
[1] "XMLInternalNodeList" "XMLNodeList"</code>
To extract data from the root node, we use the <code>xmlSApply()</code> function iteratively over all the children of the root node. 
The <code>xmlSApply</code> function returns a matrix.
To convert the preceding matrix into a data frame, we transpose the matrix using the <code>t()</code> function. 
We then extract the first two rows from the <code>cd.catalog</code> data frame:

<code>&gt; cd.catalog[1:2,]
             TITLE       ARTIST COUNTRY     COMPANY PRICE YEAR
1 Empire Burlesque    Bob Dylan     USA    Columbia 10.90 1985
2  Hide your heart Bonnie Tyler      UK CBS Records  9.90 1988</code>

XML data can be deeply nested and hence can become complex to extract. 
Knowledge of <code>XPath</code> will be helpful to access specific XML tags. 
R provides several functions such as <code>xpathSApply</code> and <code>getNodeSet</code> to locate specific elements.
<h4>Extracting HTML table data from a web page</h4>
Though it is possible to treat HTML data as a specialized form of XML, R provides specific functions to extract data from HTML tables as follows:

<code>&gt; url &lt;- "http://en.wikipedia.org/wiki/World_population"

webpage = read_html(url)
output = htmlParse(webpage)
tables = readHTMLTable(output)
world.pop = tables[[5]]

table.list = readHTMLTable(output, header=F)

u = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
webpage = read_html(u)
tables = readHTMLTable(webpage)
names(tables)
</code>
The <code>readHTMLTable()</code> function parses the web page and returns a <code>list</code> of all tables that are found on the page. 
For tables that have an <code>id</code> attribute, the function uses the <code>id</code> attribute as the name of that list element.
We are interested in extracting the "10 most populous countries," which is the fifth table; hence we use <code>tables[[5]]</code>.

<h4>Extracting a single HTML table from a web page</h4>

A single table can be extracted using the following command:

<code>&gt; table &lt;- readHTMLTable(url,which=5)</code>
Specify <code>which</code> to get data from a specific table. 
R returns a data frame.

<h2>use xpathSApply to extract html</h2>
library(RCulr)
library(XML)
 
html <- read_html("http://tonybreyal.wordpress.com/2011/11/17/cool-hand-luke-aldwych-theatre-london-2011-production/", followlocation = TRUE)

doc = htmlParse(html, asText=TRUE)
plain.text <- xpathSApply(doc, "//p", xmlValue)
cat(paste(plain.text, collapse = "\n"))

pageHeader = "http://www.hkej.com/template/dnews/jsp/toc_main.jsp"
html <- read_html(pageHeader, followlocation = TRUE)
doc = htmlParse(html, asText=TRUE)
plain.text <- xpathSApply(doc, "//a", xmlValue)
cat(paste(plain.text, collapse = "\n"))

<h2>reading XML using xml2</h2>
library(xml2)
library(purrr)

txt &lt;- '&lt;Doc name="Doc1">
    &lt;Lists Count="1">
        &lt;List Name="List1">
            &lt;Points Count="3">
                &lt;Point Id="1">
                    &lt;Tags Count ="1">"a"&lt;/Tags>
                    &lt;Point Position="1"  /> 
                &lt;/Point>
                &lt;Point Id="2">
                    &lt;Point Position="2"  /> 
                &lt;/Point>
                &lt;Point Id="3">
                    &lt;Tags Count="1">"c"&lt;/Tags>
                    &lt;Point Position="3"  /> 
                &lt;/Point>
            &lt;/Points>
        &lt;/List>
    &lt;/Lists>
&lt;/Doc>'

doc &lt;- read_xml(txt)
xml_find_all(doc, ".//Points/Point") %>% 
  map_df(function(x) {
    list(
      Point=xml_attr(x, "Id"),
      Tag=xml_find_first(x, ".//Tags") %>%  xml_text() %>%  gsub('^"|"$', "", .),
      Position=xml_find_first(x, ".//Point") %>% xml_attr("Position")
    )
  })



<h2>An Introduction to XPath: How to Get Started</h2>

XPath is a powerful language that is often used for scraping the web. 
It allows you to select nodes or compute values from an XML or HTML document and is actually one of the languages that you can use to extract web data using Scrapy. 
The other is CSS and while CSS selectors are a popular choice, XPath can actually allow you to do more.

With XPath, you can extract data based on text elements' contents, and not only on the page structure. 
So when you are scraping the web and you run into a hard-to-scrape website, XPath may just save the day (and a bunch of your time!).

This is an introductory tutorial that will walk you through the basic concepts of XPath, crucial to a good understanding of it, before diving into more complex use cases.
<h3>The basics</h3>
Consider this HTML document:

<code>&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;My page&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h2&gt;Welcome to my &lt;a href="#"&gt;page&lt;/a&gt;&lt;/h2&gt;
    &lt;p&gt;This is the first paragraph.&lt;/p&gt;
    &lt;!-- this is the end --&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

XPath handles any XML/HTML document as a tree. 
This tree's root node is not part of the document itself. 
It is in fact the parent of the document element node (<code>&lt;html&gt;</code> in case of the HTML above). 
This is how the XPath tree for the HTML document looks like:

<img class="lazy" data-src="https://blog.scrapinghub.com/hs-fs/hubfs/tree-7.png" style="background-color: gray">

As you can see, there are many node types in an XPath tree:

<li><strong>Element node:</strong> represents an HTML element, a.k.a an HTML tag.</li>
<li><strong>Attribute node:</strong> represents an attribute from an element node, e.g. 
“href” attribute in <code>&lt;a href=”http://www.example.com”&gt;example&lt;/a&gt;</code>.</li>
<li><strong>Comment node:</strong> represents comments in the document (<code>&lt;!-- … --&gt;</code>).</li>
<li><strong>Text node:</strong> represents the text enclosed in an element node (<code>example</code> in <code>&lt;p&gt;example&lt;/p&gt;</code>).</li>
</ul>
Distinguishing between these different types is useful to understand how XPath expressions work. 
Now let's start digging into XPath.

Here is how we can select the title element from the page above using an XPath expression:

/html/head/title


This is what we call a location path. 
It allows us to specify the path from the <strong>context node</strong> (in this case the root of the tree) to the element we want to select, as we do when addressing files in a file system. 
The location path above has three location steps, separated by slashes. 
It roughly means: <em>start from the ‘html’ element, look for a ‘head’ element underneath, and a ‘title’ element underneath that ‘head’</em>. 
The context node changes in each step. 
For example, the <code>head</code> node is the context node when the last step is being evaluated.

However, we usually don't know or don’t care about the full explicit node-by-node path, we just care about the nodes with a given name. 
We can select them using:

//title


Which means:<em> look in the whole tree, starting from the root of the tree (<code>//</code>) and select only those nodes whose name matches <code>title</code></em>. 
In this example, <code>//</code> is the <strong>axis</strong> and <code>title</code> is the <strong>node test</strong>.

In fact, the expressions we've just seen are using XPath's abbreviated syntax. 
Translating <code>//title</code> to the full syntax we get:

/descendant-or-self::node()/child::title


So, <code>//</code> in the abbreviated syntax is short for <code>descendant-or-self</code>, which means <em>the current node or any node below it in the tree</em>. 
This part of the expression is called the <strong>axis</strong> and it specifies a set of nodes to select from, based on their direction on the tree from the current context (downwards, upwards, on the same tree level). 
Other examples of axes are: parent, child, ancestor, etc -- we’ll dig more into this later on.

The next part of the expression, <code>node()</code>, is called a <strong>node test</strong>, and it contains an expression that is evaluated to decide whether a given node should be selected or not. 
In this case, it selects nodes from all types. 
Then we have another axis,<code>child</code>, which means <em>go to the child nodes from the current context</em>, followed by another node test, which selects the nodes named as <code>title</code>.

<blockquote>
So, the <strong>axis</strong> defines where in the tree the <strong>node test</strong> should be applied and the nodes that match the node test will be returned as a <strong>result</strong>.

</blockquote>
You can test nodes against their name or against their type.

Here are some examples of name tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>/html</td>
<td>Selects the node named <code>html</code>, which is under the root.</td>
</tr>
<tr>
<td>/html/head</td>
<td>Selects the node named <code>head</code>, which is under the <code>html</code> node.</td>
</tr>
<tr>
<td>//title</td>
<td>Selects all the <code>title</code> nodes from the HTML tree.</td>
</tr>
<tr>
<td>//h2/a</td>
<td>Selects all <code>a</code> nodes which are directly under an <code>h2</code> node.</td>
</tr>
</tbody>
</table>
And here are some examples of node type tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//comment()</td>
<td>Selects only comment nodes.</td>
</tr>
<tr>
<td>//node()</td>
<td>Selects any kind of node in the tree.</td>
</tr>
<tr>
<td>//text()</td>
<td>Selects only text nodes, such as "This is the first paragraph".</td>
</tr>
<tr>
<td>//*</td>
<td>Selects all nodes, except comment and text nodes.</td>
</tr>
</tbody>
</table>
We can also combine name and node tests in the same expression. 
For example:

//p/text()


This expression selects the text nodes from inside <code>p</code> elements. 
In the HTML snippet shown above, it would select "This is the first paragraph.".

Now, <strong>let’s see how we can further filter and specify things</strong>. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li&gt;Quote 1&lt;/li&gt;
      &lt;li&gt;Quote 2 with &lt;a href="..."&gt;link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Quote 3 with &lt;a href="..."&gt;another link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;h2&gt;Quote 4 title&lt;/h2&gt; ...&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the first <code>li</code> node from the snippet above. 
We can do this with:

//li[position() = 1]


The expression surrounded by square brackets is called a predicate and it filters the node set returned by <code>//li</code> (that is, all <code>li</code> nodes from the document) using the given condition. 
In this case it checks each node's position using the <code>position()</code> function, which returns the position of the current node in the resulting node set (notice that positions in XPath start at 1, not 0). 
We can abbreviate the expression above to:

//li[1]


Both XPath expressions above would select the following element:

    &lt;li class="quote"&gt;Quote 1&lt;/li&gt;


Check out a few more predicate examples:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//li[position()%2=0]</td>
<td>Selects the <code>li</code> elements at even positions.</td>
</tr>
<tr>
<td>//li[a]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element.</td>
</tr>
<tr>
<td>//li[a or h2]</td>
<td>Selects the <code>li</code> elements which enclose either an <code>a</code> or an <code>h2</code> element.</td>
</tr>
<tr>
<td>//li[ a [ text() = "link" ] ]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element whose text is "link". 
Can also be written as <code>//li[ a/text()="link" ]</code>.</td>
</tr>
<tr>
<td>//li[last()]</td>
<td>Selects the last <code>li</code> element in the document.</td>
</tr>
</tbody>
</table>
So, a location path is basically composed by steps, which are separated by <code>/</code> and each step can have an axis, a node test and a predicate. 
Here we have an expression composed by two steps, each one with axis, node test and predicate:

&lt;span style="font-weight: 400;"&gt;//li[ 4 ]/h2[ text() = "Quote 4 title" ]&lt;/span&gt;


And here is the same expression, written using the non-abbreviated syntax:

/descendant-or-self::node()<br>
    /child::li[ position() = 4 ]<br>
        /child::h2[ text() = "Quote 4 title" ]


We can also <strong>combine</strong> multiple XPath expressions in a single one using the union operator <code>|</code>. 
For example, we can select all <code>a</code> and <code>h2</code> elements in the document above using this expression:

//a | //h2


Now, consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li id="begin"&gt;&lt;a href="https://scrapy.org"&gt;Scrapy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://scrapinghub.com"&gt;Scrapinghub&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://blog.scrapinghub.com"&gt;Scrapinghub Blog&lt;/a&gt;&lt;/li&gt;
      &lt;li id="end"&gt;&lt;a href="http://quotes.toscrape.com"&gt;Quotes To Scrape&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the <code>a</code> elements whose link points to an HTTPS URL. 
We can do it by checking their <code>href</code> <strong>attribute</strong>:

//a[starts-with(@href, "https")]


This expression first selects all the <code>a</code> elements from the document and for each of those elements, it checks whether their <code>href</code> attribute starts with "https". 
We can access any node attribute using the <code>@attributename</code> syntax.

Here we have a few additional examples using attributes:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//a[@href="https://scrapy.org"]</td>
<td>Selects the <code>a</code> elements pointing to https://scrapy.org.</td>
</tr>
<tr>
<td>//a/@href</td>
<td>Selects the value of the <code>href</code> attribute from all the <code>a</code> elements in the document.</td>
</tr>
<tr>
<td>//li[@id]</td>
<td>Selects only the <code>li</code> elements which have an <code>id</code> attribute.</td>
</tr>
</tbody>
</table>
<h3>More on Axes</h3>
We've seen only two types of axes so far:

<li>descendant-or-self</li>
<li>child</li>
</ul>
But there's plenty more where they came from and we'll see a few examples. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;p&gt;Intro paragraph&lt;/p&gt;
    &lt;h1&gt;Title #1&lt;/h1&gt;
    &lt;p&gt;A random paragraph #1&lt;/p&gt;
    &lt;h1&gt;Title #2&lt;/h1&gt;
    &lt;p&gt;A random paragraph #2&lt;/p&gt;
    &lt;p&gt;Another one #2&lt;/p&gt;
    A single paragraph, with no markup
    &lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Now we want to extract only the first paragraph after each of the titles. 
To do that, we can use the <code>following-sibling</code> axis, which selects all the siblings after the context node. 
Siblings are nodes who are children of the same parent, for example all children nodes of the <code>body</code> tag are siblings. 
This is the expression:

//h1/following-sibling::p[1]


In this example, the context node where the <code>following-sibling</code> axis is applied to is each of the <code>h1</code> nodes from the page.

What if we want to select only the text that is right before the <code>footer</code>? We can use the <code>preceding-sibling</code> axis:

//div[@id='footer']/preceding-sibling::text()[1]


In this case, we are selecting the first text node before the <code>div</code> footer (<em>"A single paragraph, with no markup"</em>).

XPath also allows us to select elements based on their text content. 
We can use such a feature, along with the <code>parent</code> axis, to select the parent of the <code>p</code> element whose text is "Footer text":

//p[ text()="Footer text" ]/..


The expression above selects <code>&lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;</code>. 
As you may have noticed, we used <code>..</code> here as a shortcut to the <code>parent</code> axis.

As an alternative to the expression above, we could use:

//*[p/text()="Footer text"]


It selects, from all elements, the ones that have a <code>p</code> child which text is "Footer text", getting the same result as the previous expression.

You can find additional axes in the XPath specification: https://www.w3.org/TR/xpath/#axes

<h3>Wrap up</h3>
XPath is very powerful and this post is just an introduction to the basic concepts. 
If you want to learn more about it, check out these resources:

<li>http://zvon.org/comp/r/tut-XPath_1.html</li>
<li>http://fr.slideshare.net/scrapinghub/xpath-for-web-scraping</li>
<li>https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/</li>
</ul>
And <strong>stay tuned</strong>, because we will post a series with more XPath tips from the trenches in the following months.

<h2>to handle UTF</h2>
options("encoding" = "native.enc") # this is the natural environment
Sys.setlocale(category = 'LC_ALL', 'Chinese')	# to show chinese
# Sys.getlocale()
# options("encoding")

theNewsHeader = readLines("newsHeader.txt", encoding="UTF-8") # load UTF-8 file
options("encoding" = "UTF-8") # write UTF-8
sink("temp.html")

<h2><span class="gold embossts">R jsonlite to handle JSON</span></h2>
install.packages("jsonlite")
library(jsonlite)

# convert data frame to JSON array
my.json <- toJSON(mtcars)

# convert JSON array to data frame
my.df <- fromJSON(my.json)

# check data equality
all.equal(mtcars, my.df)
[1] TRUE

- set simplifyVector to FALSE, fromJSON will keep the raw JSON structure
ie, convert to list
fromJSON(json, simplifyVector = FALSE)

- fromJSON will convert multiple JSON structures to data frame
we may convert JSOn to data frame, and after fiddling, toJSON back to JSON.

- fromJSON will convert JSON matrix to R matrix

- higher order dimension JSON will be converted to R matrixs

<h2>Extract Components from Lists</h2>
Using [ ]
to extract a list components

Using [[ ]]
to extract only a single component

<h2>to view a list or dataframe</h2>
names(test), summary(test), head(test), tail(test), str(test)
typeof(test)

<h2>R function: cut</h2>
v <- c( 8, 13, 19, 3, 14, 7, 6, 12, 18, 9, 7, 14, 2, 3, 8, 11, 17)
c <- cut(v, c(0, 5, 10, 15, 20))
str(c)
 Factor w/ 4 levels "(0,5]","(5,10]",..: 2 3 4 1 3 2 2 3 4 2 ...

c # shows every element's category
#
#  [1] (5,10]  (10,15] (15,20] (0,5]   (10,15] (5,10]  (5,10]  (10,15] (15,20]
# [10] (5,10]  (5,10]  (10,15] (0,5]   (0,5]   (5,10]  (10,15] (15,20]

# Levels: (0,5] (5,10] (10,15] (15,20]

<h2>use cumsum() to create cumulative frequency graph</h2>

dataset = sample(1:20,100, replace= TRUE)
breaks = seq(0, 20, by=2) 
datasetCategory = cut(dataset, breaks, right=FALSE) 
dataset.freq = table(datasetCategory)

barplot(dataset.freq) # this show every category but not cumulative

We then compute its cumulative frequency with cumsum, add a starting zero element, and plot the graph.

cumfreq0 = c(0, cumsum(dataset.freq)) 
plot(breaks, cumfreq0,                 # plot the data 
 main="Old Faithful Eruptions",        # main title 
 xlab="dataset minutes",               # x−axis label 
 ylab="cumulative frequency graph")    # y−axis label 
lines(breaks, cumfreq0)                # join the points

<h2>to prevent scientific notation</h2>
Use a large positive value like 999 in options:
options(scipen=999)
to revert it back, the default scipen is 0

<h2>process daily data</h2>
# kline_dayqfq={"code":0,"msg":"","data":{"hk00700":{"qfqday":[["2020-01-14","410.000","400.400","413.000","396.600","26827634.000",{},"0.000","1086386.492"],

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/hkfqkline/get?_var=kline_dayqfq&param=hk00700,day,,,40,qfq"

my.json <- readLines(urlAddr, warn=F)
my.json = gsub("kline_dayqfq=","",my.json) # remove the leading command

my.dataframe = fromJSON(my.json)
my.dataframe = my.dataframe[[3]][[1]][[1]] # 40 obs., list of list
# chr "2020-01-14"   Date   1
# chr "410.000"      open   2
# chr "400.400"      close  3
# chr "413.000"      high   4
# chr "396.600"      low    5
# chr "26827634.000" Qty    6
# Named list()              7
# chr "0.000"               8
# chr "1086386.492"  Amt    9

my.dataframe[[1]][1]  # date
my.dataframe[[1]][3]  # close

for (i in 1:40){      # remove column 7
  my.dataframe[[i]] = my.dataframe[[i]][-(7:8)]
}

dataMatrix = matrix(unlist(my.dataframe), nrow=40, ncol=7)  # convert to matrix

<h2>process minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00981"
my.json <- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only list 3 is useful
my.dataframe = my.dataframe[[1]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec"
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, nearest day on top

datalist = unlist(my.list) # this is all strings in one vector


<h2>statistics of minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00388"
my.json <- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only list 3 is useful
my.dataframe = my.dataframe[[3]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec"
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, nearest day on top

datalist = unlist(my.list) # this is all strings in one vector

datalist = gsub("^.* ","",datalist) # this is the amount
datalist = round(as.numeric(datalist)/10000,0) # units in wan
datalist = sort(datalist)
datalist = datalist[-(1:20)]
datalist = datalist[-( (length(datalist)-20):length(datalist))] # remove the extremes

# max(datalist); min(datalist); length(datalist)

sections <- cut(datalist, breaks = 100)
table(sections)
barplot(table(sections))

cumulative sums
plot(cumsum(table(sections)))

<h2>R examples</h2>
https://www.datamentor.io/r-programming/examples/
http://www.rexamples.com
https://www.guru99.com/r-tutorial.html
https://r4stats.com/examples/programming/
https://www.statmethods.net/r-tutorial/index.html
http://rprogramming.net

<h2>output text to the R console in color</h2>
library(crayon)
cat(blue("Hello", "world!\n"))

Genaral styles
reset, bold
blurred (usually called ‘dim’, renamed to avoid name clash)
italic (not widely supported)
underline, inverse, hidden
strikethrough (not widely supported)

Text colors
black, red, green, yellow, blue, magenta, cyan, white
silver (usually called ‘gray’, renamed to avoid name clash)

Background colors
bgBlack, bgRed, bgGreen, bgYellow, bgBlue, bgMagenta, bgCyan, bgWhite

Styling
The styling functions take any number of character vectors as arguments, and they concatenate and style them:

Crayon defines the %+% string concatenation operator, to make it easy to assemble stings with different styles.

cat("... to highlight the " %+%
    red("search term") %+%
    " in a block of text\n")

Styles can be combined using the $ operator:
  cat(yellow$bgMagenta$bold('Hello world!\n'))
See also combine_styles().

Styles can also be nested, and then inner style takes precedence:
  cat(green(
    'I am a green line ' %+%
    blue$underline$bold('with a blue substring') %+%
    ' that becomes green again!\n'
  ))

define your own themes:
  error <- red $ bold
  warn <- magenta $ underline
  note <- cyan
  cat(error("Error: subscript out of bounds!\n"))
  cat(warn("Warning: shorter argument was recycled.\n"))
  cat(note("Note: no such directory.\n"))

See Also make_style() for using the 256 ANSI colors.

Examples
cat(blue("Hello", "world!"))
cat("... to highlight the " %+% red("search term") %+%
    " in a block of text")
cat(yellow$bgMagenta$bold('Hello world!'))
cat(green(
 'I am a green line ' %+%
 blue$underline$bold('with a blue substring') %+%
 ' that becomes green again!'
))
error <- red $ bold
warn <- magenta $ underline
note <- cyan
cat(error("Error: subscript out of bounds!\n"))
cat(warn("Warning: shorter argument was recycled.\n"))
cat(note("Note: no such directory.\n"))

<h3>style - Add Style To A String</h3>
Usage
style(string, as = NULL, bg = NULL)
cat(style("I am pink\n", "pink"))
cat(style("#4682B433\n", "#4682B433"))
cat(style("#002050\n", "#002050"))


<h3>rgb()</h3>
To use the function:
rgb(red, green, blue, alpha) : quantity of red (between 0 and 1), of green and of blue, and finally transparency (alpha).
newcolor = rgb(0.5, 0.2, 0.1, 0.8)
newcolor
"#80331ACC"

cat(style("newcolor\n", newcolor))  # note, without quotation marks


<h3>make_style</h3>
pink <- make_style("pink")
bgMaroon <- make_style(rgb(0.93, 0.19, 0.65), bg = TRUE)
cat(bgMaroon(pink("pink style.\n")))

## Create a new style for pink and maroon background
make_style(pink = "pink")
make_style(bgMaroon = rgb(0.0, 0.3, 0.3), bg = TRUE)
"pink" %in% names(styles())
"bgMaroon" %in% names(styles())

cat(style("I am pink, too!\n", "pink"))
cat(style("I am pink, too!\n", "pink", bg = "blue")) # color will change
cat(style("I am pink, too!\n", "pink", bg = "bgMaroon"))
cat(style("I am pink, too!\n", "pink", bg = "cyan"))

<h2>print strings with wordwraps</h2>

strwrap(astring, width = 110, indent = 5, exdent = 2))
use writeLines to print it
note: control characters inside string will be ignored.

astring = "Substituted with the text matched by the capturing group that can be found by counting as many opening parentheses of named or numbered capturing groups as specified by the number from right to left starting at the backreference."

writeLines(strwrap(astring, width = 110, indent = 5, exdent = 2))

<h2>R.utils withTimeout()</h2>

withTimeout() from package R.utils, in concert with tryCatch(), might provide a cleaner solution.

For example:
require(R.utils)

for(i in 1:5) {
    tryCatch(
        expr = {
            withTimeout({Sys.sleep(i); cat(i, "\n")}, 
                         timeout = 3.1)
            }, 
        TimeoutException = function(ex) cat("Timeout. Skipping.\n")
    )
}

# 1 
# 2 
# 3 
# Timeout. Skipping.
# Timeout. Skipping.

In the artificial example above:

The first argument to withTimeout() contains the code to be evaluated within each loop.

The timeout argument to withTimeout() sets the time limit in seconds.

The TimeoutException argument to tryCatch() takes a function that is to be executed when an iteration of the loop is timed out.

<h2>drawing SVG</h2>
<a href="https://cran.r-project.org/web/packages/RIdeogram/vignettes/RIdeogram.html" class="whitebut ">RIdeogram: drawing SVG graphics</a>

<a href="https://cran.r-project.org/web/packages/magick/vignettes/intro.html" class="whitebut ">Magick: Advanced Image-Processing</a>

<a href="http://ralanbutler.com/blog/2016/03/31/animated-SVG-R" class="whitebut ">Animating an SVG</a>

svglite + ggsave function
Saving a plot as an SVG

sample code:
require("ggplot2")

#some sample data
head(diamonds) 

#to see actually what will be plotted and compare 
qplot(clarity, data=diamonds, fill=cut, geom="bar")

#save the plot in a variable image to be able to export to svg
image=qplot(clarity, data=diamonds, fill=cut, geom="bar")

#This actually save the plot in a image
ggsave(file="test.svg", plot=image, width=10, height=8)

<h2>Package ‘TTR’</h2>
Technical Trading Rules
x=c(1,2,4,3,5,6,5,4,5,6,7,9,10,11,10)

Usage
SMA(x, n = 4)
EMA(x, n = 4)
DEMA(x, n = 4)
WMA(x, n = 4, wts = 1:n)
EVWMA(price, volume, n = 4)
ZLEMA(x, n = 4, ratio = NULL)
VWAP(price, volume, n = 4)
VMA(x, w, ratio = 1)
HMA(x, n = 20)
ALMA(x, n = 9, offset = 0.85, sigma = 6)

<h3>Weighted moving average WMA</h3>
<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Weighted_moving_average_weights_N%3D15.png/220px-Weighted_moving_average_weights_N%3D15.png">

<h3>Exponential moving average EMA</h3>
EMA is more exagerating
<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Exponential_moving_average_weights_N%3D15.png/220px-Exponential_moving_average_weights_N%3D15.png">

<h2>自然语言处理中的Transformer和BERT</h2>
2018年马上就要过去，回顾深度学习在今年的进展，让人印象最深刻的就是谷歌提出的应用于自然语言处理领域的BERT解决方案，BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805">
https://arxiv.org/abs/1810.04805</a>）。
BERT解决方案刷新了各大NLP任务的榜单，在各种NLP任务上都做到state of the art。
这里我把BERT说成是解决方案，而不是一个算法，因为这篇文章并没有提出新的算法模型，还是沿用了之前已有的算法模型。
BERT最大的创新点，在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的NLP任务，因此BERT这篇论文对于算法模型完全不做介绍，以至于在我直接看这篇文章的时候感觉云里雾里。
但是本文中，我会从算法模型到解决方案，进行完整的诠释。
本文中我会分3个部分进行介绍，第一部分我会大概介绍一下NLP的发展，第二部分主要讲BERT用到的算法，最后一部分讲BERT具体是怎么操作的。

<h3>一，NLP的发展</h3>
要处理NLP问题，首先要解决文本的表示问题。
虽然我们人去看文本，能够清楚明白文本中的符号表达什么含义，但是计算机只能做数学计算，需要将文本表示成计算机可以处理的形式。
最开始的方法是采用one hot，比如，我们假设英文中常用的单词有3万个，那么我们就用一个3万维的向量表示这个词，所有位置都置0，当我们想表示apple这个词时，就在对应位置设置1，如图1.1所示。
这种表示方式存在的问题就是，高维稀疏，高维是指有多少个词，就需要多少个维度的向量，稀疏是指，每个向量中大部分值都是0。
另外一个不足是这个向量没有任何含义。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-597b011ddd148eb53b5a90730b6090ae_b.jpg">

<figcaption>图1.1</figcaption>
后来出现了词向量，word embedding，用一个低维稠密的向量去表示一个词，如图1.2所示。
通常这个向量的维度在几百到上千之间，相比one hot几千几万的维度就低了很多。
词与词之间可以通过相似度或者距离来表示关系，相关的词向量相似度比较高，或者距离比较近，不相关的词向量相似度低，或者距离比较远，这样词向量本身就有了含义。
文本的表示问题就得到了解决。
词向量可以通过一些无监督的方法学习得到，比如CBOW或者Skip-Gram等，可以预先在语料库上训练出词向量，以供后续的使用。
顺便提一句，在图像中就不存在表示方法的困扰，因为图像本身就是数值矩阵，计算机可以直接处理。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-840859265e735cce77233bb42a4bee6a_b.png">

<figcaption>图1.2</figcaption>
NLP中有各种各样的任务，比如分类（Classification），问答（QA），实体命名识别（NER）等。
对于这些不同的任务，最早的做法是根据每类任务定制不同的模型，输入预训练好的embedding，然后利用特定任务的数据集对模型进行训练，如图1.3所示。
这里存在的问题就是，不是每个特定任务都有大量的标签数据可供训练，对于那些数据集非常小的任务，恐怕就难以得到一个理想的模型。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-4546b7aa51af50d3ac0c7504f965cc70_b.jpg">

<figcaption>图1.3</figcaption>
我们看一下图像领域是如何解决这个问题的。
图像分类是计算机视觉中最基本的任务，当我要解决一个小数据集的图像分类任务时，该怎么做？CV领域已经有了一套成熟的解决方案。
我会用一个通用的网络模型，比如Vgg，ResNet或者GoogleNet，在ImageNet上做预训练（pre-training）。
ImageNet有1400万张有标注的图片，包含1000个类别，这样的数据规模足以训练出一个规模庞大的模型。
在训练过程中，模型会不断的学习如何提取特征，底层的CNN网络结构会提取边缘，角，点等通用特征，模型越往上走，提取的特征也越抽象，与特定的任务更加相关。
当完成预训练之后，根据我自己的分类任务，调整最上层的网络结构，然后在小数据集里对模型进行训练。
在训练时，可以固定住底层的模型参数只训练顶层的参数，也可以对整个模型进行训练，这个过程叫做微调（fine-tuning），最终得到一个可用的模型。
总结一下，整个过程包括两步，拿一个通用模型在ImageNet上做预训练（pre-training），然后针对特定任务进行微调（fine-tuning），完美解决了特定任务数据不足的问题。
还有一个好处是，对于各种各样的任务都不再需要从头开始训练网络，可以直接拿预训练好的结果进行微调，既减少了训练计算量的负担，也减少了人工标注数据的负担。

NLP领域也引入了这种做法，用一个通用模型，在非常大的语料库上进行预训练，然后在特定任务上进行微调，BERT就是这套方案的集大成者。
BERT不是第一个，但目前为止，是效果最好的方案。
BERT用了一个已有的模型结构，提出了一整套的预训练方法和微调方法，我们在后文中再进行详细的描述。

<h3>二，算法</h3>
BERT所采用的算法来自于2017年12月份的这篇文章，Attenion Is All You Need（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762">
https://arxiv.org/abs/1706.03762</a>），同样来自于谷歌。
这篇文章要解决的是翻译问题，比如从中文翻译成英文。
这篇文章完全放弃了以往经常采用的RNN和CNN，提出了一种新的网络结构，即Transformer，其中包括encoder和decoder，我们只关注encoder。
这篇英文博客（<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">
https://jalammar.github.io/illustrated-transformer/</a>）对Transformer介绍得非常详细，有兴趣的读者可以看一下，如果不想看英文博客也可以看本文，本文中的部分图片也截取自这篇博客。

<img class="lazy" data-src="https://pic4.zhimg.com/v2-393d5284f5132c3150b294cfc5e5218f_b.jpg">

<figcaption>图2.1</figcaption>
图2.1是Transformer encoder的结构，后文中我们都简称为Transformer。
首先是输入word embedding，这里是直接输入一整句话的所有embedding。
如图2.1所示，假设我们的输入是Thinking Machines，每个词对应一个embedding，就有2个embedding。
输入embedding需要加上位置编码（Positional Encoding），为什么要加位置编码，后文会做详细介绍。
然后经过一个Multi-Head Attention结构，这个结构是算法单元中最重要的部分，我们会在后边详细介绍。
之后是做了一个shortcut的处理，就是把输入和输出按照对应位置加起来，如果了解残差网络（ResNet）的同学，会对这个结构比较熟悉，这个操作有利于加速训练。
然后经过一个归一化normalization的操作。
接着经过一个两层的全连接网络，最后同样是shortcut和normalization的操作。
可以看到，除了Multi-Head Attention，都是常规操作，没有什么难理解的。
这里需要注意的是，每个小模块的输入和输出向量，维度都是相等的，比如，Multi-Head Attention的输入和输出向量维度是相等的，否则无法进行shortcut的操作；Feed Forward的输入和输出向量维度也是相等的；最终的输出和输入向量维度也是相等的。
但是Multi-Head Attention和Feed Forward内部，向量维度会发生变化。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-4019f1ffead184e3bc00aabb41e6b6b6_b.jpg">

<figcaption>图2.2</figcaption>
我们来详细看一下Multi-Head Attention的结构。
这个Multi-Head表示多头的意思，先从最简单的看起，看看单头Attention是如何操作的。
从图2.1的橙色方块可以看到，embedding在进入到Attention之前，有3个分叉，那表示说从1个向量，变成了3个向量。
具体是怎么算的呢？我们看图2.3，定义一个WQ矩阵（这个矩阵随机初始化，通过训练得到），将embedding和WQ矩阵做乘法，得到查询向量q，假设输入embedding是512维，在图3中我们用4个小方格表示，输出的查询向量是64维，图3中用3个小方格以示不同。
然后类似地，定义WK和WV矩阵，将embedding和WK做矩阵乘法，得到键向量k；将embeding和WV做矩阵乘法，得到值向量v。
对每一个embedding做同样的操作，那么每个输入就得到了3个向量，查询向量，键向量和值向量。
需要注意的是，查询向量和键向量要有相同的维度，值向量的维度可以相同，也可以不同，但一般也是相同的。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-ac045486e0eff3b8a1eb27d2ae61a634_b.jpg">

<figcaption>图2.3</figcaption>
接下来我们计算每一个embedding的输出，以第一个词Thinking为例，参看图2.4。
用查询向量q1跟键向量k1和k2分别做点积，得到112和96两个数值。
这也是为什么前文提到查询向量和键向量的维度必须要一致，否则无法做点积。
然后除以常数8，得到14和12两个数值。
这个常数8是键向量的维度的开方，键向量和查询向量的维度都是64，开方后是8。
做这个尺度上的调整目的是为了易于训练。
然后把14和12丢到softmax函数中，得到一组加和为1的系数权重，算出来是大约是0.88和0.12。
将0.88和0.12对两个值向量v1和v2做加权求和，就得到了Thinking的输出向量z1。
类似的，可以算出Machines的输出z2。
如果一句话中包含更多的词，也是相同的计算方法。

<img class="lazy" data-src="https://pic2.zhimg.com/v2-b25bb6a8f9b57a4831b485015080b8c1_b.jpg">

<figcaption>图2.4</figcaption>
通过这样一系列的计算，可以看到，现在每个词的输出向量z都包含了其他词的信息，每个词都不再是孤立的了。
而且每个位置中，词与词的相关程度，可以通过softmax输出的权重进行分析。
如图2.5所示，这是某一次计算的权重，其中线条颜色的深浅反映了权重的大小，可以看到it中权重最大的两个词是The和animal，表示it跟这两个词关联最大。
这就是attention的含义，输出跟哪个词关联比较强，就放比较多的注意力在上面。
上面我们把每一步计算都拆开了看，实际计算的时候，可以通过矩阵来计算，如图2.6所示。

<img class="lazy" data-src="https://pic2.zhimg.com/v2-2dbdd8dfb5088d22c7dd9d05a0e1035d_b.jpg">

<figcaption>图2.5</figcaption>

<img class="lazy" data-src="https://pic4.zhimg.com/v2-a02ab6ab4cad1f8fef307ced0a4cf9d3_b.jpg" data-caption="">

<img class="lazy" data-src="https://pic2.zhimg.com/v2-d00785a9cfb835b5a345898e37b31be9_b.jpg">

<figcaption>图2.6</figcaption>
讲完了attention，再来讲Multi-Head。
对于同一组输入embedding，我们可以并行做若干组上面的操作，例如，我们可以进行8组这样的运算，每一组都有WQ，WK，WV矩阵，并且不同组的矩阵也不相同。
这样最终会计算出8组输出，我们把8组的输出连接起来，并且乘以矩阵WO做一次线性变换得到输出，WO也是随机初始化，通过训练得到，计算过程如图2.7所示。
这样的好处，一是多个组可以并行计算，二是不同的组可以捕获不同的子空间的信息。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-2afc28e06f5550d5e20a2dc290f2224e_b.jpg">

<figcaption>图2.7</figcaption>
到这里就把Transformer的结构讲完了，同样都是做NLP任务，我们来和RNN做个对比。
图2.8是个最基本的RNN结构，还有计算公式。
当计算隐向量h4时，用到了输入x4，和上一步算出来的隐向量h3，h3包含了前面所有节点的信息。
h4中包含最多的信息是当前的输入x4，越往前的输入，随着距离的增加，信息衰减得越多。
对于每一个输出隐向量h都是如此，包含信息最多得是当前的输入，随着距离拉远，包含前面输入的信息越来越少。
但是Transformer这个结构就不存在这个问题，不管当前词和其他词的空间距离有多远，包含其他词的信息不取决于距离，而是取决于两者的相关性，这是Transformer的第一个优势。
第二个优势在于，对于Transformer来说，在对当前词进行计算的时候，不仅可以用到前面的词，也可以用到后面的词。
而RNN只能用到前面的词，这并不是个严重的问题，因为这可以通过双向RNN来解决。
第三点，RNN是一个顺序的结构，必须要一步一步地计算，只有计算出h1，才能计算h2，再计算h3，隐向量无法同时并行计算，导致RNN的计算效率不高，这是RNN的固有结构所造成的，之前有一些工作就是在研究如何对RNN的计算并行化。
通过前文的介绍，可以看到Transformer不存在这个问题。
通过这里的比较，可以看到Transformer相对于RNN有巨大的优势，因此我看到有人说RNN以后会被取代。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-5bafe804c0dc77f945ade48561de63a0_b.jpg" data-caption="">

<img class="lazy" data-src="https://pic1.zhimg.com/v2-235854b916c55c54bbcad343443885c0_b.jpg">

<figcaption>图2.8</figcaption>
关于上面的第三点优势，可能有人会不认可，RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了。
为了解决时序的问题，Transformer的作者用了一个绝妙的办法，这就是我在前文提到的位置编码（Positional Encoding）。
位置编码是和word embedding同样维度的向量，将位置embedding和词embedding加在一起，作为输入embedding，如图2.9所示。
位置编码可以通过学习得到，也可以通过设置一个跟位置或者时序相关的函数得到，比如设置一个正弦或者余弦函数，这里不再多说。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-df4b9de6b9feb1971ab7225ebc4454d2_b.jpg">

<figcaption>图2.9</figcaption>
我们把图2.1的结构作为一个基本单元，把N个这样的基本单元顺序连起来，就是BERT的算法模型，如图2.10所示。
从前面的描述中可以看到，当输入有多少个embedding，那么输出也就有相同数量的embedding，可以采用和RNN采用相同的叫法，把输出叫做隐向量。
在做具体NLP任务的时候，只需要从中取对应的隐向量作为输出即可。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-81e63e36210c8e342d193be69c441e7c_b.jpg">

<figcaption>图2.10</figcaption>
<h3>三，BERT</h3>
在介绍BERT之前，我们先看看另外一套方案。
我在第一部分说过，BERT并不是第一个提出预训练加微调的方案，此前还有一套方案叫GPT，这也是BERT重点对比的方案，文章在这，Improving Language Understanding by Generative Pre-Training（<a href="https://link.zhihu.com/?target=https%3A//s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>）。
GPT的模型结构和BERT是相同的，都是图2.10的结构，只是BERT的模型规模更加庞大。
GPT是这么预训练的，在一个8亿单词的语料库上做训练，给出前文，不断地预测下一个单词。
比如这句话，Winter is coming，当给出第一个词Winter之后，预测下一个词is，之后再预测下一个词coming。
不需要标注数据，通过这种无监督训练的方式，得到一个预训练模型。

我们再来看看BERT有什么不同。
BERT来自于Bidirectional Encoder Representations from Transformers首字母缩写，这里提到了一个双向（Bidirectional）的概念。
BERT在一个33亿单词的语料库上做预训练，语料库就要比GPT大了几倍。
预训练包括了两个任务，第一个任务是随机地扣掉15%的单词，用一个掩码MASK代替，让模型去猜测这个单词；第二个任务是，每个训练样本是一个上下句，有50%的样本，下句和上句是真实的，另外50%的样本，下句和上句是无关的，模型需要判断两句的关系。
这两个任务各有一个loss，将这两个loss加起来作为总的loss进行优化。
下面两行是一个小栗子，用括号标注的是扣掉的词，用[MASK]来代替。

<b>正样本：我[MASK]（是）个算法工程师，我服务于WiFi万能钥匙这家[MASK]（公司）。
</b>
<b>负样本：我[MASK]（是）个算法工程师，今天[MASK]（股票）又跌了。
</b>
我们来对比下GPT和BERT两种预训练方式的优劣。
GPT在预测词的时候，只预测下一个词，因此只能用到上文的信息，无法利用到下文的信息。
而BERT是预测文中扣掉的词，可以充分利用到上下文的信息，这使得模型有更强的表达能力，这也是BERT中Bidirectional的含义。
在一些NLP任务中需要判断句子关系，比如判断两句话是否有相同的含义。
BERT有了第二个任务，就能够很好的捕捉句子之间的关系。
图3.1是BERT原文中对另外两种方法的预训练对比，包括GPT和ELMo。
ELMo采用的还是LSTM，这里我们不多讲ELMo。
这里会有读者困惑，这里的结构图怎么跟图2.10不一样？如果熟悉LSTM的同学，看到最右边的ELMo，就会知道那些水平相连的LSTM其实只是一个LSTM单元。
左边的BERT和GPT也是一样，水平方向的Trm表示的是同一个单元，图中那些复杂的连线表示的是词与词之间的依赖关系，BERT中的依赖关系既有前文又有后文，而GPT的依赖关系只有前文。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-287ba1129d213df7e2ed5adb7c4a440e_b.jpg">

<figcaption>图3.1</figcaption>
讲完了这两个任务，我们再来看看，如何表达这么复杂的一个训练样本，让计算机能够明白。
图3.2表示“my dog is cute, he likes playing.”的输入形式。
每个符号的输入由3部分构成，一个是词本身的embedding；第二个是表示上下句的embedding，如果是上句，就用A embedding，如果是下句，就用B embedding；最后，根据Transformer模型的特点，还要加上位置embedding，这里的位置embedding是通过学习的方式得到的，BERT设计一个样本最多支持512个位置；将3个embedding相加，作为输入。
需要注意的是，在每个句子的开头，需要加一个Classification（CLS）符号，后文中会进行介绍，其他的一些小细节就不说了。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-ec06762a57a7d7176747627dc3ee20b4_b.jpg">

<figcaption>图3.2</figcaption>
完成预训练之后，就要针对特定任务就行微调了，这里描述一下论文中的4个例子，看图3.4。
首先说下分类任务，分类任务包括对单句子的分类任务，比如判断电影评论是喜欢还是讨厌；多句子分类，比如判断两句话是否表示相同的含义。
图3.4（a）（b）是对这类任务的一个示例，左边表示两个句子的分类，右边是单句子分类。
在输出的隐向量中，取出CLS对应的向量C，加一层网络W，并丢给softmax进行分类，得到预测结果P，计算过程如图3.3中的计算公式。
在特定任务数据集中对Transformer模型的所有参数和网络W共同训练，直到收敛。
新增加的网络W是HxK维，H表示隐向量的维度，K表示分类数量，W的参数数量相比预训练模型的参数少得可怜。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-61486f520243716de645f904e3a36ac2_b.jpg">

<figcaption>图3.3</figcaption>

<img class="lazy" data-src="https://pic3.zhimg.com/v2-42514100ab16b207d2732729c85fccaa_b.jpg">

<figcaption>图3.4</figcaption>
我们再来看问答任务，如图3.4（c），以SQuAD v1.1为例，给出一个问题Question，并且给出一个段落Paragraph，然后从段落中标出答案的具体位置。
需要学习一个开始向量S，维度和输出隐向量维度相同，然后和所有的隐向量做点积，取值最大的词作为开始位置；另外再学一个结束向量E，做同样的运算，得到结束位置。
附加一个条件，结束位置一定要大于开始位置。
最后再看NER任务，实体命名识别，比如给出一句话，对每个词进行标注，判断属于人名，地名，机构名，还是其他。
如图3.4（d）所示，加一层分类网络，对每个输出隐向量都做一次判断。
可以看到，这些任务，都只需要新增少量的参数，然后在特定数据集上进行训练即可。
从实验结果来看，即便是很小的数据集，也能取得不错的效果。

<h2>Delete Files unlink("data.txt")</h2>
Delete Files and Directories. unlink deletes the file(s) or directories specified by x .

Usage. unlink(x, recursive = FALSE, force = FALSE)

<h2>scan</h2>
Read data into a vector or list from the console or file.

cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "ex.data", sep = "\n")
pp <- scan("ex.data", skip = 1, quiet = TRUE)
scan("ex.data", skip = 1)
scan("ex.data", skip = 1, nlines = 1) # only 1 line after the skipped one
scan("ex.data", what = list("","","")) # flush is F -> read "7"
scan("ex.data", what = list("","",""), flush = TRUE)
unlink("ex.data") # tidy up

## "inline" usage
scan(text = "1 2 3")

<h2>Copy an R data.frame to an Excel spreadsheet</h2>
write.excel <- function(x,row.names=FALSE,col.names=TRUE,...) {
  write.table(x,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}

write.excel(my.df)

and finally Ctr+V in Excel :)

<h2>copy a table x to the clipboard preserving the table structure</h2>
write.table(x, "clipboard", sep="\t")

write.table(x, "clipboard", sep="\t", row.names=FALSE)
write.table(x, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

s = c('aa','gb','rc')
n = c('af','rd','ac')
df = data.frame(n,s)

write.table(df, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

"af"	"aa"
"rd"	"gb"
"ac"	"rc"

<h2>read.table</h2>
reads a file into data frame in table format

x <- read.table("tp.txt",header=T,sep="\t");

<h3>copy a table from the clipboard</h3>
x <- read.table("clipboard",header=F,sep="\t");

<h2>distributed programming</h2>
reasons for distributed programming:

To speed up a process or piece of code
To scale up an interface or application for multiple users

<a href="http://spark.apache.org/docs/latest/sparkr.html" class="whitebut ">SparkR</a>: R on Apache Spark

SparkR provides an R frontend to Apache Spark and using Spark’s distributed computation engine allows us to run large scale data analysis from the R shell.

To get started you need to set up a Spark cluster. 
<a href="http://paxcel.net/blog/how-to-setup-apache-spark-standalone-cluster-on-multiple-machine/" class="whitebut ">SETUP APACHE SPARK STANDALONE CLUSTER ON MULTIPLE MACHINE</a>

The Spark documentation, without using Mesos or YARN as your cluster manager
<a href="http://spark.apache.org/docs/latest/spark-standalone.html" class="whitebut ">Spark Standalone Mode</a>

Once you have Spark set up, see <a href="https://rpubs.com/wendyu/sparkr" class="whitebut ">Wendy Yu's tutorial on SparkR</a>

She also shows how to integrate H20 with Spark which is referred to as 'Sparkling Water'.

R has been shipping with a base library parallel. 

In a nutshell, you can just do something like

mclapply(1:nCores, someFunction())
and the function someFunction() will be run in parallel over nCores. 
A default value of half your physical cores may be a good start.

<a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html" class="whitebut ">High Performance Computing</a>

<br>
<br>
<br>
<br>

<script type='text/javascript' src='readbook.js'></script>
<script>
  var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
  });
</script>
</pre>
</body>
</html>
