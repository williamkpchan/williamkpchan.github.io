<base target="_blank"><html><head><title>R Notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="../lazyload.min.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script src="D:/Dropbox/Public/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var bookid = "R Notes"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px; background-color:#044;}
</style></head><body onkeypress="chkKey()"><center>
<h1>R Notes</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<a href="https://cran.r-project.org/web/packages/" class="whitebut ">Available Packages</a>

<a href="https://www.listendata.com/2016/10/r-data-table.html" class="whitebut ">datatable</a>
<a href="https://www.listendata.com/2016/08/dplyr-tutorial.html" class="whitebut ">DPLYR</a>
<a href="https://www.listendata.com/" class="whitebut ">listendata</a>

<li>data visualization (ggplot2)</li>
<li>data manipulation (dplyr, lubridate, tidyr, stringr, readr, & forcats)</li>
<li>data analysis (combine ggplot2, dplyr to explore data and find insights)</li>
<a href="https://beckmw.wordpress.com/" class="whitebut ">R is my friend</a>

<pre>
<a href="R4DataScience.html" class="whitebut ">R4DataScience</a>

<a href="https://statisticsglobe.com/r-functions-list/" class="whitebut ">R Basic Commands of the</a>

<a href="Non-standard evaluation.html" class="whitebut ">Non-standard evaluation</a>
<a href="https://statisticsglobe.com" class="whitebut ">Statistics Globe</a>
<a href="http://www.datasciencemadesimple.com" class="whitebut ">DataScience Made Simple</a>

<a href="Libraries for Python & R.html">Libraries for Python & R</a>
<a href="sparklyr.html" class="redbut red blueblackgrad">sparklyr</a>

functions on non-tabular data
rlist is a set of tools for working with list objects.
<a href="RList Turorial.html" class="yellowbut gold purpleblackgrad">RList Turorial</a>
</pre>
<br>

</div>
<pre>
<br>
<br>
<br>
<br>
<a href="Libraries for Python & R.html">Libraries for Python & R</a>

<br>
<h2>free books for R</h2>
<a href="http://www.cookbook-r.com/" target="_blank">Cookbook for R</a>
<a href="RCookbook.html" target="_blank" class="orangesha">&diams;RCookbook</a>
<a href="https://bookdown.org" class="whitebut " target="_blank">bookdown R books</a>
<a href="https://bookdown.org/home/archive/" class="whitebut " target="_blank">bookdown all books</a>
<a href="https://bookdown.org/home/tags/r-programming/" class="whitebut " target="_blank">bookdown r-programming books</a>
<a href="https://bookdown.org/rdpeng/rprogdatascience/" class="whitebut " target="_blank">R Programming for Data Science</a>

<h2>朱松纯教授浅谈人工智能：现状、任务、构架与统一</h2>
朱松纯，加州大学洛杉矶分校UCLA统计学和计算机科学教授，视觉、认知、学习与自主机器人中心主任。
文章前四节浅显探讨什么是人工智能和当前所处的历史时期，后面六节分别探讨六个学科的重点研究问题和难点，有什么样的前沿的课题等待年轻人去探索，最后一节讨论人工智能是否以及如何成为一门成熟的科学体系。

<h3><b>目录 </b> </h3>引言
第一节 现状：正视现实
第二节 未来：一只乌鸦给我们的启示
第三节 历史：从“春秋五霸”到“战国六雄”
第四节 统一：“小数据、大任务”范式与认知构架
第五节 学科一：计算视觉 --- 从“深”到“暗”
第六节 学科二：认知推理 --- 走进内心世界
第七节 学科三：语言通讯 --- 沟通的认知基础
第八节 学科四：博弈伦理 --- 获取、共享人类的价值观
第九节 学科五：机器人学 --- 构建大任务平台
第十节 学科六：机器学习 --- 学习的终极极限与“停机问题”
第十一节 总结：智能科学 --- 牛顿与达尔文的统一  附录 中科院自动化所报告会上的问答与互动摘录 鸣谢
<h3><b>引言</b> </h3>到底什么是人工智能？
现在的研究处于什么阶段？
今后如何发展？
这是大家普遍关注的问题。
由于人工智能涵盖的学科和技术面非常广，要在短时间内全面认识、理解人工智能，别说非专业人士，就算对本行业研究人员，也是十分困难的任务。
所以，现在很多宣传与决策冲到认识之前了，由此不可避免地造成一些思想和舆论的混乱。
全面认识人工智能之所以困难，是有客观原因的。

<b>其一、人工智能是一个非常广泛的领域。</b>
当前人工智能涵盖很多大的学科，我把它们归纳为六个： （1）计算机视觉（暂且把模式识别，图像处理等问题归入其中）、 （2）自然语言理解与交流（暂且把语音识别、合成归入其中，包括对话）、 （3）认知与推理（包含各种物理和社会常识）、 （4）机器人学（机械、控制、设计、运动规划、任务规划等）、 （5）博弈与伦理（多代理人agents的交互、对抗与合作，机器人与社会融合等议题）、 （6）机器学习（各种统计的建模、分析工具和计算的方法）， 这些领域目前还比较散，目前它们正在交叉发展，走向统一的过程中。
我把它们通俗称作“战国六雄”，中国历史本来是“战国七雄”，我这里为了省事，把两个小一点的领域：博弈与伦理合并了，伦理本身就是博弈的种种平衡态。
最终目标是希望形成一个完整的科学体系，从目前闹哄哄的工程实践变成一门真正的科学Science of Intelligence。
各个领域的研究人员看人工智能，如果按照印度人的谚语可以叫做“盲人摸象”，但这显然是言语冒犯了，还是中国的文豪苏轼游庐山时说得有水准： “横看成岭侧成峰，远近高低各不同。
不识庐山真面目，只缘身在此山中。”

<b>其二，人工智能发展的断代现象。</b>
由于历史发展的原因，人工智能自1980年代以来，被分化出以上几大学科，相互独立发展，而且这些学科基本抛弃了之前30年以逻辑推理与启发式搜索为主的研究方法，取而代之的是概率统计（建模、学习）的方法。
留在传统人工智能领域（逻辑推理、搜索博弈、专家系统等）而没有分流到以上分支学科的老一辈中，的确是有很多全局视野的，但多数已经过世或退休了。
这种领域的<b>分化</b>与历史的<b>断代</b>， 客观上造成了目前的学界和产业界思路和观点相当“混乱”的局面，媒体上的混乱就更放大了。
但是，以积极的态度来看，这个局面确实为现在的年轻一代研究人员、研究生提供了一个很好的建功立业的机会和广阔的舞台。
我写这篇文章的动机在于三点： （1）为在读的研究生们、为有志进入人工智能研究领域的年轻学者开阔视野。
（2）为那些对人工智能感兴趣、喜欢思考的人们，做一个前沿的、综述性的介绍。
（3）为公众与媒体从业人员，做一个人工智能科普，澄清一些事实。
诚如屈子所言：“路漫漫其修远兮，吾将上下而求索”。

<h3><b>第一节 现状评估：正视现实 </b> </h3>人工智能的研究，简单来说，就是要通过智能的机器，延伸和增强（augment）人类在改造自然、治理社会的各项任务中的能力和效率，最终实现一个人与机器和谐共生共存的社会。
抛开科幻的空想，谈几个近期具体的应用。
无人驾驶大家听了很多，先说说军用。
军队里的一个班或者行动组，现在比如要七个人，将来可以减到五个人，另外两个用机器来替换。
其次，机器人可以用在救灾和一些危险的场景，如核泄露现场，人不能进去，必须靠机器人。
医用的例子很多：智能的假肢或外骨架（exoskeleton）与人脑和身体信号对接，增强人的行动控制能力，帮助残疾人更好生活。
此外，还有就是家庭养老等服务机器人等。



<img src="https://picb.zhimg.com/v2-1966c66ffd36461e13fc02d24ee0f09a_b.jpg">


但是，这方面的进展很不尽人意。
以前日本常常炫耀他们机器人能跳舞，中国有一次春节晚会也拿来表演了。
那都是事先编写的程序，结果一个福岛核辐射事故一下子把所有问题都暴露了，发现他们的机器人一点招都没有。
美国也派了机器人过去，同样出了很多问题。
比如一个简单的技术问题，机器人进到灾难现场，背后拖一根长长的电缆，要供电和传数据，结果电缆就被缠住了，动弹不得。
看到这里，有人要问了，教授说得不对，我们明明在网上看到美国机器人让人叹为观止的表现。
比如，这一家波士顿动力学公司（Boston Dynamics）的演示，它们的机器人，怎么踢都踢不倒呢，或者踢倒了可以自己爬起来，而且在野外丛林箭步如飞呢，还有几个负重的电驴、大狗也很酷。
这家公司本来是由美国国防部支持开发出机器人来的，被谷歌收购之后、就不再承接国防项目。
可是，谷歌发现除了烧钱，目前还找不到商业出路，最近一直待售之中。
您会问，那谷歌不是很牛吗？
DeepMind下围棋不是也一次次刺激中国人的神经吗？
有一个逆天的机器人身体、一个逆天的机器人大脑，它们都在同一个公司内部，那为什么没有做出一个人工智能的产品呢？
他们何尝不在夜以继日的奋战之中啊。



<img src="https://pic4.zhimg.com/v2-bbc33bca0e465a2b240faac4ec4224f0_b.jpg">


人工智能炒作了这么长时间，您看看周围环境，您看到机器人走到大街上了？
没有。
您看到人工智能进入家庭了吗？
其实还没有。
您可能唯一直接领教过的是基于大数据和深度学习训练出来的聊天机器人，你可能跟Ta聊过。
用我老家湖北人的话，这就叫做“扯白”--- 东扯西拉、说白话。
如果你没有被Ta气得背过气的话，要么您真的是闲得慌，要么是您真的有耐性。



<img src="https://picb.zhimg.com/v2-885607d0e641a296558a8b7ee1c1d99c_b.jpg">


为了测试技术现状，美国国防部高级研究署2015年在洛杉矶郊区Pomona做了一个DARPA Robot Challenge（DRC），悬赏了两百万美金奖给竞赛的第一名。
有很多队伍参加了这个竞赛，上图是韩国科技大学队赢了第一名，右边是他们的机器人在现场开门进去“救灾”。
后来发现内情，原来机器人所有的动作基本上是人在遥控的。
每一步、每一个场景分别有一个界面，每个学生控制一个模块。
感知、认知、动作都是人在指挥。
就是说这个机器人其实并没有自己的感知、认知、思维推理、规划的能力。
这还是一个简单的场景。
其一、整个场景都是事先设定的，各个团队也都反复操练过的。
如果是没有遇见的场景，需要灵机决断呢？
其二、整个场景还没有人出现，如果有其他人出现，需要社会活动（如语言交流、分工协作）的话，那复杂度就又要上两个数量级了。



<img src="https://picb.zhimg.com/v2-bc04bdbb633ce6486570bb2dba4a002c_b.jpg">


其实，要是完全由人手动控制，现在的机器人都可以做手术了，而且手术机器人已经在普及之中。
上图是我实验室与一家公司合作的项目，机器人可以开拉链、检查包裹、用钳子撤除炸弹等，都是可以实现的。
小结一下，现在的人工智能和机器人，关键问题是缺乏物理的常识和社会的常识“Common sense”。
这是人工智能研究最大的障碍。
那么什么是常识？
常识就是我们在这个世界和社会生存的最基本的知识：（1）它使用频率最高；
（2）它可以举一反三，推导出并且帮助获取其它知识。
这是解决人工智能研究的一个核心课题。
我自2010年来，一直在带领一个跨学科团队，攻关视觉常识的获取与推理问题。
我在自动化所做了另外一个关于视觉常识报告，也被转录成中文了，不久会发表出来。
那么是不是说，我们离真正的人工智能还很遥远呢？
其实也不然。
关键是研究的思路要找对问题和方向。
自然界已经为我们提供了很好的案例。
下面，我就来看一下，自然界给我们展示的解答。

<h3><b>第二节 未来目标： 一只乌鸦给我们的启示</b> </h3>同属自然界的鸟类，我们对比一下体型大小都差不多的乌鸦和鹦鹉。
鹦鹉有很强的语言模仿能力，你说一个短句，多说几遍，它能重复，这就类似于当前的由数据驱动的聊天机器人。
二者都可以说话，但鹦鹉和聊天机器人都不明白说话的语境和语义，也就是它们不能把说的话对应到物理世界和社会的物体、场景、人物，不符合因果与逻辑。
可是，乌鸦就远比鹦鹉聪明，它们能够制造工具，懂得各种物理的常识和人的活动的社会常识。
下面，我就介绍一只乌鸦，它生活在复杂的城市环境中，与人类交互和共存。
YouTube网上有不少这方面的视频，大家可以找来看看。
我个人认为，人工智能研究该搞一个“乌鸦图腾”， 因为我们必须认真向它们学习。



<img src="https://pic4.zhimg.com/v2-e78af10acf33ac456f0e4e740bfcb582_b.jpg">


上图a是一只乌鸦，被研究人员在日本发现和跟踪拍摄的。
乌鸦是野生的，也就是说，没人管，没人教。
它必须靠自己的观察、感知、认知、学习、推理、执行，完全自主生活。
假如把它看成机器人的话，它就在我们现实生活中活下来。
如果这是一个自主的流浪汉进城了，他要在城里活下去，包括与城管周旋。
首先，乌鸦面临一个任务，就是寻找食物。
它找到了坚果（至于如何发现坚果里面有果肉，那是另外一个例子了），需要砸碎，可是这个任务超出它的物理动作的能力。
其它动物，如大猩猩会使用工具，找几块石头，一块大的垫在底下，一块中等的拿在手上来砸。
乌鸦怎么试都不行，它把坚果从天上往下抛，发现解决不了这个任务。
在这个过程中，它就发现一个诀窍，把果子放到路上让车轧过去（图b），这就是“鸟机交互”了。
后来进一步发现，虽然坚果被轧碎了，但它到路中间去吃是一件很危险的事。
因为在一个车水马龙的路面上，随时它就牺牲了。
我这里要强调一点，这个过程是没有大数据训练的，也没有所谓监督学习，乌鸦的生命没有第二次机会。
这是与当前很多机器学习，特别是深度学习完全不同的机制。
然后，它又开始观察了，见图c。
它发现在靠近红绿路灯的路口，车子和人有时候停下了。
这时，它必须进一步领悟出红绿灯、斑马线、行人指示灯、车子停、人流停这之间复杂的因果链。
甚至，哪个灯在哪个方向管用、对什么对象管用。
搞清楚之后，乌鸦就选择了一根正好在斑马线上方的一根电线，蹲下来了（图d）。
这里我要强调另一点，也许它观察和学习的是别的地点，那个点没有这些蹲点的条件。
它必须相信，同样的因果关系，可以搬到当前的地点来用。
这一点，当前很多机器学习方法是做不到的。
比如，一些增强学习方法，让机器人抓取一些固定物体，如积木玩具，换一换位置都不行；
打游戏的人工智能算法，换一换画面，又得重新开始学习。
它把坚果抛到斑马线上，等车子轧过去，然后等到行人灯亮了（图e）。
这个时候，车子都停在斑马线外面，它终于可以从容不迫地走过去，吃到了地上的果肉。
你说这个乌鸦有多聪明，这是我期望的真正的智能。
这个乌鸦给我们的启示，至少有三点：  其一、它是一个完全自主的智能。
感知、认知、推理、学习、和执行， 它都有。
我们前面说的， 世界上一批顶级的科学家都解决不了的问题，乌鸦向我们证明了，这个解存在。
其二、你说它有大数据学习吗？
这个乌鸦有几百万人工标注好的训练数据给它学习吗？
没有，它自己把这个事通过少量数据想清楚了，没人教它。
其三、乌鸦头有多大？
不到人脑的1%大小。
人脑功耗大约是10-25瓦，它就只有0.1-0.2瓦，就实现功能了，根本不需要前面谈到的核动力发电。
这给硬件芯片设计者也提出了挑战和思路。
十几年前我到中科院计算所讲座， 就说要做视觉芯片VPU，应该比后来的GPU更超前。
我最近参与了一个计算机体系结构的大项目，也有这个目标。
讲通俗一点，我们要寻找“乌鸦”模式的智能，而不要“鹦鹉”模式的智能。
当然，我们必须也要看到，“鹦鹉”模式的智能在商业上，针对某些垂直应用或许有效。

<h3><b>第三节 历史时期：从“春秋五霸”到“战国六雄”</b></h3>要搞清楚人工智能的发展趋势，首先得回顾历史。
读不懂历史，无法预测未来。
这一节，我就结合自己的经历谈一下我的观点，不见得准确和全面。



<img src="https://pic3.zhimg.com/v2-72a8b89160f0edab30f476e2e6137638_b.jpg">


</figure><b>首先，从表面一层来看。</b>
反映在一些产业新闻和社会新闻层面上，人工智能经过了几起几落，英文叫做Boom and Bust，意思是一哄而上、一哄而散，很形象。
每次兴盛期都有不同的技术在里面起作用。
1980年代初又兴起了第二次热潮，一批吹牛的教授、研究人员登场了。
做专家系统、知识工程、医疗诊断等，中国当时也有人想做中医等系统。
虽然这次其中也有学者拿了图灵奖，但这些研究没有很好的理论根基。
1986年我上了中国科大计算机系，我对计算机专业本身不是最感兴趣，觉得那就是一个工具和技能，而人工智能方向水很深，值得长期探索，所以我很早就去选修了人工智能的研究生课程，是由自动化系一个到美国进修的老师回来开的课。
第三次热潮就是最近两年兴起的深度学习推动的。
有了以前的教训，一开始学者们都很谨慎，出来警告说我们做的是特定任务，不是通用人工智能，大家不要炒作。
但是，拦不住了。
公司要做宣传，然后，大家开始加码宣传。
这就像踩踏事件，处在前面的人是清醒的，他们叫停，可是后面大量闻信赶来的人不知情，拼命往里面挤。
人工智能的确是太重要了，谁都不想误了这趟车。
也有人认为这次是真的，不会再有冬天了。
冬天不冬天，那就要看我们现在怎么做了。
所以说，从我读大学开始，人工智能这个名词从公众视线就消失了近30年。
我现在回头看，其实它当时并没有消失，而是分化了。
研究人员分别聚集到五个大的领域或者叫做学科：计算机视觉、自然语言理解、认知科学、机器学习、机器人学。
这些领域形成了自己的学术圈子、国际会议、国际期刊，各搞各的，独立发展。
人工智能里面还有一些做博弈下棋、常识推理，还留在里面继续搞，但人数不多。
我把这30年叫做一个“分治时期”，相当于中国历史的“春秋时期”。
春秋五霸就相当于这分出去的五个学科，大家各自发展壮大。

<b>其次、从深一层的理论基础看。</b>
我把人工智能发展的60年分为两个阶段。
第一阶段：前30年以数理逻辑的表达与推理为主。
这里面有一些杰出的代表人物，如John McCarthy、Marvin Minsky、Herbert Simmon。
他们懂很多认知科学的东西，有很强的全局观念。
这些都是我读大学的时候仰慕的人物，他们拿过图灵奖和其它一堆大奖。
但是，他们的工具基本都是基于数理逻辑和推理。
这个逻辑表达的“体制”，就相当于中国的周朝，周文王建立了一个相对松散的诸侯部落体制，后来指挥不灵，就瓦解了，进入一个春秋五霸时期。
而人工智能正好也分出了五大领域。
第二阶段：后30年以概率统计的建模、学习和计算为主。
在10余年的发展之后，“春秋五霸”在1990年中期都开始找到了概率统计这个新“体制”：统计建模、机器学习、随机计算算法等。
在这个体制的转型过程中，起到核心作用的有这么几个人。
讲得通俗一点，他们属于先知先觉者，提前看到了人工智能的发展趋势，押对了方向（就相当于80年代买了微软、英特尔股票；
90年代末，押对了中国房地产的那一批人）。
他们没有进入中国媒体的宣传视野。
我简要介绍一下，从中我们也可以学习到一些治学之道。



<img src="https://pic2.zhimg.com/v2-e0d506c5ca879cfce4c183045ff25b85_b.jpg">


第一个人叫Ulf Grenander。
他从60年代就开始做随机过程和概率模型，是最早的先驱。
60年代属于百家争鸣的时期，当别的领军人物都在谈逻辑、神经网络的时候，他开始做概率模型和计算，建立了广义模式理论，试图给自然界各种模式建立一套统一的数理模型。
第二个人是Judea Pearl。
他是我在UCLA的同事，原来是做启发式搜索算法的。
80年代提出贝叶斯网络把概率知识表达于认知推理，并估计推理的不确定性。
到90年代末，他进一步研究因果推理，这又一次领先于时代。
2011年因为这些贡献他拿了图灵奖。
他是一个知识渊博、思维活跃的人，不断有原创思想。
第三个人是Leslei Valiant。
他因离散数学、计算机算法、分布式体系结构方面的大量贡献，2010年拿了图灵奖。
1984年，他发表了一篇文章，开创了computational learning theory。
他问了两个很简单、但是深刻的问题。
第一个问题：你到底要多少例子、数据才能近似地、以某种置信度学到某个概念，就是PAClearning；
第二个问题：如果两个弱分类器综合在一起，能否提高性能？
如果能，那么不断加弱分类器，就可以收敛到强分类器。
第四个人是David Mumford。
我把他放在这里，有点私心，因为他是我博士导师。
他说他60年代初本来对人工智能感兴趣。
因为他数学能力特别强，上代数几何课程的时候就发现能够证明大定理了，结果一路不可收拾，拿了菲尔茨奖。
但是，到了80年代中期，他不忘初心，还是决定转回到人工智能方向来，从计算机视觉和计算神经科学入手。
这个时期，还有一个重要的人物是做神经网络和深度学习的多伦多大学教授Hinton。
我上大学的时候，80年代后期那一次神经网络热潮，他就出名了。
他很有思想，也很坚持，是个学者型的人物。
所不同的是，他下面的团队有点像摇滚歌手，能凭着一首通俗歌曲（代码），迅速红遍大江南北。
所以，我跟那些计算机视觉的研究生和年轻人说，你们不要单纯在视觉这里做，你赶紧出去“抢地盘”，单独做视觉，已经没有多少新东西可做的了，性能调不过公司的人是一方面；
更麻烦的是，别的领域的人打进来，把你的地盘给占了。
这是必然发生的事情，现在正在发生的事情。
我的判断是，我们刚刚进入一个“战国时期”，以后就要把这些领域统一起来。
首先我们必须深入理解计算机视觉、自然语言、机器人等领域，这里面有很丰富的内容和语意。
如果您不懂这些问题domain的内涵，仅仅是做机器学习就称作人工智能专家，恐怕说不过去。
我们正在进入这么一个大集成的、大变革的时代，有很多机会让我们去探索前沿，不要辜负了这个时代。
这是我演讲的第一个部分：人工智能的历史、现状，发展的大趋势。
下面，进入我今天演讲的第二个主题：<b>用一个什么样的构架把这些领域和问题统一起来。</b>
我不敢说我有答案，只是给大家提出一些问题、例子和思路，供大家思考。
不要指望我给你提供代码，下载回去，调调参数就能发文章。

<h3><b>第四节 人工智能研究的认知构架：小数据、大任务范式</b> </h3>智能是一种现象，表现在个体和社会群体的行为过程中。
回到前面乌鸦的例子，我认为智能系统的根源可以追溯到两个基本前提条件： <b>一、物理环境客观的现实与因果链条。</b>
这是外部物理环境给乌鸦提供的、生活的边界条件。
在不同的环境条件下，智能的形式会是不一样的。
任何智能的机器必须理解物理世界及其因果链条，适应这个世界。
<b>二、智能物种与生俱来的任务与价值链条。</b>
这个任务是一个生物进化的“刚需”。
如个体的生存，要解决吃饭和安全问题，而物种的传承需要交配和社会活动。
这些基本任务会衍生出大量的其它的“任务”。
动物的行为都是被各种任务驱动的。
有了这个先天的基本条件（设计）后，下一个重要问题：是什么驱动了模型在空间中的运动，也就是学习的过程？
还是两点：  <b>一、 外来的数据。</b>
外部世界通过各种感知信号，传递到人脑，塑造我们的模型。
数据来源于观察（observation）和实践（experimentation）。
观察的数据一般用于学习各种统计模型，这种模型就是某种时间和空间的联合分布，也就是统计的关联与相关性。
实践的数据用于学习各种因果模型，将行为与结果联系在一起。
因果与统计相关是不同的概念。
<b>二、内在的任务。</b>
这就是由内在的价值函数驱动的行为、以期达到某种目的。
我们的价值函数是在生物进化过程中形成的。
因为任务的不同，我们往往对环境中有些变量非常敏感，而对其它一些变量不关心。
由此，形成不同的模型。
机器人的脑、人脑都可以看成一个模型。
任何一个模型由数据与任务来共同塑造。
现在，我们就来到一个很关键的地方。
同样是在概率统计的框架下，当前的很多深度学习方法，属于一个被我称作“<b>大数据、小任务范式</b>（big data for small task）”。
针对某个特定的任务，如人脸识别和物体识别，设计一个简单的价值函数Loss function，用大量数据训练特定的模型。
这种方法在某些问题上也很有效。

<h3><b>第五节 计算机视觉：从“深”到“暗” Dark, Beyond Deep</b> </h3>视觉是人脑最主要的信息来源，也是进入人工智能这个殿堂的大门。
我自己的研究也正是从这里入手的。
这一节以一个具体例子来介绍视觉里面的问题。
当然，很多问题远远没有被解决。



<img src="https://pic3.zhimg.com/v2-e296919138433f539992fcaa9c5e0fbc_b.jpg">


这是我家厨房的一个视角。
多年前的一个下午，我女儿放学回家，我正在写一个大的项目申请书，就拍了这一张作为例子。
图像就是一个像素的二维矩阵，可是我们感知到非常丰富的三维场景、行为的信息；
你看的时间越长，理解的也越多。
下面我列举几个被主流（指大多数研究人员）忽视的、但是很关键的研究问题。
<b>一、几何常识推理与三维场景构建。</b>
以前计算机视觉的研究，需要通过多张图像（多视角）之间特征点的对应关系，去计算这些点在三维世界坐标系的位置（SfM、SLAM）。
见下图所示，在这个三维场景中，我们的理解就可以表达成为一个层次分解（compositional）的时空因果的解译图（Spatial，Temporal and Causal Parse Graph）,简称 STC-PG。
STC-PG是一个极其重要的概念，我下面会逐步介绍。
几何重建的一个很重要的背景是，我们往往不需要追求十分精确的深度位置。
比如，人对三维的感知其实都是非常不准的，它的精确度取决于你当前要执行的任务。
在执行的过程中，你不断地根据需要来提高精度。
比如，你要去拿几米以外的一个杯子，一开始你对杯子的方位只是一个大致的估计，在你走近、伸手的过程中逐步调整精度。



<img src="https://pic1.zhimg.com/v2-e7cc2ef62238dc20c704506ccc7c351e_b.jpg">


</figure><b>二、场景识别的本质是功能推理。</b>
现在很多学者做场景的分类和分割都是用一些图像特征，用大量的图片例子和手工标注的结果去训练神经网络模型 --- 这是典型的“鹦鹉”模式。
而一个场景的定义本质上就是功能。
当你看到一个三维空间之后，人脑很快就可以想象我可以干什么：这个地方倒水，这里可以拿杯子，这里可以坐着看电视等。
现代的设计往往是复合的空间，就是一个房间可以多种功能，所以简单去分类已经不合适了。



<img src="https://pic1.zhimg.com/v2-641a654b55205b12a525a3a4064dd883_b.jpg">


有了这个理解，我们就知道：下面两张图，虽然图像特征完全不同，但是他们是同一类场景,功能上是等价的。
人的活动和行为，不管你是哪个国家、哪个历史时期，基本是不变的。
这是智能泛化的基础，也就是把你放到一个新的地区，你不需要大数据训练，马上就能理解、适应。
这是我们能够举一反三的一个基础。



<img src="https://pic4.zhimg.com/v2-882eeb9a3c6d7eb50ae0599e13602f56_b.jpg">


回到前面的那个STC-PG解译图，每个场景底下其实就分解成为一些动作和功能 （见STC-PG图中的绿色方片节点）。
由计算机想象、推理的各种功能决定对场景的分类。
想象功能就是把人的各种姿态放到三维场景中去拟合（见厨房解译图中人体线画）。
这是完全不同于当前的深度学习方法用的分类方法。
<b>三、物理稳定性与关系的推理。</b>
我们的生活空间除了满足人类的各种需求（功能、任务）之外， 另一个基本约束就是物理。
我们对图像的解释和理解被表达成为一个解译图，这个解译图必须满足物理规律，否则就是错误的。
我们对图像的理解包含了物体之间的物理关系，每个物体的支撑点在那里。
比如，下面这个图，吊灯和墙上挂的东西，如果没有支撑点，就会掉下来（右图）。
这个研究方向，MIT认知科学系的Josh Tenenbuam教授与我都做了多年。



<img src="https://pic2.zhimg.com/v2-1411e70aecad7d840086537018aac360_b.jpg">


我提出了一个新的场景理解的minimax标准：minimize instability and maximize functionality最小化不稳定性且最大化功能性。
这比以前我们做图像理解的用的MDL（最小描述长度）标准要更靠谱。
这是解决计算机视觉的基本原理，功能和物理是设计场景的基本原则。
几何尺寸是附属于功能推出来的，比如椅子的高度就是因为你要坐得舒服，所以就是你小腿的长度。
<b>四、意向、注意和预测。</b>
厨房那张图有一个人和一只狗，我们可以进一步识别其动作、眼睛注视的地方，由此推导其动机和意向。
这样我们可以计算她在干什么、想干什么，比如说她现在是渴了，还是累了。
通过时间累积之后，进而知道她知道哪些，也就是她看到了或者没有看到什么。
在时间上做预测，她下面想干什么。
只有把这些都计算出来了，机器才能更好地与人进行交互。
下面的这一张图，是多摄像机的一个综合场景的解译实例。
这是我的实验室做出来的一个视觉系统。
这个视频的理解就输出为一个大的综合的STC-PG。
在此基础上，就可以输出文字的描述（I2T）和回答提问 QA。
我们把它叫做视觉图灵测试，网址：<a href="https://link.zhihu.com/?target=http%3A//visualturingtest.com" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">visualturingtest.com</span><span class="invisible"></span></a>。



<img src="https://picb.zhimg.com/v2-88ddd9bbfb6262fa4e2df0ac98fcd383_b.jpg">


与第一节讲的机器人竞赛类似，这也是一个DARPA项目。
测试就是用大量视频，我们算出场景和人的三维的模型、动作、属性、关系等等，然后就来回答各种各样的1000多个问题。
现在一帮计算机视觉的人研究VQA（视觉问答），就是拿大量的图像和文本一起训练，这是典型的“鹦鹉”系统，基本都是“扯白”。
<b>五、任务驱动的因果推理与学习。</b>
前面我谈了场景的理解的例子，下面我谈一下物体的识别和理解，以及为什么我们不需要大数据的学习模式，而是靠举一反三的能力。
我们人是非常功利的社会动物，就是说做什么事情都是被任务所驱动的。
这一点，2000年前的司马迁就已经远在西方功利哲学之前看到了（ 《史记》 “货殖列传” ）： “天下熙熙，皆为利来；
天下攘攘，皆为利往。”
 那么，人也就带着功利的目的来看待这个世界，这叫做“teleological stance”。
这个物体是用来干什么的？
它对我有什么用？
怎么用？
 当然，有没有用是相对于我们手头的任务来决定的。
很多东西，当你用不上的时候，往往视而不见；
一旦要急用，你就会当个宝。
俗话叫做“势利眼”，没办法，这是人性！你今天干什么、明天干什么，每时每刻都有任务。
俗话又叫做“屁股决定脑袋”，一个官员坐在不同位置，他就有不同的任务与思路，位置一调，马上就“物是人非”了。
我们的知识是根据我们的任务来组织的。
那么什么叫做任务呢？
如何表达成数学描述呢？
  每个任务其实是在改变场景中的某些物体的状态。
牛顿发明了一个词，在这里被借用了：叫做fluent。
这个词还没被翻译到中文，就是一种可以改变的状态，我暂且翻译为“流态”吧。
比如，把水烧开，水温就是一个流态；
番茄酱与瓶子的空间位置关系是一个流态，可以被挤出来；
还有一些流态是人的生物状态，比如饿、累、喜悦、悲痛；
或者社会关系：从一般人，到朋友、再到密友等。
人类和动物忙忙碌碌，都是在改变各种流态，以提高我们的价值函数（利益）。
懂得这一点，我们再来谈理解图像中的三维场景和人的动作。
其实，这就是因果关系的推理。
所谓因果就是：人的动作导致了某种流态的改变。
我把这些图像之外的东西统称为“暗物质”--- Dark Matter。
物理学家认为我们可观察的物质和能量只是占宇宙总体的5%，剩下的95%是观察不到的暗物质和暗能量。
视觉与此十分相似：感知的图像往往只占5%，提供一些蛛丝马迹；
而后面的95%，包括功能、物理、因果、动机等等是要靠人的想象和推理过程来完成的。



<img src="https://pic3.zhimg.com/v2-bc87d67d41c3a0eba83a9e0c25d92bc3_b.jpg">


有了这个认识，我们来看一个例子（见下图左）。
这个例子来自我们CVPR2015年发的paper，主要作者是朱毅鑫，这也是我很喜欢的一个工作。
一个人要完成的任务是砸核桃，改变桌子上那个核桃的流态。
把这个任务交给UCLA一个学生，他从桌面上的工具里面选择了一个锤子，整个过程没有任何过人之处，因为你也会这么做。
我再强调几点： 一、这个STC-PG的表达是你想象出来的。
这个理解的过程是在你动手之前就想好了的，它里面的节点和边大多数在图像中是没有的，也就是我称作的“暗物质”。
二、这个计算的过程中，大量的运算属于“top-down”自顶向下的计算过程。
也就是用你脑皮层里面学习到的大量的知识来解释你看到的“蛛丝马迹”，形成一个合理的解。
而这种Top-down的计算过程在目前的深度多层神经网络中是没有的。
三、学习这个任务只需要极少的几个例子。
如果一个人要太多的例子，说明Ta脑袋“不开窍”，智商不够。
顺便说一句，我在UCLA讲课，期末学生会给老师评估教学质量。
一个常见的学生意见就是朱教授给的例子太少了。
那么STC-PG是如何推导出来的呢？
它的母板是一个STC-AOG，AOG就是And-Or Graph与或图。
这个与或图是一个复杂的概率语法图模型，它可以导出巨量的合乎规则的概率事件，每一个事件就是STC-PG。
这个表达与语言、认知、机器人等领域是一致的。
在我看来，这个STC-AOG是一个统一表达，它与逻辑以及DNN可以打通关节。
这里就不多讲了。
接着砸核桃的例子讲，还是朱毅鑫那篇文章的实验，这个实验很难做。
比如现在的一个任务是“铲土”，我给你一个例子什么叫铲土，然后开始测试这个智能算法（机器人）的泛化能力。
见下图。



<img src="https://pic2.zhimg.com/v2-12a9ef747619a487cdcf0d5f14d8b205_b.jpg">


第一组实验（图左）。
我给你一些工具，让你铲土，机器人第一选择挑了这个铲子，这个不是模式识别，它同时输出用这个铲子的动作、速度；
输出铲子柄的绿色地方表示它要手握的地方，这个红的表示它用来铲土的位置。
第二选择是一把刷子。
第二组实验（图中）。
假如我要把这些工具拿走，你现在用一些家里常见的物体，任务还是铲土。
它的第一选择是锅，第二选择是杯子。
二者的确都是最佳选择。
这是计算机视觉做出来的，自动的。
第三组实验（图右）。
假如我们回到石器时代，一堆石头能干什么事情？
所以我经常说，咱们石器时代的祖先，比现在的小孩聪明。
因为他们能够理解这个世界的本质，现在，工具和物体越来越特定了，一个工具做一个任务，人都变成越来越傻了。
视觉认知就退化成模式识别的问题了：从原来工具的理解变成一个模式识别。
也就是由乌鸦变鹦鹉了。
 


<img src="https://pic3.zhimg.com/v2-932d429083c802eb3bbb09af59444040_b.jpg">


我的一个理念是：计算机视觉要继续发展，必须发掘这些“dark matter”。
把图像中想象的95%的暗物质与图像中可见的5%的蛛丝马迹，结合起来思考，才能到达真正的理解。
视觉研究的未来，我用一句话来说：Go Dark， Beyond Deep --- 发掘暗，超越深。
这样一来，视觉就跟认知和语言接轨了。

<h3><b>第六节 认知推理：走进内心世界</b> </h3>上一节讲到的智能的暗物质，已经属于感知与认知的结合了。
再往里面走一步，就进入人与动物的内心世界Mind, 内心世界反映外部世界，同时受到动机任务的影响和扭曲。
研究内涵包括：  

<ul><li>Ta看到什么了？
知道什么了？
什么时候知道的？
这其实是对视觉的历史时间求积分。
</li><li>Ta现在在关注什么？
这是当前的正在执行的任务。
</li><li>Ta的意图是什么？
后面想干什么？
预判未来的目的和动机。
</li><li>Ta喜欢什么？
有什么价值函数？
这在第九节会谈到具体例子。
</li></ul>自从人工智能一开始，研究者就提出这些问题，代表人物是Minsky：society of minds，心理学研究叫做Theory of minds。
到2006年的时候，MIT认知科学系的Saxe与Kanwisher（她是我一个项目合作者）发现人的大脑皮层有一个专门的区，用于感受、推理到别人的想法：我知道你在想什么、干什么。
这是人工智能的重要部分。
现实生活中，一般非隐私性的活动中，我们是不设防的，也就是“君子坦荡荡”。



<img src="https://pic1.zhimg.com/v2-3bc95a4d3ecfef5b5894143771f9268f_b.jpg">


不光是人有这个侦察与反侦察的能力，动物也有（见上图）。
比如说这个鸟（图左），它藏果子的时候，会查看周围是否有其它鸟或者动物在那里看到它；
如果有，它就不藏，它非要找到没人看它的时候和地方藏。
这就是它在观察你，知道你知道什么。
图中是一个狐狸和水獭对峙的视频。
水獭抓到鱼了以后，发现这个狐狸在岸上盯着它呢，它知道这个狐狸想抢它嘴里叼着的鱼。
水獭就想办法把鱼藏起来，它把这个鱼藏到水底下，然后这个狐狸去找。
这说明了动物之间互相知道对方在想什么。
尽管人工智能和认知科学，以及最近机器人领域的人都对这个问题感兴趣，但是，大家以前还都是嘴上、纸上谈兵，用的是一些toy examples作为例子来分析。
要做真实世界的研究，就需要从计算机视觉入手。
计算机视觉里面的人呢，又大部分都在忙着刷榜，一时半会还没意思到这是个问题。
我的实验室就捷足先登，做了一些初步的探索，目前还在积极推进之中。



<img src="https://picb.zhimg.com/v2-cddd791f2c0513b1727268f6a8a51dbe_b.jpg">


我们首先做一个简单的试验，如上图。
这个人在厨房里，当前正在用微波炉。
有一个摄像头在看着他，就跟监控一样，也可以是机器人的眼睛(图左)。
首先能够看到他目前在看什么（图中），然后，转换视角，推算他目前看到了什么（图右）。



<img src="https://pic1.zhimg.com/v2-9d3ab07c6b3113588ef6ce1f2d56aa31_b.jpg">


上面这个图是实验的视频的截图。
假设机器人事先已经熟悉某个三维房间（图e），它在观察一个人在房间里面做事（图a）。
为了方便理解，咱们就想象这是一个养老院或者医院病房，机器人需要知道这个人现在在干什么，看什么（图c）。
它的输入仅仅是一个二维的视频（图a）。
它开始跟踪这个人的运动轨迹和眼睛注视的地方，显示在图e的那些轨迹和图f的行为分类。
然后，图d（右上角）是它估算出来的，这个人应该在看什么的图片。
也就是，它把它附体到这个人身上，来感知。
这个结果与图b对比，非常吻合。
图b是这个人带一个眼镜，眼镜有一个小摄像头记录下来的，他确实在看的东西。
这个实验结果是魏平博士提供的，他是西交大前校长郑南宁老师那里的一个青年教师，博士期间在我实验室访问，后来又回来进修。
这里面需要推测动作与物体的时空交互，动作随时间的转换，手眼协调。
然后，进一步猜他下面干什么，意图等等。
这个细节我不多讲了。
对这个人内心的状态，也可以用一个STC-AOG 和STC-PG 来表达的，见下图，大致包含四部分。



<img src="https://pic4.zhimg.com/v2-b830660dd082ba0ee42152d73f678be8_b.jpg">


</figure>

<img src="https://picb.zhimg.com/v2-7a618b5a656db8c37658b193b0b8e4cb_b.jpg">


一、时空因果的概率“与或图”，STC-AOG。
它是这个人的一个总的知识，包含了所有的可能性，我待会儿会进一步阐述这个问题。
剩下的是他对当前时空的一个表达，是一个STC-PG解译图。
此解译图包含三部分，图中表达为三个三角形，每个三角形也是一个STC-PG 解译图。
二、当前的情景situation，由上图的蓝色三角形表示。
当前的情况是什么，这也是一个解，表示视觉在0-t时间段之间对这个场景的理解的一个解译图。
三、意向与动作规划图，由上图的绿色三角形表示。
这也是一个层次化的解译图，预判他下面还会做什么事情，  四、当前的注意力，由上图的红色三角形表示。
描述他正在关注什么。
把这整个解译图放在一块，基本上代表着我们脑袋的过去、现在、未来的短暂时间内的状态。
用一个统一的STC-PG 和 STC-AOG来解释。
这是一个层次的分解。
因为是Composition， 它需要的样本就很少。
有人要说了，我的深度神经网络也有层次，还一百多层呢。
我要说的是，你那一百多层其实就只有一层，对不对？
因为你从特征做这个识别，中间的东西是什么你不知道，他不能去解释中间那些过程，只有最后一层输出物体类别。
我用下面这个图来大致总结一下。
两个人A与B或者一个人一个机器人，他们脑袋里面的表达模式。
图中是一个嵌套的递归结构,每一个椭圆代表一个大脑的内心mind。
 


<img src="https://pic2.zhimg.com/v2-01a4f492d7949fb523b8ac825552f32e_b.jpg">


每个mind除了上面谈到的知识STC-AOG 和状态STC-PG，还包含了价值函数，就是价值观，和决策函数。
价值观驱动动作，然后根据感知、行动去改变世界，这样因果就出来了。
我后面再细谈这个问题。
最底下中间的那个椭圆代表真实世界（“上帝”的mind，真相只有TA知道，我们都不知道），上面中间的那个椭圆是共识。
多个人的话就是社会共识。
在感知基础上，大家形成一个统一的东西，共同理解，我们达成共识。
比如，大家一起吃饭，菜上来了，大家都看到这个菜是什么菜，如果没有共识那没法弄。
比如，“指鹿为马”或者“皇帝的新装”，就是在这些minds之间出现了不一致的东西。
这是所谓“认识论”里面的问题。
以前，在大学学习认识论，老师讲得比较空泛，很难理解；
现在你把表达写出来，一切都清楚了。
这也是人工智能必须解决的问题。
我们要达成共识，共同的知识，然后在一个小的团体、大致社会达成共同的价值观。
当有了共同价值观的时候，就有社会道德和伦理规范，这都可以推导出来了。
俗话说，入乡随俗。
那么如何达成共识呢？
语言就是必要的形成共识的工具了。

<h3><b>第七节 语言通讯：沟通的认知基础</b> </h3>我要介绍的人工智能的第三个领域是语言、对话。
最近我两次在视觉与语言结合的研讨会上做了报告，从我自己观察的角度来谈，视觉与语言是密不可分的。
动物之间就已经有丰富的交流的方式，很多借助于肢体语言。
人的对话不一定用语言，手语、哑剧（pantomine）同样可以传递很多信息。
所以，在语言产生之前，人类就已经有了十分丰富的认知基础，也就是上一节谈的那些表达。
没有这样的认知基础，语言是空洞的符号，对话也不可能发生。
如果是人的话，我们就会热心地指那个小孩的方向，人天生是合作的，去帮助别人的，助人为乐，所以这是为什么我们人进化出来了。
猩猩不会，猩猩不指，它们没有这个动机，它们脑袋与人相比一定是缺了一块。



<img src="https://pic4.zhimg.com/v2-74a91956671e55dcda979ddfe535b34a_b.jpg">


除了需要这个认知基础，语言的研究不能脱离了视觉对外部世界的感知、机器人运动的因果推理，否则语言就是无源之水、无本之木。
这也就是为什么当前一些聊天机器人都在“扯白”。
我们先来看一个最基本的的过程：信息的一次发送。
当某甲（sender）要发送一条消息给某乙（receiver），这是一个简单的通讯communication。
这个通讯的数学模型是当年贝尔实验室香农Shannon1948年提出来的信息论。
首先把它编码，因为这样送起来比较短，比较快；
针对噪声通道，加些冗余码防错；
然后解码，某乙就拿到了这个信息。
见下图。



<img src="https://pic1.zhimg.com/v2-a1c54c6030ae46c33bd73cb84a503a84_b.jpg">


在这个通讯过程之中他有两个基本的假设。
第一、这两边共享一个码本，否则你没法解码，这是一个基本假设。
第二、就是我们有个共享的外部世界的知识在里面，我们都知道世界上正在发生什么什么事件，比如哪个股票明天要涨了，哪个地方要发生什么战争了等等。
我给你传过去的这个信息其实是一个解译图的片段（PG：parse graph）。
这个解译图的片段对于我们物理世界的一个状态或者可能发生的状态的描述。
这个状态也有可能就是我脑袋Mind里面的一个想法、感觉、流态（fluents）。
比如，很多女人拿起电话，叫做“煲粥”，就在交流内心的一些经历和感受。
Shannon的通讯理论只关心码本的建立（比如视频编解码）和通讯带宽（3G,4G，5G）。
1948年提出信息论后，尽管有很多聪明人、数学根底很强的人进到这个领域，这个领域一直没有什么大的突破。
为什么？
因为他们忽视了几个更重大的认识论的问题，避而不谈：   

<ul><li>甲应该要想一下：乙脑袋里面是否与甲有一个共同的世界模型？
否则，解码之后，乙也不能领会里面的内容？
或者会误解。
那么我发这个信息的时候，措辞要尽量减少这样的误解。
</li><li>甲还应该要想一下：为什么要发这个信息？
乙是不是已经知道了，乙关不关注这个信 息呢？
乙爱不爱听呢？
听后有什么反应？
这一句话说出去有什么后果呢？
</li><li>乙要想一下：我为什么要收这个信息呢？
你发给我是什么意图？
</li></ul>这是在认知层面的，递归循环的认知，在编码之外。
所以，通讯理论就只管发送，就像以前电报大楼的发报员，收钱发报，他们不管你发报的动机、内容和后果。
纵观人类语言，中国的象形文字实在了不起。
所谓象形文字就完全是“明码通讯”。
每个字就是外部世界的一个图片、你一看就明白了，不需要编解码。
我觉得研究自然语言的人和研究视觉统计建模的人，都要好好看看中国的甲骨文，然后，所有的事情都清楚了。
每个甲骨文字就是一张图，图是什么？
代表的就是一个解译图的片段（fragment of parse graph）。



<img src="https://pic2.zhimg.com/v2-bf72a57bfb827c10292b3b02cac0b023_b.jpg">


上面这个图是一个汉字的演变和关系图，从一本书叫做《汉字树》得来的。
几年前，我到台湾访问，发现这本丛书，很有意思。
这个图是从眼睛开始的一系列文字。
首先从具象的东西开始，这中间是一个眼睛，“目”字，把手搭在眼睛上面，孙悟空经常有这个动作，就是“看”（look）。
然后是会意，比如“省”，就是细看，明察秋毫，画一个很小的叶子在眼睛上面，指示说你看叶子里面的东西，表示你要细看。
然后开始表达抽象的概念，属性attribute、时空怎么表达，就是我们甲骨文里面，表示出发、终止，表示人的关系，人的脑袋状态，甚至表现伦理道德。
就这样，一直推演开。
所以，搞视觉认知的，要理解物体功能就要追溯到石器时代去，搞语言的要追溯到语言起源。
下图是另一个例子：日、月、山、水、木；
鸟、鸡、鱼、象、羊。
下面彩色的图是我们实验室现在用计算机视觉技术从图像中得到的一些物体的表达图模型，其实就重新发明一些更具像的甲骨文。
这项技术是由YiHong，司长长等博士做的无监督学习。
他们的算法发现了代表鸟的有头、身子和脚、水波和水草等“类甲骨文”名词符号。
这种视觉的表达模型是可解释explainable、直观的。
所以，从生成式模型的角度来看，语言就是视觉，视觉就是语言。
 


<img src="https://pic2.zhimg.com/v2-f8677169f5bd403c8fc3e5984a00110e_b.jpg">


再来看看动词。
考考你们，这是啥意思？
第一个字，两只手，一根绳子，在拖地上一个东西，拿根绳子拽。
第二个很简单，洗手。
第三是关门。
第四是援助的援字，一只手把另外一个人的手往上拉。
第五也是两个手，一个手朝下一个手朝上，啥意思？
我给你东西，你接受。
第六是争夺的争，两个手往相反的方向抢。
第七两个人在聊天。
基本上，字已经表示了人和人之间的动作细节。



<img src="https://pic1.zhimg.com/v2-bac88da99f764f526c34af25e00bf4b9_b.jpg">


现在我的实验室里，计算机也能自动学出“类甲骨文”的动词的表达，见下图。
我们学出来的这些两个人交互的动作包括：坐、玩手机、握手、人拉人等等。
我们把这些动作模型分别叫做4DHOI (4D Human-Object Interaction)、4Dhoi（4D hand-object interaction）、4DHHI (4D Human-Human Interaction)。



<img src="https://pic1.zhimg.com/v2-2f15d84814c1f224afb0f451de1b07bb_b.jpg">


我刚才说了名词和动词，还有很多其他的东西，我建议你们去研究一下，要建模型的话我们古代的甲骨文其实就是一个模型，他能够把我们世界上所有需要表达的东西都给你表达了，是一个完备了的语言模型。
现在，我们回到语言通讯、人与机器人对话的问题。
下图就是我提出的一个认知模型。



<img src="https://pic1.zhimg.com/v2-7520adb900f3caa0f0828fd788a9a074_b.jpg">


两个人之间至少要表达五个脑袋minds：我知道的东西、你知道的东西、我知道你知道的东西、你知道我知道的东西、我们共同知道的东西。
还有，对话的时候你的意图是什么等等诸多问题。
具体我不讲那么多了。



<img src="https://pic4.zhimg.com/v2-9a055387e4066bcf835134e20bd763d9_b.jpg">


最后，我想谈一点，语言与视觉更深层的联系、与数学中代数拓扑的联系。
拓扑学是什么意思？
就是说图象空间，语言空间，就是一个大集合，全集。

<h3><b>第八节 博弈伦理：获取、共享人类的价值观</b> </h3>机器人要与人交流，它必须懂得人类价值观。
哲学和经济学里面有一个基本假设，认为一个理性的人（rational agent），他的行为和决策都由利益和价值驱动，总在追求自己的利益最大化。
与此对应的是非理性的人。
对于理性的人，你通过观察他的行为和选择，就可以反向推理、学习、估算他的价值观。
我们暂时排除他有可能故意假装、迷惑我们的情况。
人与人的价值不同，就算同一个人，价值观也在改变。
本文不讨论这些社会层面的价值观，我们指的是一些最基本的、常识性的、人类共同的价值观。
比如说把房间收拾干净了，这是我们的共识。



<img src="https://pic3.zhimg.com/v2-973027fb2fb07de1bf9467f746901240_b.jpg">


上图是我做的一个简单的实验。
我把几种不同的椅子、凳子放在我办公室（左图）和实验室（右图）。
然后，我统计一下学生进来以后，他喜欢坐哪个椅子，实在不行可以坐地上。
这样我就可以得到这些椅子的排序。
A、B、C、D、E、F、G排个序，见上面的统计图。
我观察了这些人的选择，就问：为什么这个椅子比那个椅子好？
是什么好？
这其实就反映了人的脑袋里面一个基本的价值函数。
又说一遍：很普通的日常现象，蕴含深刻的道路。
苹果落地不是这样吗?大家司空见惯了，就不去问这个问题了。
见下图，比如背部、臀部、头部受多少力。
 


<img src="https://pic1.zhimg.com/v2-77b64e18261a7b2d6dc4e929ebfbf045_b.jpg">


下图中蓝色的直方图显示了六个身体部位的受力分别图。
由此我们就可以推算出每个维度的价值函数。
下面图中六条红色的曲线是负的价值函数，当人的坐姿使得各部位受力处于红线较低的值，就有较高的“价值”，也就是坐得“舒服”。
当然每个人可能不一样，有的人腰疼必须坐硬板凳子有的人喜欢坐软沙发。
这也是为什么，如果你观察到有些异样，可以推导这个人某地方可能受伤了。



<img src="https://pic2.zhimg.com/v2-dba4f4d7f49be07187778addaf6ec54b_b.jpg">


读到这里，你不禁要问：这不是与物理的势能函数，如重力场，一样吗？
对，就是一个道理。
这也是在最后一节我将要说的：达尔文与牛顿的理论体系要统一。
这对我们是常识，但是机器人必须计算出很多这样的常识，TA需要设身处地为人着想，这个就不容易了。
最近大家谈论较多的是机器人下棋，特别是下围棋，的确刺激了国人的神经。
下棋程序里面一个关键就是学习价值函数，就是每一个可能的棋局，它要有一个正确的价值判断。
谈到这里，我想顺便对比两大类学习方法。
一、归纳学习 Inductive learning。
我们通过观察大量数据样本，这些样本就是对某个时期、某个地域、某个人群达成的准平衡态的观察。
也是我前面谈过的千年文化的形成与传承。
二、演绎学习 Deductive learning。
这个东西文献中很少，也就是从价值函数（还有物理因果）出发，直接推导出这些准平衡态，在我看来，这也是一个STC-AOG。
这就要求对研究的对象有深刻的、生成式的模型和理解。

<h3><b>第九节 机器人学：构建大任务平台 </b> </h3>我在第四节谈到人工智能研究的认知构架，应该是小数据、大任务范式。
机器人就是这么一个大任务的科研平台。
它不仅要调度视觉识别、语言交流、认知推理等任务，还要执行大量的行动去改变环境。
我就不介绍机械控制这些问题了，就用市面上提供的通用机器人平台。
前面介绍过，人和机器人要执行任务，把任务分解成一连串的动作，而每个动作都是要改变环境中的流态。
我把流态分作两大类： （1）物理流态 （Physical Fluents）：如下图左边，刷漆、烧开水、拖地板、切菜。
（2）社会流态 (Social Fluents): 如下图右边，吃、喝、 追逐、搀扶，是改变自己内部生物状态、或者是与别人的关系。



<img src="https://pic2.zhimg.com/v2-c7b37a91ba12e05c8328b21d4b76fc08_b.jpg">


当机器人重建了三维场景后（在谈视觉的时候提到了，这其实是一个与任务、功能推理的迭代生成的过程），它就带着功利和任务的眼光来看这个场景。
如下图所示，哪个地方可以站，哪个地方可以坐，哪个地方可以倒水等等。
下面图中亮的地方表示可以执行某个动作。
这些图在机器人规划中又叫做Affordance Map。
意思是：这个场景可以给你提供什么？
  


<img src="https://pic1.zhimg.com/v2-29306a2c2600490a7f881ba48de67647_b.jpg">


有了这些单个基本任务的地图，机器人就可以做任务的规划。
这个规划本身就是一个层次化的表达。
文献中有多种方法，我还是把它统一称作一种STC-PG。
这个过程，其实相当复杂，因为它一边做，一边还要不断看和更新场景的模型。
因为我前面介绍过，对环境三维形状的计算精度是根据任务需要来决定的，也就是Task-Centered视觉表达。
这个动作计划的过程还要考虑因果、考虑到场景中别人的反应。
考虑的东西越多，它就越成熟，做事就得体、不莽莽撞撞。
我一开始讲到的那个机器人竞赛，这些感知和规划的任务其实都交给了一群在后台遥控的人。
下面，我就简单介绍几个我实验室得到的初步演示结果，后台没有遥控的人。
我实验室用的是一个通用的Baxter机器人，配上一个万向移动的底座和两个抓手（grippers），还有一些传感器、摄像头等。
两个抓手是不同的，左手力道大，右手灵活。
很有意思的是，如果你观察过龙虾等动物，它的两个钳子也是不同的，一个用来夹碎、一个是锯齿状的。
下图是一个博士生舒天民教会了机器人几种社交动作，比如握手。
握手看似平常，其实非常微妙。
但你走过去跟一个人握手的过程中，你其实需要多次判断对方的意图；
否则，会出现尴尬局面。
舒的论文在美国这边媒体都报道过。



<img src="https://pic4.zhimg.com/v2-9413e3617678140020c57377ff1763bb_b.jpg">


下面这个组图是机器人完成一个综合的任务。
首先它听到有人去敲门，推断有人要进来，它就去开门。
其次，它看到这个人手上拿个蛋糕盒子，双手被占了，所以需要帮助。
通过对话，它知道对方要把蛋糕放到冰箱里面，所以它就去帮人开冰箱的门（上右图）。
这个人坐下来后，他有一个动作是抓可乐罐，摇了摇，放下来。
它必须推断这个人要喝水，而可乐罐是空的（不可见的流态）。
假设它知道有可乐在冰箱，它后面就开冰箱门拿可乐，然后递给人。



<img src="https://picb.zhimg.com/v2-5adc64e80ae827087ab10dbadfb57ebc_b.jpg">


当然，这个是受限环境，要能够把样的功能做成任意一个场景的话，那就基本能接近我们前面提到的可敬的乌鸦了。
我们还在努力中！
<h3><b>第十节 机器学习：学习的极限和“停机问题” </b></h3> 前面谈的五个领域，属于各个层面上的“问题领域”，叫Domains。
我们努力把这些问题放在一个框架中来思考，寻求一个统一的表达与算法。
而最后要介绍的机器学习，是研究解决“方法领域”（Methods），研究如何去拟合、获取上面的那些知识。
打个比方，那五个领域就像是五种钉子，机器学习是研究锤子，希望去把那些钉子锤进去。
深度学习就像一把比较好用的锤子。
当然，五大领域里面的人也发明了很多锤子。
只不过最近这几年深度学习这把锤子比较流行。
网上关于机器学习的讨论很多，我这里就提出一个基本问题，与大家探讨：学习的极限与“停机问题”。
<b>首先，到底什么是学习？
 </b> 当前大家做的机器学习，其实是一个很狭义的定义，不代表整个的学习过程。
见下图。
它就包含三步： （1）你定义一个损失函数loss function 记作u，代表一个小任务，比如人脸识别，对了就奖励1，错了就是-1。
（2）你选择一个模型，比如一个10-层的神经网络，它带有几亿个参数theta，需要通过数据来拟合。
（3）你拿到大量数据，这里假设有人给你准备了标注的数据，然后就开始拟合参数了。
这个过程没有因果，没有机器人行动，是纯粹的、被动的统计学习。
目前那些做视觉识别和语音识别都是这一类。



<img src="https://pic2.zhimg.com/v2-02b30cd8645e2feb1fed99d326f0dc24_b.jpg">


其实真正的学习是一个交互的过程。
就像孔子与学生的对话，我们教学生也是这样一个过程。
学生可以问老师，老师问学生，共同思考，是一种平等交流，而不是通过大量题海、填鸭式的训练。
坦白说，我虽然是教授，现在就常常从我的博士生那里学到新知识。
这个学习过程是建立在认知构架之上的（第六节讲过的构架）。
我把这种广义的学习称作通讯学习Communicative Learning，见下图。



<img src="https://pic4.zhimg.com/v2-20c2b36a88233f59f64acab42cb0b1bf_b.jpg">


这个图里面是两个人A与B的交流，一个是老师，一个是学生，完全是对等的结构，体现了教与学是一个平等的互动过程。
每个椭圆代表一个脑袋mind，它包含了三大块：知识theta、决策函数pi、价值函数mu。
最底下的那个椭圆代表物理世界，也就是“上帝”脑袋里面知道的东西。
上面中间的那个椭圆代表双方达成的共识。
这个通讯学习的构架里面，就包含了大量的学习模式，包括以下七种学习模式（每种学习模式其实对应与图中的某个或者几个箭头），这里面还有很多模式可以开发出来。
（1）被动统计学习passive statistical learning：上面刚刚谈到的、当前最流行的学习模式，用大数据拟合模型。
（2）主动学习active learning：学生可以问老师主动要数据，这个在机器学习里面也流行过。
（3）算法教学algorithmic teaching：老师主动跟踪学生的进展和能力，然后，设计例子来帮你学。
这是成本比较高的、理想的优秀教师的教学方式。
(4) 演示学习learning from demonstration：这是机器人学科里面常用的，就是手把手叫机器人做动作。
一个变种是模仿学习immitation learning。
（5）感知因果学习perceptual causality：这是我发明的一种，就是通过观察别人行为的因果，而不需要去做实验验证，学习出来的因果模型，这在人类认知中十分普遍。
（6）因果学习causal learning：通过动手实验， 控制其它变量， 而得到更可靠的因果模型， 科学实验往往属于这一类。
（7）增强学习reinforcement learning：就是去学习决策函数与价值函数的一种方法。
我在第一节谈到过，深度学习只是这个广义学习构架里面很小的一部分，而学习又是人工智能里面一个领域。
所以，把深度学习等同于人工智能，真的是坐井观天、以管窥豹。
<b>其次，学习的极限是什么？
停机条件是什么？
</b> 我们学习、谈话的过程，其实就是某种信息在这些椭圆之间流动的过程。
那么影响这个流动的因素就很多,我列举几条如下。
（1）教与学的动机：老师要去交学生一个知识、决策、价值，首先他必须确认自己知道、而学生不知道这个事。
同理，学生去问老师，他也必须意识到自己不知道，而这个老师知道。
那么，一个关键是，双方对自己和对方有一个准确的估计。
（2）教与学的方法：如果老师准确知道学生的进度，就可以准确地提供新知识，而非重复。
这在algorithmic learning 和 perceptual causality里面很明显。
（3）智商问题：如何去测量一个机器的智商？
很多动物，有些概念你怎么教都教不会。
（4）价值函数：如果你对某些知识不感兴趣，那肯定不想学。
价值观相左的人，那根本都无法交流，更别谈相互倾听、学习了。
比如微信群里面有的人就待不了，退群了，因为他跟你不一样，收敛不到一起去，最后同一个群的人收敛到一起去了，互相增强。
这在某种程度上造成了社会的分裂。

<h3><b>第十一节 总结：智能科学 --- 牛顿与达尔文理论体系的统一</b> </h3>什么叫科学？
物理学是迄今为止发展最为完善的一门科学，我们可以借鉴物理学发展的历史。
我自己特别喜欢物理学，1986年报考中科大的时候，我填写的志愿就是近代物理（4系）。
填完志愿以后，我就回乡下去了。
我哥哥当时是市里的干部，他去高中查看我的志愿，一看报的是物理，只怕将来不好找工作，他就给我改报计算机。
当时我们都没见过计算机，他也没跟我商量，所以我是误打误撞进了这个新兴的专业，但心里总是念念不忘物理学之美。
等到开学，上《力学概论》的课，教材是当时常务副校长夫妇写的，我这里就不提名字了，大家都知道，这是科大那一代人心中永恒的记忆。
翻开书的第一页，我就被绪论的文字震撼了。
下面是一个截图，划了重点两句话，讨论如下。



<img src="https://pic1.zhimg.com/v2-06ada8888db79fe666a64db4523a3b8d_b.jpg">


（1）物理学的发展就是一部追求物理世界的统一的历史。
第一次大的统一就是牛顿的经典力学， 通过万有引力把天界星体运动与世俗的看似复杂的物体运动做了一个统一的解释。
形成一个科学的体系，从此也坚定了大家的信念： <b>“物理世界存在着完整的因果链条”。</b>
 物理学的责任就是寻找支配自然各种现象的统一的力。
这完全是一个信念，你相信了，就为此努力！自牛顿以来，300多年了，物理学家还在奋斗，逐步发现了一个美妙的宇宙模型。
智能科学的复杂之处在于：  （1）物理学面对的是一个客观的世界，当这个客观世界映射到每个人脑中， 形成一个主观与客观融合的世界，也就是每个人脑中的模型（这是统计中贝叶斯学派观点）。
这个模型又被映射到别人脑袋之中。
每个脑Mind里面包含了上百个他人的模型的估计。
由这些模型来驱动人的运动、行为。
（2）物理学可以把各种现象隔离出来研究，而我们一张图像就包含大量的模式， 人的一个简单动作后面包含了很复杂的心理活动，很难隔离开。
况且，当前以大数据集为依据的“深度学习”学派、“刷榜派”非常流行，你要把一个小问题单独拿出来研究，那在他们复杂数据集里面是讨不到什么便宜的。
文章送到他们手上，他们就“强烈拒绝”，要求你到他们数据集上跑结果。
这批人缺乏科学的思维和素养。
呜呼哀哉！ 回到前面乌鸦的例子，我在第四节讨论到，我们研究的物理与生物系统有两个基本前提：  <b>一、智能物种与生俱来的任务与价值链条。</b>
这是生物进化的“刚需”，动物的行为都是被各种任务驱动的，任务由价值函数决定，而后者是进化论中的phenotype landscape，通俗地说就是进化的适者生存。
<b>二、物理环境客观的现实与因果链条。</b>
这就是自然尺度下的物理世界与因果链条，也就是牛顿力学的东西。
说到底，人工智能要变成智能科学，它本质上必将是达尔文与牛顿这两个理论体系的统一。



<img src="https://pic3.zhimg.com/v2-e6b69d75be03c0753e36eb59a3ae3613_b.jpg">


2016年我到牛津大学开项目合作会，顺便参观了伦敦的Westminster Abbey 大教堂。
让我惊讶的是：牛顿（1642-1727）与达尔文（1809-1882）两人的墓穴相距也就2-3米远。
站在那个地点，我当时十分感慨。
这两个人可以说是彻底改变人类世界观的、最伟大的科学巨人，但是他们伟大的理论体系和思想的统一，还要等多久呢？
 这篇长文的成稿正好是深秋，让我想起唐代诗人刘禹锡的《秋词》，很能说明科研的一种境界，与大家共赏： “自古逢秋悲寂寥，我言秋日胜春朝。
晴空一鹤排云上，便引诗情到碧霄。”

<b>报告后的提问互动：</b> <b>提问一：</b>朱老师，机器怎么通过学习让它产生自我意识。
刚才您演示的那个机器人，门口有个人他要进来，Ta怎么知道自己后退把路给让出来？
  <b>朱：</b>自我意识这个问题非常重要。
我先简要介绍一下背景，再回答你的问题。
自我意识（self-awareness，consciousness）在心理学领域争议很大，以至于认知学会一度不鼓励大家去谈这个问题，这个方向的人多年拿不到研究经费。
人工智能里面有少数人在谈，但是，还不落地。
自我意识包括几点：  （1）感知体验。
我们花钱去看电影、坐过山车、旅游，其实买的就是一种体验。
这种体验是一种比较低层次的自我意识，形成一种表达（可以是我上面讲到的解译图）。
事后你也可以回味。
（2）运动体验。
我们虽然有镜子，可是除了舞蹈人员，大家并没有看到自己的行为动作。
但是， 我们对自己的体态和动作是有认知的。
我们时刻知道我们的体态和三维动作。
比如，心理学实验，把你和一群人（熟悉和不熟悉的都有）的动作步态用几个关节点做运动捕捉，记录下来，然后，就把这些点放给你看，你只看到点的运动，看不到其它信息。
你认出哪个人是你自己的比率高于认出别人，而且对视角不那么敏感。
所以，我们通过感知和运动在共同建立一个自我的三维模型。
这两者是互通的，往往得益于镜像神经元（mirror neurons）。
这是内部表达的一个关键转换机制。
机器人在这方面就比较容易实现，它有自己的三维模型，关节有传感器，又有Visualodometry， 可随时更新自己在场景中的三维位置和形态。
这一点不难。
<b>（3）自知之明。</b>
中国有个俗语叫做“人贵有自知之明”。
换句话说，一般人很难有自知之明。
对自己能力的认识，不要手高眼低、或者眼高手低。
而且这种认识是要随时更新的。
比如，喝酒后不能开车，灯光暗的时候我的物体识别能力就不那么强，就是你对自己能力变化有一个判断。
我们每天能力可能都不一样其实，这个相当复杂了。
比如，机器人进到日本福岛救灾场景，核辐射随时就在损害机器人的各种能力。
突然，哪一条线路不通了，一个关节运动受限了，一块内存被破坏了。
它必须自己知道，而后重新调整自己的任务规划。
目前人工智能要做到这一点，非常难。
刚才说的人进来、机器人知道往后退，那就是一个协调动作的规划。
你规划动作、首先要知道对方是什么动作。
比如，人与人握手就其实是非常复杂的互动过程。
为了达成这个目标，你要在脑内做模拟simulate。
<b>提问二：</b>谢谢朱教授，感觉今天听到的都是我以前从来没有听过的东西。
我有一个问题就是像机器人这种自我认识都很难，像您说的交互他还要去理解对方那个人的想法，这种信息他怎么来获取呢？
也是通过学习还是？
  <b>朱：</b>靠观察与实践。
你看别人做事你就观察到，你就能够学到每个人都不一样的价值函数，你就了解到你周围的同事，比如你们共享一个办公室，或者观察你家庭里面的人，你跟他生活的时间越长，你就越来越多的知道他怎么想问题、怎么做事，然后你跟他在交互的过程中越来越默契了。
除了观察，还有实践，就是去试探、考验对方。
夫妻之间，刚结婚会吵架，之后越吵越少了、和谐了，价值观融合大致收敛了、或者能够互相容忍了。
实在无法收敛，那就分道扬镳，到民政局办手续。
这两种情况都是我说的“学习的停机问题”。
大家之间不要再相互交流、学习了，要么心领神会、心照不宣；
要么充耳不闻、形同陌路。
<b>提问三：</b>他也是通过他自己观察到，它里面建立一个图吗？
一个解译图（parse graph）吗？
  <b>朱：</b>在我看来是这样的。
就是我必须把你脑袋里面的很多结构尽量重构出来，表达层面就是解译图，至于人脑如何在神经元层面存储这个解译图，我们不清楚。
人脑肯定有类似的表达，我脑袋里面有你的表达后，我就可以装或者演你的对各种情况的反应。
文学作家创作的时候，他脑袋里面同时要装下几十、上百号人的模型和知识表达，那些人知道什么、什么时候知道的。
读文科的人一般观察比较敏锐。
表演艺术家在这方面能力肯定也特别强。
提问四：像我们刚接触机器学习，你有没有什么推荐的，因为现在大家都在追踪训练深度网络，有没有一个推荐的，就是概率模型还是什么东西，一个数学理论或者一个数学工具。
<b>朱：</b>我的想法是这样的，首先让大家端正思想，就是你想学，探索真理和未知。
就是说在夜深人静的时候你探索真理，等你心境沉静下来，你自然就看到一些别人忽略的东西。
不要让我推荐某个工具、代码、秘籍，拿来就用。
我今天讲的东西都不是来源于某一个理论、工具，是融会贯通后的结果。
我反复告诫学生们，做科学研究不是过去那种到北京天桥看把戏，哪里热闹就往哪里钻。
我以前也谈到过一个“路灯的隐喻”，科学研究就像在一个漆黑的夜晚找钥匙，大家喜欢聚在路灯底下找，但是很可能钥匙不在那个灯底下。
<b>提问五：</b>朱老师好，非常庆幸来听这个报告，我最后一个问题很简单。
您说那几个时期，我想问一下秦朝到底什么时候能到？
到秦朝的时候，数学的哪一块你认为，可能会被用做秦朝的武器或者最厉害的那个武器是什么。
<b>朱：</b>问得很好。
什么时候会达到统一？
这个事情中国有两个说法，都有道理。
一种说法叫做“望山跑死马”。
你远远望见前面那个山快到了，你策马前行，可是马跑死都到不了，中间可能还有几条河拦住去路。
那是我们对这个事情估计不足。
第二个说法是“远在天边，近在眼前”。
能不能到达，决定于你这边的人的智慧和行动。
什么时候统一、谁来统一，这决定于我们自己努力了。
春秋和战国时期，思想家是最多的，诸子百家全部都出来了，那是一个思想激烈碰撞的时代。
我今天讲的这些东西其实都在我脑袋里面激烈的碰撞，我还有些问题想不通。
我们现在谈这个事情和框架，你觉得世界上有多少人在做？
我的观察是：极少，也许一只手就可以数得过来。
你的第二个问题，如果要统一，那最厉害的数学工具是什么？
我们要建立统一的知识表达：概率和逻辑要融合，和深度学习也要融合。
我们看看物理学是如何统一的，他们里面各种模型（四大类的力与相互作用）必须融洽，然后解释各种现象。
简单说我们需要搞清楚两点：  一、什么地方用什么模型？
 对比经典力学、电磁学、光学、统计物理、粒子物理等都有自己的现象、规律和使用范围。
我们这边也类似，各种模型有它们的范围和基础，比如我们常常听说的，吉布斯模型往往就在高熵区，稀疏模型在低熵区，与或图语法用在中熵区。
这一块除了我的实验室，世界上没有其他人研究。
二、这些模型之间如何转化？
 前面我讲了一个例子，我写了一篇关于隐式（马尔科夫场）与显式（稀疏）模型的统一与过渡的信息尺度的论文，投到CVPR会议，结果，三个评分是“（5）强烈拒绝；
（5）强烈拒绝；
（4）拒绝”。
大家根本就没想这个问题，眼睛都巴巴地看着数据集、性能提升了多少。
刷榜成了CVPR科研的重要范式。
在某些人眼中，刷榜成了唯一方式。
我以前是批判这个风气，后来一想，其实应该多鼓励。
我对那些把大众带到沟里去的学术领军人物，以前是批评，现在我特别感激Ta们。
这样我自己的学生才有更多时间去实现我们的思路。
你们都一起涌过来踩踏、乱开乱挖，我都躲不开。
我做研究喜欢清静，不去赶热闹，不去追求文章引用率这些指标。


<br>
<h2>Data Frame</h2>

Data Frame is a list of vectors of equal length

to create a dataframe:
n = c(2,3,5)
s = c('a','b','c')
b = c(TRUE, FALSE, FALSE)
df = data.frame(n,s,b)

Components of dataframe:
header, column names, data row, name of the row cell
single square bracket "[]", comma

Functions:
nrow(), ncol(), head()

Inport Data:
read.table("mydata.txt")
read.csv("mydata.csv")

retrieve the column vector by the double square bracket or the "$" operator
mtcars[[9]]
mtcars[["am"]]
mtcars$am
mtcars[,"am"]


retrieve a column slice with the single square bracket "[]"
mtcars[1]
mtcars["mpg"]
mtcars[c("mpg", "hp")]

Data frame Row Slice
mtcars[24,]
mtcars[c(3,24),]
mtcars["camaro z28",]
mtcars[c("datsun 710","camaro z28"),]

<h2># MLFundStat and Hangseng Fund Stat</h2>
#=================
MLFundStat.html
the computation is long, it is possible to cut time by adjusting the cutdate variable.
this should be modified to new version using r chart.

<h2># Start Of R</h2>
#=================
Sys.setlocale(category = 'LC_ALL', 'Chinese')

use the .Rprofile.site file to run R commands for all users when their R session starts.
D:\R-3.5.1\etc\Rprofile.site
See: Initialization at startup.

This command could be an environment set:
Sys.setenv(FAME="/opt/fame")

<a href="https://www.rdocumentation.org/packages/base/versions/3.5.0/topics/Startup">Start Of R Initialization</a>

<h2># Encoding Problems</h2>
<code>To write text UTF8 encoding on Windows</code>
Firstly, set encoding
options(encoding = "UTF-8")

To write text UTF8 encoding on Windows one has to use the <b class="gold embossts redbs borRad10">useBytes=T</b> options in functions like writeLines or readLines:

txt &lt;- "在"
<code>writeLines(txt, "test.txt", useBytes=T)

readLines("test.txt", encoding="UTF-8")</code>
[1] "在"

<code>writeLines(wholePage, theFilename, useBytes=T)</code>
The UTF-8 BOM is a sequence of bytes at the start of a text stream
(0xEF, 0xBB, 0xBF) that allows the reader to more reliably guess a file as being encoded in UTF-8.

Normally, the BOM is used to signal the endianness of an encoding, but since endianness is irrelevant to UTF-8, the BOM is unnecessary.

According to the Unicode standard, the BOM for UTF-8 files is not recommended

#=================
# Encoding Problems
Sys.getlocale()
getOption("encoding")
options(encoding = "UTF-8")
Encoding(txtstring) &lt;- "UTF-8"
Encoding(txtstring)
txtstring
Sys.setlocale
Sys.setlocale(category = 'LC_ALL', 'Chinese')
Sys.setlocale(category = "LC_ALL", locale = "chs") 
Sys.setlocale(category = "LC_ALL", locale = "cht") # fanti

Note: 
default: options("encoding" = "native.enc")
statTxtFile = "test.txt"
write("建设银行", statTxtFile, append=TRUE)
result file is ansi

add:
options("encoding" = "UTF-8")
write("建设银行", statTxtFile, append=TRUE)
result file is utf-8

mytext &lt;- "this is my text"
Encoding(mytext)

options(encoding = "UTF-8")
getOption("encoding")

options(encoding='native.enc')
getOption("encoding")


iconvlist()
theHeader = "http://qt.gtimg.cn/r=2&q=r_hk"
onecode = "02009"
con = url(paste0(theHeader,onecode), encoding = "GB2312")
thepage=readLines(con)
close(con)
Info=unlist(strsplit(thepage,"~"))
codename=Info[2]
codename
Encoding(codename)

==================
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
readLines(filename, encoding="UTF-8")
readLines(con = stdin(), n = -1L, ok = TRUE, warn = TRUE, encoding = "unknown", skipNul = FALSE)

<span class="redword"># note! the chiname encoding is ok inside R, but will be wrong when write to file by local pc locale, to solve the problem, set Sys.setlocale(category = 'LC_ALL', 'Chinese') </span>

readLines(con &lt;- file("Unicode.txt", encoding = "UCS-2LE"))
close(con)
unique(Encoding(A)) # will most likely be UTF-8
==================
guess_encoding(pageHeader)
pageHeader = repair_encoding(pageHeader, from="utf-8")
pageHeader = repair_encoding(pageHeader, "UTF-8")

iconv(pageHeader, to="UTF-8")
Encoding(pageHeader) &lt;- "UTF-8"

Sys.getlocale("LC_ALL")
https://rpubs.com/mauriciocramos/encoding
==================

Read text as UTF-8 encoding

the following reads in encoding twice and works but reasons unknown
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
[1] "Zürich"

==================
the page source claim to be using UTF-8 encoding:
meta http-equiv="Content-Type" content="text/html; charset=utf-8"

So, the question is, are they really using a different enough encoding, 
or can we just convert to utf-8, guessing that any errors will be negligible?

A quick and dirty approach just force utf-8 using iconv:

TV_Audio_Video &lt;- read_html(iconv(page_source[[1]], to = "UTF-8"), encoding = "utf8")

In general, this is a bad idea - better to specify the encoding it's from.
In this case, maybe the error is theirs, so this quick and dirty approach might be ok.


<h2>to remove leading zeros</h2>
substr(t,regexpr("[^0]",t),nchar(t))

<h2>Pop up message in windows 8.1</h2>
use the tcl/tk package in R to create a messageBox. 
Here is a very simple example:

require(tcltk)
tkmessageBox(title = "Title of message box",
                       message = "Hello, world!", icon = "info", type = "ok")

library(tcltk)
tk_messageBox(type='ok',message='I am a tkMessageBox!')

different types of messagebox (yesno, okcancel, etc).
See ?tk_messageBox.


or
use cmd
system('CMD /C "ECHO The R process has finished running && PAUSE"', 

or
use hta

in one line:
mshta "about:&lt;script>alert('Hello, world!');close()&lt;/script>"
or
mshta "javascript:alert('message');close()"
or
mshta.exe vbscript:Execute("msgbox ""message"",0,""title"":close")


mshta "about:&lt;script src='file://%~f0'>&lt;/script>&lt;script>close()&lt;/script>" %*

msg = paste0(
'mshta ',
"\"about:&lt;script>alert('Hello, world!');close()&lt;/script>\""
)

to show web page, use script to create

#=================
Pop up message in windows 8.1
c.bat:  start MessageBox.vbs "This will be shown in a popup."

MessageBox.vbs :
Set objArgs = WScript.Arguments
messageText = objArgs(0)
MsgBox messageText

in fact, save a file named test.vbs with content:
MsgBox "some message"

double click the file will run directly


# options("scipen"=999)
# format(xx, scientific=F)
# options("scipen"=100, "digits"=4)
# getOption("scipen")
# or as.integer(functionResult);

df &lt;- data.frame(matrix(ncol = 10000, nrow = 0))
colnames(df) &lt;- c("a", "b," "c")
rm(list=ls())
Extracting a Single, Simple Table
The first step is to load the ¡§XML¡¨ package, 
then use the htmlParse() function to read the html document into an R object, 
and readHTMLTable() to read the table(s) in the document. 
The length() function indicates there is a single table in the document, simplifying our work.

The plot3d() function in the rgl package
library(rgl)
open3d()
attach(mtcars)
plot3d(disp,wt,mpg, col = rainbow(10))






<h2>library(stringr)</h2>
#============
library(stringr)
library(htmltools)
library(threejs)
data(mtcars)
data &lt;- mtcars[order(mtcars$cyl),]
uv &lt;- tabulate(mtcars$cyl)
col &lt;- c(rep("red",uv[4]),rep("yellow",uv[6]),rep("blue",uv[8]))
row.names(mtcars)
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")





<h2>scatterplot3js(data[,c(3,6,1)],</h2>
#============
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")
# Gumball machine
N &lt;- 100
i &lt;- sample(3, N, replace=TRUE)
x &lt;- matrix(rnorm(N*3),ncol=3)
lab &lt;- c("small", "bigger", "biggest")
scatterplot3js(x, color=rainbow(N), labels=lab[i],
               size=i, renderer="canvas")
# Example 1 from the scatterplot3d package (cf.)
z &lt;- seq(-10, 10, 0.1)
x &lt;- cos(z)
y &lt;- sin(z)
scatterplot3js(x,y,z, color=rainbow(length(z)),
   labels=sprintf("x=%.2f, y=%.2f, z=%.2f", x, y, z))
# Interesting 100,000 point cloud example, should run this with WebGL!
N1 &lt;- 10000
N2 &lt;- 90000
x &lt;- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
y &lt;- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
z &lt;- c(rnorm(N1, sd=0.5), rpois(N2, lambda=20)-20)
col &lt;- c(rep("#ffff00",N1),rep("#0000ff",N2))
scatterplot3js(x,y,z, color=col, size=0.25)
cat("\014")	CLS Screen
#
match returns a vector of the positions
v1 &lt;- c("a","b","c","d")
v2 &lt;- c("g","x","d","e","f","a","c")
x &lt;- match(v1,v2)
6 NA  7  3
v1 %in% v2
TRUE FALSE  TRUE  TRUE
x &lt;- match(v1,v2,nomatch=-1)
6 -1  7  3
%in% returns a logical vector indicating if there is a match or not






<h2>this check whether an element is inside a group</h2>
#=============
this check whether an element is inside a group
v &lt;- c('a','b','c','e')
'b' %in% v





<h2>check vector includes in 31:37 %in% 0:36</h2>
#=============
31:37 %in% 0:36
#
dmInfo=data.matrix(Info)	# convert dataframe to matrix, but the row and column is exchanged
#
bob &lt;- data.frame(lapply(bob, as.character), stringsAsFactors=FALSE)	#Change numeric to characters
#
write.csv(Info,quote=FALSE, row.names = FALSE)	# write csv is the proper way to write the datafile
#

attach an excel file in R:
1: Install packages XLConnect and foreign and run both libraries
2: abcd &lt;- readWorksheet(loadWorkbook('file extension'),sheet=1)
#
allocate vector of size 1.7 Gb
Try memory.limit() for the current memory limit Use memory.limit (size=50000) to increase memory limit. Try using a cloud based environment, 
try using package slam
use factors 

Concatenate and Split Strings in R
==================================
use the paste() function to concatenate
strsplit() function to split
pangram &lt;- "The quick brown fox jumps over the lazy dog"
strsplit(pangram, " ")
"The"  "quick" "brown" "fox"  "jumps" "over" "the"  "lazy" "dog"

the unique elements
unique() function
unique(tolower(words))
"the"  "quick" "brown" "fox"  "jumps" "over" "lazy" "dog"

# <span class="gold">find duplicates</span>
# the intersect function is used for different set, not in inside a vector
# instead, use the duplicated function will be OK.

words = unlist(strsplit(pangram, " "))
words = tolower(words)
duplicated(words)
words[duplicated(words)]

arr = sample(1:36,6,replace=TRUE)
cat(arr, "\n")
arr[duplicated(arr)]


R split Function
================
split() function divides the data in a vector. 
unsplit() funtion do the reverse.
split(x, f, drop = FALSE, ...)
split(x, f, drop = FALSE, ...) &lt;- value
unsplit(value, f, drop = FALSE)
x: vector, data frame
f: indices
drop: discard non existing levels or not






<h2>x &lt;- read.csv("anova.csv",header=T,sep=",")</h2>
#=============
x &lt;- read.csv("anova.csv",header=T,sep=",")
Subtype,Gender,Expression
A,m,-0.54
A,m,-0.8
Split the "Expression" values into two groups based on "Gender" variable, 
"f" for female group, and 
"m" for male group:
>g &lt;- split(x$Expression, x$Gender)
>g
$f
  [1] -0.66 -1.15 -0.30 -0.40 -0.24 -0.92  0.48 -1.68 -0.80 -0.55 -0.11 -1.26
$m
  [1] -0.54 -0.80 -1.03 -0.41 -1.31 -0.43  1.01  0.14  1.42 -0.16  0.15 -0.62

Calculate the length, mean value of each group:
sapply(g,length)
  f   m 
135 146 
sapply(g,mean)
         f          m 
-0.3946667 -0.2227397

You may use lapply, return is a list:
lapply(g,mean)
unsplit() function combines the groups:
unsplit(g,x$Gender)

<h2><span class="blink red">Apply</span></h2>
=====
m &lt;- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=30, ncol=3)
apply(m, 1, mean)
a 1 in the second argument, giving the mean of each row. 
apply(m, 2, mean)giving the mean of each column. 
apply(m, 2, function(x) length(x[x&lt;0]))	# count -ve values
apply(m, 2, function(x) is.matrix(x))
apply(m, 2, is.vector)
apply(m, 2, function(x) mean(x[x>0]))

#=========
ma &lt;- matrix(c(1:4, 1, 6:8), nrow = 2)

apply(ma, 1, table)

apply(ma, 1, stats::quantile)
apply(ma, 2, mean)

apply(m, 2, function(x) length(x[x&lt;0]))

sapply lapply rollapply
sapply(1:3, function(x) x^2)

lapply return a list:
lapply(1:3, function(x) x^2)
use unlist with lapply to get a vector

sapply(1:3, function(x, y) mean(y[,x]), y=m)

A&lt;-matrix(1:9, 3,3)
B&lt;-matrix(4:15, 4,3)
C&lt;-matrix(8:10, 3,2)
MyList&lt;-list(A,B,C)
Z=sapply(MyList,"[", 1,1 )

#==========
te=matrix(1:20,nrow=2)
sapply(te,mean)	# this is a vector, order arrange in matrix direction
matrix(sapply(te,mean),nrow=2)	# this is changed to matrix

subset()
apply()
sapply()
lapply()
tapply()
aggregate()
apply 	apply a function to the rows or columns of a matrix
M &lt;- matrix(seq(1,16), 4, 4)
apply(M, 1, min)
lapply 	apply a function to each element of a list in turn and get a list back
x &lt;- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
sapply 	apply a function to each element of a list in turn, but you want a vector back
x &lt;- list(a = 1, b = 1:3, c = 10:100)
sapply(x, FUN = length)  
vapply 	squeeze some more speed out of sapply
x &lt;- list(a = 1, b = 1:3, c = 10:100)
vapply(x, FUN = length, FUN.VALUE = 0L) 

mapply 	apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in sapply

Note: 
mApply(X, INDEX, FUN, …, simplify=TRUE, keepmatrix=FALSE)
from Hmisc package

is different from 
mapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)


Examples

#Sums the 1st elements, the 2nd elements, etc. 
mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15

mapply(rep, 1:4, 4:1)
mapply(rep, times = 1:4, x = 4:1)
mapply(rep, times = 1:4, MoreArgs = list(x = 42))
mapply(function(x, y) seq_len(x) + y,
       c(a =  1, b = 2, c = 3),  # names from first
       c(A = 10, B = 0, C = -10))
word &lt;- function(C, k) paste(rep.int(C, k), collapse = "")
utils::str(mapply(word, LETTERS[1:6], 6:1, SIMPLIFY = FALSE))

mapply(function(x,y){x^y},x=c(2,3),y=c(3,4))
8 81

values1 &lt;- list(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))
values2 &lt;- list(a = c(10, 11, 12), b = c(13, 14, 15), c = c(16, 17, 18)) 
mapply(function(num1, num2) max(c(num1, num2)), values1, values2)
 a  b  c 
12 15 18 



Map 	A wrapper to mapply with SIMPLIFY = FALSE, so it is guaranteed to return a list
rapply	For when you want to apply a function to each element of a nested list structure, recursively
tapply	For when you want to apply a function to subsets of a vector and the subsets are defined by some other vector, usually a factor
lapply is a list apply which acts on a list or vector and returns a list.
sapply is a simple lapply (function defaults to returning a vector or matrix when possible)
vapply is a verified apply (allows the return object type to be prespecified)
rapply is a recursive apply for nested lists, i.e. lists within lists
tapply is a tagged apply where the tags identify the subsets
apply is generic: applies a function to a matrix's rows or columns
by	a "wrapper" for tapply. The power of by arises when we want to compute a task that tapply can't handle
aggregate can be seen as another a different way of use tapply if we use it in such a way


xx = c(1,3,5,7,9,8,6,4,2,1,5)
duplicated(xx)
xx[duplicated(xx)]

Accessing dataframe by names:
mtcars["mpg"]
QueueNo = 12
mtcars[QueueNo,"mpg"]

some functions to remember
charToRaw(key)
as.raw(key)

A motion chart is a dynamic chart to explore several indicators over time. 
subset(airquality, Temp > 80, select = c(Ozone, Temp))
subset(airquality, Day == 1, select = -Temp)
subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp > 80))
 ## sometimes requiring a logical 'subset' argument is a nuisance nm &lt;- rownames(state.x77) start_with_M &lt;- nm %in% grep("^M", nm, value = TRUE)
subset(state.x77, start_with_M, Illiteracy:Murder) # but in recent versions of R this can simply be
subset(state.x77, grepl("^M", nm), Illiteracy:Murder)

join 3 dataframes
library("plyr")
join() function
names(gdp)[3] &lt;- "GDP"
names(life_expectancy)[3] = "LifeExpectancy"
names(population)[3] = "Population"
gdp_life_exp &lt;- join(gdp, life_expectancy)
development &lt;- join(gdp_life_exp, population)

subset() function
dev_2005 &lt;- subset(development, Year == 2005)
dev_2005_big &lt;- subset(dev_2005, GDP >= 30000)

development_motion &lt;- subset(development_complete, Country %in% selection)
library(googleVis)
gvisMotionChart() function
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year")
plot(motion_graph)
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "GDP", yvar = "LifeExpectancy", sizevar = "Population")
development_motion$logGDP &lt;- log(development_motion$GDP)
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "logGDP", yvar = "LifeExpectancy", sizevar = "Population")

my_list[[1]] extracts the first element of the list my_list, and my_list[["name"]] extracts the element in my_list that is called name. 
If the list is nested you can travel down the heirarchy by recursive subsetting. 
mylist[[1]][["name"]] is the element called name inside the first element of my_list.
A data frame is just a special kind of list, so you can use double bracket subsetting on data frames too. 
my_df[[1]] will extract the first column of a data frame and my_df[["name"]] will extract the column named name from the data frame.
names() and str() is a great way to explore the structure of a list.

i in 1:ncol(df)
This is a pretty common model for a sequence: a sequence of consecutive integers designed to index over one dimension of our data.
What might surprise you is that this isn't the best way to generate such a sequence, especially when you are using for loops inside your own functions. Let's look at an example where df is an empty data frame:
df &lt;- data.frame()
1:ncol(df)
for (i in 1:ncol(df)) {
  print(median(df[[i]]))
}
Our sequence is now the somewhat non-sensical: 1, 0. You might think you wouldn't be silly enough to use a for loop with an empty data frame, but once you start writing your own functions, there's no telling what the input will be.
A better method is to use the seq_along() function.
if you grow the for loop at each iteration (e.g. using c()), your for loop will be very slow.
A general way of creating an empty vector of given length is the vector() function. 
It has two arguments: the type of the vector ("logical", "integer", "double", "character", etc.) and the length of the vector.
Then, at each iteration of the loop you must store the output in the corresponding entry of the output vector, i.e. assign the result to output[[i]]. (You might ask why we are using double brackets here when output is a vector. It's primarily for generalizability: this subsetting will work whether output is a vector or a list.)

A time series can be thought of as a vector or matrix of numbers, 
along with some information about what times those numbers were recorded. This information is stored in a ts object in R.
read in some time series data from an xlsx file using read_excel(), 
a function from the readxl package, 
and store the data as a ts object.
Use the read_excel() function to read the data from "exercise1.xlsx" into mydata.
mydata &lt;- read_excel("exercise1.xlsx")
Create a ts object called myts using the ts() function. 
myts &lt;- ts(mydata[,2:4], start = c(1981, 1), frequency = 4)

The first step in any data analysis task is to plot the data. 
Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. 
The features that you see in the plots must then be incorporated into the forecasting methods that you use. 
Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.
You will use the autoplot() function to produce time plots of the data. 
In each plot, look out for outliers, seasonal patterns, and other interesting features.
Use which.max() to spot the outlier in the gold series. 

library("fpp2")
autoplot(a10)
ggseasonplot(a10)
An interesting variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal. 
ggseasonplot(a10, polar = TRUE)
beer &lt;- window(a10, start=1992)
autoplot(beer)
ggseasonplot(beer)
Use the window() function to consider only the ausbeer data from 1992 and save this to beer. 
Set a keyword start to the appropriate year.

x &lt;- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});





<h2>x &lt;- c(sort(sample(1:20, 9)), NA)</h2>
#===================
x &lt;- c(sort(sample(1:20, 9)), NA)
y &lt;- c(sort(sample(3:23, 7)), NA)
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)

alist = readLines("alist.txt")
blist = readLines("blist.txt")
out = setdiff(blist, alist)

writeClipboard(out)





<h2># To skip 3rd iteration and go to next iteration</h2>
#===================
# To skip 3rd iteration and go to next iteration
for(n in 1:5) {
  if(n==3) next
  cat(n)
}





<h2>googleVis chart</h2>
#===================
googleVis chart
===============
library(googleVis)

Line chart
==========
df=data.frame(country=c("US", "GB", "BR"), 
              val1=c(10,13,14), val2=c(23,12,32))
Line &lt;- gvisLineChart(df)
plot(Line)

Scatter chart
=======================
# example 1
dat &lt;- data.frame(x=c(1,2,3,4,5), y1=c(0,3,7,5,2), y2=c(1,NA,0,3,2))
plot(gvisScatterChart(dat, options=list(lineWidth=2, pointSize=2, width=900, height=600)))

# example 2, women
Scatter &lt;- gvisScatterChart(women, 
               options=list(
                 legend="none", lineWidth=1, pointSize=2,
                 title="Women", vAxis="{title:'weight (lbs)'}",
                 hAxis="{title:'height (in)'}", width=900, height=600)
           )
plot(Scatter)

# example 3
ex3dat &lt;- data.frame(x=c(1,2,3,4,5,6,7,8), y1=c(0,3,7,5,2,0,8,6), y2=c(1,NA,0,3,2,6,4,2))
ex3 &lt;- gvisScatterChart(ex3dat, 
           options=list(
             legend="none", lineWidth=1, pointSize=2,
             title="ex3", vAxis="{title:'weight (lbs)'}",
             hAxis="{title:'height (in)'}", width=900, height=600)
       )
plot(ex3)
# Note: to plot timeline chart, arrange the time in x axis, beginning with -ve and the last is 1 to show the sequence


<h2>cat to a file using file(filename, open = "a")</h2>
cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "data.txt", sep = "\n")

<h3>cat append to a file, open file in "a" mode</h3>
#===================
textVector = c("First thing","Second thing","c")

catObj &lt;- file("theappend.txt", open = "a")
cat(textVector, file = catObj, sep="\n")
close(catObj)






<h2>install.packages("readr")</h2>
#===================
install.packages("readr")
library(readr)

to read rectangular data (like csv, tsv, and fwf)
readr is part of the core tidyverse
library(tidyverse)

readr supports seven file formats with seven read_ functions:

read_csv(): comma separated (CSV) files
read_tsv(): tab separated files
read_delim(): general delimited files
read_fwf(): fixed width files
read_table(): tabular files where columns are separated by white-space.
read_log(): web log files





<h2>iconv(keyword, "unknown", "GB2312")</h2>
#===================
iconv(keyword, "unknown", "GB2312")






<h2>Grabbing HTML Tags</h2>
#==========
Grabbing HTML Tags

\b[^>]*>(.*?) matches the opening and closing pair of a specific HTML tag. 

Anything between the tags is captured into the first backreference. 
The question mark in the regex makes the star lazy, to make sure it stops before the first closing tag rather than before the last, like a greedy star would do. 
This regex will not properly match tags nested inside themselves, like in one two one.

<([A-Z][A-Z0-9]*)\b[^>]*>(.*?)</\1> will match the opening and closing pair of any HTML tag. 
Be sure to turn off case sensitivity. 
The key in this solution is the use of the backreference \1 in the regex. 
Anything between the tags is captured into the second backreference. 
This solution will also not match tags nested in themselves






<h2>find the new item</h2>
#==========
find the new item

theList = c("00700","02318","02007")
newList=c("03333","01398","02007")

newList[!(newList %in% theList)]





<h2>formating numbers</h2>
#==========
formating numbers
a &lt;- seq(1,101,25)
sprintf("%03d", a)

format(round(a, 2), nsmall = 2)





<h2>the match function:</h2>
#==========
the match function:
match(x, table, nomatch = NA_integer_, incomparables = NULL)
%in%
match returns a vector of the positions of (first) matches of its first argument in its second.

Corpus&lt;- c('animalada', 'fe', 'fernandez', 'ladrillo')
Lexicon&lt;- c('animal', 'animalada', 'fe', 'fernandez', 'ladr', 'ladrillo')
Lexicon %in% Corpus

Lexicon[Lexicon %in% Corpus]





<h2>Machine Learning:</h2>
<a href="https://machinelearningmastery.com/machine-learning-in-r-step-by-step/">machine-learning-in-r-step-by-step</a>
<br>
<a href="https://lgatto.github.io/IntroMachineLearningWithR/index.html">An Introduction to Machine Learning with R</a>
<br>
<a href="https://www.r-bloggers.com/image-recognition-tutorial-in-r-using-deep-convolutional-neural-networks-mxnet-package/">mxnet</a>
<br>
<a href="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/">image classification</a>
<br>
#==========
Machine Learning:

The caret package

Caret contains wrapper functions that allow you to use the exact same functions for training and predicting with dozens of different algorithms. On top of that, it includes sophisticated built-in methods for evaluating the effectiveness of the predictions you get from the model. 

Use The Titanic dataset

Training a model
training a bunch of different decision trees and having them vote 
Random forests work pretty well in *lots* of different situations, so I often try them first.

Evaluating the model

Cross-validation is a way to evaluate the performance of a model without needing any other data than the training data. 

Making predictions on the test set

Improving the model







<h2>to handle error 404 when scraping: use tryCatch()</h2>
#==========
to handle error 404 when scraping: use tryCatch()

for (i in urls) {
    tmp &lt;- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}
#==========
try(readLines(url), silent = TRUE)

tryCatch(readLines(url), error = function (e) conditionMessage(e))






<h2>write.table</h2>
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
#==========
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
            eol = "\n", na = "NA", dec = ".", row.names = FALSE,
            col.names = FALSE, qmethod = c("escape", "double"),
            fileEncoding = "")

write.table(finalTableList, theOutputname, row.names=FALSE, col.names=FALSE, quote = FALSE, sep = "\t" )






<h2>Four normal distribution functions:</h2>
#==========
Four normal distribution functions:

<a href="https://www.r-bloggers.com/normal-distribution-functions/">Four normal distribution functions:</a>
<br>

RNORM	Generates random numbers from normal distribution	
rnorm(n, mean, sd)
rnorm(1000, 3, .25)	Generates 1000 numbers from a normal with mean 3 and sd=.25

DNORM	Probability Density Function(PDF)
dnorm(x, mean, sd)
dnorm(0, 0, .5)	Gives the density (height of the PDF) of the normal with mean=0 and sd=.5. 

PNORM	Cumulative Distribution Function
(CDF)	pnorm(q, mean, sd)
pnorm(1.96, 0, 1)	Gives the area under the standard normal curve to the left of 1.96, i.e. ~0.975

QNORM	Quantile Function – inverse of
pnorm	qnorm(p, mean, sd)
qnorm(0.975, 0, 1)	Gives the value at which the CDF of the standard normal is .975, i.e. ~1.96

Note that for all functions, leaving out the mean and standard deviation would result in default values of mean=0 and sd=1, a standard normal distribution.






<h2>pnorm students scoring higher than 84</h2>
#==========
pnorm students scoring higher than 84
> pnorm(84, mean=72, sd=15.2, lower.tail=FALSE) 
[1] 0.21492
Answer
The percentage of students scoring 84 or more in the college entrance exam is 21.5%.






<h2>plot a histogram of 1000</h2>
draws from a normal distribution with mean 10, standard deviation 2.
#==========
plot a histogram of 1000 draws from a normal distribution with mean 10, standard deviation 2.
set.seed(seed)
x = rnorm(1000, 10, 2)
plot(x)
hist(x)

Using a QQ plot. Assess the normality:
qqnorm(x)
qqline(x)

In statistics, a Q–Q (quantile-quantile) plot is a probability plot, 
which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.
First, the set of intervals for the quantiles is chosen. 
A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). 
Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.






<h2>format leading zeros</h2>
#==========
format leading zeros

formatC(1, width = 2, format = "d", flag = "0")
"01"
formatC(125, width = 5, format = "d", flag = "0")
"00125"






<h2>library(pdftools)</h2>
#==========
setwd("C:/Users/User/Desktop")
library(pdftools)
txt &lt;- pdf_text("a.pdf")
str(txt)	# 361 pages
writeClipboard(txt[1])

txt1 = gsub(".*ORIGINATOR", "", txt)
txt1 = gsub("          ", "", txt1)

list = c(13:16, 19:22, 25:28, 31:34, 37:42, 45:48, 52:58, 62:68, 71:75, 78:85, 88:95, 98:105, 108:115, 118:124, 127:133, 136:142, 145:156, 159:169, 173:202, 206:221, 225:240, 244:258, 261:274, 277:290, 294:298, 302:308, 312:318, 323:331, 334:345, 348:359)

txt1 = txt1[list]

writeClipboard(txt1)

pdf_info("a.pdf")
pdf_text("a.pdf")
pdf_fonts("a.pdf")
pdf_attachments("a.pdf")
pdf_toc("a.pdf")

toc = pdf_toc("a.pdf")
sink("test.txt")
print(toc)
sink()





<h2>library(pdftools)</h2>
#==========
library(pdftools)
txt &lt;- pdf_text("a.pdf")
str(txt)
txtList = unlist(strsplit(txt, "\\s{2,}"))

writeClipboard(txtList)






<h2>The name of the site environment variable R_ENVIRON</h2>
#==========
The name of the site environment variable R_ENVIRON
"R_HOME/etc/Renviron.site"

the default is "R_HOME/etc/Rprofile.site"

Sys.getenv("R_USER").

Examples

## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, 
so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, 
digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", 
"onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First &lt;- function() cat("\n   Welcome to R!\n\n")
.Last &lt;- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, 
set a CRAN mirror
  old &lt;- getOption("defaultPackages"); r &lt;- getOption("repos")
  r["CRAN"] &lt;- "http://my.local.cran"
  options(defaultPackages = c(old, 
"MASS"), 
repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols &lt;- Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), 
"\n")
# coo\bardoh\exabc"def'







<h2>How to Convert Factor into Numerical?</h2>
#==========
How to Convert Factor into Numerical?

When you convert factors to numeric, 
first you should convert it into characters and then convert into numeric. 
as.numeric(as.character(X))

Df$column&lt;-as.numeric(as.factor(df$column)

as.integer(as.factor(region))





<h2>options(error=recover)</h2>
#==========
options(error=recover)

recover {utils}
Browsing after an Error

This function allows the user to browse directly on any of the currently active function calls, and is suitable as an error option.
The expression options(error = recover) will make this the error option.

Usage
recover()

When called, recover prints the list of current calls, and prompts the user to select one of them.
The standard R browser is then invoked from the corresponding environment;
the user can type ordinary R language expressions to be evaluated in that environment.

Turning off the options() debugging mode in R
options(error=NULL)





<h2>Extract hyperlink from Excel file in R</h2>

#==========

library(XML)

# rename file to .zip
my.zip.file &lt;- sub("xlsx", "zip", my.excel.file)
file.copy(from = my.excel.file, to = my.zip.file)

# unzip the file
unzip(my.zip.file)

# unzipping produces a bunch of files which we can read using the XML package
# assume sheet1 has our data
xml &lt;- xmlParse("xl/worksheets/sheet1.xml")

# finally grab the hyperlinks
hyperlinks &lt;- xpathApply(xml, "//x:hyperlink/@display", namespaces="x")


<span class="redword">To repair Hyperlink address corrupted:</span>
copy file to desk top and rename to zip file
open zip file and locate: <span class="redword">\xl\worksheets\_rels</span>
open the sheet1.xml.rels with editor
remove all text: D:\Users\Lawht\AppData\Roaming\Microsoft\Excel\




<h2>Extract part of a string</h2>

#==========
x &lt;- c("75 to 79", "80 to 84", "85 to 89")
substr(x, start = 1, stop = 2)

substr(x, start, stop)
x &lt;- "1234567890"
substr(x, 5, 7)
"567"

<br>
<h2>alter grades</h2>

#==========
alter grades

locate the word
get the line location
alter the score table
#==========

locate the word
v &lt;- c('a','b','c','e')
'b' %in% v ## returns TRUE
match('b',v) ## returns the first location of 'b', in this case: 2

subv &lt;- c('a', 'f')
subv %in% v ## returns a vector TRUE FALSE
is.element(subv, v) ## returns a vector TRUE FALSE

which()
which('a' == v) #[1] 2 4 For finding all occurances as vector of indices

grep() returns a vector of integers, which indicate where matches are.
yo &lt;- c("a", "a", "b", "b", "c", "c")
grep("b", yo) # [1] 3 4

ROC&lt;-"中華民國 – 維基百科，自由的百科全書"
grep("中華民國",ROC)

Partial String Matching
pmatch("med", c("mean", "median", "mode")) # returns 2


<br>
<br>
<h2>table, cut and barplot</h2>

atab=c(1,2,3,2,1,2,3,4,5,4)
table(atab)
cut(atab, 2)
table( cut(atab, 2))
counts = table( cut(atab, 4))
barplot(counts, main="Qty", xlab="grade")


<br>
<br>
<br>
<h2>non-paste answer to concatenate two strings</h2>

capture.output(cat(counts, sep = ","))


<br>
<h2>V8 is an R interface JavaScript engine. </h2>

This package helps us execute javascript code in R

#Loading both the required libraries
library(rvest)
library(V8)

#URL with js-rendered content to be scraped
link &lt;- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'

#Read the html page content and extract all javascript codes that are inside a list

emailjs &lt;- read_html(link) %>% html_nodes('li') %>% html_nodes('script') %>% html_text()

# Create a new v8 context
ct &lt;- v8()

#parse the html content from the js output and print it as text
read_html(ct$eval(gsub('document.write','',emailjs))) %>%  html_text()

info@brewhemia.co.uk

Thus we have used rvest to extract the javascript code snippet from the desired location (that is coded in place of email ID) and used V8 to execute the javascript snippet (with slight code formatting) and output the actual email (that is hidden behind the javascript code). 

####################
Getting email address through rvest
You need a javascript engine here to process the js code.
R has got V8.

Modify your code after installing V8 package:
library(rvest)
library(V8)

link &lt;- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'
page &lt;- read_html(link)
name_html &lt;- html_nodes(page,'.placeHeading')
business_adr &lt;- html_text(adr_html)
tel_html &lt;- html_nodes(page,'.value')
business_tel &lt;- html_text(tel_html)
emailjs &lt;- page %>% html_nodes('li') %>% html_nodes('script') %>% html_text()
ct &lt;- v8()
read_html(ct$eval(gsub('document.write','',emailjs))) %>% html_text()


<br>
<br>
<h2>extract protected pdf document</h2>

library(pdftools)
setwd("C:/Users/User/Desktop")
txt &lt;- pdf_text("a.pdf")
str(txt)	# 361 pages
# copy page 1
writeClipboard(txt[1])
# copy page 2
writeClipboard(txt[2])
# copy page 3
writeClipboard(txt[3])

Convert unicode character to string format: remove "\u"

theStr = "\u9999\u6e2f\u98df\u54c1\u6295\u8d44"	#  "香港食品投资"

=============================

Sys.setlocale(category = 'LC_ALL', 'Chinese')

library(pdftools)
setwd("C:/Users/User/Desktop")
txt &lt;- pdf_text("45.pdf")
str(txt)

chi1 = gsub('\\u' , '&#x', txt[1])
chi2 = gsub('\\u' , '&#x', txt[2])
chi3 = gsub('\\u' , '&#x', txt[3])
sink("aaa.txt")
cat(chi1)
cat(chi2)
cat(chi3)
sink()



<br>
<br>
<h2>Writing an R package</h2>

  <a href="https://support.rstudio.com/hc/en-us/articles/200486488-Developing-Packages-with-RStudio">Develop Packages with RStudio</a>
<br>
<a href="http://web.mit.edu/insong/www/pdf/rpackage_instructions.pdf">rpackage_instructions.pdf</a>
<br>
<a href="https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/">Writing an R package from scratch</a>
<a href="Writing an R package.html"><span class="goldb">Writing an R package</span></a> 
<br>


<br>
<br>
<h2>table, cut and breaks</h2>

table(cut(as.numeric(resultTable[,3]), 10))
cut(as.numeric(resultTable[,3]),10)
breaks = c(seq(lower, 0, by = 5), 0, seq(0, upper, by = 5))

tableA = c(1,3,5,7,9)
tableB = c(1,3,5,7,2,4,6,8)
tableA = c(tableA, tableB)
tableA = sort(tableA)

table(tableA)

table(cut(tableA, 3))

breaks = c(seq(1, 3, by = 1), 4, seq(5, 9, by = 2))
table(cut(tableA, breaks))


<br>
<br>
<h2>List the Files in a Directory</h2>

List the Files in a Directory/Folder
list.files()

list.dirs(R.home("doc"))
list.dirs()

<br>
<br>
<h2>test url exist</h2>

library(httr)
http_error(theUrl)

<a href="https://stackoverflow.com/questions/18407177/load-image-from-website">Load image from website</a>
download.file("url", destfile="tmp.png", mode="wb")

<a href="https://jangorecki.gitlab.io/data.table/library/RCurl/html/url.exists.html">url.exists {RCurl}	</a> return true of false
<a href="https://stackoverflow.com/questions/31420210/r-check-existence-of-url-problems-with-httrget-and-url-exists">With httr use url_success()</a>
<br>

<br>
<br>
<h2>Passing arguments to R script</h2>

<a href="Passing arguments to R script.html"><span class="goldb">Passing arguments to R script</span></a> 

Rscript --vanilla testargument.R iris.txt newname

To avoid Rscript.exe loop forever for keyboard input:
use this:
cat("a string please: ");
a &lt;- readLines("stdin",n=1);


<br>
<br>
<h2>School Revision Papers</h2>

http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2018/
http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2017/
https://curriculum.gov.mt/en/Examination-Papers/Pages/list_secondary_papers.aspx
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F1-2ndTest-Eng.pdf
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F2-2ndTest-Eng.pdf
http://www.sttss.edu.hk/parents_corner/pastpaper.php

<br>
<br>
<h2>difference between 1L and 1</h2>

L specifies an integer type, rather than a double, it uses only 4 bytes per element
the function as.integer is simplified yb  "L " suffix

> str(1)
 num 1

> str(1L)
 int 1

<br>
<br>
<br>
<h2><span class="white goldbs">Datatable</span></h2>

<a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf">Datatable Cheat Sheet</a>
library(data.table)

dt=data.table(read.table("wAveTable.txt", header=TRUE, colClasses=c('character', 'numeric', 'numeric')))
colnames(dt)
"Code"   "WAve5"  "WAve10"
dt[WAve5 > 5, ]
summary(dt[WAve5 = 5, ])
summary(dt[WAve5 %between% c(7,9), ])

data.table dt subset rows using i, and manipulate columns with j, grouped according to by	dt[i, j, by]
Create a data.table	data.table(a = c(1, 2), b = c("a", "b"))
convert a data frame or a list to a data.table	setDT(df) or as.data.table(df)
Subset data.table rows using i	dt[1:2, ]
subset data.table rows based on values in one or more columns	dt[a > 5, ]
data.table Logical Operators To Use In i	>,<,<=,>=, |, !,&, is.na(),!is.na(), %in%, %like%,  %between%
data.table extract column(s) by number. Prefix column numbers with “-” to drop	dt[, c(2)]
data.table extract column(s) by name	dt[, .(b, c)]
create a data.table with new columns based on the summarized values of rows	dt[, .(x = sum(a))]
compute a data.table column based on an expression	dt[, c := 1 + 2]
compute a data.table column based on an expression but only for a subset of rows	dt[a == 1, c := 1 + 2]
compute a data.table multiple columns based on separate expressions	dt[, `:=`(c = 1 , d = 2)]
delete a data.table column	dt[, c := NULL]
convert the type of a data.table column using as.integer(), as.numeric(), as.character(), as.Date(), etc..	dt[, b := as.integer(b)]
group data.table rows by values in specified column(s)	dt[, j, by = .(a)]
group data.table and simultaneously sort rows according to values in specified column(s)	dt[, j, keyby = .(a)]
summarize data.table rows within groups	dt[, .(c = sum(b)), by = a]
create a new data.table column and compute rows within groups	dt[, c := sum(b), by = a]
extract first data.table row of groups	dt[, .SD[1], by = a]
extract last data.table row of groups	dt[, .SD[.N], by = a]
perform a sequence of data.table operations by chaining multiple “[]”	dt[…][…]
reorder a data.table according to specified columns	setorder(dt, a, -b), “-” for descending
data.table’s functions prefixed with “set” and the operator “:=”	work without “&lt;-” to alter data without making copies in memory
df &lt;- as.data.table(df)	setDT(df)
extract unique data.table rows based on columns specified in “by”. Leave out “by” to use all columns	unique(dt, by = c("a", "b"))
return the number of unique data.table rows based on columns specified in “by”	uniqueN(dt, by = c("a", "b"))
rename data.table column(s)	setnames(dt, c("a", "b"), c("x", "y"))
data.table Syntax	DT[ i , j , by], i refers to rows. j refers to columns. by refers to adding a group
data.table Syntax arguments	DT[ i , j , by], with, which, allow.cartesian, roll, rollends, .SD, .SDcols, on, mult, nomatch
data.table fread() function	to read data, mydata = fread("https://github.com/flights_2014.csv")
data.table select only 'origin' column returns a vector	dat1 = mydata[ , origin]
data.table select only 'origin' column returns a data.table	dat1 = mydata[ , .(origin)] or dat1 = mydata[, c("origin"), with=FALSE]
data.table select column	dat2 =mydata[, 2, with=FALSE]
data.table select column Multiple Columns	dat3 = mydata[, .(origin, year, month, hour)], dat4 = mydata[, c(2:4), with=FALSE]
data.table Dropping Column	adding ! sign, dat5 = mydata[, !c("origin"), with=FALSE]
data.table Dropping Multiple Columns	dat6 = mydata[, !c("origin", "year", "month"), with=FALSE]
data.table select variables that contain 'dep'	use %like% operator, dat7 = mydata[,names(mydata) %like% "dep", with=FALSE]
data.table Rename Variables	setnames(mydata, c("dest"), c("Destination"))
data.table  rename multiple variables	setnames(mydata, c("dest","origin"), c("Destination", "origin.of.flight"))
data.table find all the flights whose origin is 'JFK'	dat8 = mydata[origin == "JFK"]
data.table Filter Multiple Values	dat9 = mydata[origin %in% c("JFK", "LGA")]
data.table selects not equal to 'JFK' and 'LGA'	dat10 = mydata[!origin %in% c("JFK", "LGA")]
data.table Filter Multiple variables	dat11 = mydata[origin == "JFK" & carrier == "AA"]
data.table Indexing Set Key	tells system that data is sorted by the key column
data.table setting 'origin' as a key	setkey(mydata, origin), 'origin' key is turned on. data12 = mydata[c("JFK", "LGA")]
data.table Indexing Multiple Columns	setkey(mydata, origin, dest), key is turned on. mydata[.("JFK", "MIA")] # First key 'origin' matches “JFK” second key 'dest' matches “MIA”
data.table Indexing Multiple Columns equivalent	mydata[origin == "JFK" & dest == "MIA"]
data.table  identify the column(s) indexed by	key(mydata)
data.table sort data using setorder()	mydata01 = setorder(mydata, origin)
data.table sorting on descending order	mydata02 = setorder(mydata, -origin)
data.table Sorting Data based on multiple variables	mydata03 = setorder(mydata, origin, -carrier)
data.table Adding Columns (Calculation on rows)	use := operator, mydata[, dep_sch:=dep_time - dep_delay]
data.table Adding Multiple Columns	mydata002 = mydata[, c("dep_sch","arr_sch"):=list(dep_time - dep_delay, arr_time - arr_delay)]
data.table IF THEN ELSE Method I	mydata[, flag:= 1*(min < 50)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table IF THEN ELSE Method II	mydata[, flag:= ifelse(min < 50, 1,0)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table build a chain	DT[ ] [ ] [ ], mydata[, dep_sch:=dep_time - dep_delay][,.(dep_time,dep_delay,dep_sch)]
data.table Aggregate Columns mean	mydata[, .(mean = mean(arr_delay, na.rm = TRUE),
data.table Aggregate Columns median	median = median(arr_delay, na.rm = TRUE),
data.table Aggregate Columns min	min = min(arr_delay, na.rm = TRUE),
data.table Aggregate Columns max	max = max(arr_delay, na.rm = TRUE))]
data.table Summarize Multiple Columns	all the summary function in a bracket, mydata[, .(mean(arr_delay), mean(dep_delay))]
data.table .SD operator	implies 'Subset of Data'
data.table .SD and .SDcols operators	calculate summary statistics for a larger list of variables
data.table calculates mean of two variables	mydata[, lapply(.SD, mean), .SDcols = c("arr_delay", "dep_delay")]
data.table Summarize all numeric Columns	mydata[, lapply(.SD, mean)]
data.table Summarize with multiple statistics	mydata[, sapply(.SD, function(x) c(mean=mean(x), median=median(x)))]
data.table Summarize by group 'origin	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = origin]
data.table Summary by group useing keyby= operator	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), keyby = origin]
data.table Summarize multiple variables by group 'origin'	mydata[, .(mean(arr_delay, na.rm = TRUE), mean(dep_delay, na.rm = TRUE)), by = origin], or mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin]
data.table remove non-unique / duplicate cases with unique()	setkey(mydata, "carrier"), unique(mydata)
data.table remove duplicated	setkey(mydata, NULL), unique(mydata), Note : Setting key to NULL is not required if no key is already set.
data.table Extract values within a group	mydata[, .SD[1:2], by=carrier], selects first and second values from a categorical variable carrier.
data.table Select LAST value from a group	mydata[, .SD[.N], by=carrier]
data.table window function frank()	dt = mydata[, rank:=frank(-distance,ties.method = "min"), by=carrier], calculating rank of variable 'distance' by 'carrier'. 
data.table cumulative sum cumsum()	dat = mydata[, cum:=cumsum(distance), by=carrier]
data.table lag and lead with shift()	shift(variable_name, number_of_lags, type=c("lag", "lead")), DT &lt;- data.table(A=1:5), DT[ , X := shift(A, 1, type="lag")], DT[ , Y := shift(A, 1, type="lead")]
data.table  %between% operator to define a range	DT = data.table(x=6:10), DT[x %between% c(7,9)]
data.table %like% to find all the values that matches a pattern	DT = data.table(Name=c("dep_time","dep_delay","arrival"), ID=c(2,3,4)), DT[Name %like% "dep"] 
data.table Inner Join	Sample Data: (dt1 &lt;- data.table(A = letters[rep(1:3, 2)], X = 1:6, key = "A")), (dt2 &lt;- data.table(A = letters[rep(2:4, 2)], Y = 6:1, key = "A")), merge(dt1, dt2, by="A")
data.table Left Join	merge(dt1, dt2, by="A", all.x = TRUE)
data.table Right Join	merge(dt1, dt2, by="A", all.y = TRUE)
data.table Full Join	merge(dt1, dt2, all=TRUE)
Convert a data.table to data.frame	setDF(mydata)
convert data frame to data table	setDT(), setDT(X, key = "A")
data.table Reshape Data	dcast.data.table() and melt.data.table()
data.table Calculate total number of rows by month and then sort on descending order	mydata[, .N, by = month] [order(-N)], The .N operator is used to find count.
data.table Find top 3 months with high mean arrival delay	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = month][order(-mean_arr_delay)][1:3]
data.table Find origin of flights having average total delay is greater than 20 minutes	mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin][(arr_delay + dep_delay) > 20]
data.table Extract average of arrival and departure delays for carrier == 'DL' by 'origin' and 'dest' variables	mydata[carrier == "DL", lapply(.SD, mean, na.rm = TRUE), by = .(origin, dest), .SDcols = c("arr_delay", "dep_delay")]
data.table Pull first value of 'air_time' by 'origin' and then sum the returned values when it is greater than 300	mydata[, .SD[1], .SDcols="air_time", by=origin][air_time > 300, sum(air_time)]


<br>
<br>
<h2><span class="gold bordred1 blink">R Web Scraping</span></h2>
<a href="https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples" class="whitebut red bluebs blueblackgrad whitets blinkNmove">get started xpath selectors</a>

<a href="https://www.datacamp.com/community/tutorials/r-web-scraping-rvest">R Web Scraping Rvest</a>
<a href="http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl/">webscraping-using-readlines-and-rcurl</a>
<a href="https://www.rdocumentation.org/packages/XML/versions/3.98-1.16/topics/xmlTreeParse">xmlTreeParse, htmlTreeParse</a>
<a href="http://www.cse.chalmers.se/~chrdimi/downloads/web/getting_web_data_r4_parsing_xml_html.pdf">getting web data parsing xml html</a>
<a href="https://stackoverflow.com/questions/35479549/error-in-r-no-applicable-method-for-xpathapply">error in r no applicable method for xpathapply</a>
<a href="https://blog.rstudio.com/2015/04/21/xml2/">Parse and process XML (and HTML) with xml2</a>
==================
web_page &lt;- readLines("http://www.interestingwebsite.com")
web_page &lt;- read.csv("http://www.programmingr.com/jan09rlist.html")

    # General-purpose data wrangling
    library(tidyverse)  

    # Parsing of HTML/XML files  
    library(rvest)    

    # String manipulation
    library(stringr)   

    # Verbose regular expressions
    library(rebus)     

    # Eases DateTime manipulation
    library(lubridate)

==================
install.packages("RCurl", dependencies = TRUE)
library("RCurl")
library("XML")

past &lt;- getURL("http://www.iciba.com/past", ssl.verifypeer = FALSE)	# getURL cannot work
webpage &lt;- read_html("http://www.iciba.com/past")	# getURL cannot work

jan09_parsed &lt;- htmlTreeParse(jan09)

==================
http://www.iciba.com/past
ul class="base-list switch_part" class

library('rvest')
library(tidyverse)
url &lt;- 'http://www.iciba.com/past'
webpage &lt;- readLines(url, warn=FALSE)
webpage &lt;- read_html(webpage)
grappedData &lt;- html_nodes(webpage,'.base-list switch_part')

parseData = htmlTreeParse(webpage)

rank_data &lt;- html_text(grappedData)

html_node("#mw-content-text > div > table:nth-child(18)")
html_table()

the function htmlParse() which is equivalent to xmlParse(file, isHTML = TRUE)
output = htmlParse(webpage)
class(output)

To parse content into an R structure :
htmlTreeParse() which is equivalent to htmlParse(file, useInternalNodes = FALSE)
output = htmlTreeParse(webpage)
class(output)

htmlTreeParse(file) especially suited for parsing HTML content
returns class "XMLDocumentContent" (R data structure)
equivalent to
xmlParse(file, isHTML = TRUE, useInternalNodes = FALSE)
htmlParse(file, useInternalNodes = FALSE)

root =xmlRoot(output)
xmlChildren(output)
xmlChildren(xmlRoot(output))
XMLNodeList

Functions for a given node
Function Description
xmlName() name of the node
xmlSize() number of subnodes
xmlAttrs() named character vector of all attributes
xmlGetAttr() value of a single attribute
xmlValue() contents of a leaf node
xmlParent() name of parent node
xmlAncestors() name of ancestor nodes
getSibling() siblings to the right or to the left
xmlNamespace() the namespace (if there’s one)

to parse HTML tables using R
sched &lt;- readHTMLTable(html, stringsAsFactors = FALSE)

The html.raw object is not immediately useful because it literally contains all of the raw HTML for the entire webpage. We can parse the raw code using the xpathApply function which parses HTML based on the path argument, which in this case specifies parsing of HTML using the paragraph tag.

html.raw&lt;-htmlTreeParse('http://www.dnr.state.mn.us/lakefind/showreport.html?downum=27013300',
    useInternalNodes=T    )
html.parse&lt;-xpathApply(html.raw, "//p", xmlValue)

# evaluate input and convert to text
txt &lt;- htmlToText(url)

==================
url &lt;- 'http://www.iciba.com/past'
webpage &lt;- readLines(url, warn=FALSE)
scraping_wiki &lt;- read_html(webpage)
scraping_wiki %>% html_nodes("h1") %>% html_text()

url &lt;- 'testvibrate.html'
webpage &lt;- readLines(url, warn=FALSE)
x &lt;- read_xml(webpage)
xml_name(x)
===========

This cannot work in office
library(rvest)
Sys.setlocale(category = 'LC_ALL', 'Chinese')
webpage &lt;- read_html("http://www.iciba.com/haunt")
ullist = webpage %>% html_nodes("ul")
content = ullist[2] %>% html_text()
content = gsub("n.| |\n|adj.|adv.|prep.|vt.|vi.|&","",content)
content = gsub("，|；"," ",content) %>%  strsplit(split = " ") %>% unlist() %>% sort() %>% unique()
paste0("past","\t",capture.output(cat(content)))


<br>
<br>
<h2>R scraping html text example</h2>

<a href="http://bradleyboehmke.github.io/2015/12/scraping-html-text.html">scraping-html-text</a>
library(rvest)

scraping_wiki &lt;- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scraping_wiki %>% html_nodes("h1")
scraping_wiki %>% html_nodes("h2")
scraping_wiki %>% html_nodes("h1") %>% html_text()
scraping_wiki %>% html_nodes("h2") %>% html_text()
p_nodes &lt;- scraping_wiki %>% html_nodes("p")
length(p_nodes)
p_text &lt;- scraping_wiki %>% html_nodes("p") %>% html_text()
p_text[1]
p_text[5]

ul_text &lt;- scraping_wiki %>% html_nodes("ul") %>% html_text()
length(ul_text)
ul_text[1]
substr(ul_text[2], start = 1, stop = 200)

li_text &lt;- scraping_wiki %>% html_nodes("li") %>% html_text()
length(li_text)
li_text[1:8]
li_text[104:136]

all_text &lt;- scraping_wiki %>% html_nodes("div") %>%  html_text()

body_text &lt;- scraping_wiki %>% html_nodes("#mw-content-text") %>%  html_text()

# read the first 207 characters
substr(body_text, start = 1, stop = 207)

# read the last 73 characters
substr(body_text, start = nchar(body_text)-73, stop = nchar(body_text))

# Scraping a specific heading
scraping_wiki %>% html_nodes("#Techniques") %>%  html_text()
## [1] "Techniques"

# Scraping a specific paragraph
scraping_wiki %>% html_nodes("#mw-content-text > p:nth-child(20)") %>%  html_text()

# Scraping a specific list
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

# Scraping a specific reference list item
scraping_wiki %>% html_nodes("#cite_note-22") %>%  html_text()

# Cleaning up
library(magrittr)
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text() %>%  strsplit(split = "\n") %>% unlist() %>% .[. != ""]


library(stringr)

# read the last 700 characters
substr(body_text, start = nchar(body_text)-700, stop = nchar(body_text))

# clean up text
body_text %>% 
str_replace_all(pattern = "\n", replacement = " ") %>% 
str_replace_all(pattern = "[\\^]", replacement = " ") %>% 
str_replace_all(pattern = "\"", replacement = " ") %>% 
str_replace_all(pattern = "\\s+", replacement = " ") %>% 
str_trim(side = "both") %>% 
substr(start = nchar(body_text)-700, stop = nchar(body_text))

################
# rvest tutorials
https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/
https://blog.gtwang.org/r/rvest-web-scraping-with-r/
https://www.rdocumentation.org/packages/rvest/versions/0.3.4
https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/
https://lmyint.github.io/post/dnd-scraping-rvest-rselenium/

################
# parse guancha
library(rvest)
pageHeader="https://user.guancha.cn/main/content?id=181885"
pagesource &lt;- read_html(pageHeader)

################
# parse RTHK and metroradio
library(rvest)
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource &lt;- read_html(pageHeader)
className = ".ns2-title"
keywordList &lt;- html_nodes(pagesource, className)
html_text(keywordList)

pageHeader = "http://www.metroradio.com.hk/MetroFinance/News/NewsLive.aspx"
pagesource &lt;- read_html(pageHeader)
className = ".n13newslist"
keywordList &lt;- html_nodes(pagesource, className)
className = "a"
keywordList &lt;- html_nodes(keywordList, className)
html_text(keywordList)

################
# parse xhamster
library(rvest)
pageHeader = "https://xhamster.com/users/fredlake/photos"
pagesource &lt;- read_html(pageHeader)
className = ".xh-paginator-button"
keywordList &lt;- html_nodes(pagesource, className)
html_text(keywordList)
html_name(keywordList)
html_attrs(keywordList)

thelist = unlist(html_attrs(keywordList))

length(keywordList)
as.numeric(html_text(keywordList[length(keywordList)]))

pagesource %>% html_nodes(className) %>% html_text() %>% as.numeric()

for ( i in keywordList ) { 
 qlink &lt;- html_nodes(s, ".gallery-thumb")
 cat("Title:", html_text(qlink), "\n")
 qviews &lt;- html_nodes(s, "name")
 cat("Views:", html_text(qviews), "\n")
}
################
# parse text and href
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource &lt;- read_html(pageHeader)

className = ".ns2-title"
keywordList &lt;- html_nodes(pagesource, className)

className = "a"
a &lt;- html_nodes(keywordList, className)

html_text(a)
html_attr(a, "href")

################
# extract huanqiu.com gallery

pageHeader = "https://china.huanqiu.com/gallery/9CaKrnQhXac"
pagesource &lt;- read_html(pageHeader)
className = "article"
keywordList &lt;- html_nodes(pagesource, className)

className = "img"
img &lt;- html_nodes(keywordList, className)
html_attr(img, "src")
html_attr(img, "data-alt")

################
# html_nodes samples
html_nodes(".a1.b1")
html_nodes(".b1:not(.a1)")  # <i>Select class contains b1 not a1:</i>
html_nodes(".content__info__item__value")
html_nodes("[class='b1']")
html_nodes("center")
html_nodes("font")
html_nodes(ateam, "center")
html_nodes(ateam, "center font")
html_nodes(ateam, "center font b")
html_nodes("table") %>% .[[3]] %>% html_table()
html_nodes("td")
html_nodes() returns all nodes
html_nodes(pagesource, className)
html_nodes(pg, "div > input:first-of-type"), "value")
html_nodes(s, ".gallery-thumb")
html_nodes(s, "name")
html_nodes(xpath = '//*[@id="a"]')

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td &lt;- ateam %>% html_nodes("center") %>% html_nodes("td")
td %>% html_nodes("font")

if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_node("font")
}

# To pick out an element at specified position, use magrittr::extract2
# which is an alias for [[
library(magrittr)
ateam %>% html_nodes("table") %>% extract2(1) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% `[[`(1) %>% html_nodes("img")

# Find all images contained in the first two tables
ateam %>% html_nodes("table") %>% `[`(1:2) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% extract(1:2) %>% html_nodes("img")

# XPath selectors ---------------------------------------------
# If you prefer, you can use xpath selectors instead of css: 
html_nodes(doc, xpath = "//table//td")).

# chaining with XPath is a little trickier - you may need to vary
# the prefix you're using - // always selects from the root node
# regardless of where you currently are in the doc
ateam %>% html_nodes(xpath = "//center//font//b") %>% html_nodes(xpath = "//b")


read_html()
html_node()	# to find the first node
html_nodes(doc, "table td")	# to find the all node
html_nodes(doc, xpath = "//table//td"))

html_name()	# the name of the tag
html_tag()	# Extract the tag names
html_text()	# Extract all text inside the tag 

html_attr()	Extract the a single attribute
html_attrs()	Extract all the attributes

# html_attrs(keywordList) this cannot use id, just list all details
# html_attr(keywordList, "id") this select the ids
# html_attr(keywordList, "href") this select the hrefs

html_nodes("#titleCast .itemprop span")
html_nodes("#img_primary img")
html_nodes("div.name > strong > a")
html_attr("href")

html_text(keywordList, trim = FALSE)
html_name(keywordList)
html_children(keywordList)
html_attrs(keywordList)
html_attr(keywordList, "[href]", default = NA_character_)

parse with xml()
then extract components using 
xml_node()
xml_attr()
xml_attrs()
xml_text() and xml_name()

Parse tables into data frames with 
html_table().

Extract, modify and submit forms with 
html_form()
set_values()
submit_form().

Detect and repair encoding problems with 
guess_encoding()	Detect text encoding
repair_encoding()	repair text encoding

Navigate around a website as if you’re in a browser with 
html_session()
jump_to()
follow_link()
back()
forward()

Extract, modify and submit forms with 
html_form(), 
set_values() 
and submit_form()


The toString() function collapse the list of strings into one.

html_node(":not(#commentblock)")	# exclude tags

######### demos #########
# Inspired by https://github.com/notesofdabbler
library(rvest)
library(tidyr)

page &lt;- read_html("http://www.zillow.com/homes/for_sale/....")

houses &lt;- page %>% html_nodes(".photo-cards li article")
z_id &lt;- houses %>% html_attr("id")

address &lt;- houses %>% html_node(".zsg-photo-card-address") %>% html_text()

price &lt;- houses %>% html_node(".zsg-photo-card-price") %>% html_text() %>% readr::parse_number()

params &lt;- houses %>% html_node(".zsg-photo-card-info") %>% html_text() %>% strsplit("\u00b7")

beds &lt;- params %>% purrr::map_chr(1) %>% readr::parse_number()
baths &lt;- params %>% purrr::map_chr(2) %>% readr::parse_number()
house_area &lt;- params %>% purrr::map_chr(3) %>% readr::parse_number()

################
pagesource %>% html_nodes("table") %>% .[[3]] %>% html_table()

read_html(doc) %>% html_nodes(".b1:not(.a1)") # <i>Select class contains b1 not a1:</i>
# [1] <span class="b1"> text2 </span>

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")
# [1] <span class="b1"> text2 </span>

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
# [1] <span class="a1 b1"> text1 </span>

combine class and ID in CSS selector
div#content.sectionA  # <i>this is 'and' operation</i>

=====================
<i>select 2 classes in 1 tag</i>
Select class contains b1 not a1:
read_html(doc) %>% html_nodes(".b1:not(.a1)")

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
=====================
standard CSS selector specify either or both

html_nodes(".content__info__item__value, skill")  # <i>the comma is 'or' operation</i>
{xml_nodeset (4)}
[1] <span class="content__info__item__value duration">5h 59m 42s</span>
[2] <span class="content__info__item__value skill">Beginner + Intermediate</span>
[3] <span class="content__info__item__value released">September 26, 2013</span>
[4] <span class="content__info__item__value viewers">82,552</span>

# has both classes in_learning_page
html_nodes(".content__info__item__value.skill")   # <i>this is 'and' operation</i>
{xml_nodeset (1)}
[1] <span class="content__info__item__value skill">Beginner + Intermediate</span>


in_learning_page %>%
  html_nodes(".content__info__item__value") %>% 
  str_subset(., "viewers")

h &lt;- read_html(text)

h %>% html_nodes(xpath = '//*[@id="a"]') %>% xml_attr("value")

html_attr(html_nodes(pg, "div > input:first-of-type"), "value")

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td &lt;- ateam %>% html_nodes("center") %>% html_nodes("td")

# When applied to a list of nodes, html_nodes() returns all nodes,
# collapsing results into a new nodelist.
td %>% html_nodes("font")
# nodes, it returns a "missing" node
if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_nodes("font")


<br>
<br>
<h2>sort() rank() order()</h2>

<strong>Rank</strong> references the position of the value in the sorted vector and is in the same order as the <strong>original </strong>sequence
<strong>Order</strong> returns the position of the original value and is in the order of <strong>sorted </strong>sequence
The graphic below helps tie together the values reported by rank and order with the positions from which they come.
<img class="lazy" data-src="https://cdn-images-1.medium.com/max/800/1*3KeaXU6luJDyoatWkEwdug.jpeg">
x = c(1, 8,9, 4)
sort(x)
1 4 8 9

# the original position in the sorted order
rank(x)
1 3 4 2

# the sorted position in the original position
order(x)
1 4 2 3

<br>
<br>
<h2>Bioinformatics</h2>

<a href="https://cran.r-project.org/doc/contrib/Krijnen-IntroBioInfStatistics.pdf">Bioinformatics using R</a>
<br>
<a href="https://www.bioconductor.org/">bioconductor</a>
<br>
<a href="https://www.r-exercises.com/product/introduction-to-bioconductor-annotation-and-analysis-of-genomes-and-genomic-assays/">Introduction to Bioconductor:Annotation and Analysis of Genomes and Genomics Assays</a>

<br>
<h2>a list of dataframes, 3D data arrangement</h2>

d1 &lt;- data.frame(y1=c(1,2,3),y2=c(4,5,6))
d2 &lt;- data.frame(y1=c(3,2,1),y2=c(6,5,4))
d3 &lt;- data.frame(y1=c(7,8,9),y2=c(5,2,6))
mylist &lt;- list(d1, d2, d3)
names(mylist) &lt;- c("List1","List2","List3")

mylist[1]	# same as mylist$List1

mylist[[2]][1,2]	# access an element inside a dataframe
mylist[[2]][2,2]	# same as mylist$List2[2,2]

to concate another dataframe:

d4 &lt;- data.frame(y1=c(2,5,8),y2=c(1,4,7))
mylist[[4]] &lt;- d4

to create an empty list:
data &lt;- list()

<br>
<br>
<h2>format time string</h2>

Sys.time()

sub(".* | .*", "", Sys.time())

format(Sys.time(), '%H:%M')

gsub(":", "", format(Sys.time(), '%H:%M'))


<br>
<h2>extract 5 digit from string</h2>

activityListCode = str_replace(activityListCode, ".*\\b(\\d{5})\\b.*", "\\1")


<br>
<h2>access Components of a Data Frame</h2>
<a href="https://www.datamentor.io/r-programming/data-frame/">access Components of a Data Frame</a>
<br>

Components of data frame can be accessed like a list or like a matrix.

<h3>Accessing like a list</h3>
We can use either <code>[</code>, <code>[[</code> or <code>$</code> operator to access columns of data frame.

<code>&gt; x["Name"]
Name
1 John
2 Dora
&gt; x$Name
[1] "John" "Dora"
&gt; x[["Name"]]
[1] "John" "Dora"
&gt; x[[3]]
[1] "John" "Dora"
</code>
Accessing with <code>[[</code> or <code>$</code> is similar. However, it differs for <code>[</code> in that, indexing with <code>[</code> <span class="redword">will return us a data frame</span> but the other two will <span class="redword">reduce it into a vector</span>.


<h3>Accessing like a matrix</h3>
Data frames can be accessed like a matrix by providing index for row and column.

To illustrate this, we use datasets already available in R. Datasets that are available can be listed with the command <code>library(help = "datasets")</code>.

We will use the <code>trees</code> dataset which contains <code>Girth</code>, <code>Height</code> and <code>Volume</code> for Black Cherry Trees.

A data frame can be examined using functions like <code>str()</code> and <code>head()</code>.

<code>&gt; str(trees)
'data.frame':   31 obs. of 3 variables:
$ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...
$ Height: num  70 65 63 72 81 83 66 75 80 75 ...
$ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...
&gt; head(trees,n=3)
Girth Height Volume
1   8.3     70   10.3
2   8.6     65   10.3
3   8.8     63   10.2
</code>
We can see that <code>trees</code> is a data frame with 31 rows and 3 columns. We also display the first 3 rows of the data frame.

Now we proceed to access the data frame like a matrix.

<code>&gt; trees[2:3,]    # select 2nd and 3rd row
Girth Height Volume
2   8.6     65   10.3
3   8.8     63   10.2
&gt; trees[trees$Height &gt; 82,]    # selects rows with Height greater than 82
Girth Height Volume
6   10.8     83   19.7
17  12.9     85   33.8
18  13.3     86   27.4
31  20.6     87   77.0
&gt; trees[10:12,2]
[1] 75 79 76
</code>
We can see in the last case that the returned type is a vector since we extracted data from a single column.

This behavior can be avoided by passing the argument <code>drop=FALSE</code> as follows.

<code>&gt; trees[10:12,2, drop = FALSE]
Height
10     75
11     79
12     76
</code>


# access first row by index, returns a data.frame
x[1,]

# access first row by "name", returns a data.frame
> x["1",]

# access first row returns a vector
use as.numeric
str(as.numeric(wAveTable["1",]))

unlist which keeps the names.
str(unlist(wAveTable["1",]))

use transpose and as.vector
str(as.vector(t(wAveTable["1",])[,1]))

use only as.vector cannot convert to vector
str(as.vector(wAveTable["1",]))

# convert dataframe to matrix
data.matrix(wAveTable)


<br>
<br>
<h2>read.csv as character</h2>

wAveTable = read.csv("wAveTable.txt", sep="\t", colClasses=c('character', 'character', 'character'))

<br>
<br>
<h2>frequency manipulation</h2>

grade = c("low", "high", "medium", "high", "low", "medium", "high")

# using factor to count the frequency
foodfac &lt;- factor(grade)
summary(foodfac)
max(summary(foodfac))
min(summary(foodfac))
levels(foodfac)
nlevels(foodfac)
summary(levels(foodfac))

# use of table to count frequency:
table(grade)
sort(table(grade))

table(grade)[1]
max(table(grade))
summary(table(grade))

# this locate the max item:
table(grade)[which(table(grade) == max(table(grade)))]

# change to dataframe and find the max item:
theTable = as.data.frame(table(grade))
theTable[which(theTable$Freq == max(theTable$Freq)),]

# use of the count function in plyr:
library(plyr)
count(grade)
count(mtcars, 'gear')

# use of the which function:
which(letters == "g")
x &lt;- c(1,5,8,4,6)
which(x == 5)
which(x != 5)


<br>
<br>
<h2>5 must have R programming tools</h2>

<h4>1) RStudio</h4>
<h4>2) lintr</h4>
If you come from the world of Python, you’ve probably heard of 
<a href="https://stackoverflow.com/questions/8503559/what-is-linting" data-href="https://stackoverflow.com/questions/8503559/what-is-linting" rel="nofollow noopener" target="_blank">linting</a>. 
Essentially, linting 
<a href="https://en.wikipedia.org/wiki/Lint_%28software%29" data-href="https://en.wikipedia.org/wiki/Lint_%28software%29" rel="nofollow noopener" target="_blank">analyzes</a> your code for readability. 
It makes sure you don’t produce code that looks like this:
# This is some bad R code
<br>if ( mean(x,na.rm=T)==1) { print(“This code is bad”); } # Still bad code because this line is SO long
There are 
<em>many</em> things wrong with this code. 
For starters, the code is too long. 
Nobody likes to read code with seemingly endless lines. 
There are also no spaces after the comma in the 
<code>mean()</code> function, or any spaces between the 
<code>==</code> operator. 
Oftentimes data science is done hastily, but linting your code is a good reminder for creating portable and understandable code. 
After all, if you can’t explain what you are doing or how you are doing it, your data science job is incomplete. 

<a href="https://cran.r-project.org/web/packages/lintr/index.html" data-href="https://cran.r-project.org/web/packages/lintr/index.html" rel="nofollow noopener" target="_blank">lintr</a> is an R package, growing in popularity, that allows you to lint your code. 
Once you install lintr, linting a file is as easy as 
<code>lint(&quot;filename.R&quot;)</code> .
<h4>3) Caret</h4>
<a href="http://topepo.github.io/caret/index.html" data-href="http://topepo.github.io/caret/index.html" rel="nofollow noopener" target="_blank">Caret</a>, which you can find on 
<a href="https://cran.r-project.org/web/packages/caret/caret.pdf" data-href="https://cran.r-project.org/web/packages/caret/caret.pdf" rel="nofollow noopener" target="_blank">CRAN</a>, is central to a data scientist’s toolbox in R. 
Caret allows one to quickly develop models, set cross-validation methods and analyze model performance all in one. 
Right out of the box, Caret abstracts the various interfaces to user-made algorithms and allows you to swiftly create models from averaged neural networks to boosted trees. 
It can even handle parallel processing. 
Some of the models caret includes are: AdaBoost, Decision Trees &amp; Random Forests, Neural Networks, Stochastic Gradient Boosting, nearest neighbors, support vector machines — among the most commonly used machine learning algorithms.
<h4>4) Tidyverse</h4>
You may not have heard of 
<code>tidyverse</code> as a whole, but chances are, you’ve used one of the packages in it. 
Tidyverse is a set of unified packages meant to make data science… 
<em>easyr</em> (classic R pun). 
These packages alleviate many of the problems a data scientist may run into when dealing with data, such as loading data into your workspace, manipulating data, tidying data or visualizing data. 
Undoubtedly, these packages make dealing with data in R more efficient.
It’s incredibly easy to get Tidyverse, you just run 
<code>install.packages(&quot;tidyverse&quot;)</code> and you get:
<a href="http://ggplot2.tidyverse.org/" data-href="http://ggplot2.tidyverse.org/" rel="nofollow noopener" target="_blank">ggplot2</a>: A popular R package for creating graphics
<a href="http://dplyr.tidyverse.org/" data-href="http://dplyr.tidyverse.org/" rel="nofollow noopener" target="_blank">dplyr</a>: A popular R package for efficiently manipulating data
tidyr: An R package for tidying up data sets
<a href="http://readr.tidyverse.org/" data-href="http://readr.tidyverse.org/" rel="nofollow noopener" target="_blank">readr</a>: An R package for reading in data
<a href="http://purrr.tidyverse.org/" data-href="http://purrr.tidyverse.org/" rel="nofollow noopener" target="_blank">purrr</a>: An R package which extends R’s functional programming toolkit
<a href="http://tibble.tidyverse.org/" data-href="http://tibble.tidyverse.org/" rel="nofollow noopener" target="_blank">tibble</a>: An R package which introduces the 
<em>tibble (tbl_df)</em>, an enhancement of the data frame
By and large, ggplot2 and dplyr are some of the most common packages in the R sphere today, and you’ll see countless posts on StackOverflow on how to use either package.

<em>(Fine Print: Keep in mind, you can’t just load everything with </em>
<code>
<em>library(tidyverse)</em></code>
<em> you must load each individually!)</em>
<h4>5) Jupyter Notebooks or R Notebooks</h4>
Data science 
<em>MUST</em> be transparent and reproducible. 
For this to happen, we have to see your code! The two most common ways to do this are through 
<a href="http://jupyter.org/" data-href="http://jupyter.org/" rel="nofollow noopener" target="_blank">Jupyter Notebooks</a> or 
<a href="http://rmarkdown.rstudio.com/r_notebooks.html" data-href="http://rmarkdown.rstudio.com/r_notebooks.html" rel="nofollow noopener" target="_blank">R Notebooks</a>.
Essentially, a notebook (of either kind) allows you to run R code block by block, and show output block my block. 
We can see on the left that we are summarizing the data, then checking the output. 
After, we plot the data, then view the plot. 
All of these actions take place within the notebook, and it makes analyzing both output and code a simultaneous process. 
This can help data scientists collaborate and ease the friction of having to open up someone’s code and understand what it does. 
Additionally, notebooks also make data science 
<em>reproducible</em>, which gives validity to whatever data science work you do!
<h4>Honorable Mention: Git</h4>
Last but not least, I want to mention Git. 
Git is a 
<em>version control</em> system. 
So why use it? Well, it’s in the name. 
Git allows you to keep versions of the code you are working on. 
It also allows multiple people to work on the same project and allows those changes to be attributed to certain contributors. 
You’ve probably heard of 
<a href="http://www.github.com" data-href="http://www.github.com" rel="nofollow noopener" target="_blank">Github</a>, undoubtedly one of the most popular git servers.
You can visit my website at 
<a href="http://www.peterxeno.com" data-href="http://www.peterxeno.com" rel="nofollow noopener" target="_blank">www.peterxeno.com</a> and my Github at 
<a href="https://github.com/peterxeno" data-href="https://github.com/peterxeno" rel="nofollow noopener" target="_blank">www.github.com/peterxeno</a>


<h2>R with Javascript</h2>
<a href="R and D3.html">R and D3</a>
<a href="https://www.opencpu.org/posts/js-release-0-1/">tools for working with JavaScript in R</a>
<a href="http://www.di.fc.ul.pt/~jpn/r/langs/javascript.html">R Connecting with Javascript</a>

<h2><span class="white bordgreen1">tryCatch</span></h2>
<a href="https://codeday.me/bug/20170502/13495.html">如何在R中写trycatch</a>
<a href="https://stackoverflow.com/questions/12193779/how-to-write-trycatch-in-r">write trycatch in R</a>
<a href="http://mazamascience.com/WorkingWithData/?p=912">error Handing with tryCatch()</a>

e.g.
readUrl = function(url) {
    out = tryCatch(
        {
            message("This is the 'try' part")
            readLines(con=url, warn=FALSE) 
        },
        error=function(cond) {
            message(paste("URL does not seem to exist:", url))
            message("Here's the original error message:")
            message(cond)
            # Choose a return value in case of error
            return(NA)
        },
        warning=function(cond) {
            message(paste("URL caused a warning:", url))
            message("Here's the original warning message:")
            message(cond)
            # Choose a return value in case of warning
            return(NULL)
        },
        finally={
        # Here goes everything that should be executed at the end,
        # regardless of success or error.
        # If you want more than one expression to be executed, then you 
        # need to wrap them in curly brackets ({...}); otherwise you could
        # just have written 'finally={expression}' 
            message(paste("Processed URL:", url))
            message("Some other message at the end")
        }
    )    
    return(out)
}

e.g.
x &lt;- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});

e.g.
for (i in urls) {
    tmp &lt;- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}

a retry function:
retry &lt;- function(dothis, max = 10, init = 0){
	suppressWarnings( tryCatch({
		if(init&lt;max) dothis}, 
			error = function(e){retry(dothis, max, init = init+1)}
		)
	)
}
dothis &lt;- function(){do somthing}

<h2>Download Image</h2>
<a href="https://stackoverflow.com/questions/29110903/how-to-download-and-display-an-image-from-an-url-in-r">Download Image</a>
<br>
If I try your code it looks like the image is downloaded. However, when opened with windows image viewer it also says it is corrupt. The reason for this is that you don't have specified the mode in the download.file statement.

Try this:

download.file(y,'y.jpg', mode = 'wb')

download.file('http://78.media.tumblr.com/83a81c41926c1da585916a5c092b4789/tumblr_or0y0vdjOP1rttk8po1_1280.jpg','y.jpg', mode = 'wb')

To view the image in R, have a look at

library(jpeg)
jj &lt;- readJPEG("y.jpg",native=TRUE)
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(jj,0,0,1,1)

<h2>Download Something</h2>
Download Something
Download Something
Download Something

<h2>testShiny</h2>
setwd("D:/KPC/testShiny")
runApp("D:/KPC/testShiny")


<h2>Error in file(filename, "r", encoding = encoding)</h2>
The error indicate that either the file doesn't exist or the source() command an incorrect path. 

<h2>call a R program from another R program</h2>
source("program_B.R")

<h2>to view all the functions present in a package</h2>

To list all objects in the package use ls
ls("package:Hmisc")
Note that the package must be attached.

To list all strings
lsf.str("package:dplyr")
lsf.str("package:Hmisc")

To see the list of currently loaded packages use
search()

Alternatively calling the help would also do, even if the package is not attached:
help(package = dplyr)
help(package = Hmisc)

Finally, use RStudio which provides an autocomplete function.
So, for instance, typing Hmisc:: in the console or while editing a file will result in a popup list of all dplyr functions/objects.



<h2>cut2</h2>

Function like cut but left endpoints are inclusive.

install.packages("Hmisc")
library(Hmisc)

alist = c(-15,18,2,5,4,-7,-5,-3,-1,0,2,1,5,4,6)
breaks = c(-5,-3,-1,0,1,3,5)
table(cut2(alist, breaks))

<h2>Reference A Data Frame Column</h2>

with the double square bracket "[[]]" operator.
LastDayTable[["Vol"]] 
or
LastDayTable$Vol
or
<span class="cyanword">LastDayTable[,"Vol"] </span>

<h2>Writing data to a file</h2>
<h3 id="problem">Problem</h3>

You want to write data to a file.


<h3 id="solution">Solution</h3>

<h3 id="writing-to-a-delimited-text-file">Writing to a delimited text file</h3>

The easiest way to do this is to use <code>write.csv()</code>. By default, <code>write.csv()</code> includes row names, but these are usually unnecessary and may cause confusion.


<code># A sample data frame
data &lt;- read.table(header=TRUE, text='
 subject sex size
       1   M    7
       2   F    NA
       3   F    9
       4   M   11
 ')


# Write to a file, suppress row names
write.csv(data, "data.csv", row.names=FALSE)

# Same, except that instead of "NA", output blank cells
write.csv(data, "data.csv", row.names=FALSE, na="")

# Use tabs, suppress row names and column names
write.table(data, "data.csv", sep="\t", row.names=FALSE, col.names=FALSE) 
</code>

<h3>Saving in R data format</h3>

<code>write.csv()</code> and <code>write.table()</code> are best for interoperability with other data analysis programs. They will not, however, preserve special attributes of the data structures, such as whether a column is a character type or factor, or the order of levels in factors. In order to do that, it should be written out in a special format for R.


Below are are three primary ways of doing this:


The first method is to output R source code which, when run, will re-create the object. This should work for most data objects, but it may not be able to faithfully re-create some more complicated data objects.


<code># Save in a text format that can be easily loaded in R
dump("data", "data.Rdmpd")
# Can save multiple objects:
dump(c("data", "data1"), "data.Rdmpd")

# To load the data again: 
source("data.Rdmpd")
# When loaded, the original data names will automatically be used.
</code>

The next method is to write out individual data objects in RDS format. This format can be binary or ASCII. Binary is more compact, while ASCII will be more efficient with version control systems like Git.

<code># Save a single object in binary RDS format
saveRDS(data, "data.rds")
# Or, using ASCII format
saveRDS(data, "data.rds", ascii=TRUE)

# To load the data again:
data &lt;- readRDS("data.rds")
</code>

It’s also possible to save multiple objects into an single file, using the RData format.


<code># Saving multiple objects in binary RData format
save(data, file="data.RData")
# Or, using ASCII format
save(data, file="data.RData", ascii=TRUE)
# Can save multiple objects
save(data, data1, file="data.RData")

# To load the data again:
load("data.RData")
</code>
An important difference between <code>saveRDS()</code> and <code>save()</code> is that, with the former, when you <code>readRDS()</code> the data, you specify the name of the object, and with the latter, when you <code>load()</code> the data, the original object names are automatically used. Automatically using the original object names can sometimes simplify a workflow, but it can also be a drawback if the data object is meant to be distributed to others for use in a different environment.


<h2>Debugging a script or function</h2>
    <h3>Problem</h3>

You want to debug a script or function.


<h3>Solution</h3>

Insert this into your code at the place where you want to start debugging:


browser()

When the R interpreter reaches that line, it will pause your code and you will be able to look at and change variables.


In the browser, typing these letters will do things:


<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>c</td>
      <td>Continue</td>
    </tr>
    <tr>
      <td>n (or Return)</td>
      <td>Next step</td>
    </tr>
    <tr>
      <td>Q</td>
      <td>quit</td>
    </tr>
    <tr>
      <td>Ctrl-C</td>
      <td>go to top level</td>
    </tr>
  </tbody>
</table>

When in the browser, you can see what variables are in the current scope.


ls()

To pause and start a browser for every line in your function:


debug(myfunction)
myfunction(x)

<h3>Useful options</h3>

By default, every time you press Enter at the browser prompt, it runs the next step. This is equivalent to pressing <code class="highlighter-rouge">n</code> and then Enter. This can be annoying. To disable it use:

options(browserNLdisabled=TRUE)

To start debugging whenever an error is thrown, run this before your function which throws an error:


options(error=recover)

If you want these options to be set every time you start R, you can put them in your ~/.Rprofile file.

<h2>data.table vs data.frame</h2>
<a href="data.table vs data.frame.html">data.table vs data.frame</a>
<br>
<a href="Introduction to data.table.html">Introduction to data.table</a>
<br>
<a href="http://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html">JOINing data in R using data.table</a>
<br>
<a href="Advanced tips and tricks with data.table.html"><span class="goldwhiteb">&diams;Advanced tips and tricks with data.table</span></a>
<br>

X = data.table(a=1:5, b=6:10, c=c(5:1))

length(X[b %between% c(7,9)])
length(X[b %inrange% c(7,9)])

# inrange()
Y = data.table(a=c(8,3,10,7,-10), val=runif(5))
range = data.table(start = 1:5, end = 6:10)
Y[a %inrange% range]

https://stackoverflow.com/questions/16652533/insert-a-row-in-a-data-table
insert-a-row-in-a-data-table
dt1 &lt;- list(1,4,7)
rbind(dt1, X)

dt1 &lt;- data.table(1,4,7)
rbindlist(list(dt1, X))

===================
use data.frame
df &lt;- data.frame( name=c("John", "Adam"), date=c(3, 5) )

Extract exact matches:

subset(df, date==3)
nrow(subset(df, date==3))

Extract matches in range:

subset(df, date>4 & date&lt;6)

  name date
2 Adam    5



<a href="Data.Table Tutorial.html"><span class="silverredb">&diams;Data.Table Tutorial</span></a>
<br>

<a href="http://brooksandrew.github.io/simpleblog/articles/advanced-data-table/">Advanced tips and tricks with data.table package</a>
<br>

<br>
<h2>DiagrammeR</h2>
<a href="http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html">DiagrammeR</a><br>
<a href="DiagrammeR Docs.html">DiagrammeR Docs</a>

<br>
<h2>ROC Curves</h2>
<a href="ROC Curves.html">ROC Curves</a>
<br>
<h2>capture.output to file</h2>
capture.output(options(), file="temp.txt")

<h2>writing functions</h2>

a simple function:

square = function(x){x*x}

to square a vector:

x = c(1,3,5)
square(x)

to square a matrix:
x = cbind(c(1,3),c(5,7))
square(x)

to return a list of objects, use list():
square = function(x){return(list(x*x,x*x*x))}
square(x)

using debug() to debug:
debug(square)
square(x-a)

using print to debug in function:
square = function(x){
print(x)
print(x*x)
x*x
}

using stop() and stopifnot() to write your own error msg:
squareRoot = function(x){
	if(x&lt;0){
		stop("cannot use negative number!")
	}
	sqrt(x)
}
squareRoot(-1)

good function practices:
keep short function
write comments
try with examples
use debug and error msg

<h2>For Loop in R with Examples</h2>
<a href="https://www.guru99.com/r-for-loop.html">For Loop in R with Examples</a>

<h2>case_when and switch</h2>

switch("shape", "color" = "red", "shape" = "square", "length" = 5)

library(dplyr)
Length=3.5
mode &lt;- case_when(
                (Length < 1) ~ "Walk",
                (1 <= Length & Length < 5) ~ "bike",
                (5 <= Length & Length < 10) ~ "drive",
                (Length >= 10) ~ "fly"
          )

<h2>Calling multiple external program from R</h2>

<a href="https://stackoverflow.com/questions/21966209/calling-external-program-from-r-with-multiple-commands-in-system">Calling multiple external program from R</a>
<br>
for(i in 1:10){
cmd=paste("export FOO=",i," ; echo \"$FOO\" ",sep='')
system(cmd)
}

<h2>rmItems</h2>
# rmItems(fmList, itemList) remove itemList from fmList
 rmItems &lt;- function(fmList, itemList){return(fmList [! fmList %in% itemList])}

fmList = 1:10
itemList = c(2,4,5)
rmItems(fmList, itemList)

# remove fraudSTK
 CodeTable = rmItems(CodeTable, fraudSTK)

milList8 = c("a","b","c","d")
milList20 = c("a","b","c","f")
setdiff(fmList,itemList)


<h2>R porjects</h2>
<a href="R porjects.html">R porjects</a>

<h2>call C from R</h2>
<a href="call C from R.html">call C from R</a>

<h2>Make R Beep</h2>

<a href="https://www.geoffchappell.com/studies/windows/win32/kernel32/api/index.htm">KERNEL32 Functions</a>
<a href="https://stackoverflow.com/questions/3365657/is-there-a-way-to-make-r-beep-play-a-sound-at-the-end-of-a-script">Make R Beep</a>

rundll32.exe Kernel32.dll,Beep 550,1000
rundll32.exe cmdext.dll,MessageBeepStub
rundll32 user32.dll,MessageBeep
BOOL Beep(
  DWORD dwFreq,
  DWORD dwDuration
);

C:\Windows\Media\Delta

install.packages("audio") 
library(audio)

# play(x, rate, ...)
# x = audioSample(sin(1:8000/10), 8000)
# play(x)
# 10000 is the set of numbers, 10 is the freq code

play(sin(c(2000:1000,1500:2000) / 3))

play(sin(1:10000/3))
Sys.sleep(1)
play(sin(1:10000/4))
Sys.sleep(1)
play(sin(1:10000/5))
Sys.sleep(1)
play(sin(1:10000/6))
Sys.sleep(1)
play(sin(1:10000/7))
Sys.sleep(1)
play(sin(1:10000/8))
Sys.sleep(1)
play(sin(1:10000/9))
Sys.sleep(1)
play(sin(1:10000/10))
Sys.sleep(1)
play(sin(1:10000/20))
Sys.sleep(1)
play(sin(1:10000/30))
Sys.sleep(1)

<h2>Play a random sound</h2>
# Update all packages and "ping" when it's ready
# danger! will take a long time and may get wrong result

library(beepr)
update.packages(ask=FALSE); beep()

#Play a fanfare instead of a "ping".
beep("fanfare")
#or
beep(3)

# Play a random sound
beep(0)

beep(sound = 1, expr = NULL)
Arguments
sound character string or number specifying what sound to be played by either specifying one of the built in sounds or specifying the path to a wav file. The default is 1.
Possible sounds are:
"ping" "coin" "fanfare" "complete" "treasure" "ready" "shotgun" "mario" "wilhelm" "facebook" "sword"
beep("shotgun")

<h2>read clipboard</h2>

simply use: readClipboard()

this gives too many columns:
read.table(file = "clipboard", sep = ",")

<h2>dplyr Data Manipulation</h2>
<a href="dplyr Data Manipulation.html">dplyr Data Manipulation</a>

<h2>Language Server Protocol</h2>
<a href="D:\R-3.4.3\bin\x64\R.exe">install.packages("languageserver") Language Server Protocol:</a>
<br>
Adding features like auto complete, go to definition, or documentation on hover for a programming language takes significant effort.

<h2>to run R by batch script</h2>
Rscript.exe  alert.r
Rscript.exe  something.r

Note: Rscript.exe cannot run with Chinese

calling chrome by batch script in sequence
can also call by R

<a href="https://kknews.cc/tech/92k2828.html">R与中文那些事 R script with Chinese</a>

<a href="https://stackoverflow.com/questions/31190468/integrating-r-and-its-graphics-with-existing-javascript-html-application"><strong>R and Javascript : Execution, Libraries, Integration</strong></a>
<br>
In today’s date, R is the megastar language for <a href="https://www.cuelogic.com/big-data-solution" data-href="https://www.cuelogic.com/big-data-solution" target="_blank">big data analytics</a>. 
In this article, I will talk about on coordination, visualization and execution of R and JavaScript. 
However, you may ask the question for what reason somebody might want to incorporate R into web applications?
There are quite a few reasons for this. 
When you add R to your solution, a vast opportunity of analytics opens up like statistics, predictive data modelling, forecasting, machine learning, visualization and much more.
R is developed by statisticians, scientists or professional analysts using the script but the reports and the results generated by them on the desktop can be easily emailed or presented in the form of presentation, but that is limiting the business use and other potential uses.
If R is incorporated with JavaScript, then web delivery can happen smoothly, and it can help in making efficient business decision making. 
Integrating R into web application naturally becomes quintessential.

<h2>Create Apps with Rt</h2>
<a href="https://www.r-bloggers.com/deploying-desktop-apps-with-r/">Create Apps with R</a>


<h2>Integrate R into JavaScript</h2>
There can be various ways through which you can integrate R with JavaScript. 
Here I am discussing the following methods that I prefer for Rand Javascript integration.
<strong>1. Deploy R open</strong>
Through Deploy R opens you can easily embed results of various R functions like- data and charts into any application. 
This specific structure is an open source server-based system planned especially for R, which makes it simple to call the R code at a real time.
The workflow for this is simple: first, the programmer develops R script which is then published on the Deploy R server. 
The published R script that can be executed from any standard application using DeployR API. 
Using client libraries JavaScript now can make calls to the server. 
The results returned by the call can be embedded into the displayed or processed according to the application.
<strong>2. Open CPU JavaScript API</strong>
This offers straightforward RPC and information input/Output through Ajax strategies that can be fused in JavaScript of your HTML page.

<h2>Visualization with R and JavaScript</h2>
You can make use of numerous JavaScript libraries that help in creating web functionality for dynamic data visualizations for R.
Here I will be elaborating some of those tools like D3, Highchart, and leaflet. 
You can quickly implement these tools in your R and program knowledge of JavaScript is not mandatory for this.
As I have already mentioned that R is an open source analytical software, it can create high dimensional data visualizations. 
Ggplot2 is a standout among the most downloaded bundle that has helped R to accomplish best quality level as a data visualization tool.
Javascript then again is a scripting dialect in which R can be consolidated to make data visualisation. 
Numerous javascript libraries can help in creating great intuitive plots, some of them are d3.Js, c3.js, vis.js, plotly.js, sigma.js, dygraphs.js.
HTM widgets act as a bridge between R and JavaScript. 
It is the principal support for building connectors between two languages. 
The flow of a program for HTM widgets r can be visualized as under:
• Information is perused into R
• Data is handled (and conceivably controlled) by R
• Data is changed over to JavaScript Object Notation (JSON) arrange
• Information is bound to JavaScript
• Information is prepared (and conceivably controlled) by JavaScript
• Information is mapped to plotting highlights and rendered
Now let us discuss some of the data visualization packages:
<strong>• r d3 package</strong>
Data-driven documents or d3 is one of the popular JavaScript visualization libraries. 
D3 can produce visualization for almost everything including choropleths, scatter plots, graphs, network visualizations and many more. 
Multiple R packages are using only D3 plotting methods. 
You can refer r d3 package tutorials to learn about this.
• <strong>ggplot2</strong> <br> <br> It is really very easy to create plots in R, but you may ask me whether it is same for creating custom plots, the answer is “yes”, and that is the primary motivation behind why ggplot came into existence. 
With ggplot, you can make complex multi-layered designs effectively.
Here you can start plotting with axes then add points and lines. 
But the only drawback that it has it is relatively slower than base R, and new developers might find it difficult to learn.
• <strong>Leaflet</strong>
The leaflet has found its profound use in GIS (mapping), this is an open source library. 
The R packages that backings this is composed and kept up by RStudio and ports. 
Using this developer can create pop up text, custom zoom levels, tiles, polygon, planning and many more.
The ggmap bundle of javaScript can be utilised for the estimation of the latitude and longitude.
• <strong>Lattice</strong>
Lattice helps in plotting visualized multivariate data. 
Here you can have tilled plots that help in comparing values or subgroups of a given variable. 
Here you will discover numerous lattice highlights has been acquired as utilizes grid package for its usage. 
The underlying logic used by lattice is very much similar to base R.
<strong>• visNetwork</strong>
For the graphical representation of nodes and edges, the visual network is referred. 
Vis.js is a standout amongst the most famous library among numerous that can do this sort of plotting. 
visNetwork is the related with R package for this.
Network plots ought to be finished remembering nodes and edges. 
For visNetwork, these two should be separated into two different data frames one for the nodes and the other
<strong>• Highcarter</strong>
This is another visualization tool which is very similar to D3. 
You can use this tool for a variety of plots like line, spline, arealinerange, column range, polar chart and many more. 
For the commercial use of Highcarter, you need to get a license while for the non-commercial you don’t need one.
Highcarter library can be accessed very easily using various chart () functions. 
Using this function, you can create a plot in a single task. 
This function is very much similar to qplot() of ggplot2 of D3. 
chart () can produce different types of scenarios depending on the data inputs and specifications.
<strong>• RColor Brewer</strong>
With this package, you can use color for your plots, graphs, and maps. 
This package works nicely with schemes.
<strong>• Plotly</strong>
It is a well distinguish podium for data visualization that works inordinately with R and Python notebook. 
It has similarity with the high career as both are known for interactive plotting. 
But here you get some extra as it offers something that most of the package don’t like contour plots, candlestick chart, and 3d charts.
• <strong>SunTrust</strong>
It is the way for representing data visualization as it nicely describes the sequence of events. 
The diagram that it produces speaks about itself. 
You don’t need an explanation for the chart as it is self-explanatory.
• <strong>RGL</strong>
For creating three-dimensional plots in R you should check out RGL. 
It has comparability with lattice, and on the off chance that you are an accomplished R developer you will think that its simple.
<strong>• Threejs</strong>
This is an R package and an HTML widget that helps in incorporating several data visualization from the JavaScript library.
Some of the visualization function three are as follows:
• Graphjs: this is used for implementing 3D interactive data visualization. 
This function accepts igraph as the first argument. 
This manages definition for nodes and edges.
• Scatterplot3js: this function is used for creating three dimensional scatter plot.
• Globejs: this function of JavaScript is used for plotting surface maps and data points on earth.
• <strong>Shiny</strong>
The most significant benefit of JavaScript visualization is it can be implanted voluntarily into the web application. 
They can be injected into several frameworks, one of such context of R development is shiny.
Shiny is created and maintained by R Studio. 
It is a <a href="https://www.cuelogic.com/custom-software-development" data-href="https://www.cuelogic.com/custom-software-development" target="_blank">software application development</a> instrument, to a great extent employed for making wise interfaces with R. 
R shiny tutorial will take in more about shiny.
Shiny is a podium for facilitating R web development.
Connecting R with javascript using libraries
Web scuffling has formed into an original piece of examination as through this movement you can pucker your required information. 
But the data should be extracted before any web developer start to insert javascript render content into the web page. 
To help in such situation R has an excellent package called V8 which acts as an interface to JavaScript. 
R v8 is the most generally utilized capacity utilized for interfacing r in javascript. 
You can undoubtedly implement JS code in R without parting the current session. 
The library function used for this is rvest().
To run the JavaScript in R, we need a context handler, within that context handler you can start programming. 
Then you can export the R data into JavaScript.
Some other JavaScript libraries that help in analytical programming such as Linear Regression, SVMs etc. 
are as follows:
• Brain.js()
• Mljs
• Webdnn
• Convnetjs

<h2>Conclusion:</h2>
R and Javascript can practically unlock innumerable possibility in Data Science and Analytics. 
Both technologies are working towards developing better integrations, knowledge repositories, libraries and use cases. 
It is a good time to use both of this together. 
The future looks bright.


<a href="https://hackernoon.com/r-and-javascript-execution-libraries-integration-40a30726f295">Integrating R and Javascript/HTML Application</a>
<br>
<h2>Rserve package</h2>
There is javascript implementation of Rserve client available rserve-js.
You can call R from javascript efficiently using Rserve package. 

<h2>FastRWeb</h2>
FastRWeb is an infrastructure that allows any webserver to use R scripts for generating content on the fly, such as web pages or graphics. 
URLs are mapped to scripts and can have optional arguments that are passed to the R function run from the script. 
For example http://my.server/cgi-bin/R/foo.png?n=100 would cause FastRWeb to look up a script foo.png.R, source it and call run(n="100"). 
So for example the script could be as simple as

run &lt;- function(n=10, ...) {
   p &lt;- WebPlot(800, 600)
   n &lt;- as.integer(n)
   plot(rnorm(n), rnorm(n), col=2, pch=19)
   p
}
This can potentially then be called using JavaScript to dynamically load images and display them.

<a href="https://stackoverflow.com/questions/22179512/suggestions-needed-for-building-r-server-rest-apis-that-i-can-call-from-externa/29537593#29537593">building R server REST API's that I can call from external app</a>
<br>
<h2>httpuv</h2>
You can use httpuv to fire up a basic server then handle the GET/POST requests. The following isn't "REST" per se, but it should provide the basic framework:

library(httpuv)
library(RCurl)
library(httr)

app &lt;- list(call=function(req) {

  query &lt;- req$QUERY_STRING
  qs &lt;- httr:::parse_query(gsub("^\\?", "", query))

  status &lt;- 200L
  headers &lt;- list('Content-Type' = 'text/html')

  if (!is.character(query) || identical(query, "")) {
    body &lt;- "\r\n<html><body></body></html>"
  } else {
    body &lt;- sprintf("\r\n<html><body>a=%s</body></html>", qs$a)
  }

  ret &lt;- list(status=status,
              headers=headers,
              body=body)

  return(ret)

})

message("Starting server...")

server &lt;- startServer("127.0.0.1", 8000, app=app)
on.exit(stopServer(server))

while(TRUE) {
  service()
  Sys.sleep(0.001)
}

stopServer(server)

<br>
<h2>Cucumber Selenium</h2>
<a href="https://www.guru99.com/using-cucumber-selenium.html">Cucumber Selenium</a>
<a href="https://www.youtube.com/watch?v=ZSfOEBh9BRM">Cucumber Selenium Tutorial</a>
<br>
<a href="https://blog.gtwang.org/r/rselenium-r-selenium-browser-web-scraping-tutorial/">RSelenium：R 使用 Selenium 操控瀏覽器下載網頁資料</a>
<br>

<h2>SQL databases and R</h2>
<a href="https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html">SQL databases and R</a>

<h2>SQLite</h2>
<a href="https://db.rstudio.com/databases/sqlite/">R SQLite</a>
<br>

install.packages("RSQLite")

Or install the latest development version from GitHub with:
# install.packages("devtools")
devtools::install_github("rstats-db/RSQLite")

To install from GitHub, you’ll need a development environment.

Basic usage
library(DBI)
# Create an ephemeral in-memory RSQLite database
con &lt;- dbConnect(RSQLite::SQLite(), ":memory:")

dbListTables(con)
## character(0)
dbWriteTable(con, "mtcars", mtcars)
dbListTables(con)
## [1] "mtcars"
dbListFields(con, "mtcars")
##  [1] "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"   "gear"
## [11] "carb"
dbReadTable(con, "mtcars")
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
...

# You can fetch all results:
res &lt;- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
dbFetch(res)
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
...

dbClearResult(res)

# Or a chunk at a time
res &lt;- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
while(!dbHasCompleted(res)){
  chunk &lt;- dbFetch(res, n = 5)
  print(nrow(chunk))
}
## [1] 5
## [1] 5
## [1] 1
# Clear the result
dbClearResult(res)

# Disconnect from the database
dbDisconnect(con)
Acknowledgements
Many thanks to Doug Bates, Seth Falcon, Detlef Groth, Ronggui Huang, Kurt Hornik, Uwe Ligges, Charles Loboz, Duncan Murdoch, and Brian D. 
Ripley for comments, suggestions, bug reports, and/or patches.


<br>

<h2>Invoking the Rstudio Viewer</h2>
viewer &lt;- getOption("viewer")
viewer("<a href="https://www.rt.com/")">viewer("C:/Users/User/Desktop/Debugging with RStudio.html")</a>

<h2>to sum only elements greater than 5</h2>
a&lt;-sample.int(10,20,replace=TRUE)
sum(a[a>5])

<h2>Customizing RStudio themes</h2>
<a href="https://www.r-bloggers.com/make-rstudio-look-the-way-you-want-because-beauty-matters/">Make RStudio Beauty</a>

D:\RStudio\www\rstudio\806BBC582D6B8DF91384AD7E3EFC9A52.cache.css

<a href="https://support.rstudio.com/hc/en-us/articles/200532307-Customizing-Fonts-and-Appearance">Customizing Fonts and Appearance</a>
<br>

<h2>table()</h2>
table()的输出可以看成是一个带名字的数字向量。
可以用names()和as.numeric()分别得到名称和频数：> 

x &lt;- sample(c("a", "b", "c"), 100, replace=TRUE)
tablex = table(x)

names(tablex)
[1] "a" "b" "c"

> as.numeric(tablex)
[1] 42 25 33

可以直接把输出结果转化为数据框，as.data.frame()：> 
as.data.frame(tablex)
  x Freq
1 a   42
2 b   25
3 c   33

<h2>with(data, expr, …)</h2>
applys an expression to a dataset.
eg
with(BOD,{BOD$demand &lt;- BOD$demand + 1; print(BOD$demand)})

<h2>R regular expression</h2>
<a href="https://blog.yjtseng.info/post/regexpr/">R regex</a>

<h2>R Operator Syntax</h2>
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Syntax.html">R Operator Syntax and Precedence</a>

:: :::	access variables in a namespace
$ @	component / slot extraction
[ [[	indexing
^	exponentiation (right to left)
- +	unary minus and plus
:	sequence operator
%any%	special operators (including %% and %/%)
* /	multiply, divide
+ -	(binary) add, subtract
< > <= >= == !=	ordering and comparison
!	negation
& &&	and
| ||	or
~	as in formulae
-> ->>	rightwards assignment
&lt;- <&lt;-	assignment (right to left)
=	assignment (right to left)
?	help (unary and binary)

exampleRPackage
The exampleRPackage can be installed from github:

# install.packages("devtools")
devtools::install_github("mvuorre/exampleRPackage")

The file you are reading now is the package’s README, which describes how to create R packages with functions, data, and appropriate documentation. 


A Simple Example of Using replyr::gapply
It’s a common situation to have data from multiple processes in a “long” data format. 
It’s also natural to split that data apart to analyze or transform it, per-process — and then to bring the results of that data processing together, for comparison. 
Such a work pattern is called “Split-Apply-Combine”. 
A simple example of one such implementation, replyr::gapply, from package, replyr.

K-means clustering
K-means is a clustering techniques that subdivide the data sets into a set of k groups, where k is the number of groups pre-specified by the analyst.

Determining the optimal number of clusters: use factoextra::fviz_nbclust()


<h2>树状图</h2>
<a href="Dendrograms in R.html" target="_blank">Dendrograms in R</a>
<br>
<h2>shiny and rpanel - a quick comparison</h2>

Shiny is a package from RStudio that lets you produce interactive web pages. 
You build a page with some control widgets and a handler that does something dependent on the value of those widgets. 
You can build your interface programmatically or create a boilerplate html page that gets filled in by control and output widgets.

A conceptually similar pattern is implemented by the rpanel package, but this uses the tcltk toolkit. 
A panel is created, control widgets added, and callbacks on the controls can run R code to, for example, update a plot.

qq plot example
Here's the rpanel version:

require(rpanel)
# box-cox transform
bc.fn &lt;- function(y, lambda) {
    if (abs(lambda) < 0.001) 
        z &lt;- log(y) else z &lt;- (y^lambda - 1)/lambda
}

# qqplot of transformed data
qq.draw &lt;- function(panel) {
    z &lt;- bc.fn(panel$y, panel$lambda)
    qqnorm(z, main = paste("lambda =", round(panel$lambda, 2)))
    panel
}

# create a new panel with some initial data
panel &lt;- rp.control(y = exp(rnorm(50)), lambda = 1)

# add a slider that calls qq.draw on change
rp.slider(panel, lambda, -2, 2, qq.draw)

Run these functions and you should see a slider and a graphics window. 
Move the slider to modify the plot.
Note that this might not work too well under RStudio because of the way the embedded RStudio graphics device captures output.


And here is the shiny version, which comes in two files living in their own folder.  ui.R and server.R

First qqplot/ui.R:

library(shiny)
# this defines our page layout
shinyUI(pageWithSidebar(
  headerPanel("qqplot example"),
  sidebarPanel(
  # a slider called 'lambda':
    sliderInput("lambda", "Lambda value", min = -2, max = 2, step=0.01, value = 0)
  ),
  mainPanel(
    # the main panel is the plotted output from qqplot:
    plotOutput("qqPlot")
  )
))

and qqplot/server.R:

library(shiny)
shinyServer(function(input, output) {
    # b-c transform
    bc.fn &lt;- function(y, lambda) {
        if (abs(lambda) < 0.001) 
            z &lt;- log(y) else z &lt;- (y^lambda - 1)/lambda
    }
    # initial data
    y = exp(rnorm(50))
    # here's the qqplot method:
    output$qqPlot &lt;- reactivePlot(function() {
        z &lt;- bc.fn(y, input$lambda)
        qqnorm(z, main = paste("lambda =", round(input$lambda, 2)))
    })
})

With that done, launch the app with:

runApp("qqplot")
that should open up the page in your web browser. 
Hit break, stop, or control-C to quit.


Notes
The rpanel plot updates as you drag the slider, whereas shiny updates only when you let go of the slider.

I find that when I hit Control-C and break a running shiny app, then my tcltk windows go all unresponsive until I quit R and start again. 
Threading issues? This is on Linux. 
I've always had problems with tcltk widgets going unresponsive on me, or ending up unkillable.

The shiny UI looks, well, “shiny”, but the rpanel interface looks a bit old and not very exciting (if you can get excited by user interfaces).

Using the tkrplot package, you can build integrated rpanel packages with controls and plots in the same window. 
Without it, you are stuck with separate graphics and control windows.

Which should I use?
How do I know?! Shiny looks better, but I do like the update on drag of rpanel - it gives you much better feedback as you control the plot. 
Maybe this can be done in shiny with some additional work.

I don't really like the two-file method of shiny. 
Looking at the code I see the files just get sourced in, so conceivably it could be possible to run shiny apps just by specifying the shinyServer and shinyUI functions - but shiny monitors the server.R and ui.R file for changes and updates the application, which is quite nice.

So there's the basic existential dilemma. 
Choice. 
I can even throw some more things into the mix if you want - there' RServe, or RApache with gWidgetsWWW and probably many many more. 
I'm sure we can all agree that the days of needing Java and Apache Tomcat to deploy R applications to the web are now over (http://sysbio.mrc-bsu.cam.ac.uk/Rwui/tutorial/quick_tour.html).

I might try and implement some more of the rpanel examples in shiny shortly. 
Or why don't you have a go, and publish your works here?

<h2>R GUI 視窗程式設計</h2>
<a href="http://www.hmwu.idv.tw/web/R/F01-hmwu_R-GUI-Design.pdf">R GUI 視窗程式設計 tcltk/tcltk2, rpanel</a>
<a href="http://adrian.waddell.ch/EssentialSoftware/Rtcltk_geometry.pdf">Rtcltk_geometry</a>

<a href="http://rstudio-pubs-static.s3.amazonaws.com/2666_f0de0980ac9048d0a71d0f507cd83c3f.html">shiny and rpanel - a quick comparison</a>

<h3>rpanel sample</h3>
<a href="https://www.academia.edu/370260/rpanel_Simple_Interactive_Controls_for_R_Functions_Using_the_tcltk_Package_9999_">rpanel: Simple Interactive Controls for R Functions Using the tcltk Package </a>
<a href="https://www.academia.edu/attachments/1880059/download_file?st=MTU2NDAxOTg4MywxODIuMjM5LjExNS4zNg%3D%3D&s=swp-splash-header">download rpanel sample</a>
<a href="www.stats.gla.ac.uk/~adrian/rpanel">The `rpanel' package</a>
<a href="http://www.stats.gla.ac.uk/~adrian/rpanel/scripts/rpanel-paper-scripts.r">Simple Interactive Controls for R Functions scripts</a>

library(rpanel)
x11(width=4,height=4)
qq.draw &lt;- function(panel)
 { z &lt;- bc.fn(panel$y, panel$lambda)
   qqnorm(z, main = paste("lambda =",round(panel$lambda, 2)))
   panel
 }
 panel &lt;- rp.control(y = exp(rnorm(50)), lambda = 1)
 rp.slider(panel, lambda, -2, 2, qq.draw,showvalue = TRUE)


<h3>create a  matrix</h3>
A = matrix( 
c(2, 4, 3, 1, 5, 7), # the data elements 
nrow=2,              # number of rows 
ncol=3,              # number of columns 
byrow = TRUE)        # fill matrix by rows 


<h2>cross tabulations</h2>
<a href="Contingency Table.html" class="bordred2 borRad10 green whitebs">Contingency Table</a>  <a href="Xtabs exercises.html" class="bordred2 borRad10 white whitebs">Xtabs exercises</a> 

a chart is different from a table
a chart is a graphic representation
a table is a numeric representation

frequaency table is a single row table

cross tabulations, 列联表, contingency tables, 又称交互分类表 按两个或更多变量分类时所列出的频数表。

R provides many methods for creating frequency and contingency tables. 

generate frequency tables using the table( ) function, table( ) function can also create cross tab, table( ) can also generate multidimensional tables based on 3 or more categorical variables.

generate tables of proportions using the prop.table( ) function
generate marginal frequencies using margin.table( )

# 2-Way Frequency Table using table() function
attach(mtcars)
mytable &lt;- table(mtcars$gear,mtcars$cyl) # A will be rows, B will be columns 
mytable # print table 

# 2-Way Frequency Table using xtabs()
y = xtabs(~ cyl + gear, mtcars)	# xtabs gives row and col labels

margin.table(mytable, 1) # A frequencies (summed over B) 
margin.table(mytable, 2) # B frequencies (summed over A)

prop.table(mytable) # cell percentages
prop.table(mytable, 1) # row percentages 
prop.table(mytable, 2) # column percentages

# 3-Way Frequency Table 
mytable &lt;- table(A, B, C) 
mytable &lt;- table(mtcars$gear,mtcars$cyl,mtcars$mpg)
mytable

# 3-Way Frequency Table
mytable &lt;- xtabs(~A+B+c, data=mydata)
mytable &lt;- xtabs(~gear+cyl+mpg, mtcars)
summary(mytable) # chi-square test of indepedence

<a href="https://www.statmethods.net/stats/frequencies.html">Frequencies and Crosstabs</a>

<h2>parallel 平行計算</h2>
<a href="https://blog.gtwang.org/r/r-parallel-computing-module-tutorial/">R 的 parallel 平行計算套件使用教學與範例</a>
<br>
<a href="How-to go parallel in R.html">How-to go parallel in R</a>
<br>
<h2>edply</h2>
<a href="https://www.r-bloggers.com/edply-combining-plyr-and-expand-grid/">edply: combining plyr and expand.grid</a>

<h2>column merge two tables</h2>
lista = c(1:5)
listb = c(6:10)
listc = paste0(lista, "  ",listb)
lista
listb
listc

data from files:
lista = readLines("list1.txt")
listb = readLines("list2.txt")
listc = paste0(lista, "  ",listb)
sink("list3.txt")
cat(listc, sep="\n")
sink()

note: may use cbind in dataframe
lista = c(1:5)
listb = c(6:10)
listc = c(11:15)
MC = matrix()  # this is an empty matrix

MB = matrix( c(lista,listb,listc), nrow=5, ncol=3)  # a 3 column matrix
MC = cbind(MB[,1],MB[,3])   # now MC is a two column matrix

<h2>chop in blocks</h2>
groupPageNum = 7
theList = 1:78
if(length(theList)%%groupPageNum==0){
  pageNo = length(theList)%/%groupPageNum
}else{
  pageNo = length(theList)%/%groupPageNum +1
}
pageNo


for(page in 1:pageNo){
  if(length(theList) > groupPageNum){
	thepage= theList[1:groupPageNum]
	theList= theList[-(1:groupPageNum)]
     arrangePages(thepage)
	page = page + 1
  }else{
     arrangePages(theList)
  }
}


<h2>remove items</h2>

fmList=c('02917','01876','01960','03938','02951','02952','06820','06110','03601','01895')
itemList=c('02718','02696')

commons = fmList[fmList %in% itemList]
cat("\n\nnumber of Items to remove: ", length(commons), "\n")
for(item in commons){fmList = fmList[-(which(fmList == item))]}
fmList

<h2>extract chinanews images</h2>
http://www.chinanews.com/tp/hd2011/2019/10-20/909276.shtml
copy the thumb address and replace ending 320x300.jpg with 1000x2000

<h2>cut(x,breaks)</h2>
x = sort(rnorm(13,5,12))
x
-15.0 -11.3  -3.2   1.0   3.8   6.1   7.6   7.8  10.7 13.7  15.4  15.9  23.4

cut(x,5)
(-15.1,-7.36] (-15.1,-7.36] (-7.36,0.339] (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (8.03,15.7]   (8.03,15.7]   (8.03,15.7]   (15.7,23.5]   (15.7,23.5]  
Levels: (-15.1,-7.36] (-7.36,0.339] (0.339,8.03] (8.03,15.7] (15.7,23.5]

<h2>R GUI: RGtk or Tcl/Tk, gWidgets</h2>
<a href="http://www.ggobi.org/rgtk2/">RGtk2</a>

<a href="https://www.r-bloggers.com/playing-with-guis-in-r-with-rgtk2/">Playing with GUIs in R with RGtk2</a>

<a href="https://www.r-bloggers.com/gui-building-in-r-gwidgets-vs-deducer/">GUI building in R: gWidgets vs Deducer</a>

require("RGtk2")

window &lt;- gtkWindow()
window["title"] &lt;- "Calculator"

frame &lt;- gtkFrameNew("Calculate")
window$add(frame)

box1 &lt;- gtkVBoxNew()
box1$setBorderWidth(30)
frame$add(box1)   #add box1 to the frame

box2 &lt;- gtkHBoxNew(spacing= 10) #distance between elements
box2$setBorderWidth(24)

TextToCalculate&lt;- gtkEntryNew() #text field with expresion to calculate
TextToCalculate$setWidthChars(25)
box1$packStart(TextToCalculate)

label = gtkLabelNewWithMnemonic("Result") #text label
box1$packStart(label)

result&lt;- gtkEntryNew() #text field with result of our calculation
result$setWidthChars(25)
box1$packStart(result)

box2 &lt;- gtkHBoxNew(spacing= 10) # distance between elements
box2$setBorderWidth(24)
box1$packStart(box2)

Calculate &lt;- gtkButton("Calculate")
box2$packStart(Calculate,fill=F) #button which will start calculating

Sin &lt;- gtkButton("Sin") #button to paste sin() to TextToCalculate
box2$packStart(Sin,fill=F)

Cos &lt;- gtkButton("Cos") #button to paste cos() to TextToCalculate
box2$packStart(Cos,fill=F)

model&lt;-rGtkDataFrame(c("double","integer"))
combobox &lt;- gtkComboBox(model)
#combobox allowing to decide whether we want result as integer or double

crt &lt;- gtkCellRendererText()
combobox$packStart(crt)
combobox$addAttribute(crt, "text", 0)

gtkComboBoxSetActive(combobox,0)
box2$packStart(combobox)

DoCalculation&lt;-function(button)
{

  if ((TextToCalculate$getText())=="") return(invisible(NULL)) #if no text do nothing

   #display error if R fails at calculating
   tryCatch(
      if (gtkComboBoxGetActive(combobox)==0)
   result$setText((eval(parse(text=TextToCalculate$getText()))))
   else (result$setText(as.integer(eval(parse(text=TextToCalculate$getText()))))),
   error=function(e)
      {
      ErrorBox &lt;- gtkDialogNewWithButtons("Error",window, "modal","gtk-ok", GtkResponseType["ok"])
      box1 &lt;- gtkVBoxNew()
      box1$setBorderWidth(24)
      ErrorBox$getContentArea()$packStart(box1)

      box2 &lt;- gtkHBoxNew()
      box1$packStart(box2)

      ErrorLabel &lt;- gtkLabelNewWithMnemonic("There is something wrong with your text!")
      box2$packStart(ErrorLabel)
      response &lt;- ErrorBox$run()


      if (response == GtkResponseType["ok"])
         ErrorBox$destroy()

      }
   )

}


  PasteSin&lt;-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"sin()",sep=""))

}

PasteCos&lt;-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"cos()",sep=""))

}

#however button variable was never used inside 
#functions, without it gSignalConnect would not work
gSignalConnect(Calculate, "clicked", DoCalculation)
gSignalConnect(Sin, "clicked", PasteSin)
gSignalConnect(Cos, "clicked", PasteCos)
Now it works like planned.



library(RGtk2)
createWindow &lt;- function()
{
    window &lt;- gtkWindow()
    label &lt;- gtkLabel("Hello World")
    window$add(label)
}
createWindow()
gtk.main() # this will create error

# using this will loop dead
gtkMain()


<h2>To keep the scripts and algorithm secret</h2>
by saving functions using save(). 
For example, here's a function f() you want to keep secret:

f &lt;- function(x, y) { return(x + y)}

Save it :
save(f, file = 'C:\\Users\\Joyce\\Documents\\R\\Secret.rda')

And when you want to use the function:
load("C:\\Users\\Joyce\\Documents\\R\\Secret.rda")

Save all functions in separate files, 
put them in a folder and have one plain old .R script
loading them all in and executing whatever.
Zip the whole thing up and distribute it to whoever.
Maybe even compile it into a package.
Effectively the whole thing would be read-only then.

This solution isn't that great though.
You can still see the function in R by typing the name of the function
so it's not hidden in that sense.
But if you open the .rda files their contents are all garbled.
It all depends really on how experienced the recipients of your code are with R.

One form of having encrypted code is implemented in the petals function in the TeachingDemos package.

it would only take intermediate level programing skills to find the hidden code,
however it does take deliberate effort and the user would not be able to claim having seen the code by accident.
You would then need some type of license agreement in place to enforce any no peeking agreements.

Well you are going to need R installed on the deployment machine.

<h2>Test if characters are in a string</h2>
grepl("abc", "abcde")
note: RE will be applied, take care of the expression

<h2>get password</h2>
install.packages("getPass")
pass = getPass::getPass(msg = "PASSWORD: ", noblank = FALSE, forcemask = FALSE)

<h2>tryCatch loop</h2>
  retrieveData &lt;- function(urlAddr){      
    retryCounter = 0
    while(retryCounter < 20) {
      cat("..",retryCounter," ") 
      retriveFile &lt;- tryCatch(readLines(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter &lt;- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

  retrieveData &lt;- function(urlAddr){      
    retryCounter = 1
    while(retryCounter < 20) {
      cat("..try ",retryCounter," ") 
      retriveFile &lt;- tryCatch(read_html(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter &lt;- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

<h2>變異數分析, 方差分析 ANOVA </h2>
<a href="http://personality-project.org/r/r.guide/r.anova.html">r.anova</a>
<br>
<a href="https://alex59638.pixnet.net/blog/post/403137005-用r進行anova%28變方分析%29">ANOVA可分析多組間的差異 變異數分析 (ANOVA)</a>
<a href="http://programmermagazine.github.io/201310/htm/article3.html">主成分分析 Principle Component Analysis</a>
<br>
<h2>常用統計檢驗法簡介</h2>

T.test(又稱 T 檢定、T檢驗、t.test，以下簡稱T檢驗)
T檢驗主要用於檢定樣本的平均值，這是一項重點。

如果要看一個樣本的平均是否等於某值，要用 T 檢驗。

如果要看兩個樣本的平均是否相等，要用 T 檢驗。

T 檢驗分成三種類別
1.單樣本T檢驗(One smaple T test)
2.獨立雙樣本T檢驗
3.配對雙樣本T檢驗

要看 30 個男生的身高是否等於 180，用單樣本T檢驗。
[R語法:t.test(樣本,mu=平均)]

要看 A 班與 B 班男生身高是否相等，用獨立雙樣本T檢驗。
[R語法:t.test(A樣本,B樣本)]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A樣本,B樣本,paired=TURE)]

每一種 T 檢驗都還可以再分成雙尾與單尾檢驗。
[R語法:t.test(樣本,mu=平均,alternative= "two.sided")]

two.sided代表等於，就是雙尾的意思，也可以改成單尾的大於"greater"或是單尾的小於"less"。

重點只有"檢驗平均等於某值時"是雙尾，"檢驗平均小於某值時"是單尾，"檢驗平均大於某值時"是單尾。
請看到這裡後不要再講單尾或是雙尾了，一點意義也沒有，講大於等於小於就好了。
但Eecel沒有大於小於的選項，只有單尾雙尾，因此要自己判斷是大於還是小於(從樣本平均看即可)。
[Eecel語法:TTEST(A樣本,B樣本,2,2)]，2代表雙尾，改成1就變成單尾。

要看 30 個男生的身高是否大於 180，用單樣本T檢驗
[R語法:t.test(樣本,mu=180),alternative="greater"]

要看 A 班與 B 班男生身高差異是否小於 30，用獨立雙樣本T檢驗
[R語法:t.test(A,B,mu=30,alternative="less")]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A,B,mu=30,paired=T)]
[Eecel語法:TTEST(A樣本,B樣本,2,1)]

其中獨立雙樣本T檢驗(Welch Two smaple T test)還細分成變異數相等或變異數不相等兩種，這要看你母體與取樣的方法，如果不確定，嚴格一點是認為不相等的。

變異數相等
[R語法:t.test(A,B,mu=0,var.equal=T)]

變異數不相等
[R語法:t.test(A,B,mu=0,var.equal=F)]

<h3>卡方檢定 chi-square test(以下簡稱卡方檢定)</h3>
卡方檢驗用於確認樣本是否符合某種分配
骰子丟一百次，每面的機率是否為1/6</a>)，
或是兩個屬性之間是否有所關聯(男生是否比較容易選擇藍色商品)。

這其實是一樣的概念，假設兩個屬性之間無關，其分佈上應該會呈現隨機;
如果兩個屬性有關，例如男生喜歡藍色商品，在同樣的其況下，男生買藍色商品的次數會比男生買紅色商品的次數多，也就是不符合隨機的分配(理論上無關的話次數會一樣多)。

卡方檢定分成三種
1.適合度檢定（Goodness of fit test）
2.獨立性檢定（Test of independence）
3.同質性檢定 (Test of Homogeneity)

其實獨立性與同質性檢定是同一個東西，只是問法不一樣而已卡方適合度檢定用來檢驗樣本是否服從某種分佈，這種分佈你的心裡要有底，比方隨機(丟骰子各面是1/6)，孟德爾的紅花白花是3:1等等，如果你不知道要選擇哪種分佈，那就不能用卡方適合度檢定。

紅花969株，白花360株，檢驗是否符合孟德爾3:1的分佈，用卡方適合度檢定
chisq.test(c(969,360),p=c(0.75,0.25))
#次數表放第一個變數,p後面接機率，機率合要等於1[R語法:chisq.test(次數表,p=機率)]

骰子1000次，檢驗每面是否為1/6的分佈，用卡方適合度檢定
x=ceiling(runif(1000)*6)#丟1000次骰子, ceiling是無條件進位，讓數值落在1~6的整數
table(x)
#這是卡方檢定的重點，必須輸入統計次數，知道骰出1的有幾次，2的有幾次
chisq.test(table(x),p=c(1/6,1/6,1/6,1/6,1/6,1/6))
#次數表放第一個，p後面接分佈的機率，本次是6個1/6。

[R語法:chisq.test(次數表,p=機率)]

<h3>費雪精確性檢定 Fisher's exact test</h3>
類似卡方檢定的小樣本方式，通常用於樣本小於20的狀況，案例是猜八杯茶是先加奶還是先加茶。
fisher.test(table(real,guess))

<h2>變異數分析 ANOVA</h2>
兩組資料連續看是否有差異，用t.test，兩組以上則用ANOVA，其虛無假說H0:u1=u2=u3=...un。
若p值小於0.05，則認為並非所有的資料來自同一個母體。

若要知道到底是哪組資料不同，可使用 
pairwise.t.test(Y, B, p.adjust.method="none")
其中Y為資料列，B為組別列，並且不調整p值。
雙因子變異數分析
aov(cardspent~factor(region)*factor(gender)
使用*符號而不是+


<h2>Logistic Regression</h2>
Logistic regression, also called a logit model, is used to model dichotomous outcome variables. 

Logistic regression is a method for fitting a regression curve, y = f(x), when y is a categorical variable. 
The typical use of this model is predicting y given a set of predictors x. The predictors can be continuous, categorical or a mix of both.

一般的線性迴歸都是連續數值，例如身高或體重。
但有些情況下的應變數為類別，例如生還與否(1或0)，就可以採用Logistic Regression。

Logistic Regression有幾項要點，
1.他需要應變數為類別變項
2.他會給出一個式子，帶入自變數後(可為連續變項或類別變項)，會得出一個值
3.這個值稱為勝算比


以鐵達尼號乘客名單的資料作為範例分析
model1&lt;-glm data="titanic_passenger," family="binomial(link=" formula="survival~fare," logit="" na.action="na.exclude)&lt;/p"&gt;summary(model1)
其中fare 對 survival 的對數機率為 0.013108
勝算比為exp(0.013108)=1.013085
多一英鎊，多1%生還率。
參考資料
<a href="https://sites.google.com/site/rlearningsite/catagory/logit" target="_blank">Logistic迴歸模型</a>
<a href="http://xn--r-vc8at2mlrkqvkh65cu2ccyjyqb/" target="_blank">R语言逻辑回归分析</a>
<a href="https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/" target="_blank">How to perform a Logistic Regression in R</a><!---glm-->


<a href="http://ccckmit.wikidot.com/r:optimize">一維空間優化方法：optimize()</a>
<br>
<a href="http://ccckmit.wikidot.com/r:main">R 統計軟體 作者：陳鍾誠</a>
<br>
<a href="http://programmermagazine.github.io/201309/htm/article3.html">R 統計軟體(6) – 迴歸分析</a>
<br>

<h2>Advanced Statistics Tree-Based Models</h2>
<a href="https://www.datacamp.com/community/tutorials/decision-trees-R">Decision Trees in R</a>
<br>
<a href="https://www.guru99.com/r-decision-trees.html">Decision Tree in R with Example</a>
<br>
<a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a>
<br>
<a href="http://www.di.fc.ul.pt/~jpn/r/tree/tree.html">Classification & Regression Trees</a>
<br>
<a href="https://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html">Classification and Regression Trees with the rpart.plot package</a>
<br>

<h2>Grouping functions (tapply, by, aggregate) and the *apply family</h2>
R has many *apply functions.
Much of the functionality of the *apply family is covered by the extremely popular <code>plyr</code> package, the base functions remain useful and worth knowing.

<li><strong>apply</strong> - <em>When you want to apply a function to the rows or columns of a matrix (and higher-dimensional analogues); not generally advisable for data frames as it will coerce to a matrix first.</em>

<code># Two dimensional matrix
M &lt;- matrix(seq(1,16), 4, 4)

# apply min to rows
apply(M, 1, min)
[1] 1 2 3 4

# apply max to columns
apply(M, 2, max)
[1]  4  8 12 16

# 3 dimensional array
M &lt;- array( seq(32), dim = c(4,4,2))

# Apply sum across each M[*, , ] - i.e Sum across 2nd and 3rd dimension, look from top is an area
apply(M, 1, sum)
# Result is one-dimensional
[1] 120 128 136 144

# Apply sum across each M[*, *, ] - i.e Sum across 3rd dimension
apply(M, c(1,2), sum)
# Result is two-dimensional
     [,1] [,2] [,3] [,4]
[1,]   18   26   34   42
[2,]   20   28   36   44
[3,]   22   30   38   46
[4,]   24   32   40   48
</code>

If you want row/column means or sums for a 2D matrix, be sure to investigate the highly optimized, lightning-quick <code>colMeans</code>, <code>rowMeans</code>, <code>colSums</code>, <code>rowSums</code>.</li>
<li><strong>lapply</strong> - <em>When you want to apply a function to each element of a list in turn and get a list back.</em>

This is the workhorse of many of the other *apply functions. 
Peel back their code and you will often find <code>lapply</code> underneath.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
$a 
[1] 1
$b 
[1] 3
$c 
[1] 91
lapply(x, FUN = sum) 
$a 
[1] 1
$b 
[1] 6
$c 
[1] 5005</code></li>
<li><strong>sapply</strong> - <em>When you want to apply a function to each element of a list in turn, but you want a <strong>vector</strong> back, rather than a list.</em>

If you find yourself typing <code>unlist(lapply(...))</code>, stop and consider <code>sapply</code>.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Compare with above; a named vector, not a list 
sapply(x, FUN = length)  
a  b  c   
1  3 91

sapply(x, FUN = sum)   
a    b    c    
1    6 5005 
</code>

In more advanced uses of <code>sapply</code> it will attempt to coerce the result to a multi-dimensional array, if appropriate. 
For example, if our function returns vectors of the same length, <code>sapply</code> will use them as columns of a matrix:

<code>sapply(1:5,function(x) rnorm(3,x))
</code>

If our function returns a 2 dimensional matrix, <code>sapply</code> will do essentially the same thing, treating each returned matrix as a single long vector:

<code>sapply(1:5,function(x) matrix(x,2,2))</code>

Unless we specify <code>simplify = "array"</code>, in which case it will use the individual matrices to build a multi-dimensional array:

<code>sapply(1:5,function(x) matrix(x,2,2), simplify = "array")</code>

Each of these behaviors is of course contingent on our function returning vectors or matrices of the same length or dimension.</li>
<li><strong>vapply</strong> - <em>When you want to use <code>sapply</code> but perhaps need to squeeze some more speed out of your code.</em>

For <code>vapply</code>, you basically give R an example of what sort of thing your function will return, which can save some time coercing returned values to fit in a single atomic vector.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Note that since the advantage here is mainly speed, this
# example is only for illustration. 
We're telling R that
# everything returned by length() should be an integer of length 1. 

vapply(x, FUN = length, FUN.VALUE = 0L) 
a  b  c  
1  3 91
</code></li>
<li><strong>mapply</strong> - <em>For when you have several data structures (e.g. 
vectors, lists) and you want to apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in <code>sapply</code>.</em>

This is multivariate in the sense that your function must accept multiple arguments.

<code>#Sums the 1st elements, the 2nd elements, etc. 

mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15
#To do rep(1,4), rep(2,3), etc.
mapply(rep, 1:4, 4:1)   
[[1]]
[1] 1 1 1 1

[[2]]
[1] 2 2 2

[[3]]
[1] 3 3

[[4]]
[1] 4
</code></li>
<li><strong>Map</strong> - <em>A wrapper to <code>mapply</code> with <code>SIMPLIFY = FALSE</code>, so it is guaranteed to return a list.</em>

<code>Map(sum, 1:5, 1:5, 1:5)
[[1]]
[1] 3

[[2]]
[1] 6

[[3]]
[1] 9

[[4]]
[1] 12

[[5]]
[1] 15
</code></li>
<li><strong>rapply</strong> - <em>For when you want to apply a function to each element of a <strong>nested list</strong> structure, recursively.</em>

To give you some idea of how uncommon <code>rapply</code> is, I forgot about it when first posting this answer! Obviously, I'm sure many people use it, but YMMV. 
<code>rapply</code> is best illustrated with a user-defined function to apply:

<code># Append ! to string, otherwise increment
myFun &lt;- function(x){
    if(is.character(x)){
      return(paste(x,"!",sep=""))
    }
    else{
      return(x + 1)
    }
}

#A nested list structure
l &lt;- list(a = list(a1 = "Boo", b1 = 2, c1 = "Eeek"), 
          b = 3, c = "Yikes", 
          d = list(a2 = 1, b2 = list(a3 = "Hey", b3 = 5)))


# Result is named vector, coerced to character          
rapply(l, myFun)

# Result is a nested list like l, with values altered
rapply(l, myFun, how="replace")
</code></li>
<li><strong>tapply</strong> - <em>For when you want to apply a function to <strong>subsets</strong> of a vector and the subsets are defined by some other vector, usually a factor.</em>

The black sheep of the *apply family, of sorts. 
The help file's use of the phrase "ragged array" can be a bit <a href="https://stackoverflow.com/questions/6297201/explain-r-tapply-description/6297396#6297396">confusing</a>, but it is actually quite simple.

A vector:

<code>x &lt;- 1:20</code>

A factor (of the same length!) defining groups:

<code>y &lt;- factor(rep(letters[1:5], each = 4))</code>

Add up the values in <code>x</code> within each subgroup defined by <code>y</code>:

<code>tapply(x, y, sum)  
 a  b  c  d  e  
10 26 42 58 74 
</code>

More complex examples can be handled where the subgroups are defined by the unique combinations of a list of several factors. 
<code>tapply</code> is similar in spirit to the split-apply-combine functions that are common in R (<code>aggregate</code>, <code>by</code>, <code>ave</code>, <code>ddply</code>, etc.) Hence its black sheep status.</li>

<b>Slice vector</b>
We can use lapply() or sapply() interchangeable to slice a data frame. 
We create a function, below_average(), that takes a vector of numerical values and returns a vector that only contains the values that are strictly above the average. 

below_ave &lt;- function(x) {  
    ave &lt;- mean(x) 
    return(x[x > ave])
}

Compare both results with the identical() function.
dataf_s&lt;- sapply(dataf, below_ave)
dataf_l&lt;- lapply(dataf, below_ave)
identical(dataf_s, dataf_l)


<h2>Principal Component Methods</h2>
<a href="Principal Component Methods.html">Principal Component Methods</a>


<h2>NLP techniques</h2>
<a href="NLP techniques.html">NLP techniques</a>
<br>


<h2>RMySQL R connect to MySQL</h2>
root
asdf1234
SHOW DATABASES

# 1. Library
library(RMySQL)

# 2. Settings
db_user &lt;- 'root'
db_password &lt;- 'asdf1234'
db_name &lt;- 'sampledb'
# db_table &lt;- 'example'
db_table &lt;- 'world'

db_host &lt;- '127.0.0.1' # for local access
db_port &lt;- 3306

# 3. Read data from db
mydb &lt;-  dbConnect(MySQL(), user = db_user, password = db_password,
         dbname = db_name, host = db_host, port = db_port)
s &lt;- paste0("select * from ", db_table)
rs &lt;- dbSendQuery(mydb, s)
df &lt;-  fetch(rs, n = -1)
on.exit(dbDisconnect(mydb))

<h2>convert R {xml_node} to plain text while preserving the tags</h2>
className = "#icnt"
keywordList &lt;- html_nodes(pagesource, className)
as.character(keywordList)

<h2>convert R objects into a binary format</h2>
x &lt;- list(1, 2, 3)
serialize(x, NULL)
The serialize() function is used to convert individual R objects into a binary format that can be communicated across an arbitrary connection. This may get sent to a file, but it could get sent over a network or other connection.

<h2>Convert an R Object to a Character String</h2>
x &lt;- c("a", "b", "aaaaaaaaaaa")
toString(x)
toString(x, width = 8)


<h2>html_node, html_nodes</h2>
html_node retrieves the first element it encounter, 
while html_nodes returns each matching element in the page as a list.

use html_nodes instead of html_node.

The toString() function collapse the list of strings into one.

library(rvest)
pagesource &lt;- read_html("url")

testpost &lt;- pagesource %>% 
  html_nodes("#contentmiddle>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>Excluding Nodes in RVest</h2>
library(rvest)
pagesource &lt;- read_html("url")

testpost &lt;- pagesource %>% 
  html_nodes("#content>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>xml_remove()</h2>
By using xml_remove(), you can literally remove any nodes

text &lt;- '
<table> <tr class="alt">
     <td>1</td>
     <td>2</td>
     <td class="hidden">3</td>
   </tr>
   <tr class="tr0 close notule">
     <td colspan="9">4</td> </tr>
</table>'

html_tree &lt;- read_html(text)

#select nodes you want to remove
hidden_nodes &lt;- html_tree %>% html_nodes(".hidden")
close_nodes &lt;- html_tree %>% html_nodes(".tr0.close.notule")

#remove those nodes
xml_remove(hidden_nodes)
xml_remove(close_nodes)

html_tree %>% html_table()


<h2>view all xml_nodeset class object (output of rvest::html_nodes)</h2>
print.AsIs(keywordList)

<h2>Install package loaclly</h2>
# 安装export包
if(!require(export)){
install.packages('export')
require(export)
}

下载安装包文件
打开git bash，执行命令：
git clone https://github.com/tomwenseleers/export.git

BUILD 安装包文件
R CMD BUILD export

安装包压缩文件
R CMD INSTALL

测试export包是否可以使用
require(export)

<h2>e1071 package Support vector machine</h2>
<a href="e1071 package SVM.html" class="whitebut ">e1071 package SVM</a>

<h2>substitute()</h2>
a &lt;- 1
b &lt;- 2
substitute(a + b + z) ## a + b + z

<h2>When to use CPUs vs GPUs vs TPUs?</h2>
Behind every machine learning algorithm is hardware crunching away at multiple gigahertz. 
You may have noticed several processor options when setting up Kaggle notebooks, but which one is best for you? In this blog post, we compare the relative advantages and disadvantages of using CPUs (<a href="https://www.intel.com/content/www/us/en/products/processors/xeon.html" target="_blank">Intel Xeon</a>*) vs GPUs (<a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a>) vs TPUs (<a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a>) for training machine learning models that were written using <a href="https://keras.io/" target="_blank">tf.keras</a> (Figure 1**). 
We’re hoping this will help you make sense of the options and select the right choice for your project.

<img class="lazy" data-src="https://miro.medium.com/max/1466/1*suXcuHEe29aKLPrQnXGBrg.png">

How we prepared the test
In order to compare the performance of CPUs vs GPUs vs TPUs for accomplishing common data science tasks, we used the <a href="https://www.tensorflow.org/datasets/catalog/tf_flowers" target="_blank">tf_flowers dataset</a> to train a convolutional neural network, and then the exact same code was run three times using the three different backends (CPUs vs GPUs vs TPUs; GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM). 
The accompanying <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">tutorial notebook</a> demonstrates a few best practices for getting the best performance out of your TPU.
For example:

Using a dataset of sharded files (<a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">e.g., .TFRecord</a>)
Using the <a href="https://www.tensorflow.org/guide/data" target="_blank">tf.data</a> API to pass the training data to the TPU
Using large batch sizes (e.g. 
batch_size=128)

By adding these precursory steps to your workflow, it is possible to avoid a common I/O bottleneck that otherwise prevents the TPU from operating at its full potential. 
You can find additional tips for optimizing your code to run on TPUs by visiting the official <a href="https://www.kaggle.com/docs/tpu" target="_blank">Kaggle TPU documentation</a>.
How the hardware performed
The most notable difference between the three hardware types that we tested was the speed that it took to train a model using <a href="https://keras.io/" target="_blank">tf.keras</a>. 
The tf.keras library is one of the most popular machine learning frameworks because tf.keras makes it easy to quickly experiment with new ideas. 
If you spend less time writing code then you have more time to perform your calculations, and if you spend less time waiting for your code to run, then you have more time to evaluate new ideas (Figure 2). 
tf.keras and TPUs are a powerful combination when participating in <a href="https://kaggle.com/c/flower-classification-with-tpus" target="_blank">machine learning competitions</a>!

<img class="lazy" data-src="https://miro.medium.com/max/1438/1*bqmG-YzgJzVeLbQ5Ym1iFg.png">
For our first experiment, we used the same code (a modified version*** of the <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">official tutorial notebook</a>) for all three hardware types, which required using a very small batch size of 16 in order to avoid out-of-memory errors from the CPU and GPU. 
Under these conditions, we observed that TPUs were responsible for a ~100x speedup as compared to CPUs and a ~3.5x speedup as compared to GPUs when training an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model (Figure 3). 
Because TPUs operate more efficiently with large batch sizes, we also tried increasing the batch size to 128 and this resulted in an additional ~2x speedup for TPUs and out-of-memory errors for GPUs and CPUs. 
Under these conditions, the TPU was able to train an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model more than 7x as fast as the GPU from the previous experiment****.

<img class="lazy" data-src="https://miro.medium.com/max/1438/1*p2X9DQcq9K5Iu76Kk82vrg.png">
The observed speedups for model training varied according to the type of model, with Xception and Vgg16 performing better than ResNet50 (Figure 4). Model training was the only type of task where we observed the TPU to outperform the GPU by such a large margin. 
For example, we observed that in our hands the TPUs were ~3x faster than CPUs and ~3x slower than GPUs for performing a small number of predictions (TPUs perform exceptionally when making predictions in some situations such as when <a href="https://docs.google.com/presentation/d/1O49AkNyYV48n0X4nWr7KE-5aask88pz9gBSQ26ZG-5o/edit#slide=id.g50ce3d3866_0_1590" target="_blank">making predictions</a> on very large batches, which were not present in this experiment).

<img class="lazy" data-src="https://miro.medium.com/max/46/1*p7U2zlYn9O5Yvjluh2P-dg.png">
<img class="lazy" data-src="https://miro.medium.com/max/1466/1*p7U2zlYn9O5Yvjluh2P-dg.png">
To supplement these results, we note that <a href="https://arxiv.org/abs/1907.10701" target="_blank">Wang<em> et. 
al</em></a> have developed a rigorous benchmark called ParaDnn [1] that can be used to compare the performance of different hardware types for training machine learning models. 
By using this method Wang<em> et. 
al</em> were able to conclude that the performance benefit for parameterized models ranged from 1x to 10x, and the performance benefit for real models ranged from 3x to 6.8x when a TPU was used instead of a GPU (Figure 5). 
TPUs perform best when combined with sharded datasets, large batch sizes, and large models.

<img class="lazy" data-src="https://miro.medium.com/max/1466/1*QbP2CPDZH5BQWlnaTtW3oA.png">
Price considerations when training models
While our comparisons treated the hardware equally, there is a sizeable difference in pricing. TPUs are ~5x as expensive as GPUs (<a href="https://cloud.google.com/compute/gpus-pricing" target="_blank">$1.46/hr</a> for a <a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a> GPU vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$8.00/hr</a> for a <a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a> vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$4.50/hr</a> for the TPUv2 with “on-demand” access on <a href="https://cloud.google.com/pricing/" target="_blank">GCP</a> ). 
If you are trying to optimize for cost then it makes sense to use a TPU if it will train your model at least 5 times as fast as if you trained the same model using a GPU.
We consistently observed model training speedups on the order of ~5x when the data was stored in <a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">a sharded format</a> in a <a href="https://www.kaggle.com/paultimothymooney/how-to-move-data-from-kaggle-to-gcs-and-back" target="_blank">GCS bucket</a> then passed to the TPU in large batch sizes, and therefore we recommend TPUs to cost-conscious consumers that are familiar with the <a href="http://tf.data" target="_blank">tf.data</a> API.
Some machine learning practitioners prioritize the reduction of model training time as opposed to prioritizing the reduction of model training costs. 
For someone that just wants to train their model as fast as possible, the TPU is the best choice. 
If you spend less time training your model, then you have more time to iterate upon new ideas. 
But don’t take our word for it — you can evaluate the performance benefits of CPUs, GPUs, and TPUs by running your own code in a <a href="https://www.kaggle.com/docs/kernels#the-kernels-environment" target="_blank">Kaggle Notebook</a>, free-of-charge. 
Kaggle users are already having a lot of fun and success experimenting with TPUs and text data: check out <a href="https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127333" target="_blank">this forum post</a> that describes how TPUs were used to train a BERT transformer model to win $8,000 (2nd prize) in a recent <a href="https://www.kaggle.com/c/tensorflow2-question-answering" target="_blank">Kaggle competition</a>.
Which hardware option should you choose?
In summary, we recommend CPUs for their versatility and for their large memory capacity. 
GPUs are a great alternative to CPUs when you want to speed up a variety of data science workflows, and TPUs are best when you specifically want to train a machine learning model as fast as you possibly can.
You can get better results by optimizing your code for the specific hardware that you are using and we think it would be especially interesting to compare runtimes for code that has been optimized for a GPU to runtimes for code that has been optimized for a TPU. 
For example, it would be interesting to record the time that it takes to train a gradient-boosted model using a GPU-accelerated library such as <a href="https://rapids.ai/" target="_blank">RAPIDS.ai</a> and then to compare that to the time that it takes to train a deep learning model using a TPU-accelerated library such as <a href="https://keras.io/" target="_blank">tf.keras</a>.
What is the least amount of time that one can train an accurate machine learning model? How many different ideas can you evaluate in a single day? When used in combination with tf.keras, TPUs allow machine learning practitioners to spend less time writing code and less time waiting for their code to run — leaving more time to evaluate new ideas and improve one’s performance in <a href="http://kaggle.com/c/flower-classification-with-tpus" target="_blank">Kaggle Competitions</a>.

<h3>Footnotes</h3>* CPU types vary according to variability. 
In addition to the Intel Xeon CPUs, you can also get assigned to either Intel Skylake, Intel Broadwell, or Intel Haswell CPUs. 
GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM).
** Image for Figure 1 from <a href="https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference" target="_blank">https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference,</a> with permission.
*** The tutorial notebook was modified to keep the parameters (e.g. 
batch_size, learning_rate, etc) consistent between the three different backends.
**** CPU and GPU experiments used a batch size of 16 because it allowed the Kaggle notebooks to run from top to bottom without memory errors or 9-hr timeout errors. 
Only TPU-enabled notebooks were able to run successfully when the batch size was increased to 128.


<h2>Diff function – Difference between elements of vector</h2>
Differences between elements of a vector

diff(x, lag = 1, differences = 1)
x – numeric vector
lag-an integer indicating how many lags to use.
Difference- order of difference

# diff in r examples
> x=c(1,2,3,5,8,13,21)
> diff(x)
[1] 1 1 2 3 5 8

The diff function provides the option “lag”.
The default specification of this option is 1.

If we want to increase the size of the lag, we can specify the lag option within the diff command as follows:

x &lt;- c(5, 2, 10, 1, 3)
diff(x, lag = 2)                # Apply diff with lag
# 5 -1 -7

Example of difference function in R with lag 1 and differences 2:

#difference function in R with lag=1 and differences=2

diff(c(2,3,5,18,4,6,4),lag=1,differences=2)
First it is differenced with lag=1 and the result is again differenced with lag=1
So the output will be
[1]   1  11  -27   16   -4

ie. get the lag difference result, and then redo the difference again on the result:
2,3,5,18,4,6,4
  1,2,13,-14,2,-2
    1,11,-27,16,-4

<h2>cut2 function</h2>
cut2(x, cuts, m, g, levels.mean, digits, minmax=TRUE, oneval=TRUE)
Cut a Numeric Variable into Intervals
but left endpoints are inclusive and labels are of the form [lower, upper), except that last interval is [lower,upper].

x &lt;- runif(1000, 0, 100)
z &lt;- cut2(x, c(10,20,30))
table(z)
table(cut2(x, g=10))      # quantile groups
table(cut2(x, m=50))      # group x into intevals with at least 50 obs.

<h2>To clear up the memory</h2>
rm(list = ls())
.rs.restartR() # this will restart

memory.size(max=T) # gives the amount of memory obtained by the OS
memory.size(max=F) # gives the amount of memory being used
m = matrix(runif(10e7), 10000, 1000)
memory.size(max=F)

To clear up the memory
gc()
memory.size(max=F)
# still some memory being used

<h2>remove XML nodes</h2>
<a href="https://cran.r-project.org/web/packages/xml2/vignettes/modification.html" class="whitebut ">Node Modification</a>
<a href="https://cran.r-project.org/web/packages/XML/XML.pdf" class="whitebut ">Package XML</a>

#find parent nodes
parent&lt;- review %>% html_nodes("blockquote")

#find children nodes to exclude
toremove&lt;-parent %>% html_node("div.bbcode_container")

#remove nodes
xml_remove(toremove)

The xml_remove() can be used to remove a node (and it’s children) from a tree. 

library(XML)
r &lt;- xmlRoot(doc)
removeNodes(r[names(r) == "location"])

<h2>Comment out block of code</h2>

if(FALSE) {
  all your code
}


<h2>Reading XML data</h2>
Data in XML format are rarely organized in a way that would allow the xmlToDataFrame function to work. 
You're better off extracting everything in lists and then binding the lists together in a data frame:

require(XML)
data &lt;- xmlParse("http://forecast.weather.gov/MapClick.php?lat=29.803&lon=-82.411&FcstType=digitalDWML")

xml_data &lt;- xmlToList(data)

<code>&gt; install.packages("XML")</code>
<code>&gt; library(XML)
text = paste0("&lt;bookstore>&lt;book>","&lt;title>Everyday Italian&lt;/title>","&lt;author>Giada De Laurentiis&lt;/author>","&lt;year>2005&lt;/year>","&lt;/book>&lt;/bookstore>")
</code>
Parse the XML file
<code>xmldoc &lt;- xmlParse(text)
rootNode &lt;- xmlRoot(xmldoc)
rootNode[1]

xmlToDataFrame(nodes = getNodeSet(xmldoc, "//title"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//author"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//book"))

newdf = xmlToDataFrame(getNodeSet(xmldoc, "//book"))
newdf = xmlToDataFrame(getNodeSet(xmldoc, "//title"))
</code>

Extract XML data:

<code>&gt; data &lt;- xmlSApply(rootNode,function(x) xmlSApply(x, xmlValue))</code>

text = paste0("&lt;CD>","&lt;TITLE>Empire Burlesque&lt;/TITLE>","&lt;ARTIST>Bob Dylan&lt;/ARTIST>","&lt;COUNTRY>USA&lt;/COUNTRY>","&lt;COMPANY>Columbia&lt;/COMPANY>","&lt;PRICE>10.90&lt;/PRICE>","&lt;YEAR>1985&lt;/YEAR>","&lt;/CD>")
<code>xmldoc &lt;- xmlParse(text)
rootNode &lt;- xmlRoot(xmldoc)
rootNode[1]</code>

Convert the extracted data into a data frame:

<code>&gt; cd.catalog &lt;- data.frame(t(data),row.names=NULL)</code>

Verify the results

The <code>xmlParse</code> function returns an object of the <code>XMLInternalDocument</code> class, which is a C-level internal data structure.
The <code>xmlRoot()</code> function gets access to the root node and its elements. 
We check the first element of the root node:

<code>&gt; rootNode[1]

$CD
&lt;CD&gt;
  &lt;TITLE&gt;Empire Burlesque&lt;/TITLE&gt;
  &lt;ARTIST&gt;Bob Dylan&lt;/ARTIST&gt;
  &lt;COUNTRY&gt;USA&lt;/COUNTRY&gt;
  &lt;COMPANY&gt;Columbia&lt;/COMPANY&gt;
  &lt;PRICE&gt;10.90&lt;/PRICE&gt;
  &lt;YEAR&gt;1985&lt;/YEAR&gt;
&lt;/CD&gt;
attr(,"class")
[1] "XMLInternalNodeList" "XMLNodeList"</code>
To extract data from the root node, we use the <code>xmlSApply()</code> function iteratively over all the children of the root node. 
The <code>xmlSApply</code> function returns a matrix.
To convert the preceding matrix into a data frame, we transpose the matrix using the <code>t()</code> function. 
We then extract the first two rows from the <code>cd.catalog</code> data frame:

<code>&gt; cd.catalog[1:2,]
             TITLE       ARTIST COUNTRY     COMPANY PRICE YEAR
1 Empire Burlesque    Bob Dylan     USA    Columbia 10.90 1985
2  Hide your heart Bonnie Tyler      UK CBS Records  9.90 1988</code>

XML data can be deeply nested and hence can become complex to extract. 
Knowledge of <code>XPath</code> will be helpful to access specific XML tags. 
R provides several functions such as <code>xpathSApply</code> and <code>getNodeSet</code> to locate specific elements.
<h4>Extracting HTML table data from a web page</h4>
Though it is possible to treat HTML data as a specialized form of XML, R provides specific functions to extract data from HTML tables as follows:

<code>&gt; url &lt;- "http://en.wikipedia.org/wiki/World_population"

webpage = read_html(url)
output = htmlParse(webpage)
tables = readHTMLTable(output)
world.pop = tables[[5]]

table.list = readHTMLTable(output, header=F)

u = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
webpage = read_html(u)
tables = readHTMLTable(webpage)
names(tables)
</code>
The <code>readHTMLTable()</code> function parses the web page and returns a <code>list</code> of all tables that are found on the page. 
For tables that have an <code>id</code> attribute, the function uses the <code>id</code> attribute as the name of that list element.
We are interested in extracting the "10 most populous countries," which is the fifth table; hence we use <code>tables[[5]]</code>.

<h4>Extracting a single HTML table from a web page</h4>

A single table can be extracted using the following command:

<code>&gt; table &lt;- readHTMLTable(url,which=5)</code>
Specify <code>which</code> to get data from a specific table. 
R returns a data frame.

<h2>use xpathSApply to extract html</h2>
library(RCulr)
library(XML)
 
html &lt;- read_html("http://tonybreyal.wordpress.com/2011/11/17/cool-hand-luke-aldwych-theatre-london-2011-production/", followlocation = TRUE)

doc = htmlParse(html, asText=TRUE)
plain.text &lt;- xpathSApply(doc, "//p", xmlValue)
cat(paste(plain.text, collapse = "\n"))

pageHeader = "http://www.hkej.com/template/dnews/jsp/toc_main.jsp"
html &lt;- read_html(pageHeader, followlocation = TRUE)
doc = htmlParse(html, asText=TRUE)
plain.text &lt;- xpathSApply(doc, "//a", xmlValue)
cat(paste(plain.text, collapse = "\n"))

<h2>reading XML using xml2</h2>
library(xml2)
library(purrr)

txt &lt;- '&lt;Doc name="Doc1">
    &lt;Lists Count="1">
        &lt;List Name="List1">
            &lt;Points Count="3">
                &lt;Point Id="1">
                    &lt;Tags Count ="1">"a"&lt;/Tags>
                    &lt;Point Position="1"  /> 
                &lt;/Point>
                &lt;Point Id="2">
                    &lt;Point Position="2"  /> 
                &lt;/Point>
                &lt;Point Id="3">
                    &lt;Tags Count="1">"c"&lt;/Tags>
                    &lt;Point Position="3"  /> 
                &lt;/Point>
            &lt;/Points>
        &lt;/List>
    &lt;/Lists>
&lt;/Doc>'

doc &lt;- read_xml(txt)
xml_find_all(doc, ".//Points/Point") %>% 
  map_df(function(x) {
    list(
      Point=xml_attr(x, "Id"),
      Tag=xml_find_first(x, ".//Tags") %>%  xml_text() %>%  gsub('^"|"$', "", .),
      Position=xml_find_first(x, ".//Point") %>% xml_attr("Position")
    )
  })



<h2>An Introduction to XPath: How to Get Started</h2>

XPath is a powerful language that is often used for scraping the web. 
It allows you to select nodes or compute values from an XML or HTML document and is actually one of the languages that you can use to extract web data using Scrapy. 
The other is CSS and while CSS selectors are a popular choice, XPath can actually allow you to do more.

With XPath, you can extract data based on text elements' contents, and not only on the page structure. 
So when you are scraping the web and you run into a hard-to-scrape website, XPath may just save the day (and a bunch of your time!).

This is an introductory tutorial that will walk you through the basic concepts of XPath, crucial to a good understanding of it, before diving into more complex use cases.
<h3>The basics</h3>
Consider this HTML document:

<code>&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;My page&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h2&gt;Welcome to my &lt;a href="#"&gt;page&lt;/a&gt;&lt;/h2&gt;
    &lt;p&gt;This is the first paragraph.&lt;/p&gt;
    &lt;!-- this is the end --&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

XPath handles any XML/HTML document as a tree. 
This tree's root node is not part of the document itself. 
It is in fact the parent of the document element node (<code>&lt;html&gt;</code> in case of the HTML above). 
This is how the XPath tree for the HTML document looks like:

<img class="lazy" data-src="https://blog.scrapinghub.com/hs-fs/hubfs/tree-7.png" style="background-color: gray">

As you can see, there are many node types in an XPath tree:

<li><strong>Element node:</strong> represents an HTML element, a.k.a an HTML tag.</li>
<li><strong>Attribute node:</strong> represents an attribute from an element node, e.g. 
“href” attribute in <code>&lt;a href=”http://www.example.com”&gt;example&lt;/a&gt;</code>.</li>
<li><strong>Comment node:</strong> represents comments in the document (<code>&lt;!-- … --&gt;</code>).</li>
<li><strong>Text node:</strong> represents the text enclosed in an element node (<code>example</code> in <code>&lt;p&gt;example&lt;/p&gt;</code>).</li>
</ul>
Distinguishing between these different types is useful to understand how XPath expressions work. 
Now let's start digging into XPath.

Here is how we can select the title element from the page above using an XPath expression:

/html/head/title


This is what we call a location path. 
It allows us to specify the path from the <strong>context node</strong> (in this case the root of the tree) to the element we want to select, as we do when addressing files in a file system. 
The location path above has three location steps, separated by slashes. 
It roughly means: <em>start from the ‘html’ element, look for a ‘head’ element underneath, and a ‘title’ element underneath that ‘head’</em>. 
The context node changes in each step. 
For example, the <code>head</code> node is the context node when the last step is being evaluated.

However, we usually don't know or don’t care about the full explicit node-by-node path, we just care about the nodes with a given name. 
We can select them using:

//title


Which means:<em> look in the whole tree, starting from the root of the tree (<code>//</code>) and select only those nodes whose name matches <code>title</code></em>. 
In this example, <code>//</code> is the <strong>axis</strong> and <code>title</code> is the <strong>node test</strong>.

In fact, the expressions we've just seen are using XPath's abbreviated syntax. 
Translating <code>//title</code> to the full syntax we get:

/descendant-or-self::node()/child::title


So, <code>//</code> in the abbreviated syntax is short for <code>descendant-or-self</code>, which means <em>the current node or any node below it in the tree</em>. 
This part of the expression is called the <strong>axis</strong> and it specifies a set of nodes to select from, based on their direction on the tree from the current context (downwards, upwards, on the same tree level). 
Other examples of axes are: parent, child, ancestor, etc -- we’ll dig more into this later on.

The next part of the expression, <code>node()</code>, is called a <strong>node test</strong>, and it contains an expression that is evaluated to decide whether a given node should be selected or not. 
In this case, it selects nodes from all types. 
Then we have another axis,<code>child</code>, which means <em>go to the child nodes from the current context</em>, followed by another node test, which selects the nodes named as <code>title</code>.

<blockquote>
So, the <strong>axis</strong> defines where in the tree the <strong>node test</strong> should be applied and the nodes that match the node test will be returned as a <strong>result</strong>.

</blockquote>
You can test nodes against their name or against their type.

Here are some examples of name tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>/html</td>
<td>Selects the node named <code>html</code>, which is under the root.</td>
</tr>
<tr>
<td>/html/head</td>
<td>Selects the node named <code>head</code>, which is under the <code>html</code> node.</td>
</tr>
<tr>
<td>//title</td>
<td>Selects all the <code>title</code> nodes from the HTML tree.</td>
</tr>
<tr>
<td>//h2/a</td>
<td>Selects all <code>a</code> nodes which are directly under an <code>h2</code> node.</td>
</tr>
</tbody>
</table>
And here are some examples of node type tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//comment()</td>
<td>Selects only comment nodes.</td>
</tr>
<tr>
<td>//node()</td>
<td>Selects any kind of node in the tree.</td>
</tr>
<tr>
<td>//text()</td>
<td>Selects only text nodes, such as "This is the first paragraph".</td>
</tr>
<tr>
<td>//*</td>
<td>Selects all nodes, except comment and text nodes.</td>
</tr>
</tbody>
</table>
We can also combine name and node tests in the same expression. 
For example:

//p/text()


This expression selects the text nodes from inside <code>p</code> elements. 
In the HTML snippet shown above, it would select "This is the first paragraph.".

Now, <strong>let’s see how we can further filter and specify things</strong>. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li&gt;Quote 1&lt;/li&gt;
      &lt;li&gt;Quote 2 with &lt;a href="..."&gt;link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Quote 3 with &lt;a href="..."&gt;another link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;h2&gt;Quote 4 title&lt;/h2&gt; ...&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the first <code>li</code> node from the snippet above. 
We can do this with:

//li[position() = 1]


The expression surrounded by square brackets is called a predicate and it filters the node set returned by <code>//li</code> (that is, all <code>li</code> nodes from the document) using the given condition. 
In this case it checks each node's position using the <code>position()</code> function, which returns the position of the current node in the resulting node set (notice that positions in XPath start at 1, not 0). 
We can abbreviate the expression above to:

//li[1]


Both XPath expressions above would select the following element:

    &lt;li class="quote"&gt;Quote 1&lt;/li&gt;


Check out a few more predicate examples:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//li[position()%2=0]</td>
<td>Selects the <code>li</code> elements at even positions.</td>
</tr>
<tr>
<td>//li[a]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element.</td>
</tr>
<tr>
<td>//li[a or h2]</td>
<td>Selects the <code>li</code> elements which enclose either an <code>a</code> or an <code>h2</code> element.</td>
</tr>
<tr>
<td>//li[ a [ text() = "link" ] ]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element whose text is "link". 
Can also be written as <code>//li[ a/text()="link" ]</code>.</td>
</tr>
<tr>
<td>//li[last()]</td>
<td>Selects the last <code>li</code> element in the document.</td>
</tr>
</tbody>
</table>
So, a location path is basically composed by steps, which are separated by <code>/</code> and each step can have an axis, a node test and a predicate. 
Here we have an expression composed by two steps, each one with axis, node test and predicate:

&lt;span style="font-weight: 400;"&gt;//li[ 4 ]/h2[ text() = "Quote 4 title" ]&lt;/span&gt;


And here is the same expression, written using the non-abbreviated syntax:

/descendant-or-self::node()<br>
    /child::li[ position() = 4 ]<br>
        /child::h2[ text() = "Quote 4 title" ]


We can also <strong>combine</strong> multiple XPath expressions in a single one using the union operator <code>|</code>. 
For example, we can select all <code>a</code> and <code>h2</code> elements in the document above using this expression:

//a | //h2


Now, consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li id="begin"&gt;&lt;a href="https://scrapy.org"&gt;Scrapy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://scrapinghub.com"&gt;Scrapinghub&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://blog.scrapinghub.com"&gt;Scrapinghub Blog&lt;/a&gt;&lt;/li&gt;
      &lt;li id="end"&gt;&lt;a href="http://quotes.toscrape.com"&gt;Quotes To Scrape&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the <code>a</code> elements whose link points to an HTTPS URL. 
We can do it by checking their <code>href</code> <strong>attribute</strong>:

//a[starts-with(@href, "https")]


This expression first selects all the <code>a</code> elements from the document and for each of those elements, it checks whether their <code>href</code> attribute starts with "https". 
We can access any node attribute using the <code>@attributename</code> syntax.

Here we have a few additional examples using attributes:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//a[@href="https://scrapy.org"]</td>
<td>Selects the <code>a</code> elements pointing to https://scrapy.org.</td>
</tr>
<tr>
<td>//a/@href</td>
<td>Selects the value of the <code>href</code> attribute from all the <code>a</code> elements in the document.</td>
</tr>
<tr>
<td>//li[@id]</td>
<td>Selects only the <code>li</code> elements which have an <code>id</code> attribute.</td>
</tr>
</tbody>
</table>
<h3>More on Axes</h3>
We've seen only two types of axes so far:

<li>descendant-or-self</li>
<li>child</li>
</ul>
But there's plenty more where they came from and we'll see a few examples. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;p&gt;Intro paragraph&lt;/p&gt;
    &lt;h1&gt;Title #1&lt;/h1&gt;
    &lt;p&gt;A random paragraph #1&lt;/p&gt;
    &lt;h1&gt;Title #2&lt;/h1&gt;
    &lt;p&gt;A random paragraph #2&lt;/p&gt;
    &lt;p&gt;Another one #2&lt;/p&gt;
    A single paragraph, with no markup
    &lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Now we want to extract only the first paragraph after each of the titles. 
To do that, we can use the <code>following-sibling</code> axis, which selects all the siblings after the context node. 
Siblings are nodes who are children of the same parent, for example all children nodes of the <code>body</code> tag are siblings. 
This is the expression:

//h1/following-sibling::p[1]


In this example, the context node where the <code>following-sibling</code> axis is applied to is each of the <code>h1</code> nodes from the page.

What if we want to select only the text that is right before the <code>footer</code>? We can use the <code>preceding-sibling</code> axis:

//div[@id='footer']/preceding-sibling::text()[1]


In this case, we are selecting the first text node before the <code>div</code> footer (<em>"A single paragraph, with no markup"</em>).

XPath also allows us to select elements based on their text content. 
We can use such a feature, along with the <code>parent</code> axis, to select the parent of the <code>p</code> element whose text is "Footer text":

//p[ text()="Footer text" ]/..


The expression above selects <code>&lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;</code>. 
As you may have noticed, we used <code>..</code> here as a shortcut to the <code>parent</code> axis.

As an alternative to the expression above, we could use:

//*[p/text()="Footer text"]


It selects, from all elements, the ones that have a <code>p</code> child which text is "Footer text", getting the same result as the previous expression.

You can find additional axes in the XPath specification: https://www.w3.org/TR/xpath/#axes

<h3>Wrap up</h3>
XPath is very powerful and this post is just an introduction to the basic concepts. 
If you want to learn more about it, check out these resources:

<li>http://zvon.org/comp/r/tut-XPath_1.html</li>
<li>http://fr.slideshare.net/scrapinghub/xpath-for-web-scraping</li>
<li>https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/</li>
</ul>
And <strong>stay tuned</strong>, because we will post a series with more XPath tips from the trenches in the following months.

<h2>to handle UTF</h2>
options("encoding" = "native.enc") # this is the natural environment
Sys.setlocale(category = 'LC_ALL', 'Chinese')	# to show chinese
# Sys.getlocale()
# options("encoding")

theNewsHeader = readLines("newsHeader.txt", encoding="UTF-8") # load UTF-8 file
options("encoding" = "UTF-8") # write UTF-8
sink("temp.html")

<h2><span class="gold embossts">R jsonlite to handle JSON</span></h2>
install.packages("jsonlite")
library(jsonlite)

# convert data frame to JSON array
my.json &lt;- toJSON(mtcars)

# convert JSON array to data frame
my.df &lt;- fromJSON(my.json)

# check data equality
all.equal(mtcars, my.df)
[1] TRUE

- set simplifyVector to FALSE, fromJSON will keep the raw JSON structure
ie, convert to list
fromJSON(json, simplifyVector = FALSE)

- fromJSON will convert multiple JSON structures to data frame
we may convert JSOn to data frame, and after fiddling, toJSON back to JSON.

- fromJSON will convert JSON matrix to R matrix

- higher order dimension JSON will be converted to R matrixs

<h2>Extract Components from Lists</h2>
Using [ ]
to extract a list components

Using [[ ]]
to extract only a single component

<h2>to view a list or dataframe</h2>
names(test), summary(test), head(test), tail(test), str(test)
typeof(test)

<h2>R function: cut</h2>
v &lt;- c( 8, 13, 19, 3, 14, 7, 6, 12, 18, 9, 7, 14, 2, 3, 8, 11, 17)
c &lt;- cut(v, c(0, 5, 10, 15, 20))
str(c)
 Factor w/ 4 levels "(0,5]","(5,10]",..: 2 3 4 1 3 2 2 3 4 2 ...

c # shows every element's category
#
#  [1] (5,10]  (10,15] (15,20] (0,5]   (10,15] (5,10]  (5,10]  (10,15] (15,20]
# [10] (5,10]  (5,10]  (10,15] (0,5]   (0,5]   (5,10]  (10,15] (15,20]

# Levels: (0,5] (5,10] (10,15] (15,20]

<h2>use cumsum() to create cumulative frequency graph</h2>

dataset = sample(1:20,100, replace= TRUE)
breaks = seq(0, 20, by=2) 
datasetCategory = cut(dataset, breaks, right=FALSE) 
dataset.freq = table(datasetCategory)

barplot(dataset.freq) # this show every category but not cumulative

We then compute its cumulative frequency with cumsum, add a starting zero element, and plot the graph.

cumfreq0 = c(0, cumsum(dataset.freq)) 
plot(breaks, cumfreq0,                 # plot the data 
 main="Old Faithful Eruptions",        # main title 
 xlab="dataset minutes",               # x−axis label 
 ylab="cumulative frequency graph")    # y−axis label 
lines(breaks, cumfreq0)                # join the points

<h2>to prevent scientific notation</h2>
Use a large positive value like 999 in options:
options(scipen=999)
to revert it back, the default scipen is 0

<h2>process daily data</h2>
# kline_dayqfq={"code":0,"msg":"","data":{"hk00700":{"qfqday":[["2020-01-14","410.000","400.400","413.000","396.600","26827634.000",{},"0.000","1086386.492"],

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/hkfqkline/get?_var=kline_dayqfq&param=hk00700,day,,,40,qfq"

my.json &lt;- readLines(urlAddr, warn=F)
my.json = gsub("kline_dayqfq=","",my.json) # remove the leading command

my.dataframe = fromJSON(my.json)
my.dataframe = my.dataframe[[3]][[1]][[1]] # 40 obs., list of list
# chr "2020-01-14"   Date   1
# chr "410.000"      open   2
# chr "400.400"      close  3
# chr "413.000"      high   4
# chr "396.600"      low    5
# chr "26827634.000" Qty    6
# Named list()              7
# chr "0.000"               8
# chr "1086386.492"  Amt    9

my.dataframe[[1]][1]  # date
my.dataframe[[1]][3]  # close

for (i in 1:40){      # remove column 7
  my.dataframe[[i]] = my.dataframe[[i]][-(7:8)]
}

dataMatrix = matrix(unlist(my.dataframe), nrow=40, ncol=7)  # convert to matrix

<h2>process minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00981"
my.json &lt;- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only the third item is useful
# List of 3
#  $ code: int 0
#  $ msg : chr ""
#  $ data:List of 1

my.dataframe = my.dataframe[[3]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec"
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, most recent day on top

datalist = unlist(my.list) # this is all strings in one vector


<h2>statistics of minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00388"
my.json &lt;- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only list 3 is useful
# List of 3
#  $ code: int 0
#  $ msg : chr ""
#  $ data:List of 1

my.dataframe = my.dataframe[[3]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec", 5 obs for 5days
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, nearest day on top

datalist = unlist(my.list) # this is all strings in one vector

datalist = gsub("^.* ","",datalist) # this is the amount
datalist = round(as.numeric(datalist)/10000,0) # units in wan
datalist = sort(datalist)
datalist = datalist[-(1:20)]
datalist = datalist[-( (length(datalist)-20):length(datalist))] # remove the extremes

# max(datalist); min(datalist); length(datalist)

sections &lt;- cut(datalist, breaks = 100)
table(sections)
barplot(table(sections))

cumulative sums
plot(cumsum(table(sections)))

<h2>R examples</h2>
https://www.datamentor.io/r-programming/examples/
http://www.rexamples.com
https://www.guru99.com/r-tutorial.html
https://r4stats.com/examples/programming/
https://www.statmethods.net/r-tutorial/index.html
http://rprogramming.net

<h2>output text to the R console in color</h2>
library(crayon)
cat(blue("Hello", "world!\n"))

Genaral styles
reset, bold
blurred (usually called ‘dim’, renamed to avoid name clash)
italic (not widely supported)
underline, inverse, hidden
strikethrough (not widely supported)

Text colors
black, red, green, yellow, blue, magenta, cyan, white
silver (usually called ‘gray’, renamed to avoid name clash)

Background colors
bgBlack, bgRed, bgGreen, bgYellow, bgBlue, bgMagenta, bgCyan, bgWhite

Styling
The styling functions take any number of character vectors as arguments, and they concatenate and style them:

Crayon defines the %+% string concatenation operator, to make it easy to assemble stings with different styles.

cat("... to highlight the " %+%
    red("search term") %+%
    " in a block of text\n")

Styles can be combined using the $ operator:
  cat(yellow$bgMagenta$bold('Hello world!\n'))
See also combine_styles().

Styles can also be nested, and then inner style takes precedence:
  cat(green(
    'I am a green line ' %+%
    blue$underline$bold('with a blue substring') %+%
    ' that becomes green again!\n'
  ))

define your own themes:
  error &lt;- red $ bold
  warn &lt;- magenta $ underline
  note &lt;- cyan
  cat(error("Error: subscript out of bounds!\n"))
  cat(warn("Warning: shorter argument was recycled.\n"))
  cat(note("Note: no such directory.\n"))

See Also make_style() for using the 256 ANSI colors.

Examples
cat(blue("Hello", "world!"))
cat("... to highlight the " %+% red("search term") %+%
    " in a block of text")
cat(yellow$bgMagenta$bold('Hello world!'))
cat(green(
 'I am a green line ' %+%
 blue$underline$bold('with a blue substring') %+%
 ' that becomes green again!'
))
error &lt;- red $ bold
warn &lt;- magenta $ underline
note &lt;- cyan
cat(error("Error: subscript out of bounds!\n"))
cat(warn("Warning: shorter argument was recycled.\n"))
cat(note("Note: no such directory.\n"))

<h3>style - Add Style To A String</h3>
Usage
style(string, as = NULL, bg = NULL)
cat(style("I am pink\n", "pink"))
cat(style("#4682B433\n", "#4682B433"))
cat(style("#002050\n", "#002050"))


<h3>rgb()</h3>
To use the function:
rgb(red, green, blue, alpha) : quantity of red (between 0 and 1), of green and of blue, and finally transparency (alpha).
newcolor = rgb(0.5, 0.2, 0.1, 0.8)
newcolor
"#80331ACC"

cat(style("newcolor\n", newcolor))  # note, without quotation marks


<h3>make_style</h3>
pink &lt;- make_style("pink")
bgMaroon &lt;- make_style(rgb(0.93, 0.19, 0.65), bg = TRUE)
cat(bgMaroon(pink("pink style.\n")))

## Create a new style for pink and maroon background
make_style(pink = "pink")
make_style(bgMaroon = rgb(0.0, 0.3, 0.3), bg = TRUE)
"pink" %in% names(styles())
"bgMaroon" %in% names(styles())

cat(style("I am pink, too!\n", "pink"))
cat(style("I am pink, too!\n", "pink", bg = "blue")) # color will change
cat(style("I am pink, too!\n", "pink", bg = "bgMaroon"))
cat(style("I am pink, too!\n", "pink", bg = "cyan"))

<h2>print strings with wordwraps</h2>

strwrap(astring, width = 110, indent = 5, exdent = 2))
use writeLines to print it
note: control characters inside string will be ignored.

astring = "Substituted with the text matched by the capturing group that can be found by counting as many opening parentheses of named or numbered capturing groups as specified by the number from right to left starting at the backreference."

writeLines(strwrap(astring, width = 110, indent = 5, exdent = 2))

<h2>R.utils withTimeout()</h2>

withTimeout() from package R.utils, in concert with tryCatch(), might provide a cleaner solution.

For example:
require(R.utils)

for(i in 1:5) {
    tryCatch(
        expr = {
            withTimeout({Sys.sleep(i); cat(i, "\n")}, 
                         timeout = 3.1)
            }, 
        TimeoutException = function(ex) cat("Timeout. Skipping.\n")
    )
}

# 1 
# 2 
# 3 
# Timeout. Skipping.
# Timeout. Skipping.

In the artificial example above:

The first argument to withTimeout() contains the code to be evaluated within each loop.

The timeout argument to withTimeout() sets the time limit in seconds.

The TimeoutException argument to tryCatch() takes a function that is to be executed when an iteration of the loop is timed out.

<h2>drawing SVG</h2>
<a href="https://cran.r-project.org/web/packages/RIdeogram/vignettes/RIdeogram.html" class="whitebut ">RIdeogram: drawing SVG graphics</a>

<a href="https://cran.r-project.org/web/packages/magick/vignettes/intro.html" class="whitebut ">Magick: Advanced Image-Processing</a>

<a href="http://ralanbutler.com/blog/2016/03/31/animated-SVG-R" class="whitebut ">Animating an SVG</a>

svglite + ggsave function
Saving a plot as an SVG

sample code:
require("ggplot2")

#some sample data
head(diamonds) 

#to see actually what will be plotted and compare 
qplot(clarity, data=diamonds, fill=cut, geom="bar")

#save the plot in a variable image to be able to export to svg
image=qplot(clarity, data=diamonds, fill=cut, geom="bar")

#This actually save the plot in a image
ggsave(file="test.svg", plot=image, width=10, height=8)

<h2>Package ‘TTR’</h2>
Technical Trading Rules
x=c(1,2,4,3,5,6,5,4,5,6,7,9,10,11,10)

Usage
SMA(x, n = 4)
EMA(x, n = 4)
DEMA(x, n = 4)
WMA(x, n = 4, wts = 1:n)
EVWMA(price, volume, n = 4)
ZLEMA(x, n = 4, ratio = NULL)
VWAP(price, volume, n = 4)
VMA(x, w, ratio = 1)
HMA(x, n = 20)
ALMA(x, n = 9, offset = 0.85, sigma = 6)

<h3>Weighted moving average WMA</h3>
<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Weighted_moving_average_weights_N%3D15.png/220px-Weighted_moving_average_weights_N%3D15.png">

<h3>Exponential moving average EMA</h3>
EMA is more exagerating
<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Exponential_moving_average_weights_N%3D15.png/220px-Exponential_moving_average_weights_N%3D15.png">

<h2>自然语言处理中的Transformer和BERT</h2>
2018年马上就要过去，回顾深度学习在今年的进展，让人印象最深刻的就是谷歌提出的应用于自然语言处理领域的BERT解决方案，BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805">
https://arxiv.org/abs/1810.04805</a>）。
BERT解决方案刷新了各大NLP任务的榜单，在各种NLP任务上都做到state of the art。
这里我把BERT说成是解决方案，而不是一个算法，因为这篇文章并没有提出新的算法模型，还是沿用了之前已有的算法模型。
BERT最大的创新点，在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的NLP任务，因此BERT这篇论文对于算法模型完全不做介绍，以至于在我直接看这篇文章的时候感觉云里雾里。
但是本文中，我会从算法模型到解决方案，进行完整的诠释。
本文中我会分3个部分进行介绍，第一部分我会大概介绍一下NLP的发展，第二部分主要讲BERT用到的算法，最后一部分讲BERT具体是怎么操作的。

<h3>一，NLP的发展</h3>
要处理NLP问题，首先要解决文本的表示问题。
虽然我们人去看文本，能够清楚明白文本中的符号表达什么含义，但是计算机只能做数学计算，需要将文本表示成计算机可以处理的形式。
最开始的方法是采用one hot，比如，我们假设英文中常用的单词有3万个，那么我们就用一个3万维的向量表示这个词，所有位置都置0，当我们想表示apple这个词时，就在对应位置设置1，如图1.1所示。
这种表示方式存在的问题就是，高维稀疏，高维是指有多少个词，就需要多少个维度的向量，稀疏是指，每个向量中大部分值都是0。
另外一个不足是这个向量没有任何含义。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-597b011ddd148eb53b5a90730b6090ae_b.jpg">

<figcaption>图1.1</figcaption>
后来出现了词向量，word embedding，用一个低维稠密的向量去表示一个词，如图1.2所示。
通常这个向量的维度在几百到上千之间，相比one hot几千几万的维度就低了很多。
词与词之间可以通过相似度或者距离来表示关系，相关的词向量相似度比较高，或者距离比较近，不相关的词向量相似度低，或者距离比较远，这样词向量本身就有了含义。
文本的表示问题就得到了解决。
词向量可以通过一些无监督的方法学习得到，比如CBOW或者Skip-Gram等，可以预先在语料库上训练出词向量，以供后续的使用。
顺便提一句，在图像中就不存在表示方法的困扰，因为图像本身就是数值矩阵，计算机可以直接处理。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-840859265e735cce77233bb42a4bee6a_b.png">

<figcaption>图1.2</figcaption>
NLP中有各种各样的任务，比如分类（Classification），问答（QA），实体命名识别（NER）等。
对于这些不同的任务，最早的做法是根据每类任务定制不同的模型，输入预训练好的embedding，然后利用特定任务的数据集对模型进行训练，如图1.3所示。
这里存在的问题就是，不是每个特定任务都有大量的标签数据可供训练，对于那些数据集非常小的任务，恐怕就难以得到一个理想的模型。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-4546b7aa51af50d3ac0c7504f965cc70_b.jpg">

<figcaption>图1.3</figcaption>
我们看一下图像领域是如何解决这个问题的。
图像分类是计算机视觉中最基本的任务，当我要解决一个小数据集的图像分类任务时，该怎么做？CV领域已经有了一套成熟的解决方案。
我会用一个通用的网络模型，比如Vgg，ResNet或者GoogleNet，在ImageNet上做预训练（pre-training）。
ImageNet有1400万张有标注的图片，包含1000个类别，这样的数据规模足以训练出一个规模庞大的模型。
在训练过程中，模型会不断的学习如何提取特征，底层的CNN网络结构会提取边缘，角，点等通用特征，模型越往上走，提取的特征也越抽象，与特定的任务更加相关。
当完成预训练之后，根据我自己的分类任务，调整最上层的网络结构，然后在小数据集里对模型进行训练。
在训练时，可以固定住底层的模型参数只训练顶层的参数，也可以对整个模型进行训练，这个过程叫做微调（fine-tuning），最终得到一个可用的模型。
总结一下，整个过程包括两步，拿一个通用模型在ImageNet上做预训练（pre-training），然后针对特定任务进行微调（fine-tuning），完美解决了特定任务数据不足的问题。
还有一个好处是，对于各种各样的任务都不再需要从头开始训练网络，可以直接拿预训练好的结果进行微调，既减少了训练计算量的负担，也减少了人工标注数据的负担。

NLP领域也引入了这种做法，用一个通用模型，在非常大的语料库上进行预训练，然后在特定任务上进行微调，BERT就是这套方案的集大成者。
BERT不是第一个，但目前为止，是效果最好的方案。
BERT用了一个已有的模型结构，提出了一整套的预训练方法和微调方法，我们在后文中再进行详细的描述。

<h3>二，算法</h3>
BERT所采用的算法来自于2017年12月份的这篇文章，Attenion Is All You Need（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762">
https://arxiv.org/abs/1706.03762</a>），同样来自于谷歌。
这篇文章要解决的是翻译问题，比如从中文翻译成英文。
这篇文章完全放弃了以往经常采用的RNN和CNN，提出了一种新的网络结构，即Transformer，其中包括encoder和decoder，我们只关注encoder。
这篇英文博客（<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">
https://jalammar.github.io/illustrated-transformer/</a>）对Transformer介绍得非常详细，有兴趣的读者可以看一下，如果不想看英文博客也可以看本文，本文中的部分图片也截取自这篇博客。

<img class="lazy" data-src="https://pic4.zhimg.com/v2-393d5284f5132c3150b294cfc5e5218f_b.jpg">

<figcaption>图2.1</figcaption>
图2.1是Transformer encoder的结构，后文中我们都简称为Transformer。
首先是输入word embedding，这里是直接输入一整句话的所有embedding。
如图2.1所示，假设我们的输入是Thinking Machines，每个词对应一个embedding，就有2个embedding。
输入embedding需要加上位置编码（Positional Encoding），为什么要加位置编码，后文会做详细介绍。
然后经过一个Multi-Head Attention结构，这个结构是算法单元中最重要的部分，我们会在后边详细介绍。
之后是做了一个shortcut的处理，就是把输入和输出按照对应位置加起来，如果了解残差网络（ResNet）的同学，会对这个结构比较熟悉，这个操作有利于加速训练。
然后经过一个归一化normalization的操作。
接着经过一个两层的全连接网络，最后同样是shortcut和normalization的操作。
可以看到，除了Multi-Head Attention，都是常规操作，没有什么难理解的。
这里需要注意的是，每个小模块的输入和输出向量，维度都是相等的，比如，Multi-Head Attention的输入和输出向量维度是相等的，否则无法进行shortcut的操作；Feed Forward的输入和输出向量维度也是相等的；最终的输出和输入向量维度也是相等的。
但是Multi-Head Attention和Feed Forward内部，向量维度会发生变化。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-4019f1ffead184e3bc00aabb41e6b6b6_b.jpg">

<figcaption>图2.2</figcaption>
我们来详细看一下Multi-Head Attention的结构。
这个Multi-Head表示多头的意思，先从最简单的看起，看看单头Attention是如何操作的。
从图2.1的橙色方块可以看到，embedding在进入到Attention之前，有3个分叉，那表示说从1个向量，变成了3个向量。
具体是怎么算的呢？我们看图2.3，定义一个WQ矩阵（这个矩阵随机初始化，通过训练得到），将embedding和WQ矩阵做乘法，得到查询向量q，假设输入embedding是512维，在图3中我们用4个小方格表示，输出的查询向量是64维，图3中用3个小方格以示不同。
然后类似地，定义WK和WV矩阵，将embedding和WK做矩阵乘法，得到键向量k；将embeding和WV做矩阵乘法，得到值向量v。
对每一个embedding做同样的操作，那么每个输入就得到了3个向量，查询向量，键向量和值向量。
需要注意的是，查询向量和键向量要有相同的维度，值向量的维度可以相同，也可以不同，但一般也是相同的。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-ac045486e0eff3b8a1eb27d2ae61a634_b.jpg">

<figcaption>图2.3</figcaption>
接下来我们计算每一个embedding的输出，以第一个词Thinking为例，参看图2.4。
用查询向量q1跟键向量k1和k2分别做点积，得到112和96两个数值。
这也是为什么前文提到查询向量和键向量的维度必须要一致，否则无法做点积。
然后除以常数8，得到14和12两个数值。
这个常数8是键向量的维度的开方，键向量和查询向量的维度都是64，开方后是8。
做这个尺度上的调整目的是为了易于训练。
然后把14和12丢到softmax函数中，得到一组加和为1的系数权重，算出来是大约是0.88和0.12。
将0.88和0.12对两个值向量v1和v2做加权求和，就得到了Thinking的输出向量z1。
类似的，可以算出Machines的输出z2。
如果一句话中包含更多的词，也是相同的计算方法。

<img class="lazy" data-src="https://pic2.zhimg.com/v2-b25bb6a8f9b57a4831b485015080b8c1_b.jpg">

<figcaption>图2.4</figcaption>
通过这样一系列的计算，可以看到，现在每个词的输出向量z都包含了其他词的信息，每个词都不再是孤立的了。
而且每个位置中，词与词的相关程度，可以通过softmax输出的权重进行分析。
如图2.5所示，这是某一次计算的权重，其中线条颜色的深浅反映了权重的大小，可以看到it中权重最大的两个词是The和animal，表示it跟这两个词关联最大。
这就是attention的含义，输出跟哪个词关联比较强，就放比较多的注意力在上面。
上面我们把每一步计算都拆开了看，实际计算的时候，可以通过矩阵来计算，如图2.6所示。

<img class="lazy" data-src="https://pic2.zhimg.com/v2-2dbdd8dfb5088d22c7dd9d05a0e1035d_b.jpg">

<figcaption>图2.5</figcaption>

<img class="lazy" data-src="https://pic4.zhimg.com/v2-a02ab6ab4cad1f8fef307ced0a4cf9d3_b.jpg" data-caption="">

<img class="lazy" data-src="https://pic2.zhimg.com/v2-d00785a9cfb835b5a345898e37b31be9_b.jpg">

<figcaption>图2.6</figcaption>
讲完了attention，再来讲Multi-Head。
对于同一组输入embedding，我们可以并行做若干组上面的操作，例如，我们可以进行8组这样的运算，每一组都有WQ，WK，WV矩阵，并且不同组的矩阵也不相同。
这样最终会计算出8组输出，我们把8组的输出连接起来，并且乘以矩阵WO做一次线性变换得到输出，WO也是随机初始化，通过训练得到，计算过程如图2.7所示。
这样的好处，一是多个组可以并行计算，二是不同的组可以捕获不同的子空间的信息。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-2afc28e06f5550d5e20a2dc290f2224e_b.jpg">

<figcaption>图2.7</figcaption>
到这里就把Transformer的结构讲完了，同样都是做NLP任务，我们来和RNN做个对比。
图2.8是个最基本的RNN结构，还有计算公式。
当计算隐向量h4时，用到了输入x4，和上一步算出来的隐向量h3，h3包含了前面所有节点的信息。
h4中包含最多的信息是当前的输入x4，越往前的输入，随着距离的增加，信息衰减得越多。
对于每一个输出隐向量h都是如此，包含信息最多得是当前的输入，随着距离拉远，包含前面输入的信息越来越少。
但是Transformer这个结构就不存在这个问题，不管当前词和其他词的空间距离有多远，包含其他词的信息不取决于距离，而是取决于两者的相关性，这是Transformer的第一个优势。
第二个优势在于，对于Transformer来说，在对当前词进行计算的时候，不仅可以用到前面的词，也可以用到后面的词。
而RNN只能用到前面的词，这并不是个严重的问题，因为这可以通过双向RNN来解决。
第三点，RNN是一个顺序的结构，必须要一步一步地计算，只有计算出h1，才能计算h2，再计算h3，隐向量无法同时并行计算，导致RNN的计算效率不高，这是RNN的固有结构所造成的，之前有一些工作就是在研究如何对RNN的计算并行化。
通过前文的介绍，可以看到Transformer不存在这个问题。
通过这里的比较，可以看到Transformer相对于RNN有巨大的优势，因此我看到有人说RNN以后会被取代。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-5bafe804c0dc77f945ade48561de63a0_b.jpg" data-caption="">

<img class="lazy" data-src="https://pic1.zhimg.com/v2-235854b916c55c54bbcad343443885c0_b.jpg">

<figcaption>图2.8</figcaption>
关于上面的第三点优势，可能有人会不认可，RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了。
为了解决时序的问题，Transformer的作者用了一个绝妙的办法，这就是我在前文提到的位置编码（Positional Encoding）。
位置编码是和word embedding同样维度的向量，将位置embedding和词embedding加在一起，作为输入embedding，如图2.9所示。
位置编码可以通过学习得到，也可以通过设置一个跟位置或者时序相关的函数得到，比如设置一个正弦或者余弦函数，这里不再多说。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-df4b9de6b9feb1971ab7225ebc4454d2_b.jpg">

<figcaption>图2.9</figcaption>
我们把图2.1的结构作为一个基本单元，把N个这样的基本单元顺序连起来，就是BERT的算法模型，如图2.10所示。
从前面的描述中可以看到，当输入有多少个embedding，那么输出也就有相同数量的embedding，可以采用和RNN采用相同的叫法，把输出叫做隐向量。
在做具体NLP任务的时候，只需要从中取对应的隐向量作为输出即可。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-81e63e36210c8e342d193be69c441e7c_b.jpg">

<figcaption>图2.10</figcaption>
<h3>三，BERT</h3>
在介绍BERT之前，我们先看看另外一套方案。
我在第一部分说过，BERT并不是第一个提出预训练加微调的方案，此前还有一套方案叫GPT，这也是BERT重点对比的方案，文章在这，Improving Language Understanding by Generative Pre-Training（<a href="https://link.zhihu.com/?target=https%3A//s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>）。
GPT的模型结构和BERT是相同的，都是图2.10的结构，只是BERT的模型规模更加庞大。
GPT是这么预训练的，在一个8亿单词的语料库上做训练，给出前文，不断地预测下一个单词。
比如这句话，Winter is coming，当给出第一个词Winter之后，预测下一个词is，之后再预测下一个词coming。
不需要标注数据，通过这种无监督训练的方式，得到一个预训练模型。

我们再来看看BERT有什么不同。
BERT来自于Bidirectional Encoder Representations from Transformers首字母缩写，这里提到了一个双向（Bidirectional）的概念。
BERT在一个33亿单词的语料库上做预训练，语料库就要比GPT大了几倍。
预训练包括了两个任务，第一个任务是随机地扣掉15%的单词，用一个掩码MASK代替，让模型去猜测这个单词；第二个任务是，每个训练样本是一个上下句，有50%的样本，下句和上句是真实的，另外50%的样本，下句和上句是无关的，模型需要判断两句的关系。
这两个任务各有一个loss，将这两个loss加起来作为总的loss进行优化。
下面两行是一个小栗子，用括号标注的是扣掉的词，用[MASK]来代替。

<b>正样本：我[MASK]（是）个算法工程师，我服务于WiFi万能钥匙这家[MASK]（公司）。
</b>
<b>负样本：我[MASK]（是）个算法工程师，今天[MASK]（股票）又跌了。
</b>
我们来对比下GPT和BERT两种预训练方式的优劣。
GPT在预测词的时候，只预测下一个词，因此只能用到上文的信息，无法利用到下文的信息。
而BERT是预测文中扣掉的词，可以充分利用到上下文的信息，这使得模型有更强的表达能力，这也是BERT中Bidirectional的含义。
在一些NLP任务中需要判断句子关系，比如判断两句话是否有相同的含义。
BERT有了第二个任务，就能够很好的捕捉句子之间的关系。
图3.1是BERT原文中对另外两种方法的预训练对比，包括GPT和ELMo。
ELMo采用的还是LSTM，这里我们不多讲ELMo。
这里会有读者困惑，这里的结构图怎么跟图2.10不一样？如果熟悉LSTM的同学，看到最右边的ELMo，就会知道那些水平相连的LSTM其实只是一个LSTM单元。
左边的BERT和GPT也是一样，水平方向的Trm表示的是同一个单元，图中那些复杂的连线表示的是词与词之间的依赖关系，BERT中的依赖关系既有前文又有后文，而GPT的依赖关系只有前文。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-287ba1129d213df7e2ed5adb7c4a440e_b.jpg">

<figcaption>图3.1</figcaption>
讲完了这两个任务，我们再来看看，如何表达这么复杂的一个训练样本，让计算机能够明白。
图3.2表示“my dog is cute, he likes playing.”的输入形式。
每个符号的输入由3部分构成，一个是词本身的embedding；第二个是表示上下句的embedding，如果是上句，就用A embedding，如果是下句，就用B embedding；最后，根据Transformer模型的特点，还要加上位置embedding，这里的位置embedding是通过学习的方式得到的，BERT设计一个样本最多支持512个位置；将3个embedding相加，作为输入。
需要注意的是，在每个句子的开头，需要加一个Classification（CLS）符号，后文中会进行介绍，其他的一些小细节就不说了。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-ec06762a57a7d7176747627dc3ee20b4_b.jpg">

<figcaption>图3.2</figcaption>
完成预训练之后，就要针对特定任务就行微调了，这里描述一下论文中的4个例子，看图3.4。
首先说下分类任务，分类任务包括对单句子的分类任务，比如判断电影评论是喜欢还是讨厌；多句子分类，比如判断两句话是否表示相同的含义。
图3.4（a）（b）是对这类任务的一个示例，左边表示两个句子的分类，右边是单句子分类。
在输出的隐向量中，取出CLS对应的向量C，加一层网络W，并丢给softmax进行分类，得到预测结果P，计算过程如图3.3中的计算公式。
在特定任务数据集中对Transformer模型的所有参数和网络W共同训练，直到收敛。
新增加的网络W是HxK维，H表示隐向量的维度，K表示分类数量，W的参数数量相比预训练模型的参数少得可怜。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-61486f520243716de645f904e3a36ac2_b.jpg">

<figcaption>图3.3</figcaption>

<img class="lazy" data-src="https://pic3.zhimg.com/v2-42514100ab16b207d2732729c85fccaa_b.jpg">

<figcaption>图3.4</figcaption>
我们再来看问答任务，如图3.4（c），以SQuAD v1.1为例，给出一个问题Question，并且给出一个段落Paragraph，然后从段落中标出答案的具体位置。
需要学习一个开始向量S，维度和输出隐向量维度相同，然后和所有的隐向量做点积，取值最大的词作为开始位置；另外再学一个结束向量E，做同样的运算，得到结束位置。
附加一个条件，结束位置一定要大于开始位置。
最后再看NER任务，实体命名识别，比如给出一句话，对每个词进行标注，判断属于人名，地名，机构名，还是其他。
如图3.4（d）所示，加一层分类网络，对每个输出隐向量都做一次判断。
可以看到，这些任务，都只需要新增少量的参数，然后在特定数据集上进行训练即可。
从实验结果来看，即便是很小的数据集，也能取得不错的效果。

<h2>Delete Files unlink("data.txt")</h2>
Delete Files and Directories. unlink deletes the file(s) or directories specified by x .

Usage. unlink(x, recursive = FALSE, force = FALSE)

<h2>scan</h2>
Read data into a vector or list from the console or file.

cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "ex.data", sep = "\n")
pp &lt;- scan("ex.data", skip = 1, quiet = TRUE)
scan("ex.data", skip = 1)
scan("ex.data", skip = 1, nlines = 1) # only 1 line after the skipped one
scan("ex.data", what = list("","","")) # flush is F -> read "7"
scan("ex.data", what = list("","",""), flush = TRUE)
unlink("ex.data") # tidy up

## "inline" usage
scan(text = "1 2 3")

<h2>Copy an R data.frame to an Excel spreadsheet</h2>
write.excel &lt;- function(x,row.names=FALSE,col.names=TRUE,...) {
  write.table(x,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}

write.excel(my.df)

and finally Ctr+V in Excel :)

<h2>copy a table x to the clipboard preserving the table structure</h2>
write.table(x, "clipboard", sep="\t")

write.table(x, "clipboard", sep="\t", row.names=FALSE)
write.table(x, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

s = c('aa','gb','rc')
n = c('af','rd','ac')
df = data.frame(n,s)

write.table(df, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

"af"	"aa"
"rd"	"gb"
"ac"	"rc"

<h2>read.table</h2>
reads a file into data frame in table format

x &lt;- read.table("tp.txt",header=T,sep="\t");

<h3>copy a table from the clipboard</h3>
x &lt;- read.table("clipboard",header=F,sep="\t");

<h2>distributed programming</h2>
reasons for distributed programming:

To speed up a process or piece of code
To scale up an interface or application for multiple users

<a href="http://spark.apache.org/docs/latest/sparkr.html" class="whitebut ">SparkR</a>: R on Apache Spark

SparkR provides an R frontend to Apache Spark and using Spark’s distributed computation engine allows us to run large scale data analysis from the R shell.

To get started you need to set up a Spark cluster. 
<a href="http://paxcel.net/blog/how-to-setup-apache-spark-standalone-cluster-on-multiple-machine/" class="whitebut ">SETUP APACHE SPARK STANDALONE CLUSTER ON MULTIPLE MACHINE</a>

The Spark documentation, without using Mesos or YARN as your cluster manager
<a href="http://spark.apache.org/docs/latest/spark-standalone.html" class="whitebut ">Spark Standalone Mode</a>

Once you have Spark set up, see <a href="https://rpubs.com/wendyu/sparkr" class="whitebut ">Wendy Yu's tutorial on SparkR</a>

She also shows how to integrate H20 with Spark which is referred to as 'Sparkling Water'.

R has been shipping with a base library parallel. 

In a nutshell, you can just do something like

mclapply(1:nCores, someFunction())
and the function someFunction() will be run in parallel over nCores. 
A default value of half your physical cores may be a good start.

<a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html" class="whitebut ">High Performance Computing</a>

<h2>matrix operation</h2>
MatA &lt;- matrix(1:9, nrow = 3)  
MatB &lt;- matrix(9:1, nrow = 3)  
MatA + MatB

> A &lt;- matrix(c(2,3,-2,1,2,2),3,2)
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
Is Something a Matrix
> <span class="orange">is.matrix(A)</span>

[1] TRUE

> is.vector(A)

[1] FALSE
<span class="orange">Multiplication by a Scalar</span>
> c &lt;- 3
> c*A

     [,1] [,2]
[1,]    6    3
[2,]    9    6
[3,]   -6    6
<span class="orange">Matrix Addition & Subtraction</span>
> B &lt;- matrix(c(1,4,-2,1,2,1),3,2)
> B

     [,1] [,2]
[1,]    1    1
[2,]    4    2
[3,]   -2    1

> C &lt;- A + B
> C 

     [,1] [,2]
[1,]    3    2
[2,]    7    4
[3,]   -4    3

> D &lt;- A - B
> D

     [,1] [,2]
[1,]    1    0
[2,]   -1    0
[3,]    0    1

<span class="orange">Matrix Multiplication</span>
> D &lt;- matrix(c(2,-2,1,2,3,1),2,3)
> D

     [,1] [,2] [,3]
[1,]    2    1    3
[2,]   -2    2    1

> C &lt;- D %*% A
> C

     [,1] [,2]
[1,]    1   10
[2,]    0    4

> C &lt;- A %*% D
> C

     [,1] [,2] [,3]
[1,]    2    4    7
[2,]    2    7   11
[3,]   -8    2   -4

> D &lt;- matrix(c(2,1,3),1,3)
> D

     [,1] [,2] [,3]
[1,]    2    1    3

> C &lt;- D %*% A
> C

     [,1] [,2]
[1,]    1   10

> C &lt;- A %*% D

Error in A %*% D : non-conformable arguments
<span class="orange">Transpose of a Matrix</span>
> AT &lt;- t(A)
> AT

     [,1] [,2] [,3]
[1,]    2    3   -2
[2,]    1    2    2

> ATT &lt;- t(AT)
>ATT

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
Common Vectors
<span class="orange">Unit Vector</span>
> U &lt;- matrix(1,3,1)
> U

     [,1]
[1,]    1
[2,]    1
[3,]    1
<span class="orange">Zero Vector</span>
> Z &lt;- matrix(0,3,1)
> Z

     [,1]
[1,]    0
[2,]    0
[3,]    0
Common Matrices
<span class="orange">Unit Matrix</span>
> U &lt;- matrix(1,3,2)
> U

     [,1] [,2]
[1,]    1    1
[2,]    1    1
[3,]    1    1
<span class="orange">Zero Matrix</span>
> Z &lt;- matrix(0,3,2)
> Z

     [,1] [,2]
[1,]    0    0
[2,]    0    0
[3,]    0    0
<span class="orange">Diagonal Matrix</span>
> S &lt;- matrix(c(2,3,-2,1,2,2,4,2,3),3,3)
> S

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    2
[3,]   -2    2    3

> D &lt;- diag(S)
> D

[1] 2 2 3

> D &lt;- diag(diag(S))
> D

     [,1] [,2] [,3]
[1,]    2    0    0
[2,]    0    2    0
[3,]    0    0    3
<span class="orange">Identity Matrix</span>
> I &lt;- diag(c(1,1,1))
> I

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
<span class="orange">Symmetric Matrix</span>
> C &lt;- matrix(c(2,1,5,1,3,4,5,4,-2),3,3)
> C

     [,1] [,2] [,3]
[1,]    2    1    5
[2,]    1    3    4
[3,]    5    4   -2

> CT &lt;- t(C)
> CT

     [,1] [,2] [,3]
[1,]    2    1    5
[2,]    1    3    4
[3,]    5    4   -2
<span class="orange">Inverse of a Matrix</span>
> A &lt;- matrix(c(4,4,-2,2,6,2,2,8,4),3,3)
> A

     [,1] [,2] [,3]
[1,]    4    2    2
[2,]    4    6    8
[3,]   -2    2    4


> AI &lt;- solve(A)
> AI

     [,1] [,2] [,3]
[1,]  1.0 -0.5  0.5
[2,] -4.0  2.5 -3.0
[3,]  2.5 -1.5  2.0

> A %*% AI

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1

> AI %*% A

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
<span class="orange">Inverse & Determinant of a Matrix</span>
> C &lt;- matrix(c(2,1,6,1,3,4,6,4,-2),3,3)
> C

     [,1] [,2] [,3]
[1,]    2    1    6
[2,]    1    3    4
[3,]    6    4   -2

> CI &lt;- solve(C)
CI

           [,1]        [,2]        [,3]
[1,]  0.2156863 -0.25490196  0.13725490
[2,] -0.2549020  0.39215686  0.01960784
[3,]  0.1372549  0.01960784 -0.04901961

> d &lt;- det(C)
> d

[1] -102
<span class="orange">Rank of a Matrix</span>
> A &lt;- matrix(c(2,3,-2,1,2,2,4,7,0),3,3)
> A

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    7
[3,]   -2    2    0

> matA &lt;- qr(A)
> matA$rank

[1] 3

> A &lt;- matrix(c(2,3,-2,1,2,2,4,6,-4),3,3)
> A

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    6
[3,]   -2    2   -4

> matA &lt;- qr(A)
> matA$rank

[1] 2

# note column 3 is 2 times column 1
Number of Rows & Columns
> X &lt;- matrix(c(3,2,4,3,2,-2,6,1),4,2)
> X

     [,1] [,2]
[1,]    3    2
[2,]    2   -2
[3,]    4    6
[4,]    3    1

> <span class="orange">dim(X)</span>

[1] 4 2

> r &lt;- nrow(X)
> r

[1] 4

> c &lt;- ncol(X)
> c

[1] 2
Computing Column & Row Sums
# note the uppercase S

> A &lt;- matrix(c(2,3,-2,1,2,2),3,2)
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2

> c &lt;- colSums(A)
> c

[1] 3 5

> r &lt;- rowSums(A)
> r

[1] 3 5 0

> a &lt;- sum(A)
> a

[1] 8
<span class="orange">Computing Column & Row Means</span>
# note the uppercase M

> cm &lt;- colMeans(A)
> cm

[1] 1.000000 1.666667

> rm &lt;- rowMeans(A)
> rm

[1] 1.5 2.5 0.0

> m &lt;- mean(A)
> m

[1] 1.333333
<span class="orange">Horizontal Concatenation</span>
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2

> B &lt;- matrix(c(1,3,2,1,4,2),3,2)
> B

     [,1] [,2]
[1,]    1    1
[2,]    3    4
[3,]    2    2

> C &lt;- cbind(A,B)
> C

     [,1] [,2] [,3] [,4]
[1,]    2    1    1    1
[2,]    3    2    3    4
[3,]   -2    2    2    2
<span class="orange">Vertical Concatenation (Appending)</span>
> C &lt;- rbind(A,B)
> C

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
[4,]    1    1
[5,]    3    4
[6,]    2    2

<span class="orange">Matrix Operations in R</span>
A * B	Element-wise multiplication
A %*% B	Matrix multiplication
A %o% B	Outer product. AB'
crossprod(A,B)
crossprod(A)	A'B and A'A respectively.
t(A)	Transpose
diag(x)	Creates diagonal matrix with elements of x in the principal diagonal
diag(A)	Returns a vector containing the elements of the principal diagonal
diag(k)	If k is a scalar, this creates a k x k identity matrix. Go figure.
solve(A, b)	Returns vector x in the equation b = Ax (i.e., A-1b)
solve(A)	Inverse of A where A is a square matrix.
ginv(A)	Moore-Penrose Generalized Inverse of A.
ginv(A) requires loading the MASS package.
y&lt;-eigen(A)	y$val are the eigenvalues of A
y$vec are the eigenvectors of A
y&lt;-svd(A)	Single value decomposition of A.
y$d = vector containing the singular values of A
y$u = matrix with columns contain the left singular vectors of A
y$v = matrix with columns contain the right singular vectors of A
R &lt;- chol(A)	Choleski factorization of A. Returns the upper triangular factor, such that R'R = A.
y &lt;- qr(A)	QR decomposition of A.
y$qr has an upper triangle that contains the decomposition and a lower triangle that contains information on the Q decomposition.
y$rank is the rank of A.
y$qraux a vector which contains additional information on Q.
y$pivot contains information on the pivoting strategy used.
cbind(A,B,...)	Combine matrices(vectors) horizontally. Returns a matrix.
rbind(A,B,...)	Combine matrices(vectors) vertically. Returns a matrix.
rowMeans(A)	Returns vector of row means.
rowSums(A)	Returns vector of row sums.
colMeans(A)	Returns vector of column means.
colSums(A)	Returns vector of column sums.

<h2>UCLA stat</h2>
<a href="http://www.philender.com/courses/intro/" class="whitebut ">ucla.edu Introduction to Research Design and Statistics</a>
<a href="http://www.philender.com/courses/linearmodels/" class="whitebut ">Linear Statistical Models: Regression & Anova, Better Living Through Linear Models</a>
<a href="http://www.philender.com/courses/multivariate/" class="whitebut ">Multivariate Statistical Analysis</a>

<h2>R Linear Algebra</h2>
R is especially handy with linear algebra. 
Its built-in data types like vectors and matrices mesh well with built-in functions like eigenvalue and determinant solvers and dynamic indexing capabilities.

<h3>Vector Assignment</h3>
x &lt;- c(1, 2, 3, 4)
In most contexts, <code>&lt;-</code> can be switched with <code>=</code>.
The function <code>assign()</code> can also be used:
assign(&#x27;x&#x27;, c(1, 2, 3, 4))
Assignments can also be made in the other direction:
c(1, 2, 3, 4) -&gt; x

<h3>Vector Operations</h3>Vectors can also be used in a variety of ways.
The operation<code> y &lt;- c(x, 0, x)</code> would assign a vector <code>1, 2, 3, 4, 0, 1, 2, 3, 4 </code>to variable <code>y</code>.
Vectors can be freely multiplied and added by constants:
v &lt;- 2*x + y + 1
Note that this operation is valid even when <code>x</code> and <code>y</code> are different lengths. 
In this case, R will simply recycle x (sometimes fractionally) until it meets the length of y. 
Since y is 9 numbers long and x is 4 units long, x will be repeated 2.25 times to match the length of y.
The arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code> can all be used. 
<code>log</code>, <code>exp</code>, <code>sin</code>, <code>cos</code>, <code>tan</code>, <code>sqrt</code>, and more can also be used. 
<code>max(x)</code> and <code>min(x)</code> represent the largest and smallest elements of a vector <code>x</code>, and <code>length(x)</code> is the number of elements in <code>x</code>. 
<code>sum(x)</code> gives the total of the elements in <code>x</code>, and <code>prod(x)</code> their product.
<code>mean(x)</code> calculates the sample mean, and <code>var(x)</code> returns the sample variance. 
<code>sort(x)</code> returns a vector of the same size as x with elements arranged in increasing order.

<h3>Generating Sequences</h3>R has many methods for generating sequences of numbers. 
<code>1:30</code> is the same as <code>c(1, 2, …, 29, 30)</code>. 
The colon as the highest priority in an expression, so<code>2*1:15</code> will return <code>c(2, 4, …, 28, 30)</code> instead of <code>c(2, 3, …, 14, 15)</code>.
30:1 may be used to generate the sequence backwards.
The <code>seq()</code> function can also be used to generate sequences. 
<code>seq(2,10) </code>returns the same vector as <code>2:10</code>. 
In <code>seq()</code>, one can also specify the length of the step in which to take: <code>seq(1,2,by=0.5)</code> returns <code>c(1, 1.5, 2)</code>.
A similar function is <code>rep()</code>, which replicates an object in various ways. 
For example, <code>rep(x, times=5)</code> will return five copies of <code>x</code> end-to-end.

<h3>Logical Vectors</h3>Logical values in R are TRUE, FALSE, and NA. 
Logical vectors are set by conditions. 
<code>val &lt;- x &gt; 13</code> sets <code>val</code> as a vector of the same length as <code>x</code> with values <code>TRUE</code> where the condition is met and <code>FALSE</code> where the condition is not.
The logical operators in r are <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==</code>, and <code>!=</code>, which mean less than, less than or equal to, greater than, greater than or equal to, equality, and inequality.

<h3>Missing Values</h3>The function <code>is.na(x)</code> returns a logical vector of the same size as <code>x</code> with <code>TRUE</code> if the corresponding element to <code>x</code> is <code>NA</code>.
<code>x == NA</code> is different from <code>is.na(x)</code> since <code>NA</code> is not a value but a marker for an unavailable quantity.
A second type of ‘missing value’ is that which is produced by numerical computation, such as <code>0/0</code>. 
In this case, <code>NaN</code> (Not a Number) values are treated as <code>NA</code> values; that is, <code>is.na(x)</code> will return <code>TRUE</code> for both <code>NA</code> and <code>NaN</code> values. 
<code>is.nan(x)</code> can be used only for identifying <code>NaN</code> values.

<h3>Indexing Vectors</h3>The first kind of indexing is through a logical vector. 
<code>y &lt;- x[!is.na(x)]</code> sets <code>y</code> to the values of <code>x</code> that are not equal to <code>NA</code> or <code>NaN</code>.
<code>(x+1)[(!is.na(x)) &amp; x&gt;0] -&gt; z</code> sets <code>z</code> to the values of <code>x+1</code> that are not <code>Na</code> or <code>NaN</code> and larger than 0.
A second method is with a vector of positive integral quantities. 
In this case, the values must be in the set <code>{1, 2, …, length(x)}</code>. 
The corresponding elements of the vector are selected and concatenated in that order to form a result. 
It is important to remember that unlike in other languages, the first index in R is 1 and not 0.
<code>x[1:10]</code> returns the first 10 elements of <code>x</code>, assuming <code>length(x)</code> is not less than 10. 
<code>c(‘x’, ‘y’)[rep(c(1,2,2,1), times=4)]</code> produces a character vector of length 16, where <code>‘x’, ‘y’, ‘y’, ‘x’</code> is repeated four times.
A vector of negative integral numbers specifies the values to be excluded rather than included. 
<code>y &lt;- x[-(1:5)]</code> sets <code>y</code> to all but the first five values of <code>x</code>.
Lastly, a vector of character strings can be used when an object has a names attribute to identify its components. 
With fruit <code>&lt;- c(1, 2, 3, 4)</code>, one can set the names of each index of the vector fruit with <code>names(fruit) &lt;- c(‘mango’, ‘apple’, ‘banana’, ‘orange’)</code>. 
Then, one can call the elements by name with <code>lunch &lt;- fruit[c(‘apple’, ‘orange’)]</code>.
The advantage of this is that alphanumeric names can sometimes be easier to remember than indices.
Note that an indexed expression can also appear on the receiving end of an assignment, in which the assignment is only performed on those elements of a vector. 
For example, <code>x[is.na(x)] &lt;- 0</code> replaces all <code>NA</code> and <code>NaN</code> values in vector <code>x</code> with the value <code>0</code>.
Another example: <code>y[y&lt;0] &lt;- -y[y&lt;0]</code> has the same effect as <code>y &lt;- abs(y)</code>. 
The code simply replaces all the values that are less than 0 with the negative of that value.
<h3>Arrays &amp; Matrices</h3>
<h3>Arrays</h3>An array is a subscripted collection of data entries, not necessarily numeric.
A dimension vector is a vector of non-negative integers. 
If the length is <em>k</em> then the array is <em>k</em>-dimensional. 
The dimensions are indexed from one up to the values given in the dimension vector.
A vector can be used by R as an array as its <code>dim </code>attribute. 
If <code>z</code> were a vector of 1500 elements, the assignment <code>dim(z) &lt;- c(100, 5, 3)</code> would mean <code>z</code> is now treated as a 100 by 5 by 3 array.

<h3>Array Indexing</h3>Individual elements of an array can be referenced by giving the name of the array followed by the subscripts in square brackets, separated by columns.
A 3 by 4 by 6 vector <code>a</code> could have its first value called via <code>a[1, 1, 1]</code> and its last value called via <code>a[3, 4, 6]</code>.
<code>a[,,]</code> represents the entire array; hence, <code>a[1,1,]</code> takes the first row of the first 2-dimensional cross-section in <code>a</code>.

<h3>Indexing Matrices</h3>The following code generates a 4 by 5 array: <code>x &lt;- array(1:20, dim = c(4,5))</code>.
Arrays are specified by a vector of values and the dimensions of the matrix. 
Values are calculated top-down first, left-right second.
<code>array(1:4, dim = c(2,2))</code> would return
1 3
2 4
and not
1 2
3 4
Negative indices are not allowed in index matrices. 
<code>NA</code> and zero values are allowed.

<h3>Outer Product of 2 Arrays</h3>An important operation on arrays is the outer product. 
If <code>a</code> and <code>b</code> are two numeric arrays, their outer product is an array whose dimension vector is obtained by concatenating the two dimension vectors and whose data vector is achieved by forming all possible products of elements of the data vector of <code>a</code> with those of <code>b</code>. 
The outer product is calculated with the operator <code>%o%</code>:
<code>ab &lt;- a %o% b</code>
Another way to achieve this is
<code>ab &lt;- outer(a, b, ‘*’)</code>
In fact, any function can be applied on two arrays using the outer() function. 
Suppose we define a function <code>f &lt;- function(x, y) cos(y)/(1+x²)</code>. 
The function could be applied to two vectors <code>x</code> and <code>y</code> via <code>z &lt;- outer(x, y, f)</code>.

<h3>Demonstration: All Possible Determinants of 2x2 Single-Digit Matrices</h3>Consider the determinants of 2 by 2 matrices [a, b; c, d] where each entry is a non-negative integer from 0 to 9. 
The problem is to find the determinants of all possible matrices in this form and represent the frequency of which the value occurs with a high density plot.
Rephrased, find the probability distribution of the determinant if each digit is chosen independently and uniformly at random.
One clever way of doing this uses the outer(0 function twice.
d &lt;- outer(0:9,0:9)
fr &lt;- table(outer(d, d, ‘-’))
plot(fr, xlab = ‘Determinant’, ylab = ‘Frequency’)
The first line assigns d to this matrix:
The second line uses the outer() function again to calculate all possible determinants, and the last line plots it.
Generalized Transpose of an Array</h3>The function <code>aperm(a, perm)</code> can be used to permute an array a. 
The argument perm must be the permutation of the integers {1,…, <em>k</em>} where <em>k</em> is the number of subscripts in <em>a</em>. 
The result of the function is an array of the same size as a but with the old dimension given by <code>perm[j]</code> becoming the new <code>j-th</code> dimension.
An easy way to think about it is a generalization of transposition for matrices. 
If <code>A</code> is a matrix, then <code>B</code> is simply the transpose of <code>A</code>:
B &lt;- aperm(A, c(2, 1))
In these special cases the function <code>t()</code> performs a transposition.

<h3>Matrix Multiplication</h3>The operator %*% is used for matrix multiplication. 
If <code>A</code> and <code>B</code> are square matrices of the same size, <code>A*B</code> is the element-wise product of the two matrices. 
<code>A %*% B</code> is the dot product (matrix product).
If x is a vector, then <code>x %*% A %*% x</code> is a quadratic form.
<code>crossprod()</code> performs cross-products; thus, <code>crossprod(X, y)</code> is the same as the operation <code>t(X) %*% y</code>, but more efficient.
<code>diag(v)</code>, where <code>v</code> is a vector, gives a diagonal matrix with elements of the vector as the diagonal entries. 
<code>diag(M)</code>, where <code>m</code> is a matrix, gives the vector of the main diagonal entries of <code>M</code> (the same convention as in Matlab). 
<code>diag(k)</code>, where <code>k</code> is a single numeric value, returns a <code>k</code> by <code>k</code> identity matrix.

<h3>Linear Equations and Inversion</h3>Solving linear equations is the inverse of matrix multiplication. 
When
b &lt;- A %*% x
with only <code>A</code> and <code>b</code> given, vector <code>x</code> is the solution of the linear equation system. 
This can be solved quickly in R with
solve(A, b)

<h3>Eigenvalues and Eigenvectors</h3>The function <code>eigen(Sm)</code> calculates the eigenvalues and eigenvectors of a symmetric matrix Sm. 
The result is a list, with the first element named values and the second named vectors. 
<code>ev &lt;- eigen(Sm)</code> assigns this list to <code>ev</code>.
<code>ev$val</code> is the vector of eigenvalues of <code>Sm</code> and <code>ev$vec</code> the matrix of corresponding eigenvectors.
For large matrices, it is better to avoid computing the eigenvectors if they are not needed by using the expression
evals &lt;- eigen(Sm, only.values = TRUE)$values

<h3>Singular Value Decomposition and Determinants</h3>The function <code>svd(m)</code> takes an arbitrary matrix argument, <code>m</code>, and calculates the singular value decomposition of <code>m</code>. 
This consists of a matrix of orthonormal columns <code>U</code> with the same column space as <code>m</code>, a second matrix of orthonormal columns <code>V</code> whose column space is the row space of <code>m</code> and a diagonal matrix of positive entries <code>D</code> such that
m = U %*% D %*% t(V)
<code>det(m)</code> can be used to calculate the determinant of a square matrix <code>m</code>.

<h3>Least Squares Fitting &amp; QR Decomposition</h3>The function <code>lsfit()</code> returns a list giving results of a least squares fitting procedure. 
An assignment like
ans &lt;- lsfit(X, y)
gives results of a least squares fit where y is the vector of observations and X is the design matrix.
<code>ls.diag()</code> can be used for regression diagnostics.
A closely related function is qr().
b &lt;- qr.coef(Xplus,y)
fit &lt;- qr.fitted(Xplus,y)
res &lt;- qr.resid(Xplus,y)
These compute the orthogonal projection of <code>y</code> onto the range of <code>X</code> in <code>fit</code>, the projection onto the orthogonal complement in <code>res</code> and the coefficient vector for the projection in <code>b</code>.

<h3>Forming Partitioned <code>Matrices</code></h3>Matrices can be built up from other vectors and matrices with the functions <code>cbind()</code> and <code>rbind()</code>.
<code>cbind()</code> forms matrices by binding matrices horizontally (column-wise), and <code>rbind()</code> binds matrices vertically (row-wise).
In the assignment <code>X &lt;- cbind(arg_1, arg_2, arg_3, …)</code> the arguments to <code>cbind()</code> must be either vectors of any length, or columns with the same column size (the same number of rows).
<code>rbind()</code> performs a corresponding operation for rows.

<h2>unable to install rvest package</h2>
Error: package or namespace load failed for ‘xml2’ in loadNamespace

install.packages("tidyverse")  # might need other dependencies installed in Rstudio

<h2>tcl/tk package to create messageBox</h2>
library(tcltk)

tkmessageBox(
 title = "Hello Friends Title",
 message = "Hello, world! message",
 icon = "warning",
 detail="This is the message details",
 type = "ok")

tk_messageBox(
message, 
icon = c("error", "info", "question", "warning")
type = c("ok", "okcancel", "yesno", "yesnocancel", "retrycancel", "abortretryignore"),
default = "", ...)

must be -default, -detail, -icon, -message, -parent, -title, or -type.

Arguments
title
character
string specifying title for dialog window

message
character
string specifying message displayed inside the alert extra arguments

A list of other arguments is shown here:
default character string specifying the default button of the dialog

detail
character string specifying a secondary message, usually displayed in a smaller font under the main message

parent
object of the class tkwin representing the window of the application for which this dialog is being posted.

type
character
string specifying predefined set of buttons to be displayed (askquestion only).

Possible values are:
 abortretryignore
  displays three buttons whose symbolic names are ‘abort’, ‘retry’ and ‘ignore’

 ok
  displays one button whose symbolic name is ‘ok’

 okcancel
  displays two buttons whose symbolic names are ‘ok’ and ‘cancel’

 retrycancel
  displays two buttons whose symbolic names are ‘retry’ and ‘cancel’

 yesno
  displays two buttons whose symbolic names are ‘yes’ and ‘no’

 yesnocancel displays three buttons whose symbolic names are ‘yes’, ‘no’ and ‘cancel’

<h2>Machine Learning in R for beginners</h2>
This small tutorial is meant to introduce you to the basics of machine learning in R: it will show you how to use R to work with KNN.

<h3>Introducing: Machine Learning in R</h3>

Machine learning is a branch in computer science that studies the design of algorithms that can learn. 
Typical machine learning tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. 
These tasks are learned through available data that were observed through experiences or instructions, for example. 
Machine learning hopes that including the experience into its tasks will eventually improve the learning. 
The ultimate goal is to improve the learning in such a way that it becomes automatic, so that humans like ourselves don’t need to interfere any more.

This small tutorial is meant to introduce you to the basics of machine learning in R: more specifically, it will show you how to use R to work with the well-known machine learning algorithm called “KNN” or <em>k</em>-nearest neighbors.


<h3>Using R For <em>k</em>-Nearest Neighbors (KNN)</h3>

The KNN or <em>k</em>-nearest neighbors algorithm is one of the simplest machine learning algorithms and is an example of instance-based learning, where new data are classified based on stored, labeled instances.

More specifically, the distance between the stored data and the new instance is calculated by means of some kind of a similarity measure. 
This similarity measure is typically expressed by a distance measure such as the Euclidean distance, cosine similarity or the Manhattan distance.

In other words, the similarity to the data that was already in the system is calculated for any new data point that you input into the system.

Then, you use this similarity value to perform predictive modeling. 
Predictive modeling is either classification, assigning a label or a class to the new instance, or regression, assigning a value to the new instance. 
Whether you classify or assign a value to the new instance depends of course on your how you compose your model with KNN.

The <em>k</em>-nearest neighbor algorithm adds to this basic algorithm that after the distance of the new point to all stored data points has been calculated, the distance values are sorted and the <em>k</em>-nearest neighbors are determined. 
The labels of these neighbors are gathered and a majority vote or weighted vote is used for classification or regression purposes.

In other words, the higher the score for a certain data point that was already stored, the more likely that the new instance will receive the same classification as that of the neighbor. 
In the case of regression, the value that will be assigned to the new data point is the mean of its <em>k</em> nearest neighbors.

<h3>Step One. Get Your Data</h3>

Machine learning usually starts from observed data. 
You can take your own data set or browse through other sources to find one.

<h3>Built-in Datasets of R</h3>

This tutorial uses the Iris data set, which is very well-known in the area of machine learning. 
This dataset is built into R, so you can take a look at this dataset by typing the following into your console:

iris

# Print first lines
head(iris)

<h3>Step Two. Know Your Data</h3>

Just looking or reading about your data is certainly not enough to get started!

You need to get your hands dirty, explore and visualize your data set and even gather some more domain knowledge if you feel the data is way over your head.

Probably you’ll already have the domain knowledge that you need, but just as a reminder, all flowers contain a sepal and a petal. 
The sepal encloses the petals and is typically green and leaf-like, while the petals are typically colored leaves. 
For the iris flowers, this is just a little bit different, as you can see in the following picture:

<img src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png" />

<h3>Initial Overview Of The Data Set</h3>

First, you can already try to get an idea of your data by making some graphs, such as histograms or boxplots. 
In this case, however, scatter plots can give you a great idea of what you’re dealing with: it can be interesting to see how much one variable is affected by another.

In other words, you want to see if there is any correlation between two variables.

You can make scatterplots with the <a href="http://www.rdocumentation.org/packages/ggvis"><code>ggvis</code> package</a>, for example.

<strong>Note</strong> that you first need to load the <code>ggvis</code> package:

<code># Load in `ggvis`
library(ggvis)

# Iris scatter plot
iris %&gt;% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %&gt;% layer_points()</code>

<img alt="correlation iris" height="499" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/plot_768312428.png" width="817" />

You see that there is a high correlation between the sepal length and the sepal width of the Setosa iris flowers, while the correlation is somewhat less high for the Virginica and Versicolor flowers: the data points are more spread out over the graph and don’t form a cluster like you can see in the case of the Setosa flowers.

The scatter plot that maps the petal length and the petal width tells a similar story:

<code>iris %&gt;% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %&gt;% layer_points()</code>

<img alt="scatterplot iris" height="500" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/plot_675020181.png" width="817" />

You see that this graph indicates a positive correlation between the petal length and the petal width for all different species that are included into the Iris data set. 
Of course, you probably need to test this hypothesis a bit further if you want to be really sure of this:

# Overall correlation `Petal.Length` and `Petal.Width`
cor(iris$Petal.Length, iris$Petal.Width)

# Return values of `iris` levels 
x=levels(iris$Species)

# Print Setosa correlation matrix
print(x[1])
cor(iris[iris$Species==x[1],1:4])

# Print Versicolor correlation matrix
print(x[2])
cor(iris[iris$Species==x[2],1:4])

# Print Virginica correlation matrix
print(x[3])
cor(iris[iris$Species==x[3],1:4])

You see that when you combined all three species, the correlation was a bit stronger than it is when you look at the different species separately: the overall correlation is 0.96, while for Versicolor this is 0.79. 
Setosa and Virginica, on the other hand, have correlations of petal length and width at 0.31 and 0.32 when you round up the numbers.

<b>Tip</b>: are you curious about ggvis, graphs or histograms in particular? Check out our <a href="https://www.datacamp.com/community/tutorials/make-histogram-basic-r/">histogram tutorial</a> and/or <a href="https://www.datacamp.com/courses/ggvis-data-visualization-r-tutorial/">ggvis course</a>.

After a general visualized overview of the data, you can also view the data set by entering

# Return all `iris` data
iris

# Return first 5 lines of `iris`
head(iris)

# Return structure of `iris`
str(iris)

However, as you will see from the result of this command, this really isn’t the best way to inspect your data set thoroughly: the data set takes up a lot of space in the console, which will impede you from forming a clear idea about your data. 
It is therefore a better idea to inspect the data set by executing <code>head(iris)</code> or <code>str(iris)</code>.

Note that the last command will help you to clearly distinguish the data type <code>num</code> and the three levels of the <code>Species</code> attribute, which is a factor. 
This is very convenient, since many R machine learning classifiers require that the target feature is coded as a factor.

Remember that factor variables represent categorical variables in R. 
They can thus take on a limited number of different values.

A quick look at the <code>Species</code> attribute through tells you that the division of the species of flowers is 50-50-50. 
On the other hand, if you want to check the percentual division of the <code>Species</code> attribute, you can ask for a table of proportions:

# Division of `Species`
table(iris$Species) 

# Percentual division of `Species`
round(prop.table(table(iris$Species)) * 100, digits = 1)

<strong>Note</strong> that the <code>round</code> argument rounds the values of the first argument, <code>prop.table(table(iris$Species))*100</code> to the specified number of digits, which is one digit after the decimal point. 
You can easily adjust this by changing the value of the <code>digits</code> argument.

<h3>Profound Understanding Of Your Data</h3>

Let’s not remain on this high-level overview of the data! R gives you the opportunity to go more in-depth with the <code>summary()</code> function. 
This will give you the minimum value, first quantile, median, mean, third quantile and maximum value of the data set Iris for numeric data types. 
For the class variable, the count of factors will be returned:

# Summary overview of `iris`
summary(....) 

# Refined summary overview
summary(....[c("Petal.Width", "Sepal.Width")])

As you can see, the <code>c()</code> function is added to the original command: the columns <code>petal width</code> and <code>sepal width</code> are concatenated and a summary is then asked of just these two columns of the Iris data set.

<h3>Step Three. Where To Go Now?</h3>

After you have acquired a good understanding of your data, you have to decide on the use cases that would be relevant for your data set. 
In other words, you think about what your data set might teach you or what you think you can learn from your data. 
From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.
From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.

<strong>Tip</strong>: keep in mind that the more familiar you are with your data, the easier it will be to assess the use cases for your specific data set. 
The same also holds for finding the appropriate machine algorithm.

For this tutorial, the Iris data set will be used for classification, which is an example of predictive modeling. 
The last attribute of the data set, <code>Species</code>, will be the target variable or the variable that you want to predict in this example.

<strong>Note</strong> that you can also take one of the numerical classes as the target variable if you want to use KNN to do regression.

<h3>Step Four. Prepare Your Workspace</h3>

Many of the algorithms used in machine learning are not incorporated by default into R. 
You will most probably need to download the packages that you want to use when you want to get started with machine learning.

<b>Tip</b>: got an idea of which learning algorithm you may use, but not of which package you want or need? You can find a pretty complete overview of all the packages that are used in R <a href="http://www.rdocumentation.org/domains/MachineLearning">right here</a>.

To illustrate the KNN algorithm, this tutorial works with the package <code>class</code>:

library(.....)

If you don’t have this package yet, you can quickly and easily do so by typing the following line of code:

<code>install.packages(&quot;&lt;package name&gt;&quot;)</code>

<strong>Remember</strong> the nerd tip: if you’re not sure if you have this package, you can run the following command to find out!

<code>any(grepl(&quot;&lt;name of your package&gt;&quot;, installed.packages()))</code>

<h3>Step Five. Prepare Your Data</h3>

After exploring your data and preparing your workspace, you can finally focus back on the task ahead: making a machine learning model. 
However, before you can do this, it’s important to also prepare your data. 
The following section will outline two ways in which you can do this: by normalizing your data (if necessary) and by splitting your data in training and testing sets.

<h3>Normalization</h3>

As a part of your data preparation, you might need to normalize your data so that its consistent. 
For this introductory tutorial, just remember that normalization makes it easier for the KNN algorithm to learn. 
There are two types of normalization:

<ul><li>example normalization is the adjustment of each example individually, while</li><li>feature normalization indicates that you adjust each feature in the same way across all examples.</li>
</ul>

So when do you need to normalize your dataset?

In short: when you suspect that the data is not consistent.

You can easily see this when you go through the results of the <code>summary()</code> function. 
Look at the minimum and maximum values of all the (numerical) attributes. 
If you see that one attribute has a wide range of values, you will need to normalize your dataset, because this means that the distance will be dominated by this feature.

For example, if your dataset has just two attributes, X and Y, and X has values that range from 1 to 1000, while Y has values that only go from 1 to 100, then Y’s influence on the distance function will usually be overpowered by X’s influence.

When you normalize, you actually adjust the range of all features, so that distances between variables with larger ranges will not be over-emphasised.

<b>Tip</b>: go back to the result of <code>summary(iris)</code> and try to figure out if normalization is necessary.

The Iris data set doesn’t need to be normalized: the <code>Sepal.Length</code> attribute has values that go from 4.3 to 7.9 and <code>Sepal.Width</code> contains values from 2 to 4.4, while <code>Petal.Length</code>’s values range from 1 to 6.9 and <code>Petal.Width</code> goes from 0.1 to 2.5. 
All values of all attributes are contained within the range of 0.1 and 7.9, which you can consider acceptable.

Nevertheless, it’s still a good idea to study normalization and its effect, especially if you’re new to machine learning. 
You can perform feature normalization, for example, by first making your own <code>normalize()</code> function.

You can then use this argument in another command, where you put the results of the normalization in a data frame through <code>as.data.frame()</code> after the function <code>lapply()</code> returns a list of the same length as the data set that you give in. 
Each element of that list is the result of the application of the <code>normalize</code> argument to the data set that served as input:

<code>YourNormalizedDataSet &lt;- as.data.frame(lapply(YourDataSet, normalize))</code>

Test this in the DataCamp Light chunk below!

# Build your own `normalize()` function
normalize &lt;- function(x) {
num &lt;- x - min(x)
denom &lt;- max(x) - min(x)
return (num/denom)
}

# Normalize the `iris` data
iris_norm &lt;- .............(......(iris[1:4], normalize))

# Summarize `iris_norm`
summary(.........)

For the Iris dataset, you would have applied the <code>normalize</code> argument on the four numerical attributes of the Iris data set (<code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code>, <code>Petal.Width</code>) and put the results in a data frame.

<strong>Tip</strong>: to more thoroughly illustrate the effect of normalization on the data set, compare the following result to the summary of the Iris data set that was given in step two.

<h3>Training And Test Sets</h3>

In order to assess your model’s performance later, you will need to divide the data set into two parts: a training set and a test set.

The first is used to train the system, while the second is used to evaluate the learned or trained system. 
In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.

One last look on the data set teaches you that if you performed the division of both sets on the data set as is, you would get a training class with all species of “Setosa” and “Versicolor”, but none of “Virginica”. 
The model would therefore classify all unknown instances as either “Setosa” or “Versicolor”, as it would not be aware of the presence of a third species of flowers in the data.

In short, you would get incorrect predictions for the test set.

You thus need to make sure that all three classes of species are present in the training model. 
What’s more, the amount of instances of all three species needs to be more or less <em>equal</em> so that you do not favour one or the other class in your predictions.

To make your training and test sets, you first set a seed. 
This is a number of R’s random number generator. 
The major advantage of setting a seed is that you can get the same sequence of random numbers whenever you supply the same seed in the random number generator.

<code>set.seed(1234)</code>

Then, you want to make sure that your Iris data set is shuffled and that you have an equal amount of each species in your training and test sets.

You use the <code>sample()</code> function to take a sample with a size that is set as the number of rows of the Iris data set, or 150. 
You sample with replacement: you choose from a vector of 2 elements and assign either 1 or 2 to the 150 rows of the Iris data set. 
The assignment of the elements is subject to probability weights of 0.67 and 0.33.

<code>ind &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))</code>

<strong>Note</strong> that the <code>replace</code> argument is set to <code>TRUE</code>: this means that you assign a 1 or a 2 to a certain row and then reset the vector of 2 to its original state. 
This means that, for the next rows in your data set, you can either assign a 1 or a 2, each time again. 
The probability of choosing a 1 or a 2 should not be proportional to the weights amongst the remaining items, so you specify probability weights. 
Note also that, even though you don’t see it in the DataCamp Light chunk, the seed has still been set to <code>1234</code>.

<strong>Remember</strong> that you want your training set to be 2/3 of your original data set: that is why you assign “1” with a probability of 0.67 and the “2”s with a probability of 0.33 to the 150 sample rows.

You can then use the sample that is stored in the variable <code>ind</code> to define your training and test sets:

# Compose training set
iris.training &lt;- ....[ind==1, 1:4]

# Inspect training set
head(................)

# Compose test set
iris.test &lt;- ....[ind==2, 1:4]

# Inspect test set
head(...........)

<strong>Note</strong> that, in addition to the 2/3 and 1/3 proportions specified above, you don’t take into account all attributes to form the training and test sets. 
Specifically, you only take <code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code> and <code>Petal.Width</code>. 
This is because you actually want to predict the fifth attribute, <code>Species</code>: it is your target variable. 
However, you do want to include it into the KNN algorithm, otherwise there will never be any prediction for it.

You therefore need to store the class labels in factor vectors and divide them over the training and test sets:
# Compose `iris` training labels
iris.trainLabels &lt;- iris[ind==1,5]

# Inspect result
print(iris.trainLabels)

# Compose `iris` test labels
iris.testLabels &lt;- iris[ind==2, 5]

# Inspect result
print(iris.testLabels)

<h3>Step Six. The Actual KNN Model</h3>

<h3>Building Your Classifier</h3>

After all these preparation steps, you have made sure that all your known (training) data is stored. 
No actual model or learning was performed up until this moment. 
Now, you want to find the <em>k</em> nearest neighbors of your training set.

An easy way to do these two steps is by using the <code>knn()</code> function, which uses the Euclidian distance measure in order to find the <em>k</em>-nearest neighbours to your new, unknown instance. 
Here, the <em>k</em> parameter is one that you set yourself.

As mentioned before, new instances are classified by looking at the majority vote or weighted vote. 
In case of classification, the data point with the highest score wins the battle and the unknown instance receives the label of that winning data point. 
If there is an equal amount of winners, the classification happens randomly.

<strong>Note</strong>: the <em>k</em> parameter is often an odd number to avoid ties in the voting scores.

# Build the model
iris_pred &lt;- ...(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)

# Inspect `iris_pred`
.........

You store into <code>iris_pred</code> the <code>knn()</code> function that takes as arguments the training set, the test set, the train labels and the amount of neighbours you want to find with this algorithm. 
The result of this function is a factor vector with the predicted classes for each row of the test data.

<strong>Note</strong> that you don’t want to insert the test labels: these will be used to see if your model is good at predicting the actual classes of your instances!

You see that when you inspect the the result, <code>iris_pred</code>, you’ll get back the factor vector with the predicted classes for each row of the test data.

<h3>Step Seven. Evaluation of Your Model</h3>

An essential next step in machine learning is the evaluation of your model’s performance. 
In other words, you want to analyze the degree of correctness of the model’s predictions.

For a more abstract view, you can just compare the results of <code>iris_pred</code> to the test labels that you had defined earlier:

# Put `iris.testLabels` in a data frame
irisTestLabels &lt;- data.frame(................)

# Merge `iris_pred` and `iris.testLabels` 
merge &lt;- data.frame(........., ...............)

# Specify column names for `merge`
names(.....) &lt;- c("Predicted Species", "Observed Species")

# Inspect `merge` 
merge


You see that the model makes reasonably accurate predictions, with the exception of one wrong classification in row 29, where “Versicolor” was predicted while the test label is “Virginica”.

This is already some indication of your model’s performance, but you might want to go even deeper into your analysis. 
For this purpose, you can import the package <code>gmodels</code>:

<code>install.packages(&quot;package name&quot;)</code>

However, if you have already installed this package, you can simply enter

<code>library(gmodels)</code>

Then you can make a cross tabulation or a contingency table. 
This type of table is often used to understand the relationship between two variables. 
In this case, you want to understand how the classes of your test data, stored in <code>iris.testLabels</code> relate to your model that is stored in <code>iris_pred</code>:

<code>CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)</code>

<img alt="Crosstable iris knn" height="698" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/Screenshot-2015-03-24-20.05.32.png" width="926" />

<strong>Note</strong> that the last argument <code>prop.chisq</code> indicates whether or not the chi-square contribution of each cell is included. 
The chi-square statistic is the sum of the contributions from each of the individual cells and is used to decide whether the difference between the observed and the expected values is significant.

From this table, you can derive the number of correct and incorrect predictions: one instance from the testing set was labeled <code>Versicolor</code> by the model, while it was actually a flower of species <code>Virginica</code>. 
You can see this in the first row of the “Virginica” species in the <code>iris.testLabels</code> column. 
In all other cases, correct predictions were made. 
You can conclude that the model’s performance is good enough and that you don’t need to improve the model!

<a href="https://www.datacamp.com/courses/" target="_blank"><img alt="Learn Python for Data Science With DataCamp" src="http://community.datacamp.com.s3.amazonaws.com/community/production/ckeditor_assets/pictures/293/content_blog_banner.png" /></a>

<h3>Machine Learning in R with <code>caret</code></h3>

In the previous sections, you have gotten started with supervised learning in R via the KNN algorithm. 
As you might not have seen above, machine learning in R can get really complex, as there are various algorithms with various syntax, different parameters, etc. 
Maybe you’ll agree with me when I say that remembering the different package names for each algorithm can get quite difficult or that applying the syntax for each specific algorithm is just too much.

That’s where the <code>caret</code> package can come in handy: it’s short for “Classification and Regression Training” and offers everything you need to know to solve supervised machine learning problems: it provides a uniform interface to a ton of machine learning algorithms. 
If you’re a bit familiar with Python machine learning, you might see similarities with <code>scikit-learn</code>!

In the following, you’ll go through the steps as they have been outlined above, but this time, you’ll make use of <code>caret</code> to classify your data. 
Note that you have already done a lot of work if you’ve followed the steps as they were outlined above: you already have a hold on your data, you have explored it, prepared your workspace, etc. 
Now it’s time to preprocess your data with <code>caret</code>!

As you have done before, you can study the effect of the normalization, but you’ll see this later on in the tutorial.

You already know what’s next! Let’s split up the data in a training and test set. 
In this case, though, you handle things a little bit differently: you split up the data based on the labels that you find in <code>iris$Species</code>. 
Also, the ratio is in this case set at 75-25 for the training and test sets.

# Create index to split based on labels  
index &lt;- createDataPartition(iris$Species, p=0.75, list=FALSE)

# Subset training set with index
iris.training &lt;- iris[.......,]

# Subset test set with index
iris.test &lt;- iris[-.........,]

You’re all set to go and train models now! But, as you might remember, <code>caret</code> is an extremely large project that includes a lot of algorithms. 
If you’re in doubt on what algorithms are included in the project, you can get a list of all of them. 
Pull up the list by running <code>names(getModelInfo())</code>, just like the code chunk below demonstrates. 
Next, pick an algorithm and train a model with the <code>train()</code> function:

# Overview of algos supported by caret
names(getModelInfo())

# Train a model
model_knn &lt;- train(iris.training[, 1:4], iris.training[, 5], method='knn')

Note that making other models is extremely simple when you have gotten this far; You just have to change the <code>method</code> argument, just like in this example:

<code>model_cart &lt;- train(iris.training[, 1:4], iris.training[, 5], method='rpart2')</code>

Now that you have trained your model, it’s time to predict the labels of the test set that you have just made and evaluate how the model has done on your data:

# Predict the labels of the test set
predictions&lt;-predict(object=model_knn,iris.test[,1:4])

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions,iris.test[,5])

Additionally, you can try to perform the same test as before, to examine the effect of preprocessing, such as scaling and centering, on your model. 
Run the following code chunk:


# Train the model with preprocessing
model_knn &lt;- train(iris.training[, 1:4], iris.training[, 5], method='knn', preProcess=c("center", "scale"))

# Predict values
predictions&lt;-predict.train(object=model_knn,iris.test[,1:4], type="raw")

# Confusion matrix
confusionMatrix(predictions,iris.test[,5])

<h3>Move On To Big Data</h3>

Congratulations! You’ve made it through this tutorial!

This tutorial was primarily concerned with performing basic machine learning algorithm KNN with the help of R. 
The Iris data set that was used was small and overviewable; Not only did you see how you can perform all of the steps by yourself, but you’ve also seen how you can easily make use of a uniform interface, such as the one that <code>caret</code> offers, to spark your machine learning.

But you can do so much more!

If you have experimented enough with the basics presented in this tutorial and other machine learning algorithms, you might want to find it interesting to go further into R and data analysis.

<h2>Machine Learning in R with Example</h2>
As a kid, you might have come across a picture of a fish and you would have been told by your kindergarten teachers or parents that this is a fish and it has some specific features associated with it like it has fins, gills, a pair of eyes, a tail and so on. 

Now, whenever your brain comes across an image with those set of features, it automatically registers it as a fish because your brain has <em>learned </em>that it is a fish.

That's how our brain functions but what about a machine? If the same image is fed to a machine, how will the machine identify it to be a fish?

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture6.png">

This is where M<em>achine Learning</em> comes in. 

We'll keep on feeding images of a fish to a computer with the tag "fish" until the <em>machine learns all the features associated</em> with a<em> fish. 
</em>

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture7.png">

Once the machine learns all the features associated with a fish, we will feed it new data to determine how much has it learned.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture8.png">

In other words,<em> Raw Data/Training Data </em>is given to the machine, so that it <em>learns </em>all the features associated with the <em>Training Data. 
</em>Once, the learning is done, it is given <em>New Data/Test Data </em>to determine how well the machine has learned.

Let us move ahead in this Machine Learning with R blog and understand about types of Machine Learning.
<h3><strong>Types of Machine Learning</strong></h3><h3><strong>Supervised Learning: </strong></h3>

Supervised Learning algorithm learns from a known data-set(Training Data) which has labels to make predictions.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture9.png">

Regression and Classification are some examples of Supervised Learning.
<h4><strong>#Classification:</strong></h4>
Classification determines to which set of categories does a new observation belongs i.e. 

a classification algorithm learns all the features and labels of the training data and when new data is given to it, it has to assign labels to the new observations depending on what it has learned from the training data.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture10.png">

For this example, if the first observation is given the label "Man" then it is rightly classified but if it is given the label "Woman", the classification is wrong. 

Similarly for the second observation, if the label given is "Woman", it is rightly classified, else the classification is wrong.
<h4><strong>#Regression: </strong></h4>
Regression is a supervised learning algorithm which helps in determining how does one variable influence another variable.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture11.png">

Over here, "living_area" is the independent variable and "price" is the dependent variable i.e. 

we are determining how does "price" vary with respect to "living_area".
<li><h3><strong>Unsupervised Learning:</strong></h3></li>

Unsupervised learning algorithm draws inferences from data which does not have labels.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture12.png">

<em>Clustering</em> is an example of unsupervised learning. 

"K-means", "Hierarchical", "Fuzzy C-Means" are some examples of clustering algorithms.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture13.png">

In this example, the set of observations is divided into two clusters. 

Clustering is done on the basis of similarity between the observations. 

There is a high intra-cluster similarity and low inter-cluster similarity i.e. 

there is a very high similarity between all the buses but low similarity between the buses and cars.
<li><h3><strong>Reinforcement Learning:</strong></h3></li>

Reinforcement Learning is a type of machine learning algorithm where the <em>machine/agent</em> in an <em>environment </em>learns ideal behavior in order to maximize its performance. 
Simple reward feedback is required for the agent to learn its behavior, this is known as the <em>reinforcement signal</em>.
<h3><strong><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/pacman.png">
</strong></h3>
Let's take <em>pacman</em> for example. 

As long as pacman keeps eating food, it earns points but when it crashes against a monster it loses it's life. 

Thus pacman learns that it needs to eat more food and avoid monsters so as to improve it's performance.
<h3><strong>Implementing Machine Learning with R:</strong></h3><h3><strong>Linear Regression:</strong></h3>
We'll be working with the diamonds data-set to implement linear regression algorithm:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/diamond.png">

Description of the data-set:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/diamond_description.png">

Prior to building any model on the data, we are supposed to split the data into "train" and "test" sets. 

The model will be built on the "train" set and it's accuracy will be checked on the "test" set.

We need to load the "caTools" package to split the data into two sets.

<code>library(caTools)</code>
"caTools" package provides a function "sample.split()" which helps in splitting the data.

<code>sample.split(diamonds$price,SplitRatio = 0.65)-&gt;split_index</code>
65% of the observations from price column have been assigned the "true" label and the rest 35% have been assigned "false" label.

<code>subset(diamonds,split_index==T)-&gt;train
subset(diamonds,split_index==F)-&gt;test</code>
All the observations which have "true" label have been stored in the "<em>train" object</em> and those observations having "false" label have been assigned to the "test" set.

Now that the splitting is done and we have our "train" and "test" sets, it's time to build the linear regression model on the training set.

We'll be using the "lm()" function to build the linear regression model on the "train" data. 

We are determining the <em>price</em> of the diamonds with respect to all other variables of the data-set. 

The built model is stored in the object "mod_regress".

<code>lm(price~.,data = train)-&gt;mod_regress</code><em> </em>
Now, that we have built the model, we need to make predictions on the "test" set. 

"predict()" function is used to get predictions. 

It takes two arguments: the <em>built model</em> and the <em>test set. 
</em>The predicted results are stored in the "result_regress" object.

<code>predict(mod_regress,test)-&gt;result_regress</code>
Let's bind the actual price values from the "test" data-set and the predicted values into a single data-set using the "cbind()" function. 

The new data-frame is stored in "Final_Data"

<code>cbind(Actual=test$price,Predicted=result_regress)-&gt;Final_Data</code> 
<code>as.data.frame(Final_Data)-&gt;Final_Data</code>
A glance at the "Final_Data" which comprises of actual values and predicted values:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data.png">

Let's find the error by subtracting the predicted values from the actual values and add this error as a new column to the "Final_Data":

<code>(Final_Data$Actual- Final_Data$Predicted)-&gt;error</code>
<code>cbind(Final_Data,error)-&gt;Final_Data</code>
A glance at the "Final_Data" which also comprises of the error in prediction:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data2.png">

Now, we'll go ahead and calculate "<em>Root Mean Square Error" </em>which gives an aggregate error for all the predictions

<code>rmse1&lt;-sqrt(mean(Final_Data$error^2))</code> 
<code>rmse1</code>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rmse1.png">

Going ahead, let's build another model, so that we can compare the accuracy of both these models and determine which is a better one.

We'll build a new linear regression model on the "train" set but this time, we'll be dropping the &#8216;x' and &#8216;y' columns from the independent variables i.e. 

the "price" of the diamonds is determined by all the columns except &#8216;x' and &#8216;y'.

The model built is stored in "mod_regress2": 

<code>lm(price~.-y-z,data = train)-&gt;mod_regress2</code>
The predicted results are stored in "result_regress2"<br> 

<code>predict(mod_regress2,test)-&gt;result_regress2</code>
Actual and Predicted values are combined and stored in "Final_Data2":

<code>cbind(Actual=test$price,Predicted=result_regress2)-&gt;Final_Data2</code> 
<code>as.data.frame(Final_Data2)-&gt;Final_Data2</code>
Let's also add the error in prediction to "Final_Data2"

<code>(Final_Data2$Actual- Final_Data2$Predicted)-&gt;error2</code>
<code>cbind(Final_Data2,error2)-&gt;Final_Data2</code>
A glance at "Final_Data2":

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data2.png">

Finding Root Mean Square Error to get the aggregate error:

<code>rmse2&lt;-sqrt(mean(Final_Data2$error^2))</code>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rmse2.png">

We see that "rmse2" is marginally less than "rmse1" and hence the second model is marginally better than the first model.
<h3><strong>Classification:</strong></h3>
We'll be working with the "car_purchase" data-set to implement <em>recursive partitioning </em>which is a classification algorithm.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/car-1.png">

Let's split the data into "train" and "test" sets using "sample.split()" function from "caTools" package.

<code>library(caTools)</code>
65% of the observations from &#8216;Purchased' column will be assigned "TRUE" labels and the rest will be assigned "FALSE" labels.

<code>sample.split(car_purchase$Purchased,SplitRatio = 0.65)-&gt;split_values</code>
All those observations which have "TRUE" label will be stored into &#8216;train' data and those observations having "FALSE" label will be assigned to &#8216;test' data.

<code>subset(car_purchase,split_values==T)-&gt;train_data</code>
<code>subset(car_purchase,split_values==F)-&gt;test_data</code>
Time to build the Recursive Partitioning algorithm:

We'll start off by loading the &#8216;rpart' package:

<code>library(rpart)</code>
"Purchased" column will be the dependent variable and all other columns are the independent variables i.e. 

we are determining whether the person has bought the car or not with respect to all other columns. 

The model is built on the "train_data" and the result is stored in "mod1".

<code>rpart(Purchased~.,data = train_data)-&gt;mod1</code>
Let's plot the result:

<code>plot(mod1,margin = 0.1)</code> <code>text(mod1,pretty = T,cex=0.8)</code>
<strong><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rplot.png">
</strong>

Now, let's go ahead and predict the results on "test_data". 

We are giving the built rpart model "mod1" as the first argument, the test set "test_data" as the second argument and prediction type as "class" for the third argument. 

The result is stored in &#8216;result1' object. 

<code>predict(mod1,test_data,type = "class")-&gt;result1</code>
Let's evaluate the accuracy of the model using "confusionMatrix()" function from caret package.

<code>library(caret)</code> <code></code><code>confusionMatrix(table(test_data$Purchased,result1))</code>
<strong><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/confusion.png">
</strong>

The confusion matrix tells us that out of the 90 observations where the person did not buy the car, 79 observations have been rightly classified as "No" and 11 have been wrongly classified as "YES". 

Similarly, out of the 50 observations where the person actually bought the car, 47 have been rightly classified as "YES" and 3 have been wrongly classified as "NO".

We can find the accuracy of the model by dividing the correct predictions with total predictions i.e. 

(79+47)/(79+47+11+3).
<h3><strong>K-Means Clustering:</strong></h3>
We'll work with "iris" data-set to implement k-means clustering:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/iris.png">

Let's remove the "Species" column and create a new data-set which comprises only the first four columns from the &#8216;iris' data-set.

<code>iris[1:4]-&gt;iris_k</code>
Let us take the number of clusters to be 3. 

"Kmeans()" function takes the input data and the number of clusters in which the data is to be clustered. 

The syntax is : kmeans( data, k) where k is the number of cluster centers.

<code>kmeans(iris_k,3)-&gt;k1</code>
Analyzing the clustering:

<code>str(k1)</code>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture15.png">

The str() function gives the structure of the kmeans which includes various parameters like withinss, betweenss, etc, analyzing which you can find out the performance of kmeans.

betweenss : Between sum of squares i.e. 

Intracluster similarity

withinss : Within sum of square i.e. 

Intercluster similarity

totwithinss : Sum of all the withinss of all the clusters i.e.Total intra-cluster similarity

A good clustering will have a lower value of "tot.withinss" and higher value of "betweenss" which depends on the number of clusters &lsquo;k&rsquo; chosen initially. 

The time is ripe to become an expert in Machine Learning to take advantage of new opportunities that come your way. 

This brings us to the end of this "<em><strong>Machine Learning with R</strong></em>" blog. 

I hope this blog was informative fruitful.

<h2>put the whole if else statement in one line</h2>
if (TRUE) 1 else 3

You have to use {} for allows the if statement to have more than one line. 

<h2>quit R</h2>
if logout, 
quit("yes")

<h2>Run R scripts from the Windows command line (CMD)</h2>
This use library RDCOMClient to send summary information to colleges with Microsoft Outlook

There are two ways to do that.

use batch file looks like this.
"C:\Program Files\R\R-3.4.3\bin\Rscript.exe" C:\Users\myusername\Documents\R\Send_Outlook_Email.R

The second one looks like this.
"C:\Program Files\R\R-3.4.3\bin\R.exe" CMD BATCH C:\Users\myusername\Documents\R\Send_Outlook_Email.R

Remember to use quotation marks when there is space in the file path.

<h2>Locate the position of patterns in a string</h2>
library("stringr")
fruit &lt;- "apple banana pear pineapple"
str_locate(fruit, "ea")
str_locate_all(fruit, "ea")

str_locate, an integer matrix.
First column gives start postion of match, and second column gives end position

str_locate_all a list of integer matrices.

<h3>trim string</h3>
x &lt;- "This string is moderately long"
strtrim(x, 20)

<h2>find the max length of string in array</h2>
setwd("D:/Dropbox/MyDocs/R misc Jobs/Learning Exercise/QuizData")
WordTableFIle <&lt;- readLines("EnglishWordList.txt", encoding="UTF-8", warn = FALSE)
WordTable <&lt;- matrix(unlist(strsplit(WordTableFIle, split = "\\t")), ncol=1, byrow=TRUE) # make it one column
maxNum = max(nchar(WordTable))
maxNum
WordTable[which(nchar(WordTable) == maxNum)]

<h2>Convert Character Vector between Encodings</h2>
iconv(x, from, to, sub=NA)

‘i’ stands for ‘internationalization’.

Usage
iconv(x, from, to, sub=NA)

Arguments
x	A character vector.
from	A character string describing the current encoding.

to	A character string describing the target encoding.

sub	character string. If not NA it is used to replace any non-convertible bytes in the input.
(This would normally be a single character, but can be more.
If "byte", the indication is "&lt;xx>" with the hex code of the byte.

Details
The names of encodings and which ones are available (and indeed, if any are) is platform-dependent. 
On systems that support R's iconv you can use "" for the encoding of the current locale, as well as "latin1" and "UTF-8".


iconvlist()
On many platforms iconvlist provides an alphabetical list of the supported encodings. 
On others, the information is on the man page for iconv(5) or elsewhere in the man pages (and beware that the system command iconv may not support the same set of encodings as the C functions R calls). 
Unfortunately, the names are rarely common across platforms.

Elements of x which cannot be converted (perhaps because they are invalid or because they cannot be represented in the target encoding) will be returned as NA unless sub is specified.

Some versions of iconv will allow transliteration by appending //TRANSLIT to the to encoding: see the examples.

Value
A character vector of the same length and the same attributes as x.

Note
Not all platforms support these functions. 

See Also
localeToCharset, file.

Examples
## Not run: 
iconvlist()

## convert from Latin-2 to UTF-8: two of the glibc iconv variants.
iconv(x, "ISO_8859-2", "UTF-8")
iconv(x, "LATIN2", "UTF-8")

## Both x below are in latin1 and will only display correctly in a
## latin1 locale.
(x &lt;- "fa\xE7ile")
charToRaw(xx &lt;- iconv(x, "latin1", "UTF-8"))
## in a UTF-8 locale, print(xx)

iconv(x, "latin1", "ASCII")          #   NA
iconv(x, "latin1", "ASCII", "?")     # "fa?ile"
iconv(x, "latin1", "ASCII", "")      # "faile"
iconv(x, "latin1", "ASCII", "byte")  # "faile"

# Extracts from R help files
(x &lt;- c("Ekstr\xf8m", "J\xf6reskog", "bi\xdfchen Z\xfcrcher"))
iconv(x, "latin1", "ASCII//TRANSLIT")
iconv(x, "latin1", "ASCII", sub="byte")
## End(Not run)

<h2>encoding error with read_html</h2>
<a href="https://stackoverflow.com/users/4350463/hodgenovice" class="whitebut ">hodgenovice R expert</a>
<a href="https://stackoverflow.com/questions/45290452/encoding-error-with-read-html" class="whitebut ">encoding error with read_html</a>
url = "http://www.chinanews.com/scroll-news/news1.html"
thekeyword = "新闻"
read_page = lapply(unique(iconvlist()), function(encoding_attempt) {

  # Optional print statement to show progress since this takes time
  # print(match(encoding_attempt, iconvlist()) / length(iconvlist()))

  read_attempt = tryCatch(expr=read_html(url, encoding=encoding_attempt),
                           error=function(condition) NA,
                           warning=function(condition) message(condition))
  read_attempt = as.character(read_attempt)
  fisrtLine = grep(thekeyword, read_attempt)
  if(length(fisrtLine)>0){
    cat(encoding_attempt, "\n")
    cat(read_attempt[fisrtLine], "\n")
  }
})

names(read_page) = unique(iconvlist())


# 2. See which encodings correctly display some complex characters
read_phrase = lapply(read_page, function(encoded_page) 
  if(!is.na(encoded_page))
    html_text(html_nodes(encoded_page, ".content_right")))

# ended up with encodings which could be sensible...
encoding_shortlist = names(read_phrase)[read_phrase == "新闻"]
encoding_shortlist

sink("testResult.txt")
print(read_page)
sink()

retriveFile &lt;- as.character(read_html(url, warn=F, encoding = "UTF-16"))
fisrtLine = grep(thekeyword, retriveFile)
fisrtLine

<h2>Object-oriented programming (OOP)</h2>
a programming paradigm based on the concept of "objects", 

which can contain data, in the form of fields (often known as attributes or properties), 

and code, in the form of procedures (often known as methods). 

A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of "this" or "self"). 

In OOP, computer programs are designed by making them out of objects that interact with one another.

OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.

Many of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. 

Significant object-oriented languages include Java, C++, C#, Python, R, PHP, JavaScript, Ruby, Perl, Object Pascal, Objective-C, Dart, Swift, Scala, Kotlin, Common Lisp, MATLAB, and Smalltalk.


<h2>Reference classes</h2>
<!-- http://www.inside-r.org/r-doc/methods/ReferenceClasses -->
R has three object oriented (OO) systems: [[S3]], [[S4]] and Reference Classes (where the latter were for a while referred to as [[R5]], yet their official name is Reference Classes). 
This page describes this new reference-based class system.

Reference Classes (or refclasses) are new in R 2.12. 
They fill a long standing need for mutable objects that had previously been filled by non-core packages like R.oo, proto and mutatr. 
While the core functionality is solid, reference classes are still under active development and some details will change. 
The most up-to-date documentation for Reference Classes can always be found in ?ReferenceClasses.

There are two main differences between reference classes and S3 and S4:

Refclass objects use message-passing OO
Refclass objects are mutable: the usual R copy on modify semantics do not apply

These properties makes this object system behave much more like Java and C#. 
Surprisingly, the implementation of reference classes is almost entirely in R code - they are a combination of S4 methods and environments. 
This is a testament to the flexibility of S4.

Particularly suited for: simulations where you’re modelling complex state, GUIs.

Note that when using reference based classes we want to minimise side effects, and use them only where mutable state is absolutely required. 
The majority of functions should still be “functional”, and side effect free. 
This makes code easier to reason about (because you don’t need to worry about methods changing things in surprising ways), and easier for other R programmers to understand.

Limitations: can’t use enclosing environment - because that’s used for the object.

<h3>Classes and instances</h3>
Creating a new reference based class is straightforward: you use setRefClass. 
Unlike setClass from S4, you want to keep the results of that function around, because that’s what you use to create new objects of that type:

# Or keep reference to class around.
Person = setRefClass("Person")
Person$new()

A reference class has three main components, given by three arguments to setRefClass:

contains, the classes which the class inherits from. 
These should be other reference class objects:

setRefClass("Polygon")
setRefClass("Regular")

# Specify parent classes
setRefClass("Triangle", contains = "Polygon")
setRefClass("EquilateralTriangle", 
  contains = c("Triangle", "Regular"))

fields are the equivalent of slots in S4. 
They can be specified as a vector of field names, or a named list of field types:

setRefClass("Polygon", fields = c("sides"))
setRefClass("Polygon", fields = list(sides = "numeric"))

The most important property of refclass objects is that they are mutable, or equivalently they have reference semantics:

    Polygon = setRefClass("Polygon", fields = c("sides"))
    square = Polygon$new(sides = 4)
    
    triangle = square
    triangle$sides = 3
    
    square$sides        

methods are functions that operate within the context of the object and can modify its fields. 
These can also be added after object creation, as described below.

setRefClass("Dist")
setRefClass("DistUniform", c("a", "b"), "Dist", methods = list(
  mean = function() {
    (a + b) / 2
  }
))

You can also add methods after creation:

# Instead of creating a class all at once:
Person = setRefClass("Person", methods = list(
  say_hello = function() message("Hi!")
))

# You can build it up piece-by-piece
Person = setRefClass("Person")
Person$methods(say_hello = function() message("Hi!"))

It’s not currently possible to modify fields because adding fields would invalidate existing objects that didn’t have those fields.

The object returned by setRefClass (or retrieved later by getRefClass) is called a generator object. 
It has methods:

new for creating new objects of that class. 
The new method takes named arguments specifying initial values for the fields

methods for modifying existing or adding new methods

help for getting help about methods

fields to get a list of fields defined for class

lock locks the named fields so that their value can only be set once

accessors a convenience method that automatically sets up accessors of the form getXXX and setXXX.

<h3>Methods</h3>
Refclass methods are associated with objects, not with functions, and are called using the special syntax obj$method(arg1, arg2, ...). 
(You might recall we’ve seen this construction before when we called functions stored in a named list). 
Methods are also special because they can modify fields. 
This is different

We’ve also seen this construct before, when we used closures to create mutable state. 
Reference classes work in a similar manner but give us some extra functionality:

inheritance
a way of documenting methods
a way of specifying fields and their types

Modify fields with &lt;=. 
Will call accessor functions if defined.

Special fields: .self (Don’t use fields with names starting with . 
as these may be used for special purposes in future versions.)

initialize

<h3>Common methods</h3>
Because all refclass classes inherit from the same superclass, envRefClass, they a have common set of methods:

obj$callSuper:

obj$copy: creates a copy of the current object. 
This is necessary because Reference Classes classes don’t behave like most R objects, which are copied on assignment or modification.

obj$field: named access to fields. 
Equivalent to slots for S4. 
obj$field("xxx") the same as obj$xxx. 
obj$field("xxx", 5) the same as obj$xxx = 5

obj$import(x) coerces into this object, and obj$export(Class) coerces a copy of obj into that class. 
These should be super classes.

obj$initFields

<h1>R S3 Class</h1>
In this article, you will learn to work with S3 classes (one of the three class systems in R programming).

S3 class is the most popular and prevalent class in R programming language.

Most of the classes that come predefined in R are of this type. 
The fact that it is simple and easy to implement is the reason behind this.

<h2>How to define S3 class and create S3 objects?</h2>
S3 class has no formal, predefined definition.

Basically, a list with its class attribute set to some class name, is an S3 object. 
The components of the list become the member variables of the object.

Following is a simple example of how an S3 object of class student can be created.

> # create a list with required components
> s = list(name = "John", age = 21, GPA = 3.5)
> # name the class appropriately
> class(s) = "student"
> # That's it! we now have an object of class "student"
> s
$name
[1] "John"
$age
[1] 21
$GPA
[1] 3.5
attr(,"class")
[1] "student"

This might look awkward for programmers coming from C++, Python etc. 
where there are formal class definitions and objects have properly defined attributes and methods.

In R S3 system, it's pretty ad hoc. 
You can convert an object's class according to your will with objects of the same class looking completely different. 
It's all up to you.

<h2>How to use constructors to create objects?</h2>
It is a good practice to use a function with the same name as class (not a necessity) to create objects.

This will bring some uniformity in the creation of objects and make them look similar.

We can also add some integrity check on the member attributes. 
Here is an example. 
Note that in this example we use the attr() function to set the class attribute of the object.

# a constructor function for the "student" class
student = function(n,a,g) {
# we can add our own integrity checks
if(g>4 || g&lt;0)  stop("GPA must be between 0 and 4")
value = list(name = n, age = a, GPA = g)
# class can be set using class() or attr() function
attr(value, "class") = "student"
value
}

Here is a sample run where we create objects using this constructor.

> s = student("Paul", 26, 3.7)
> s
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7
attr(,"class")
[1] "student"
> class(s)
[1] "student"
> s = student("Paul", 26, 5)
Error in student("Paul", 26, 5) : GPA must be between 0 and 4
> # these integrity check only work while creating the object using constructor
> s = student("Paul", 26, 2.5)
> # it's up to us to maintain it or not
> s$GPA = 5

<h2>Methods and Generic Functions</h2>
In the above example, when we simply write the name of the object, its internals get printed.

In interactive mode, writing the name alone will print it using the print() function.

> s
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7
attr(,"class")
[1] "student"

Furthermore, we can use print() with vectors, matrix, data frames, factors etc. 
and they get printed differently according to the class they belong to.

How does print() know how to print these variety of dissimilar looking object?

The answer is, print() is a generic function. 
Actually, it has a collection of a number of methods. 
You can check all these methods with methods(print).

> methods(print)
[1] print.acf*                                   
[2] print.anova*
...
[181] print.xngettext*                             
[182] print.xtabs*                                 
Non-visible functions are asterisked

We can see methods like print.data.frame and print.factor in the above list.

When we call print() on a data frame, it is dispatched to print.data.frame().

If we had done the same with a factor, the call would dispatch to print.factor(). 
Here, we can observe that the method names are in the form generic_name.class_name(). 
This is how R is able to figure out which method to call depending on the class.

Printing our object of class "student" looks for a method of the form print.student(), but there is no method of this form.

So, which method did our object of class "student" call?

It called print.default(). 
This is the fallback method which is called if no other match is found. 
Generic functions have a default method.

There are plenty of generic functions like print(). 
You can list them all with methods(class="default").

> methods(class="default")
[1] add1.default*            aggregate.default*      
[3] AIC.default*             all.equal.default
...

<h2>How to write your own method?</h2>
Now let us implement a method print.student() ourself.

print.student = function(obj) {
cat(obj$name, "\n")
cat(obj$age, "years old\n")
cat("GPA:", obj$GPA, "\n")
}

Now this method will be called whenever we print() an object of class "student".

In S3 system, methods do not belong to object or class, they belong to generic functions. 
This will work as long as the class of the object is set.

> # our above implemented method is called
> s
Paul 
26 years old
GPA: 3.7 
> # removing the class attribute will restore as previous
> unclass(s)
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7

<h2>Writing Your Own Generic Function</h2>
It is possible to make our own generic function like print() or plot(). 
Let us first look at how these functions are implemented.

> print
function (x, ...) 
UseMethod("print")
&lt;bytecode: 0x0674e230>
&lt;environment: namespace:base>
> plot
function (x, y, ...) 
UseMethod("plot")
&lt;bytecode: 0x04fe6574>
&lt;environment: namespace:graphics>

We can see that they have a single call to UseMethod() with the name of the generic function passed to it. 
This is the dispatcher function which will handle all the background details. 
It is this simple to implement a generic function.

For the sake of example, we make a new generic function called grade.

grade = function(obj) {
UseMethod("grade")
}

A generic function is useless without any method. 
Let us implement the default method.

grade.default = function(obj) {
cat("This is a generic function\n")
}

Now let us make method for our class "student".

grade.student = function(obj) {
cat("Your grade is", obj$GPA, "\n")
}

A sample run.

> grade(s)
Your grade is 3.7

In this way, we implemented a generic function called grade and later a method for our class.

<h1>R Inheritance</h1>
In this article, you’ll learn everything about inheritance in R. 
More specifically, how to create inheritance in S3, S4 and Reference classes, and use them efficiently in your program.

Inheritance is one of the key features of object-oriented programming which allows us to define a new class out of existing classes.

This is to say, we can derive new classes from existing base classes and adding new features. 
We don’t have to write from scratch. 
Hence, inheritance provides reusability of code.

Inheritance forms a hierarchy of class just like a family tree. 
Important thing to note is that the attributes define for a base class will automatically be present in the derived class.

Moreover, the methods for the base class will work for the derived.

<img class="alignnone size-full wp-image-79" src="https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance.jpg" alt="Inheritance in R Programming" srcset="https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance.jpg 740w, https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance-300x162.jpg 300w" sizes="(max-width: 740px) 100vw, 740px">

Below, we discuss how inheritance is carried out for the three different class systems in R programming language.

<h2>Inheritance in S3 Class</h2>
S3 classes do not have any fixed definition. 
Hence attributes of S3 objects can be arbitrary.

Derived class, however, inherit the methods defined for base class. 
Let us suppose we have a function that creates new objects of class student as follows.

student = function(n,a,g) {
value = list(name=n, age=a, GPA=g)
attr(value, "class") = "student"
value
}

Furthermore, we have a method defined for generic function print() as follows.

print.student = function(obj) {
cat(obj$name, "\n")
cat(obj$age, "years old\n")
cat("GPA:", obj$GPA, "\n")
}

Now we want to create an object of class InternationalStudent which inherits from student.

This is be done by assigning a character vector of class names like class(obj) = c(child, parent).

> # create a list
> s = list(name="John", age=21, GPA=3.5, country="France")
> # make it of the class InternationalStudent which is derived from the class student
> class(s) = c("InternationalStudent","student")
> # print it out
> s
John 
21 years old
GPA: 3.5

We can see above that, since we haven’t defined any method of the form print.InternationalStudent(), the method print.student() got called. 
This method of class student was inherited.

Now let us define print.InternationalStudent().

print.InternationalStudent = function(obj) {
cat(obj$name, "is from", obj$country, "\n")
}

This will overwrite the method defined for class student as shown below.

> s
John is from France

We can check for inheritance with functions like inherits() or is().

> inherits(s,"student")
[1] TRUE
> is(s,"student")
[1] TRUE

<h2>Inheritance in S4 Class</h2>
Since S4 classes have proper definition, derived classes will inherit both attributes and methods of the parent class.

Let us define a class student with a method for the generic function show().

# define a class called student
setClass("student",
slots=list(name="character", age="numeric", GPA="numeric")
)
# define class method for the show() generic function
setMethod("show",
"student",
function(object) {
cat(object@name, "\n")
cat(object@age, "years old\n")
cat("GPA:", object@GPA, "\n")
}
)

Inheritance is done during the derived class definition with the argument contains as shown below.

# inherit from student
setClass("InternationalStudent",
slots=list(country="character"),
contains="student"
)

Here we have added an attribute country, rest will be inherited from the parent.

> s = new("InternationalStudent",name="John", age=21, GPA=3.5, country="France")
> show(s)
John 
21 years old
GPA: 3.5

We see that method define for class student got called when we did show(s).

We can define methods for the derived class which will overwrite methods of the base class, like in the case of S3 systems.

<h2>Inheritance in Reference Class</h2>
Inheritance in reference class is very much similar to that of the S4 class. 
We define in the contains argument, from which base class to derive from.

Here is an example of student reference class with two methods inc_age() and dec_age().

student = setRefClass("student",
fields=list(name="character", age="numeric", GPA="numeric"),
methods=list(
inc_age = function(x) {
age &lt;= age + x
},
dec_age = function(x) {
age &lt;= age - x
}
)
)

Now we will inherit from this class. 
We also overwrite dec_age() method to add an integrity check to make sure age is never negative.

InternationalStudent = setRefClass("InternationalStudent",
fields=list(country="character"),
contains="student",
methods=list(
dec_age = function(x) {
if((age - x)&lt;0)  stop("Age cannot be negative")
age &lt;= age - x
}
)
)

Let us put it to test.

> s = InternationalStudent(name="John", age=21, GPA=3.5, country="France")
> s$dec_age(5)
> s$age
[1] 16
> s$dec_age(20)
Error in s$dec_age(20) : Age cannot be negative
> s$age
[1] 16

In this way, we are able to inherit from the parent class.


<h2>R Reference Class</h2>
In this article, you will learn to work with reference classes in R programming which is one of the three class systems (other two are S3 and S4).

Reference class in R programming is similar to the object oriented programming we are used to seeing in common languages like C++, Java, Python etc.

Unlike <a title="R S3 Class" href="/r-programming/S3-class">S3</a> and <a title="R S4 class" href="/r-programming/S4-class">S4 classes</a>, methods belong to class rather than generic functions. 
Reference class are internally implemented as S4 classes with an environment added to it.

<h3>How to define a reference class?</h3>
Defining reference class is similar to defining a S4 class. 
Instead of setClass() we use the setRefClass() function.

> setRefClass("student")

Member variables of a class, if defined, need to be included in the class definition. 
Member variables of reference class are called fields (analogous to slots in S4 classes).

Following is an example to define a class called student with 3 fields, name, age and GPA.

> setRefClass("student", fields = list(name = "character", age = "numeric", GPA = "numeric"))

<h3>How to create a reference objects?</h3>
The function setRefClass() returns a generator function which is used to create objects of that class.

> student = setRefClass("student",
fields = list(name = "character", age = "numeric", GPA = "numeric"))
> # now student() is our generator function which can be used to create new objects
> s = student(name = "John", age = 21, GPA = 3.5)
> s
Reference class object of class "student"
Field "name":
[1] "John"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h3>How to access and modify fields?</h3>
Fields of the object can be accessed using the $ operator.

> s$name
[1] "John"
> s$age
[1] 21
> s$GPA
[1] 3.5

Similarly, it is modified by reassignment.

> s$name = "Paul"
> s
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h4>Warning Note</h4>

In R programming, objects are copied when assigned to new variable or passed to a function (pass by value). 
For example.

> # create list a and assign to b
> a = list("x" = 1, "y" = 2)
> b = a
> # modify b
> b$y = 3
> # a remains unaffected
> a
$x
[1] 1
$y
[1] 2
> # only b is modified
> b
$x
[1] 1
$y
[1] 3

But this is not the case with reference objects. 
Only a single copy exist and all variables reference to the same copy. 
Hence the name, reference.

> # create reference object a and assign to b
> a = student(name = "John", age = 21, GPA = 3.5)
> b = a
> # modify b
> b$name = "Paul"
> # a and b both are modified
> a
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5
> b
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

This can cause some unwanted change in values and be the source of strange bugs. 
We need to keep this in mind while working with reference objects. 
To make a copy, we can use the copy() method made availabe to us.

> # create reference object a and assign a’s copy to b
> a = student(name = "John", age = 21, GPA = 3.5)
> b = a$copy()
> # modify b
> b$name = "Paul"
> # a remains unaffected
> a
Reference class object of class "student"
Field "name":
[1] "John"
Field "age":
[1] 21
Field "GPA":
[1] 3.5
> # only b is modified
> b
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h3>Reference Methods</h3>
Methods are defined for a reference class and do not belong to generic functions as in S3 and S4 classes.

All reference class have some methods predefined because they all are inherited from the superclass envRefClass.

> student
Generator for class "student":
Class fields:
Name:       name       age       GPA
Class: character   numeric   numeric
Class Methods:  
"callSuper", "copy", "export", "field", "getClass", "getRefClass", 
"import", "initFields", "show", "trace", "untrace", "usingMethods"
Reference Superclasses:  
"envRefClass"

We can see class methods like copy(), field() and show() in the above list. 
We can create our own methods for the class.

This can be done during the class definition by passing a list of function definitions to methods argument of setRefClass().

student = setRefClass("student",
fields = list(name = "character", age = "numeric", GPA = "numeric"),
methods = list(
inc_age = function(x) {
age &lt;= age + x
},
dec_age = function(x) {
age &lt;= age - x
}
)
)

In the above section of our code, we defined two methods called inc_age() and dec_age(). 
These two method modify the field age.

Note that we have to use the non-local assignment operator &lt;= since age isn’t in the method’s local environment. 
This is important.

Using the simple assignment operator = would have created a local variable called age, which is not what we want. 
R will issue a warning in such case.

Here is a sample run where we use the above defined methods.

> s = student(name = "John", age = 21, GPA = 3.5)
> s$inc_age(5)
> s$age
[1] 26
> s$dec_age(10)
> s$age
[1] 16

<h2>R S4 Class</h2>
In this article, you’ll learn everything about S4 classes in R; how to define them, create them, access their slots, and use them efficiently in your program.

Unlike <a title="R S3 class" href="/r-programming/S3-class">S3 classes</a> and <a title="R object" href="/r-programming/object-class-introduction">objects</a> which lacks formal definition, we look at S4 class which is stricter in the sense that it has a formal definition and a uniform way to create objects.

This adds safety to our code and prevents us from accidentally making naive mistakes.

<h3>How to define S4 Class?</h3>
S4 class is defined using the setClass() function.

In R terminology, member variables are called slots. 
While defining a class, we need to set the name and the slots (along with class of the slot) it is going to have.

<h4>Example 1: Definition of S4 class</h4>
setClass("student", slots=list(name="character", age="numeric", GPA="numeric"))

In the above example, we defined a new class called student along with three slots it’s going to have name, age and GPA.

There are other optional arguments of setClass() which you can explore in the help section with ?setClass.

<h3>How to create S4 objects?</h3>
S4 objects are created using the new() function.

<h4>Example 2: Creation of S4 object</h4>
> # create an object using new()
> # provide the class name and value for slots
> s = new("student",name="John", age=21, GPA=3.5)
> s
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.5

We can check if an object is an S4 object through the function isS4().

> isS4(s)
[1] TRUE

The function setClass() returns a generator function.

This generator function (usually having same name as the class) can be used to create new objects. 
It acts as a constructor.

> student = setClass("student", slots=list(name="character", age="numeric", GPA="numeric"))
> student
class generator function for class “student” from package ‘.GlobalEnv’
function (...) 
new("student", ...)

Now we can use this constructor function to create new objects.

Note above that our constructor in turn uses the new() function to create objects. 
It is just a wrap around.

<h4>Example 3: Creation of S4 objects using generator function</h4>
> student(name="John", age=21, GPA=3.5)
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.5

<h3>How to access and modify slot?</h3>
Just as components of a <a title="R list" href="/r-programming/list">list</a> are accessed using $, slot of an object are accessed using @.

<h4>Accessing slot</h4>
> s@name
[1] "John"
> s@GPA
[1] 3.5
> s@age
[1] 21

<h4>Modifying slot directly</h4>
A slot can be modified through reassignment.

> # modify GPA
> s@GPA = 3.7
> s
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.7

<h4>Modifying slots using slot() function</h4>
Similarly, slots can be access or modified using the slot() function.

> slot(s,"name")
[1] "John"
> slot(s,"name") = "Paul"
> s
An object of class "student"
Slot "name":
[1] "Paul"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.7

<h3>Methods and Generic Functions</h3>
As in the case of S3 class, methods for S4 class also belong to generic functions rather than the class itself. 
Working with S4 generics is pretty much similar to S3 generics.

You can list all the S4 generic functions and methods available, using the function showMethods().

<h4>Example 4: List all generic functions</h4>
> showMethods()
Function: - (package base)
Function: != (package base)
...
Function: trigamma (package base)
Function: trunc (package base)

Writing the name of the object in interactive mode prints it. 
This is done using the S4 generic function show().

You can see this function in the above list. 
This function is the S4 analogy of the S3 print() function.

<h4>Example 5: Check if a function is a generic function</h4>
> isS4(print)
[1] FALSE
> isS4(show)
[1] TRUE

We can list all the methods of show generic function using showMethods(show).

<h4>Example 6: List all methods of a generic function</h4>
> showMethods(show)
Function: show (package methods)
object="ANY"
object="classGeneratorFunction"
...
object="standardGeneric"
(inherited from: object="genericFunction")
object="traceable"

<h3>How to write your own method?</h3>
We can write our own method using setMethod() helper function.

For example, we can implement our class method for the show() generic as follows.

setMethod("show",
"student",
function(object) {
cat(object@name, "\n")
cat(object@age, "years old\n")
cat("GPA:", object@GPA, "\n")
}
)

Now, if we write out the name of the object in interactive mode as before, the above code is executed.

> s = new("student",name="John", age=21, GPA=3.5)
> s    # this is same as show(s)
John 
21 years old
GPA: 3.5

In this way we can write our own S4 class methods for generic functions.

<h2>Write text to a file</h2>
fileConn<-file("output.txt")
writeLines(c("Hello","World"), fileConn)
close(fileConn)

sink("outfile.txt")
cat("hello")
cat("world")
sink()

cat("Hello",file="outfile.txt",sep="\n")
cat("World",file="outfile.txt",append=TRUE)
cat("hello","world",file="output.txt",sep="\n",append=TRUE)
file.show("outfile.txt")

txt <- "Hallo\nWorld"
writeLines(txt, "outfile.txt")

library(tidyverse)
c('Hello', 'World') %>% write_lines( "output.txt")

writeLines() with sink()
sink("tempsink", type="output")
writeLines("Hello\nWorld")
sink()
file.show("tempsink", delete.file=TRUE)

text = c("Hello", "World")
write.table(text, file = "output.txt", col.names = F, row.names = F, quote = F)

<h2>Play birthday music</h2>
<a href="https://stackoverflow.com/questions/31782580/how-can-i-play-birthday-music-using-r" class="whitebut ">play-birthday-music</a>
library("dplyr")
library("audio")
notes = c(A = 0, B = 2, C = 3, D = 5, E = 7, F = 8, G = 10)
pitch = "D D E D G F# D D E D A G D D D5 B G F# E C5 C5 B G A G"
duration = c( rep( c(0.75, 0.25, 1, 1, 1, 2), 2),
              0.75, 0.25, 1, 1, 1, 1, 1, 0.75, 0.25, 1, 1, 1, 2)
bday = data_frame(pitch = strsplit(pitch, " ")[[1]], duration = duration)

bday =
  bday %>%
  mutate(octave = substring(pitch, nchar(pitch)) %>%
          {suppressWarnings(as.numeric(.))} %>%
           ifelse(is.na(.), 4, .),
           note = notes[substr(pitch, 1, 1)],
           note = note + grepl("#", pitch) -
           grepl("b", pitch) + octave * 12 + 12 * (note < 3),
           freq = 2 ^ ((note - 60) / 12) * 440)

tempo = 120
sample_rate = 44100 # this is MP3 sample freq, the freq resolution is 40Hz
                    # the A4 freq is 440Hz
                    # the A#4 freq is 466Hz
                    # the Ab4 freq is 415Hz

# A3 (220) A4 (440) A5 (880) C6 (1046.502)

make_sine = function(freq, duration) {
  wave = sin( seq(0, duration /tempo *60, 1 /sample_rate) *freq *2 *pi)
  fade = seq(0, 1, 50 /sample_rate)
  wave * c(fade, rep(1, length(wave) - 2 * length(fade)), rev(fade))
}

bday_wave = mapply(make_sine, bday$freq, bday$duration) %>% do.call("c", .)

play(bday_wave)

There's a few points to note.
The default octave for the notes is octave 4, where A4 is at 440 Hz (the note used to tune the orchestra).
Octaves change over at C, so C3 is one semitone higher than B2.
The reason for the fade in make_sine is that without it there are audible pops when starting and stopping notes.

simple way:
library("audio")
bday = load.wave(bday_file)
play(bday)

<h2>S4 objects, slot</h2>
A slot can be seen as a part, element or a "property" of S4 objects. 
Say you have a car object, then you can have the slots "price", "number of doors", "type of engine", "mileage".

Slots can be accessed in numerous ways :
> aCar@price
> slot(aCar,"typeEngine")

<h2>Read a UTF-8 text file with BOM</h2>
library("data.table")
theName = "file_name.csv"
thetempData = fread(theName , encoding = "UTF-8", stringsAsFactors = F)

<h2>Data Cleanup: Remove NA</h2>
data = data[!is.na(data)]

<h3>Identifying missing values</h3>
We can test for the presence of missing values via the is.na() function.

<code># remove na in r - test for missing values (is.na example)
test &lt;- c(1,2,3,NA)
is.na(test)</code>

In the example above, is.na() will return a vector indicating which elements have a na value.

<h3>na.omit() &#8211; remove rows with na from a list</h3>

This is the easiest option. 
The na.omit() function returns a list without any rows that contain na values. 

try na.omit() or na.exclude()
max( na.omit(vec) )

<code># remove na in r - remove rows - na.omit function / option
ompleterecords &lt;- na.omit(datacollected) </code>

Passing your data frame through the na.omit() function is a simple way to purge incomplete records from your analysis. 
It is an efficient way to remove na values in r.

<h3>complete.cases() &#8211; returns vector of rows with na values</h3>

The na.omit() function relies on the sweeping assumption that the dropped rows (removed the na values) are similar to the typical member of the dataset. 

We accomplish this with the complete.cases() function. 
This r function will examine a dataframe and return a vector of the rows which contain missing values. 
We can examine the dropped records and purge them if we wish.

<code># na in R - complete.cases example
fullrecords &lt;-  collecteddata[!complete.cases(collecteddata)] droprecords &lt;-  collecteddata[complete.cases(collecteddata)] </code>

<h3>Fix in place using&nbsp;na.rm</h3>

For certain statistical functions in R, you can guide the calculation around a missing value through including the na.rm parameter (na.rm=True). 
The rows with na values are retained in the dataframe but excluded from the relevant calculations. 
Support for this parameter varies by package and function, so please check the documentation for your specific package.

This is often the best option if you find there are significant trends in the observations with na values. 
Use the na.rm parameter to guide your code around the missing values and proceed from there. 
We prepared a guide to using na.rm.

<h3>NA Values and regression analysis</h3>

Removal of missing values can distort a regression analysis. 
This is particularly true if you are working with higher order or more complicated models. 
Fortunately, there are several options in the common packages for working around these issues.

If you are using the lm function, it includes a na.action option. 
As part of defining your model, you can indicate how the regression function should handle missing values. 
Two possible choices are na.omit and na.exclude. 
na.omit will omit all rows from the calculations. 
The na.exclude option removes na values from the R calculations but makes an additional adjustment (padding out vectors with missing values) to maintain the integrity of the residual analytics and predictive calculations. 
This is often more effective that procedures that delete rows from the calculations.

You also have the option of attempting to &#8220;heal&#8221; the data using custom procedures. 
In this situation, map is.na against the data set to generate a logical vector that identifies which rows need to be adjusted. 
From there, you can build your own &#8220;healing&#8221; logic.

<h2>create a empty zero-length vector</h2>
numeric()
logical()
character()
integer()
double()
raw()
complex() 
vector('numeric')
vector('character')
vector('integer')
vector('double')
vector('raw')
vector('complex')
All return 0 length vectors of the appropriate atomic modes.


<h2>Export R tables to HTML</h2>
library(tableHTML)
#create an html table 
tableHTML(mtcars)

#and to export in a file
write_tableHTML(tableHTML(mtcars), file = 'myfile.html')

<h2>h2o max_depth</h2>
Available in: GBM, DRF, XGBoost, Isolation Forest

Hyperparameter: yes

Description
This specifies the maximum depth to which each tree will be built. 
A single tree will stop splitting when there are no more splits that satisfy the min_rows parameter, if it reaches max_depth, or if there are no splits that satisfy this min_split_improvement parameter.

In general, deeper trees can seem to provide better accuracy on a training set because deeper trees can overfit your model to your data. 
Also, the deeper the algorithm goes, the more computing time is required. 
This is especially true at depths greater than 10. 
At depth 4, 8 nodes, for example, you need 8 * 100 * 20 trials to complete this splitting for the layer.

One way to determine an appropriate value for max_depth is to run a quick Cartesian grid search. 
Each model in the grid search will use early stopping to tune the number of trees using the validation set AUC, as before. 
The examples below are also available in the GBM Tuning Tutorials folder on GitHub.

The max_depth default value varies depending on the algorithm.

library(h2o)
h2o.init()
# import the titanic dataset
df <- h2o.importFile(path = "http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv")
dim(df)
head(df)
tail(df)
summary(df, exact_quantiles = TRUE)

# pick a response for the supervised problem
response <- "survived"

# the response variable is an integer.
# we will turn it into a categorical/factor for binary classification
df[[response]] <- as.factor(df[[response]])

# use all other columns (except for the name) as predictors
predictors <- setdiff(names(df), c(response, "name"))

# split the data for machine learning
splits <- h2o.splitFrame(data = df,
                         ratios = c(0.6, 0.2),
                         destination_frames = c("train", "valid", "test"),
                         seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]

# Establish a baseline performance using a default GBM model trained on the 60% training split
# We only provide the required parameters, everything else is default
gbm <- h2o.gbm(x = predictors, y = response, training_frame = train)

# Get the AUC on the validation set
h2o.auc(h2o.performance(gbm, newdata = valid))
# The AUC is over 94%, so this model is highly predictive!
[1] 0.9480135

# Determine the best max_depth value to use during a hyper-parameter search.
# Depth 10 is usually plenty of depth for most datasets, but you never know
hyper_params = list( max_depth = seq(1, 29, 2) )
# or hyper_params = list( max_depth = c(4, 6, 8, 12, 16, 20) ), which is faster for larger datasets

grid <- h2o.grid(
  hyper_params = hyper_params,

  # full Cartesian hyper-parameter search
  search_criteria = list(strategy = "Cartesian"),

  # which algorithm to run
  algorithm = "gbm",

  # identifier for the grid, to later retrieve it
  grid_id = "depth_grid",

  # standard model parameters
  x = predictors,
  y = response,
  training_frame = train,
  validation_frame = valid,

  # more trees is better if the learning rate is small enough
  # here, use "more than enough" trees - we have early stopping
  ntrees = 10000,

  # smaller learning rate is better, but because we have learning_rate_annealing,
  # we can afford to start with a bigger learning rate
  learn_rate = 0.05,

  # learning rate annealing: learning_rate shrinks by 1% after every tree
  # (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,

  # sample 80% of rows per tree
  sample_rate = 0.8,

  # sample 80% of columns per split
  col_sample_rate = 0.8,

  # fix a random number generator seed for reproducibility
  seed = 1234,

  # early stopping once the validation AUC doesn't improve by at least
  # 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = "AUC",

  # score every 10 trees to make early stopping reproducible
  # (it depends on the scoring interval)
  score_tree_interval = 10)

# by default, display the grid search results sorted by increasing logloss
# (because this is a classification task)
grid

# sort the grid models by decreasing AUC
sorted_grid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sorted_grid

# find the range of max_depth for the top 5 models
top_depths = sortedGrid@summary_table$max_depth[1:5]
min_depth = min(as.numeric(top_depths))
max_depth = max(as.numeric(top_depths))

> sorted_grid
#H2O Grid Details
Grid ID: depth_grid
Used hyper parameters:
 -  max_depth
Number of models: 15
Number of failed models: 0
Hyper-Parameter Search Summary: ordered by decreasing auc
     max_depth           model_ids                auc
  1         13  depth_grid_model_6 0.9552831783601015
...
  15         1  depth_grid_model_0 0.9478162862778248

It appears that max_depth values of 9 to 27 are best suited for this dataset, which is unusually deep.

<h2>Median</h2>
The middle most value in a data series is called the median.
The median() function is used in R to calculate this value.

data = c( 1, 2, 2, 2,3,3, 4, 7, 9 )
median(data) # Find the median 3

<h2>find mode</h2>
the value that has highest number of occurrences in a dataset
R does not have a standard in-built mode function

data = c( 1, 2, 2, 2,3,3, 4, 7, 9 )
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(data) # 2

<h2>h2o Course Prerequisites</h2>
sample codes
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/merging-data.html
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/max_depth.html.

familiar with pandas
http://pandas.pydata.org/pandas-docs/stable/10min.html

R and Python+Pandas
https://www.slideshare.net/ajayohri/python-for-r-users

Basic Stats
https://mathwithbaddrawings.com/2016/07/13/why-not-to-trust-statistics/
http://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm

most important to understand the normal distribution and standard deviation:
https://en.wikipedia.org/wiki/Standard_deviation


https://students.brown.edu/seeing-theory
linear regression

advice intermixed with xkcd cartoons on stats:
http://livefreeordichotomize.com/2016/12/15/hill-for-the-data-scientist-an-xkcd-story

Confusion Matrix
http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/


Bias/Variance
http://scott.fortmann-roe.com/docs/BiasVariance.html
https://elitedatascience.com/bias-variance-tradeoff
https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bias.2C_variance_and_mean_squared_error

<h2>droplevels</h2>
removes unused levels of a factor. 
x <- factor(c(3, 4, 8, 1, 5, 4, 4, 5))        # Example factor vector
x <- x[- 1]                                   # Delete first entry

Our example vector consists of five factor levels: 1, 3, 4, 5, and 8. 
However, the vector itself does not include the value 3. 
The factor level 3 might therefore be dropped. 

x_drop <- droplevels(x)                       # Apply droplevels in R
x_drop
# 4 8 1 5 4 4 5
# Levels: 1 4 5 8

<h2>h2o samples</h2>
library(h2o)
h2o.init()

h2oiris <- as.h2o( droplevels(iris[1:100,]))

h2oiris
class(h2oiris)
h2o.levels(h2oiris, 5)

write.csv( mtcars, file = 'mtcars.csv') # create local data
h2omtcars <- h2o.importFile( path = 'mtcars.csv')
h2omtcars

h2obin <- h2o.importFile( path = 'https://stats.idre.ucla.edu/stat/data/binary.csv') # load online data

gbmModel <- h2o.gbm( x = c('Month', 'DayOfWeek', 'Distance'), y = 'IsDepDelayed', training_frame = airlinesTrainData) # train model use GBM

h2o.varimp(gbmModel) # find variable importance

xgBoostModel <- h2o.xgboost( x = c('Month', 'DayOfWeek', 'Distance'), y = 'IsDepDelayed', training_frame = airlinesTrainData) # xgb model

h2o.predict( gbmModel, airlinesTrainData) # predict

# https://stats.idre.ucla.edu/other/dae/
# http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html
# http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#defining-a-gbm-model
# https://dzone.com/articles/how-do-you-measure-if-your-customer-churn-predicti

<h2>Gradient Boosting Machine</h2>
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. 

H2O's Gradient Boosting Machine (GBM) offers a Stochastic GBM, which can increase performance quite a bit compared to the original GBM implementation.

Now we will train a basic GBM model

The GBM model will infer the response distribution from the response encoding if not specified explicitly through the "distribution" argument.
A seed is required for reproducibility.

gbm_fit1 <- h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit1", seed = 1)

Next we will increase the number of trees used in the GBM by setting "ntrees=500".

The default number of trees in an H2O GBM is 50, so this GBM will trained using ten times the default.

Increasing the number of trees in a GBM is one way to increase performance of the model, however, you have to be careful not to overfit your model to the training data by using too many trees.

To automatically find the optimal number of trees, you must use H2O's early stopping functionality.

This example will not do that, however, the following example will.

gbm_fit2 <- h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit2",
    #validation_frame = valid, only used if stopping_rounds > 0
  ntrees = 500, seed = 1)

We will again set "ntrees = 500", however, this time we will use early stopping in order to prevent overfitting (from too many trees).

All of H2O's algorithms have early stopping available, however early stopping is not enabled by default (with the exception of Deep Learning).

There are several parameters that should be used to control early stopping.

The three that are common to all the algorithms are: "stopping_rounds", "stopping_metric" and "stopping_tolerance".

The stopping metric is the metric by which you'd like to measure performance, and so we will choose AUC here.

The "score_tree_interval" is a parameter specific to the Random Forest model and the GBM.

Setting "score_tree_interval = 5" will score the model after every five trees.

The parameters we have set below specify that the model will stop training after there have been three scoring intervals where the AUC has not increased more than 0.0005.

Since we have specified a validation frame, the stopping tolerance will be computed on validation AUC rather than training AUC. 

gbm_fit3 <- h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit3",
  validation_frame = valid,  #only used if stopping_rounds > 0
  ntrees = 500, score_tree_interval = 5,      #used for early stopping
                stopping_rounds = 3,          #used for early stopping
                stopping_metric = "AUC",      #used for early stopping
                stopping_tolerance = 0.0005,  #used for early stopping
                seed = 1)

Let's compare the performance of the two GBMs

gbm_perf1 <- h2o.performance(model = gbm_fit1, newdata = test)
gbm_perf2 <- h2o.performance(model = gbm_fit2, newdata = test)
gbm_perf3 <- h2o.performance(model = gbm_fit3, newdata = test)

# Print model performance
gbm_perf1
gbm_perf2
gbm_perf3

# Retreive test set AUC
h2o.auc(gbm_perf1)  # 0.682765594191
h2o.auc(gbm_perf2)  # 0.671854616713
h2o.auc(gbm_perf3)  # 0.68309902855


To examine the scoring history, use the "scoring_history" method on a trained model.
If "score_tree_interval" is not specified, it will score at various intervals, as we can see for "h2o.scoreHistory()" below.
However, regular 5-tree intervals are used for "h2o.scoreHistory()".
The "gbm_fit2" was trained only using a training set (no validation set), so the scoring history is calculated for training set performance metrics only.

h2o.scoreHistory(gbm_fit2)


When early stopping is used, we see that training stopped at 105 trees instead of the full 500.
Since we used a validation set in "gbm_fit3", both training and validation performance metrics are stored in the scoring history object.
Take a look at the validation AUC to observe that the correct stopping tolerance was enforced.

h2o.scoreHistory(gbm_fit3)

Look at scoring history for third GBM model

plot(gbm_fit3, timestep = "number_of_trees", metric = "AUC")
plot(gbm_fit3, timestep = "number_of_trees", metric = "logloss")

4. Deep Learning
H2O's Deep Learning algorithm is a multilayer feed-forward artificial neural network.
It can also be used to train an autoencoder.
In this example we will train a standard supervised prediction model.

Train a default DL
First we will train a basic DL model with default parameters.
The DL model will infer the response distribution from the response encoding if it is not specified explicitly through the "distribution" argument.
H2O's DL will not be reproducible if it is run on more than a single core, so in this example, the performance metrics below may vary slightly from what you see on your machine.
In H2O's DL, early stopping is enabled by default, so below, it will use the training set and default stopping parameters to perform early stopping.

dl_fit1 <- h2o.deeplearning(x = x,
     y = y,
     training_frame = train,
     model_id = "dl_fit1",
     seed = 1)

Train a DL with new architecture and more epochs.

Next we will increase the number of epochs used in the GBM by setting "epochs=20" (the default is 10).
Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model to your training data.
To automatically find the optimal number of epochs, you must use H2O's early stopping functionality.
Unlike the rest of the H2O algorithms, H2O's DL will use early stopping by default, so for comparison we will first turn off early stopping.
We do this in the next example by setting "stopping_rounds=0".

dl_fit2 <- h2o.deeplearning(
  x = x, y = y, training_frame = train, model_id = "dl_fit2",
     #validation_frame = valid, only used if stopping_rounds > 0
     epochs = 20, hidden= c(10,10),
     stopping_rounds = 0,  # disable early stopping
     seed = 1)
Train a DL with early stopping This example will use the same model parameters as "dl_fit2".
This time, we will turn on  early stopping and specify the stopping criterion.
We will also pass a validation set, as is recommended for early stopping.

dl_fit3 <- h2o.deeplearning(
  x = x, y = y, training_frame = train, model_id = "dl_fit3",
     validation_frame = valid,  #in DL, early stopping is on by default
     epochs = 20, hidden = c(10,10),
     score_interval = 1,           #used for early stopping
     stopping_rounds = 3,          #used for early stopping
     stopping_metric = "AUC",      #used for early stopping
     stopping_tolerance = 0.0005,  #used for early stopping
     seed = 1)


Let's compare the performance of the three DL models
dl_perf1 <- h2o.performance(model = dl_fit1, newdata = test)
dl_perf2 <- h2o.performance(model = dl_fit2, newdata = test)
dl_perf3 <- h2o.performance(model = dl_fit3, newdata = test)

Print model performance
dl_perf1
dl_perf2
dl_perf3

# Retreive test set AUC
h2o.auc(dl_perf1)  # 0.6774335
h2o.auc(dl_perf2)  # 0.678446
h2o.auc(dl_perf3)  # 0.6770498

# Scoring history
h2o.scoreHistory(dl_fit3)
# Scoring History: 
  timestamp   duration  training_speed   epochs
1 2016-05-03 10:33:29  0.000 sec                  0.00000
2 2016-05-03 10:33:29  0.347 sec 424697 rows/sec  0.86851
3 2016-05-03 10:33:30  1.356 sec 601925 rows/sec  6.09185
4 2016-05-03 10:33:31  2.348 sec 717617 rows/sec 13.05168
5 2016-05-03 10:33:32  3.281 sec 777538 rows/sec 20.00783
6 2016-05-03 10:33:32  3.345 sec 777275 rows/sec 20.00783

# iterations        samples training_MSE training_r2
1          0       0.000000  
2          1   99804.000000      0.14402     0.03691
3          7  700039.000000      0.14157     0.05333
4         15 1499821.000000      0.14033     0.06159
5         23 2299180.000000      0.14079     0.05853
6         23 2299180.000000      0.14157     0.05333
# training_logloss training_AUC training_lift
1                     
2          0.45930      0.66685       2.20727
3          0.45220      0.68133       2.59354
4          0.44710      0.67993       2.70390
5          0.45100      0.68192       2.81426
6          0.45220      0.68133       2.59354
# training_classification_error validation_MSE validation_r2
1             
2                       0.36145        0.14682       0.03426
3                       0.33647        0.14500       0.04619
4                       0.37126        0.14411       0.05204
5                       0.32868        0.14474       0.04793
6                       0.33647        0.14500       0.04619
# validation_logloss validation_AUC validation_lift
1    
2            0.46692        0.66582         2.53209
3            0.46256        0.67354         2.64124
4            0.45789        0.66986         2.44478
5            0.46292        0.67117         2.70672
6            0.46256        0.67354         2.64124
# validation_classification_error
1         
2  0.37197
3  0.34716
4  0.34385
5  0.36544
6  0.34716

# Look at scoring history for third DL model
plot(dl_fit3, timestep = "epochs", metric = "AUC")


5. Naive Bayes model
The Naive Bayes (NB) algorithm does not usually beat an algorithm like a Random Forest or GBM, however it is still a popular algorithm, especially in the text domain (when your input is text encoded as "Bag of Words", for example).
The Naive Bayes algorithm is for binary or multiclass classification problems only, not regression.
Therefore, your response must be a factor instead of a numeric.

First we will train a basic NB model with default parameters. 

nb_fit1 <- h2o.naiveBayes(
  x = x, y = y, training_frame = train, model_id = "nb_fit1")

Train a NB model with Laplace Smoothing
One of the few tunable model parameters for the Naive Bayes algorithm is the amount of Laplace smoothing. 
The H2O Naive Bayes model will not use any Laplace smoothing by default.

nb_fit2 <- h2o.naiveBayes(
  x = x, y = y, training_frame = train, model_id = "nb_fit2", laplace = 6)

Let's compare the performance of the two NB models
nb_perf1 <- h2o.performance(model = nb_fit1, newdata = test)
nb_perf2 <- h2o.performance(model = nb_fit2, newdata = test)

# Print model performance
nb_perf1
nb_perf2

# Retreive test set AUC
h2o.auc(nb_perf1)  # 0.6488014
h2o.auc(nb_perf2)  # 0.6490678

<h2>Confusion Matrix</h2>
<a href="http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" class="whitebut ">the basic yes/no confusion matrix</a>

<a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="whitebut ">Bias/Variance</a>

<a href="https://elitedatascience.com/bias-variance-tradeoff" class="whitebut ">the Bias-Variance Tradeoff</a>

<a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bias.2C_variance_and_mean_squared_error" class="whitebut ">shows biased estimator can be better than perfectly unbiased estimator</a>

<a href="https://www.coursera.org/learn/machine-learning-h2o/home/welcome" class="whitebut ">Practical Machine Learning on H2O</a>

<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/merging-data.html" class="whitebut ">Merging Two Datasets</a>

<a href="https://www.h2o.ai/download/" class="whitebut ">Downloads</a>

<h2>Print Strings without Quotes</h2>
   print(resultTable, quote = FALSE, row.names = FALSE, col.names = FALSE)
   write.table(resultTable,quote = FALSE,  row.names = FALSE, sep = "\t")

<h2>The apply family</h2>
<a href="https://nicercode.github.io/guides/repeating-things/" class="whitebut ">repeating-things</a>

There are several related function in R which allow you to apply some function to a series of objects (eg. vectors, matrices, dataframes or files). They include:
lapply
sapply
tapply
aggregate
mapply
apply

Each repeats a function or operation on a series of elements, but they differ in the data types they accept and return. What they all in common is that <strong>order of iteration is not important</strong>.  This is crucial. If each each iteration is independent, then you can cycle through them in whatever order you like. Generally, we argue that you should only use the generic looping functions <code>for</code>, <code>while</code>, and <code>repeat</code> when the order or operations <strong>is</strong> important. Otherwise reach for one of the apply tools.

<h2>lapply and sapply</h2>

<code>lapply</code> applies a function to each element of a list (or vector), collecting results in a list.  <code>sapply</code> does the same, but will try to <em>simplify</em> the output if possible.

Lists are a very powerful and flexible data structure that few people seem to know about. Moreover, they are the building block for other data structures, like <code>data.frame</code> and <code>matrix</code>. To access elements of a list, you use the double square bracket, for example <code>X[[4]]</code> returns the fourth element of the list <code>X</code>. If you don’t know what a list is, we suggest you <a href="http://cran.r-project.org/doc/manuals/R-intro.html#Lists-and-data-frames">read more about them</a>, before you proceed.
<h3>Basic syntax</h3>
<code>result &lt;- lapply(a list or vector, a function, ...)</code>

This code will also return a list, stored in <code>result</code>, with same number of elements as <code>X</code>.

<h3>Usage</h3>
lapply is great for building analysis pipelines, where you want to repeat a series of steps on a large number of similar objects.  The way to do this is to have a series of lapply statements, with the output of one providing the input to another:
<code>first.step &lt;- lapply(X, first.function) second.step &lt;- lapply(first.step, next.function)</code>

The challenge is to identify the parts of your analysis that stay the same and those that differ for each call of the function. The trick to using <code>lapply</code> is to recognise that only one item can differ between different function calls.

It is possible to pass in a bunch of additional arguments to your function, but these must be the same for each call of your function. For example, let’s say we have a function <code>test</code> which takes the path of a file, loads the data, and tests it against some hypothesised value H0. We can run the function on the file
“myfile.csv” as follows.

<code>result &lt;- test(&quot;myfile.csv&quot;, H0=1)</code>

We could then run the test on a bunch of files using lapply:

<code>files &lt;- c(&quot;myfile1.csv&quot;, &quot;myfile2.csv&quot;, &quot;myfile3.csv&quot;) result &lt;- lapply(files, test, H0=1)</code>

But notice, that in this example, the <strong>only this that differs</strong> between the runs is a single number in the file name. So we could save ourselves typing these by adding an extra step to generate the file names

<code>files &lt;- lapply(1:10, function(x){paste0(&quot;myfile&quot;, x, &quot;.csv&quot;)}) result &lt;- lapply(files, test, H0=1)</code>

The nice things about that piece of code is that it would extend as long as we wanted, to 10000000 files, if needed.

<h3>Example - plotting temperature for many sites using open weather data</h3>

Let’s look at the weather in some eastern Australian cities over the last couple of days.  The website
<a href="http://openweathermap.org">openweathermap.com</a> provides access to all sorts of neat data, lots of it essentially real time.  We’ve parcelled up some on the nicercode website to use.  In theory, this sort of analysis script could use the weather data directly, but we don’t want to hammer their website too badly.  The code used to generate these files is <a href="https://gist.github.com/richfitz/5795029">here</a>.

We want to look at the temperatures over the last few days for the cities

<code>cities &lt;- c(&quot;Melbourne&quot;, &quot;Sydney&quot;, &quot;Brisbane&quot;, &quot;Cairns&quot;)</code>

The data are stored in a url scheme where the Sydney data is at
<a href="http://nicercode.github.io/guides/repeating-things/data/Sydney.csv">http://nicercode.github.io/guides/repeating-things/data/Sydney.csv</a> and so on.  

The URLs that we need are therefore:

<code>urls &lt;-
 sprintf(&quot;http://nicercode.github.io/guides/repeating-things/data/%s.csv&quot;,
         cities) urls</code>

<code>[1] "http://nicercode.github.io/guides/repeating-things/data/Melbourne.csv"
[2] "http://nicercode.github.io/guides/repeating-things/data/Sydney.csv"   
[3] "http://nicercode.github.io/guides/repeating-things/data/Brisbane.csv" 
[4] "http://nicercode.github.io/guides/repeating-things/data/Cairns.csv"   </code>

We can write a function to download a file if it does not exist:

<code>download.maybe &lt;- function(url, refetch=FALSE, path=&quot;.&quot;) {
 dest &lt;- file.path(path, basename(url))
 if (refetch || !file.exists(dest))
   download.file(url, dest)
 dest
}</code>
and then run that over the urls:

<code>path &lt;- &quot;data&quot; dir.create(path, showWarnings=FALSE) files &lt;- sapply(urls, download.maybe, path=path) names(files) &lt;- cities</code>

Notice that we never specify the order of which file is downloaded in which order; we just say “apply this function (<code>download.maybe</code>) to this list of urls.  We also pass the <code>path</code> argument to every function call.  So it was as if we’d written

<code>download.maybe(urls[[1]], path=path) download.maybe(urls[[2]], path=path) download.maybe(urls[[3]], path=path) download.maybe(urls[[4]], path=path)</code>
but much less boring, and scalable to more files.

The first column, <code>time</code> of each file is a string representing date and time, which needs processing into R’s native time format (dealing with times in R (or frankly, in any language) is a complete pain).  In a real case, there might be many steps involved in processing each file.  We can make a function like this:

<code>load.file &lt;- function(filename) {
 d &lt;- read.csv(filename, stringsAsFactors=FALSE)
 d$time &lt;- as.POSIXlt(d$time)
 d
}</code>
that reads in a file given a filename, and then apply that function to each filename using <code>lapply</code>:

<code>data &lt;- lapply(files, load.file) names(data) &lt;- cities</code>

We now have a <strong>list</strong>, where each element is a <code>data.frame</code> of weather data:

<code>head(data$Sydney)</code>

<code>             time  temp temp.min temp.max
1 2013-06-13 23:00:00 12.66     8.89    16.11
2 2013-06-14 00:00:00 15.90    12.22    20.00
3 2013-06-14 02:00:00 18.44    16.11    20.00
4 2013-06-14 03:00:00 18.68    16.67    20.56
5 2013-06-14 04:00:00 19.41    17.78    22.22
6 2013-06-14 05:00:00 19.10    17.78    22.22</code>

We can use <code>lapply</code> or <code>sapply</code> to easy ask the same question to each element of this list.  For example, how many rows of data are there?

<code>sapply(data, nrow)</code>

<code>Melbourne    Sydney  Brisbane    Cairns 
      97        99        99        80 </code>

What is the hottest temperature recorded by city?

<code>sapply(data, function(x) max(x$temp))</code>

<code>Melbourne    Sydney  Brisbane    Cairns 
   12.85     19.41     22.00     31.67 </code>
or, estimate the autocorrelation function for each set:

<code>autocor &lt;- lapply(data, function(x) acf(x$temp, lag.max=24))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-221.png" alt="plot of chunk unnamed-chunk-22" /> <img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-222.png" alt="plot of chunk unnamed-chunk-22" /> <img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-223.png" alt="plot of chunk unnamed-chunk-22" /> <img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-224.png" alt="plot of chunk unnamed-chunk-22" />

<code>plot(autocor$Sydney, main=&quot;Sydney&quot;)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-225.png" alt="plot of chunk unnamed-chunk-22" />

<code>plot(autocor$Cairns, main=&quot;Cairns&quot;)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-226.png" alt="plot of chunk unnamed-chunk-22" />

I find that for loops can be easier to plot data, partly because there is nothing to <em>collect</em> (or combine) at each iteration.

<code>xlim &lt;- range(sapply(data, function(x) range(x$time))) ylim &lt;- range(sapply(data, function(x) range(x[-1]))) plot(data[[1]]$time, data[[1]]$temp, ylim=ylim, type=&quot;n&quot;,
    xlab=&quot;Time&quot;, ylab=&quot;Temperature&quot;) cols &lt;- 1:4 for (i in seq_along(data))
 lines(data[[i]]$time, data[[i]]$temp, col=cols[i])</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-23.png" alt="plot of chunk unnamed-chunk-23" />

<code>plot(data[[1]]$time, data[[1]]$temp, ylim=ylim, type=&quot;n&quot;,
    xlab=&quot;Time&quot;, ylab=&quot;Temperature&quot;) mapply(function(x, col) lines(x$time, x$temp, col=col),
      data, cols)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-24.png" alt="plot of chunk unnamed-chunk-24" />

<code>$Melbourne
NULL

$Sydney
NULL

$Brisbane
NULL

$Cairns
NULL</code>

<h3>Parallelising your code</h3>

Another great feature of lapply is that is <strong>makes it really easy to parallelise your code</strong>. All computers now contain multiple CPUs, and these can all be put to work using the great <a href="http://www.rforge.net/multicore/">multicore package</a>.

<code>result &lt;- lapply(x, f)   #apply f to x using a single core and lapply
library(multicore) result &lt;- mclapply(x, f) #same thing using all the cores in your machine</code>

<h2>tapply and aggregate</h2>

In the case above, we had naturally “split” data; we had a vector of city names that led to a list of different data.frames of weather data.  Sometimes the “split” operation depends on a factor.  For example, you might have an experiment where you measured the size of plants at different levels of added fertiliser - you then want to know the mean height as a function of this treatment.

However, we’re actiually going to use some data on <a href="https://github.com/audy/smalldata">ratings of seinfeld episodes</a>, taken from the [Internet movie Database]
(http://www.reddit.com/r/dataisbeautiful/comments/1g7jw2/seinfeld_imdb_episode_ratings_oc/).

<code>library(downloader) if (!file.exists(&quot;seinfeld.csv&quot;))
 download(&quot;https://raw.github.com/audy/smalldata/master/seinfeld.csv&quot;,
          &quot;seinfeld.csv&quot;) dat &lt;- read.csv(&quot;seinfeld.csv&quot;, stringsAsFactors=FALSE)</code>

Columns are Season (number), Episode (number), Title (of the episode), Rating (according to IMDb) and Votes (to construct the rating).

<code>head(dat)</code>

<code>  Season Episode             Title Rating Votes
1      1       2      The Stakeout    7.8   649
2      1       3       The Robbery    7.7   565
3      1       4    Male Unbonding    7.6   561
4      1       5     The Stock Tip    7.8   541
5      2       1 The Ex-Girlfriend    7.7   529
6      2       1        The Statue    8.1   509</code>

Make sure it’s sorted sensibly

<code>dat &lt;- dat[order(dat$Season, dat$Episode),]</code>

Biologically, this could be Site / Individual / ID / Mean size /
Things measured.

Hypothesis: Seinfeld used to be funny, but got progressively less good as it became too mainstream.  Or, does the mean episode rating per season decrease?

Now, we want to calculate the average rating per season:

<code>mean(dat$Rating[dat$Season == 1])</code>

<code>[1] 7.725</code>

<code>mean(dat$Rating[dat$Season == 2])</code>

<code>[1] 8.158</code>
and so on until:

<code>mean(dat$Rating[dat$Season == 9])</code>

<code>[1] 8.323</code>

As with most things, we <em>could</em> automate this with a for loop:

<code>seasons &lt;- sort(unique(dat$Season)) rating  &lt;- numeric(length(seasons)) for (i in seq_along(seasons))
 rating[i] &lt;- mean(dat$Rating[dat$Season == seasons[i]])</code>

That’s actually not that horrible to do.  But we it could be nicer.  We first <strong>split</strong> the ratings by season:

<code>ratings.split &lt;- split(dat$Rating, dat$Season) head(ratings.split)</code>

<code>$`1`
[1] 7.8 7.7 7.6 7.8

$`2`
[1] 7.7 8.1 8.0 7.9 7.8 8.5 8.7 8.5 8.0 8.0 8.4 8.3

$`3`
[1] 8.3 7.5 7.8 8.1 8.3 7.3 8.7 8.5 8.5 8.6 8.1 8.4 8.5 8.7 8.6 7.8 8.3
[18] 8.6 8.7 8.6 8.0 8.5 8.6

$`4`
[1] 8.4 8.3 8.6 8.5 8.7 8.6 8.1 8.2 8.7 8.4 8.3 8.7 8.5 8.6 8.3 8.2 8.4
[18] 8.5 8.4 8.7 8.7 8.4 8.5

$`5`
[1] 8.6 8.4 8.4 8.4 8.3 8.2 8.1 8.5 8.5 8.3 8.0 8.1 8.6 8.3 8.4 8.5 7.9
[18] 8.0 8.5 8.7 8.5

$`6`
[1] 8.1 8.4 8.3 8.4 8.2 8.3 8.5 8.4 8.3 8.2 8.1 8.4 8.6 8.2 7.5 8.4 8.2
[18] 8.5 8.3 8.4 8.1 8.5 8.2</code>

Then use sapply to loop over this list, computing the mean

<code>rating &lt;- sapply(ratings.split, mean)</code>

Then if we wanted to apply a different function (say, compute the per-season standard error) we could just do:

<code>se &lt;- function(x)
 sqrt(var(x) / length(x)) rating.se &lt;- sapply(ratings.split, se)
plot(rating ~ seasons, ylim=c(7, 9), pch=19) arrows(seasons, rating - rating.se, seasons, rating + rating.se,
      code=3, angle=90, length=0.02)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-34.png" alt="plot of chunk unnamed-chunk-34" />

But there’s still repetition there.  Let’s abstract that away a bit.

Suppose we want a:
 1. response variable (like Rating was)
 2. grouping variable (like Season was)
 3. function to apply to each level

This just writes out <em>exactly</em> what we had before

<code>summarise.by.group &lt;- function(response, group, func) {
 response.split &lt;- split(response, group)
 sapply(response.split, func)
}</code>

We can compute the mean rating by season again:

<code>rating.new &lt;- summarise.by.group(dat$Rating, dat$Season, mean)</code>
which is the same as what we got before:

<code>identical(rating.new, rating)</code>

<code>[1] TRUE</code>

Of course, we’re not the first people to try this.  This is <strong>exactly</strong> what the <code>tapply</code> function does (but with a few bells and whistles, especially around missing values, factor levels, additional arguments and multiple grouping factors at once).

<code>tapply(dat$Rating, dat$Season, mean)</code>

<code>1     2     3     4     5     6     7     8     9 
7.725 8.158 8.304 8.465 8.343 8.283 8.441 8.423 8.323 </code>

So using <code>tapply</code>, you can do all the above manipulation in a single line.

There are a couple of limitations of <code>tapply</code>.

The first is that getting the season out of <code>tapply</code> is quite hard.  We could do:

<code>as.numeric(names(rating))</code>

<code>[1] 1 2 3 4 5 6 7 8 9</code>

But that’s quite ugly, not least because it involves the conversion numeric -&gt; string -&gt; numeric.

Better could be to use

<code>sort(unique(dat$Season))</code>

<code>[1] 1 2 3 4 5 6 7 8 9</code>

But that requires knowing what is going on inside of <code>tapply</code> (that unique levels are sorted and data are returned in that order).

I suspect that this approach:

<code>first &lt;- function(x) x[[1]] tapply(dat$Season, dat$Season, first)</code>

<code>1 2 3 4 5 6 7 8 9 
1 2 3 4 5 6 7 8 9 </code>
is probably the most fool-proof, but it’s certainly not pretty.

However, the returned format is extremely flexible.  If you do:

The <code>aggregate</code> function provides a simplfied interface to <code>tapply</code> that avoids this issue.  It has two interfaces: the first is similar to what we used before, but the grouping variable now must be a list or data frame:

<code>aggregate(dat$Rating, dat[&quot;Season&quot;], mean)</code>

<code>  Season     x
1      1 7.725
2      2 8.158
3      3 8.304
4      4 8.465
5      5 8.343
6      6 8.283
7      7 8.441
8      8 8.423
9      9 8.323</code>

(note that <code>dat["Season"]</code> returns a one-column data frame).  The column ‘x’ is our response variable, Rating, grouped by season.  We can get its name included in the column names here by specifying the first argument as a <code>data.frame</code> too:

<code>aggregate(dat[&quot;Rating&quot;], dat[&quot;Season&quot;], mean)</code>

<code>  Season Rating
1      1  7.725
2      2  8.158
3      3  8.304
4      4  8.465
5      5  8.343
6      6  8.283
7      7  8.441
8      8  8.423
9      9  8.323</code>

The other interface is the formula interface, that will be familiar from fitting linear models:

<code>aggregate(Rating ~ Season, dat, mean)</code>

<code>  Season Rating
1      1  7.725
2      2  8.158
3      3  8.304
4      4  8.465
5      5  8.343
6      6  8.283
7      7  8.441
8      8  8.423
9      9  8.323</code>

This interface is really nice; we can get the number of votes here too.

<code>aggregate(cbind(Rating, Votes) ~ Season, dat, mean)</code>

<code>  Season Rating Votes
1      1  7.725 579.0
2      2  8.158 533.0
3      3  8.304 496.7
4      4  8.465 497.0
5      5  8.343 452.5
6      6  8.283 385.7
7      7  8.441 408.0
8      8  8.423 391.4
9      9  8.323 415.0</code>

If you have multiple grouping variables, you can write things like:
&lt;div class=&#8217;bogus-wrapper&#8217;&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;div class=&#8221;highlight&#8221;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#8221;gutter&#8221;&gt;&lt;pre class=&#8221;line-numbers&#8221;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#8217;code&#8217;&gt;&lt;pre&gt;<code>aggregate(response ~ factor1 + factor2, dat, function)</code>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;

to apply a function to each pair of levels of <code>factor1</code> and <code>factor2</code>.

<h2>replicate</h2>

This is great in Monte Carlo simulation situations.  For example.
Suppose that you flip a fair coin n times and count the number of heads:

<code>trial &lt;- function(n)
 sum(runif(n) &lt; 0.5) # could have done a binomial draw...</code>

You can run the trial a bunch of times:

<code>trial(10)</code>

<code>[1] 4</code>

<code>trial(10)</code>

<code>[1] 4</code>

<code>trial(10)</code>

<code>[1] 6</code>
and get a feel for the results.  If you want to replicate the trial
100 times and look at the distribution of results, you could do:

<code>replicate(100, trial(10))</code>

<code>  [1] 4 4 5 6 8 5 5 7 3 5 6 4 4 3 5 3 6 7 2 6 6 4 5 4 4 4 4 5 6 5 4 2 6 5 6
[36] 5 6 8 5 6 4 5 4 5 5 5 4 7 3 5 5 6 4 6 4 6 4 4 4 6 3 5 5 7 6 7 5 3 4 4
[71] 5 6 8 5 6 2 5 7 6 3 5 9 3 7 6 4 5 3 7 3 3 7 6 8 5 4 6 7 4 3</code>
and then you could plot these:

<code>plot(table(replicate(10000, trial(50))))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-49.png" alt="plot of chunk unnamed-chunk-49" />

<h3>for loops</h3>

“<code>for</code>” loops shine where the output of one iteration depends on the result of the previous iteration.

Suppose you wanted to model random walk.  Every time step, with 50% probability move left or right.

Start at position 0

<code>x &lt;- 0</code>

Move left or right with probability p (0.5 = unbiased)

<code>p &lt;- 0.5</code>

Update the position

<code>x &lt;- x + if (runif(1) &lt; p) -1 else 1</code>

Let’s abstract the update into a function:

<code>step &lt;- function(x, p=0.5)
 x + if (runif(1) &lt; p) -1 else 1</code>

Repeat a bunch of times:

<code>x &lt;- step(x) x &lt;- step(x)</code>

To find out where we got to after 20 steps:

<code>for (i in 1:20)
 x &lt;- step(x)</code>

If we want to collect where we’re up to at the same time:

<code>nsteps &lt;- 200 x &lt;- numeric(nsteps + 1) x[1] &lt;- 0 # start at 0 for (i in seq_len(nsteps))
 x[i+1] &lt;- step(x[i]) plot(x, type=&quot;l&quot;)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-56.png" alt="plot of chunk unnamed-chunk-56" />

Pulling <em>that</em> into a function:

<code>random.walk &lt;- function(nsteps, x0=0, p=0.5) {
 x &lt;- numeric(nsteps + 1)
 x[1] &lt;- x0
 for (i in seq_len(nsteps))
   x[i+1] &lt;- step(x[i])
 x
}</code>

We can then do 30 random walks:

<code>walks &lt;- replicate(30, random.walk(100)) matplot(walks, type=&quot;l&quot;, lty=1, col=rainbow(nrow(walks)))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-58.png" alt="plot of chunk unnamed-chunk-58" />

Of course, in this case, if we think in terms of vectors we can actually implement random walk using implicit vectorisation:

<code>random.walk &lt;- function(nsteps, x0=0, p=0.5)
 cumsum(c(x0, ifelse(runif(nsteps) &lt; p, -1, 1)))
walks &lt;- replicate(30, random.walk(100)) matplot(walks, type=&quot;l&quot;, lty=1, col=rainbow(nrow(walks)))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-59.png" alt="plot of chunk unnamed-chunk-59" />

Which reinforces one of the advantages of thinking in terms of functions: you can change the implementation detail without the rest of the program changing.

<h2>increase the max print rows limit</h2>
getOption("max.print")
options(max.print=999999)

<h2>parallel processing: foreach package</h2>
loops are incredibly inefficient at processing data in R.

iters&lt;-10  #number of iterations in the loop
ls&lt;-vector('list',length=iters)  #vector for appending output
strt&lt;-Sys.time()   #start time
for(i in 1:iters){   #loop
    cat(i,'\n')    #counter
    to.ls&lt;-rnorm(1e6)
    to.ls&lt;-summary(to.ls)
    ls[[i]]&lt;-to.ls    #export
}

print(Sys.time()-strt)   #end time
# Time difference of 2.944168 secs

repeated the above code with an increasing number of iterations, 10 to 100 at intervals of 10.

iters&lt;-seq(10,100,by=10)   #iterations to time
times&lt;-numeric(length(iters))  #output time vector for  iteration sets
for(val in 1:length(iters)){   #loop over iteration sets
    cat(val,' of ', length(iters),'\n')
    to.iter&lt;-iters[val]
    ls&lt;-vector('list',length=to.iter)    #vector for appending output
    strt&lt;-Sys.time()    #start time
    for(i in 1:to.iter){    #same for loop as before
        cat(i,'\n')
        to.ls&lt;-rnorm(1e6)
        to.ls&lt;-summary(to.ls)
        ls[[i]]&lt;-to.ls          #export
    }
    times[val]&lt;-Sys.time()-strt    #end time
}

library(ggplot2)   #plot the times
to.plo&lt;-data.frame(iters,times)
ggplot(to.plo,aes(x=iters,y=times)) + 
    geom_point() +
    geom_smooth() + 
    theme_bw() + 
    scale_x_continuous('No. of loop iterations') + 
    scale_y_continuous ('Time in seconds')

<img src="https://beckmw.files.wordpress.com/2014/01/seq_time1.jpg">
Fig: Processing time as a function of number of iterations for a simple loop.

The processing time increases linearly with the number of iterations.  
Again, processing time is not extensive for the above example.  
Suppose we wanted to run the example with ten thousand iterations.  
We can predict how long that would take based on the linear relationship between time and iterations.

mod&lt;-lm(times~iters)    #predict times
predict(mod,newdata=data.frame(iters=1e4))/60
# 45.75964


This is all well and good if we want to wait around for 45 minutes.  
Running the loop in parallel would greatly decrease this time.  
I want to first illustrate the problem of running loops in sequence before I show how this can done using the foreach package.  
If the above code is run with <code>1e4</code> iterations, a quick look at the performance metrics in the task manager (Windows 7 OS) gives you an idea of how hard your computer is working to process the code.  
My machine has eight processors and you can see that only a fraction of them are working while the script is running.

<img src="https://beckmw.files.wordpress.com/2014/01/proc1.jpg">
Fig: Resources used during sequential processing of a <code>for</code> loop.

Running the code using foreach will make full use of the computer&#8217;s processors.
Individual chunks of the loop are sent to each processor so that the entire process can be run in parallel rather than in sequence.  
Here&#8217;s how to run the code with <code>1e4</code> iterations in parallel.
That is, each processor gets a finite set of the total number of iterations, i.e., iterations 1&#8211;100 goes to processor one, iterations 101&#8211;200 go to processor two, etc. The output from each processor is then comiled after the iterations are completed.  

#import packages
library(foreach)
library(doParallel)
iters&lt;-1e4   #number of iterations

#setup parallel backend to use 8 processors
cl&lt;-makeCluster(8)
registerDoParallel(cl)

#start time
strt&lt;-Sys.time()

#loop
ls&lt;-foreach(icount(iters)) %dopar% {
    to.ls&lt;-rnorm(1e6)
    to.ls&lt;-summary(to.ls)
    to.ls
    }

print(Sys.time()-strt)
stopCluster(cl)

#Time difference of 10.00242 mins

Running the loop in parallel decreased the processing time about four-fold.  
Although the loop generally looks the same as the sequential version, several parts of the code have changed.  
First, we are using the <code>foreach</code> function rather than <code>for</code> to define our loop.  
The syntax for specifying the iterator is slightly different with <code>foreach</code> as well, i.e., <code>icount(iters)</code> tells the function to repeat the loop a given number of times based on the value assigned to <code>iters</code>.  
Additionally, the convention <code>%dopar%</code> specifies that the code is to be processed in parallel if a backend has been registered (using <code>%do%</code> will run the loop sequentially).  
The functions <code>makeCluster</code> and <code>registerDoParallel</code> from the <a href="http://cran.r-project.org/web/packages/doParallel/index.html" title="doParallel">doParallel</a> package are used to create the parallel backend.  
Another important issue is the method for recombining the data after the chunks are processed. By default, <code>foreach</code> will append the output to a list which we&#8217;ve saved to an object.  
The default method for recombining output can be changed using the <code>.combine</code> argument.  
Also be aware that packages used in the evaluated expression must be included with the <code>.packages</code> argument.
The processors should be working at full capacity if the the loop is executed properly.  
Note the difference here compared to the first loop that was run in sequence.

<img src="https://beckmw.files.wordpress.com/2014/01/proc2.jpg">
Fig: Resources used during parallel processing of a <code>for</code> loop.

A few other issues are worth noting when using the foreach package.  
These are mainly issues I&#8217;ve encountered and I&#8217;m sure others could contribute to this list.  
The foreach package does not work with all types of loops.  
For example, I chose the above example to use a large number (<code>1e6</code>) of observations with the <code>rnorm</code> function.  
I can&#8217;t say for certain the exact type of data that works best, but I have found that functions     hat take a long time when run individually are generally handled very well.  
Interestingly, decreasing the number of observations and increasing the number of iterations may cause the processors to not run at maximum efficiency (try <code>rnorm(100)</code> with <code>1e5</code> iterations). I also haven&#8217;t had much success running repeated models in parallel.  
The functions work but the processors never seem to reach max efficiency.  
The system statistics should cue you off as to whether or not the functions are working.
I also find it bothersome that monitoring progress seems is an issue with parallel loops.  
A simple call using <code>cat</code> to return the iteration in the console does not work with parallel loops.  
The most practical solution I&#8217;ve found is described <a href="http://vikparuchuri.com/blog/monitoring-progress-inside-foreach-loop/" title="here">here</a>, which involves exporting information to a separate file that tells you how far the loop has progressed.  
Also, be very aware of your RAM when running processes in parallel.  
I&#8217;ve found that it&#8217;s incredibly easy to max out the memory, which not only causes the function to stop working correctly, but also makes your computer run like garbage.  
Finally, I&#8217;m a little concerned that I might be destroying my processors by running them at maximum capacity.  
The fan always runs at full blast leading me to believe that critical meltdown is imminent.  
I&#8217;d be pleased to know if this is an issue or not.
That&#8217;s it for now.  
I have to give credit to <a href="http://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf" title="this tutorial">this tutorial</a> for a lot of the information in this post.  
<h3>Vectorised</h3>
E = sapply(1:10000, function(n) {max.eig(5, 1)})
summary(E)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.7615  1.9150  2.2610  2.3160  2.6470  5.2800

Here eigenvalues are calculated from 10000 function calls, all of which use the same parameters. 
The distribution of the resulting eigenvalues is plotted in the histogram below. 
Generating these data took a couple of seconds on my middle-of-the-range laptop. 
Not a big wait. 
But it was only using one of the four cores on the machine, so in principle it could have gone faster.

We can make things more interesting by varying the dimensions of the matrix.
sapply(1:5, function(n) {max.eig(n, 1)})

Or changing both the dimensions (taking on integral values between 1 and 5) and the standard deviation (running through 1, 2 and 3).
sapply(1:5, function(n) {sapply(1:3, function(m) {max.eig(n, m)})})

The results are presented in an intuitive matrix. Everything up to this point is being done serially.
<h3>Enter foreach</h3>

library(foreach)

At first sight, the foreach library provides a slightly different interface for vectorisation. We’ll start off with simple repetition.
times(10) %do% max.eig(5, 1)

That just executes the function with the same arguments 10 times over. If we want to systematically vary the parameters, then instead of times() we use foreach().
foreach(n = 1:5) %do% max.eig(n, 1)

The results are returned as a list, which is actually more reminiscent of the behaviour of lapply() than sapply(). But we can get something more compact by using the .combine option.
foreach(n = 1:5, .combine = c) %do% max.eig(n, 1)


That’s better. Now, what about varying both the dimensions and standard deviation? We can string together multiple calls to foreach() using the %:% nesting operator.
foreach(n = 1:5) %:% foreach(m = 1:3) %do% max.eig(n, m)

I have omitted the output because it consists of nested lists: it’s long and somewhat ugly. But again we can use the .combine option to make it more compact.
foreach(n = 1:5, .combine = rbind) %:% foreach(m = 1:3) %do% max.eig(n, m)

foreach(n = 1:5, .combine = cbind) %:% foreach(m = 1:3) %do% max.eig(n, m)

You can choose between combining using cbind() or rbind() depending on whether you want the output from the inner loop to form the columns or rows of the output. 
There’s lots more magic to be done with .combine. 
You can find the details in the informative article <a href="http://r.adu.org.za/web/packages/foreach/vignettes/foreach.pdf" rel="nofollow" target="_blank">Using The foreach Package</a> by Steve Weston.

You can also use foreach() to loop over multiple variables simultaneously.
foreach(n = 1:5, m = 1:5) %do% max.eig(n, m)

But this is still all serial…

<h3>Filtering</h3>
One final capability before we move on to parallel execution, is the ability to add in a filter within the foreach() statement.

library(numbers)
foreach(n = 1:100, .combine = c) %:% when (isPrime(n)) %do% n

Here we identify the prime numbers between 1 and 100 by simply looping through the entire sequence of values and selecting only those that satisfy the condition in the when() clause. Of course, there are more efficient ways to do this, but this notation is rather neat.
<h3>Going Parallel</h3>
Making the transition from serial to parallel is as simple as changing %do% to %dopar%.
foreach(n = 1:5) %dopar% max.eig(n, 1)

Warning message:
executing %dopar% sequentially: no parallel backend registered

The warning gives us pause for thought: maybe it was not quite that simple? Yes, indeed, there are additional requirements. You need first to choose a parallel backend. And here, again, there are a few options. We will start with the most accessible, which is the multicore backend.
<h3>Multicore</h3>
Multicore processing is provided by the doMC library. You need to load the library and tell it how many cores you want to use.
library(doMC)
registerDoMC(cores=4)

Let’s make a comparison between serial and parallel execution times.

library(rbenchmark)

benchmark(
+     foreach(n = 1:50) %do% max.eig(n, 1),
+     foreach(n = 1:50) %dopar% max.eig(n, 1)
+ )

The overall execution time is reduced, but not by the factor of 4 that one might expect. 
This is due to the additional burden of having to distribute the job over the multiple cores. 
The tradeoff between communication and computation is one of the major limitations of parallel computing, but if computations are lengthy and there is not too much data to move around then the gains can be excellent.

On a single machine you are limited by the number of cores. But if you have access to a cluster then you can truly take things to another level.
<h3>Cluster</h3>
The foreach() functionality can be applied to a cluster using the doSNOW library. We will start by using doSNOW to create a collection of R instances on a single machine using a SOCK cluster.

library(doSNOW)
cluster = makeCluster(4, type = "SOCK")
registerDoSNOW(cluster)

benchmark(
+     foreach(n = 1:50) %do% max.eig(n, 1),
+     foreach(n = 1:50) %dopar% max.eig(n, 1)
+ )

stopCluster(cluster)

There is an improvement in execution time which is roughly comparable to what we got with the multicore implementation. Note that when you are done, you need to shut down the cluster.

Next we will create an <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface" rel="nofollow" target="_blank">MPI</a> cluster consisting of 20 threads.
cluster = makeCluster(20, type = "MPI")
#
registerDoSNOW(cluster)
#
benchmark(
+     foreach(n = 1:100) %do% max.eig(n, 1),
+     foreach(n = 1:100) %dopar% max.eig(n, 1)
+ )


There is an improvement in performance, with the parallel job running roughly 3 times as quickly.

How about a slightly more complicated example? We will try running some bootstrap calculations. We start out with the serial implementation.
random.data <- matrix(rnorm(1000000), ncol = 1000)

bmed <- function(d, n) median(d[n])

library(boot)
#
sapply(1:100, function(n) {sd(boot(random.data[, n], bmed, R = 10000)$t)})

First we generated a big array of normally distributed random numbers. Then we used sapply to calculate bootstrap estimates for the standard deviation of the median for each columns of the matrix.

The parallel implementation requires a little more work: first we need to make the global data (the random matrix and the bootstrap function) available across the cluster.
clusterExport(cluster, c("random.data", "bmed"))

Then we spread the jobs out over the cluster nodes. We will do this first using clusterApply(), which is part of the snow library and is the cluster analogue of sapply(). It returns a list, so to get a nice compact representation we use unlist().
results = clusterApply(cluster, 1:100, function(n) {
+     library(boot)
+     sd(boot(random.data[, n], bmed, R = 10000)$t)
+ })
head(unlist(results))


The foreach implementation is a little neater.
results = foreach(n = 1:100, .combine = c) %dopar% {
    library(boot); sd(boot(random.data[, n], bmed, R = 10000)$t)
}
head(results)

stopCluster(cluster)

The key in both cases is that the boot library must be loaded on each of the cluster nodes as well so that its functionality is available. Simply loading the library on the root node is not enough!

<h2>repeating timer by r asynchronously</h2>
The future package:

library("future")
plan(multiprocess)

myfun <- function() {
  future(fun2())

  return(1+1)
}
Unless fun2() is function used purely for its side effects, you typically want to retrieve the value of that future expression, which you do as:

f <- future(fun2())
y <- fun3()
v <- value(f)
z <- v + y
An alternative is to use the %<-% operator as in:

v %<-% fun2()
y <- fun3()
z <- v + y
FYI, if you use

plan(cluster, workers = c("n1", "n3", "remote.server.org"))
then the future expression is resolved on one of those machines. Using

plan(future.BatchJobs::batchjobs_slurm)
will cause it to be resolved via a Slurm job scheduler queue.

<a href="https://rstudio.github.io/promises/articles/futures.html" class="whitebut ">Launching tasks with future</a>
<a href="https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html" class="whitebut ">A Future for R</a>
<a href="https://appsilon.com/an-example-of-how-to-use-the-new-r-promises-package/" class="whitebut ">example of new R promises package</a>

<h2>Set a timer in R to execute a program</h2>
executing same code block every 15 seconds:

interval = 15
x = data.frame()

repeat {
  startTime = Sys.time()
  x = rbind.data.frame(x, sum(data)) #replace this line with your code/functions
  sleepTime = startTime + interval - Sys.time()
  if (sleepTime > 0)
    Sys.sleep(sleepTime)
}

Or:
print_test<-function(x){
    if(condition)
    {
        Sys.sleep(x);
        cat("hello world");
        print_test(x);
    }
}
print_test(15)

<h2>What Is a Formula in R?</h2>
Formula allow you to capture two things:

An unevaluated expression
The context or environment in which the expression was created

In R the tilde operator ~ characterizes formulas With this operator, you say: "capture the meaning of this code, without evaluating it" You can think of a formula in R as a "quoting" operator

<code># A formula
d &lt;- y ~ x + b</code>

The variable on the left-hand side of a tilde (~) is called the "dependent variable", while the variables on the right-hand side are called the "independent variables" and are joined by plus signs +.

You can access the elements of a formula with the help of the square brackets: [[and ]].

<code>f &lt;- y ~ x + b 
# Retrieve the elements at index 1 and 2
f[[1]] ## "~"
f[[2]] ## y
f[[3]] ## x + b</code>

<h3>Why Use Formulae in R?</h3>
Formulas are powerful, general-purpose tools that allow you to capture the values of variables without evaluating them so that they can be interpreted by the function

Also, you use these R objects to express a relationship between variables.

For example, in the first line of code in the code chunk below, you say "y is a function of x, a, and b"

<code>y ~ x + a + b
## y ~ x + a + b</code>

More complex formulas like the code chunk below:

<code>Sepal.Width ~ Petal.Width | Species
## Sepal.Width ~ Petal.Width | Species</code>

Where you mean to say "the sepal width is a function of petal width, conditioned on species"

<h3>Using Formulas in R</h3>

<h2>How To Create a Formula in R</h2>
1.With the help of ~ operator 2.Some times you need or want to create a formula from an R object, such as a string. In such cases, you can use the formula or as.formula() function

<code>"y ~ x1 + x2"
## [1] "y ~ x1 + x2"
h &lt;- as.formula("y ~ x1 + x2")
h &lt;- formula("y ~ x1 + x2")</code>

<h2>How To Concatenate Formulae</h2>
To glue or bring multiple formulas together, you have two option:

Create separate variables for each formula and then use list()

<code># Create variables
i &lt;- y ~ x
j &lt;- y ~ x + x1
k &lt;- y ~ x + x1 + x2

# Concatentate
formulae &lt;- list(as.formula(i),as.formula(j),as.formula(k))</code>

Use the lapply() function, where you pass in a vector with all of your formulas as a first argument and as.formula as the function that you want to apply to each element of that vector

<code># Join all with "c()"
l &lt;- c(i, j, k)

# Apply "as.formula" to all elements of "f"
lapply(l, as.formula)
[[1]] ## y ~ x
[[2]] ## y ~ x + x1
[[3]] ## y ~ x + x1 + x2</code>

<h3>Formula Operators</h3>
"+" for joining
"-" for removing terms
":" for interaction
"*" for crossing
"%in%" for nesting
"^" for limit crossing to the specified degree

<code># Use multiple independent variables
y ~ x1 + x2 ## y ~ x1 + x2
# Ignore objects in an analysis
y ~ x1 - x2 ## y ~ x1 - x2</code>

What if you want to actually perform an arithmetic operation? you have a couple of solutions:

1.You can calculate and store all of the variables in advance 2.You use the I() or "as-is" operator: y ~ x + I(x^2)

<h3>How To Inspect Formulas in R</h3>
You saw functions such as attributes(), typeof(), class(), etc

To examine and compare different formulae, you can use the terms() function:

<code>m &lt;- formula("y ~ x1 + x2")
terms(m)
## y ~ x1 + x2
## attr(,"variables")
## list(y, x1, x2)
## attr(,"factors")
##    x1 x2
## y   0  0
## x1  1  0
## x2  0  1
## attr(,"term.labels")
## [1] "x1" "x2"
## attr(,"order")
## [1] 1 1
## attr(,"intercept")
## [1] 1
## attr(,"response")
## [1] 1
## attr(,".Environment")
## &lt;environment: R_GlobalEnv&gt;
class(m)
## [1] "formula"
typeof(m)
## [1] "language"
attributes(m)
## $class
## [1] "formula"
## 
## $.Environment
## &lt;environment: R_GlobalEnv&gt;</code>

If you want to know the names of the variables in the model, you can use all.vars.

<code>print(all.vars(m))
## [1] "y"  "x1" "x2"</code>

To modify formulae without converting them to character you can use the update() function:

<code>update(y ~ x1 + x2, ~. + x3)
## y ~ x1 + x2 + x3
y ~ x1 + x2 + x3
## y ~ x1 + x2 + x3</code>

Double check whether you variable is a formula by passing it to the is.formula() function.

<code># Load "plyr"
library(plyr)

# Check "m"
is.formula(m)
## [1] TRUE</code>

<h3>When To Use Formulas</h3>
1.Modeling Functions
2.Graphical Functions in R

<h3>R Formula Packages</h3>
1.Formula Package
2.formula.tools

<h2>dplyr samples</h2>
<a href="https://github.com/Apress/r-data-science-quick-reference" class="whitebut ">R data science quick reference</a>

library(tidyverse)
iris_df <- as_tibble(iris)
print(iris_df, n = 3)
head(iris_df$Species)

## ============
iris_df %>% select(Sepal.Length, Species) %>% print(n = 3)
iris_df %>% select(-Species) %>% print(n = 3)
iris_df %>% select(-Species, -Sepal.Length) %>% print(n = 3)

<h2>get rid of all non-ASCII characters.</h2>

Texts = c("Let the stormy clouds chase, everyone from the place ☁  ♪ ♬",
    "See you soon brother ☮ ",
    "A boring old-fashioned message" ) 

gsub("[^\x01-\x7F]", "", Texts)
[1] "Let the stormy clouds chase, everyone from the place    "
[2] "See you soon brother  "                                  
[3] "A boring old-fashioned message"

Details: You can specify character classes in regex's with [ ]. 
When the class description starts with ^ it means everything except these characters. 
Here, I have specified everything except characters 1-127, i.e. 
everything except standard ASCII and I have specified that they should be replaced with the empty string.

<h2>Display a popup from a batch file</h2>
The goal is to display a popup, the calling batch file must stop and wait the popup closing.

<h3>Using powershell</h3>
echo calling popup
powershell [Reflection.Assembly]::LoadWithPartialName("""System.Windows.Forms""");[Windows.Forms.MessageBox]::show("""rgagnon.com""", """HowTo""",0)>nul
echo we are back!

<h3>Using MHTA</h3>
echo calling popup
mshta javascript:alert("rgagnon.com\n\nHowTo!");close();
echo we are back!
Regular CMD
echo calling popup
START /WAIT CMD /C "ECHO rgagnon.com && ECHO HowTo && ECHO. && PAUSE"
echo we are back!

<h3>Using JScript</h3>
 @if (@x)==(@y) @end /***** jscript comment ******
     @echo off
     echo calling popup
     cscript //E:JScript //nologo "%~f0" "%~nx0" %*
     echo we are back!
     exit /b 0
 @if (@x)==(@y) @end ******  end comment *********/

var wshShell = WScript.CreateObject("WScript.Shell");
wshShell.Popup("HowTo", -1, "rgagnon.com", 16);

<h2>select after the nth word form a string</h2>
library(stringr)
a<-"starting anything goes from here now"
res <- gsub("^(\\w+\\s){1}","",a)
res
res <- gsub("^(\\w+\\s){3}","",a)
res
res <- gsub("^(\\w+\\s){4}","",a)
res

<h2>Significant network analysis packages</h2>
<a href="https://www.jessesadler.com/post/network-analysis-with-r/" class="redbut goldbs orangets">Introduction to Network Analysis with R</a>

<a href="http://statnet.org/" class="whitebut ">statnet</a>
<a href="https://igraph.org/" class="whitebut ">igraph</a>
<a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="whitebut ">tidygraph</a>
<a href="https://www.data-imaginist.com/2017/ggraph-introduction-layouts/" class="whitebut ">ggraph</a>
<a href="http://www.viznetwork.com/" class="whitebut ">vizNetwork</a>
<a href="https://christophergandrud.github.io/networkD3/" class="whitebut ">networkD3</a>

<h3>Basic Managerial Applications of Network Analysis</h3>
Plans the projects by analyzing the project activities.

Projects are broken down to individual tasks or activities, which are arranged in logical sequence. 

It is also decided that which tasks will be performed simultaneously and which other sequentially.

A network diagram is prepared, which presents visually the relationship between all the activities involved and the cost for different activities. 

Network analysis helps designing, planning, coordi­nating, controlling and in decision-making in order to accom­plish the project economically in the minimum available time with the limited available resources. 

The network analysis fulfills the objectives of reducing total time, cost, idle resources, interruptions and conflicts. 

Managerial applications of network analysis are as follows:
Assembly line scheduling,
Research and development,
Inventory planning and control,
Shifting of manufacturing plant from one site to another,
Launching of new products and advertising campaigns,
Control of traffic flow in cities,
Budget and audit procedures,
Launching space programmes,
Installation of new equipments,
Long-range planning and developing staffing plans, etc.

Network techniques:
A number of network techniques:
PERT- Programme Evaluation and Review Technique
CPM- Critical Path Method
RAMS- Resource Allocation and Multi-project Scheduling
PEP- Programme Evolution Procedure
COPAC- Critical Operating Production Allocation Control
MAP- Manpower Allocation Procedure
RPSM- Resource Planning and Scheduling Method
LCS- Least Cost Scheduling
MOSS- Multi-Operation Scheduling System
PCS- Project Control System
GERT- Graphical Evaluation Review Technique.

<h2>shows exactly two decimal places for the number</h2>
format(round(x, 2), nsmall = 2)

<h2>R Customizing Startup</h2>
<a href="https://github.com/rstudio/rstudio/issues/5454" class="whitebut ">Tab for spaces setting</a>
<a href="https://github.com/rstudio/rstudio/issues/4448" class="whitebut ">Spaces per tab option</a>
<a href="https://www.statmethods.net/interface/customizing.html" class="whitebut ">R Customizing Startup</a>

<h2>R courses</h2>
<a href="https://www.datacamp.com/courses/free-introduction-to-r">free-introduction-to-r</a>
<a href="https://www.datacamp.com/courses/intermediate-r">intermediate-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-the-tidyverse">introduction-to-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/data-manipulation-with-dplyr">data-manipulation-with-dplyr</a>
<a href="https://www.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2">introduction-to-data-visualization-with-ggplot2</a>
<a href="https://www.datacamp.com/courses/introduction-to-importing-data-in-r">introduction-to-importing-data-in-r</a>
<a href="https://www.datacamp.com/courses/cleaning-data-in-r">cleaning-data-in-r</a>
<a href="https://www.datacamp.com/courses/joining-data-with-dplyr">joining-data-with-dplyr</a>
<a href="https://www.datacamp.com/courses/intermediate-data-visualization-with-ggplot2">intermediate-data-visualization-with-ggplot2</a>
<a href="https://www.datacamp.com/courses/exploratory-data-analysis-in-r">exploratory-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/correlation-and-regression-in-r">correlation-and-regression-in-r</a>
<a href="https://www.datacamp.com/courses/supervised-learning-in-r-classification">supervised-learning-in-r-classification</a>
<a href="https://www.datacamp.com/courses/introduction-to-data-in-r">introduction-to-data-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-regression-in-r">introduction-to-regression-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-statistics-in-r">introduction-to-statistics-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-writing-functions-in-r">introduction-to-writing-functions-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-r-for-finance">introduction-to-r-for-finance</a>
<a href="https://www.datacamp.com/courses/case-study-exploratory-data-analysis-in-r">case-study-exploratory-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-importing-data-in-r">intermediate-importing-data-in-r</a>
<a href="https://www.datacamp.com/courses/multiple-and-logistic-regression-in-r">multiple-and-logistic-regression-in-r</a>
<a href="https://www.datacamp.com/courses/building-web-applications-with-shiny-in-r">building-web-applications-with-shiny-in-r</a>
<a href="https://www.datacamp.com/courses/data-visualization-in-r">data-visualization-in-r</a>
<a href="https://www.datacamp.com/courses/supervised-learning-in-r-regression">supervised-learning-in-r-regression</a>
<a href="https://www.datacamp.com/courses/writing-efficient-r-code">writing-efficient-r-code</a>
<a href="https://www.datacamp.com/courses/working-with-dates-and-times-in-r">working-with-dates-and-times-in-r</a>
<a href="https://www.datacamp.com/courses/unsupervised-learning-in-r">unsupervised-learning-in-r</a>
<a href="https://www.datacamp.com/courses/machine-learning-with-caret-in-r">machine-learning-with-caret-in-r</a>
<a href="https://www.datacamp.com/courses/time-series-analysis-in-r">time-series-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/manipulating-time-series-data-with-xts-and-zoo-in-r">manipulating-time-series-data-with-xts-and-zoo-in-r</a>
<a href="https://www.datacamp.com/courses/cluster-analysis-in-r">cluster-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/data-manipulation-with-datatable-in-r">data-manipulation-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/reporting-with-rmarkdown">reporting-with-rmarkdown</a>
<a href="https://www.datacamp.com/courses/working-with-data-in-the-tidyverse">working-with-data-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/forecasting-in-r">forecasting-in-r</a>
<a href="https://www.datacamp.com/courses/string-manipulation-with-stringr-in-r">string-manipulation-with-stringr-in-r</a>
<a href="https://www.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r">fundamentals-of-bayesian-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-r-for-finance">intermediate-r-for-finance</a>
<a href="https://www.datacamp.com/courses/introduction-to-portfolio-analysis-in-r">introduction-to-portfolio-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-probability-in-r">foundations-of-probability-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-inference-in-r">foundations-of-inference-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-text-analysis-in-r">introduction-to-text-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/joining-data-with-datatable-in-r">joining-data-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/credit-risk-modeling-in-r">credit-risk-modeling-in-r</a>
<a href="https://www.datacamp.com/courses/modeling-with-data-in-the-tidyverse">modeling-with-data-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/working-with-web-data-in-r">working-with-web-data-in-r</a>
<a href="https://www.datacamp.com/courses/parallel-programming-in-r">parallel-programming-in-r</a>
<a href="https://www.datacamp.com/courses/visualizing-geospatial-data-in-r">visualizing-geospatial-data-in-r</a>
<a href="https://www.datacamp.com/courses/importing-and-managing-financial-data-in-r">importing-and-managing-financial-data-in-r</a>
<a href="https://www.datacamp.com/courses/linear-algebra-for-data-science-in-r">linear-algebra-for-data-science-in-r</a>
<a href="https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models-in-r">hierarchical-and-mixed-effects-models-in-r</a>
<a href="https://www.datacamp.com/courses/case-study-exploring-baseball-pitching-data-in-r">case-study-exploring-baseball-pitching-data-in-r</a>
<a href="https://www.datacamp.com/courses/case-studies-manipulating-time-series-data-in-r">case-studies-manipulating-time-series-data-in-r</a>
<a href="https://www.datacamp.com/courses/differential-expression-analysis-with-limma-in-r">differential-expression-analysis-with-limma-in-r</a>
<a href="https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r">analyzing-election-and-polling-data-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-tensorflow-in-r">introduction-to-tensorflow-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-statistical-modeling-in-r">intermediate-statistical-modeling-in-r</a>
<a href="https://www.datacamp.com/courses/structural-equation-modeling-with-lavaan-in-r">structural-equation-modeling-with-lavaan-in-r</a>
<a href="https://www.datacamp.com/courses/bond-valuation-and-analysis-in-r">bond-valuation-and-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/garch-models-in-r">garch-models-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-functional-programming-with-purrr">foundations-of-functional-programming-with-purrr</a>
<a href="https://www.datacamp.com/courses/anomaly-detection-in-r">anomaly-detection-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-interactive-data-visualization-with-plotly-in-r">intermediate-interactive-data-visualization-with-plotly-in-r</a>
<a href="https://www.datacamp.com/courses/network-analysis-in-the-tidyverse">network-analysis-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/market-basket-analysis-in-r">market-basket-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/financial-analytics-in-r">financial-analytics-in-r</a>
<a href="https://www.datacamp.com/courses/visualizing-big-data-with-trelliscope-in-r">visualizing-big-data-with-trelliscope-in-r</a>
<a href="https://www.datacamp.com/courses/choice-modeling-for-marketing-in-r">choice-modeling-for-marketing-in-r</a>
<a href="https://www.datacamp.com/courses/handling-missing-data-with-imputations-in-r">handling-missing-data-with-imputations-in-r</a>
<a href="https://www.datacamp.com/courses/forecasting-product-demand-in-r">forecasting-product-demand-in-r</a>
<a href="https://www.datacamp.com/courses/defensive-r-programming">defensive-r-programming</a>
<a href="https://www.datacamp.com/courses/intermediate-functional-programming-with-purrr">intermediate-functional-programming-with-purrr</a>
<a href="https://www.datacamp.com/courses/analyzing-us-census-data-in-r">analyzing-us-census-data-in-r</a>
<a href="https://www.datacamp.com/courses/life-insurance-products-valuation-in-r">life-insurance-products-valuation-in-r</a>
<a href="https://www.datacamp.com/courses/mixture-models-in-r">mixture-models-in-r</a>
<a href="https://www.datacamp.com/courses/data-visualization-with-lattice-in-r">data-visualization-with-lattice-in-r</a>
<a href="https://www.datacamp.com/courses/fraud-detection-in-r">fraud-detection-in-r</a>
<a href="https://www.datacamp.com/courses/designing-and-analyzing-clinical-trials-in-r">designing-and-analyzing-clinical-trials-in-r</a>
<a href="https://www.datacamp.com/courses/chip-seq-with-bioconductor-in-r">chip-seq-with-bioconductor-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-regular-expressions-in-r">intermediate-regular-expressions-in-r</a>
<a href="https://www.datacamp.com/courses/survey-and-measurement-development-in-r">survey-and-measurement-development-in-r</a>
<a href="https://www.datacamp.com/courses/feature-engineering-in-r">feature-engineering-in-r</a>
<a href="https://www.datacamp.com/courses/human-resources-analytics-exploring-employee-data-in-r">human-resources-analytics-exploring-employee-data-in-r</a>
<a href="https://www.datacamp.com/courses/scalable-data-processing-in-r">scalable-data-processing-in-r</a>
<a href="https://www.datacamp.com/courses/practicing-statistics-interview-questions-in-r">practicing-statistics-interview-questions-in-r</a>
<a href="https://www.datacamp.com/courses/practicing-machine-learning-interview-questions-in-r">practicing-machine-learning-interview-questions-in-r</a>
<a href="https://www.datacamp.com/courses/human-resources-analytics-predicting-employee-churn-in-r">human-resources-analytics-predicting-employee-churn-in-r</a>
<a href="https://www.datacamp.com/courses/optimizing-r-code-with-rcpp">optimizing-r-code-with-rcpp</a>
<a href="https://www.datacamp.com/courses/predictive-analytics-using-networked-data-in-r">predictive-analytics-using-networked-data-in-r</a>
<a href="https://www.datacamp.com/courses/case-studies-network-analysis-in-r">case-studies-network-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/building-response-models-in-r">building-response-models-in-r</a>
<a href="https://www.datacamp.com/courses/business-process-analytics-in-r">business-process-analytics-in-r</a>
<a href="https://www.datacamp.com/courses/interactive-data-visualization-with-rbokeh">interactive-data-visualization-with-rbokeh</a>
<a href="https://www.datacamp.com/courses/r-for-sas-users">r-for-sas-users</a>
<a href="https://www.datacamp.com/courses/probability-puzzles-in-r">probability-puzzles-in-r</a>
<a href="https://www.datacamp.com/courses/time-series-with-datatable-in-r">time-series-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/data-privacy-and-anonymization-in-r">data-privacy-and-anonymization-in-r</a>
<a href="https://www.datacamp.com/courses/course-creation-at-datacamp">course-creation-at-datacamp</a>
<a href="https://www.datacamp.com/projects/796">projects/796</a>
<a href="https://www.datacamp.com/projects/78">projects/78</a>
<a href="https://www.datacamp.com/projects/758">projects/758</a>
<a href="https://www.datacamp.com/projects/74">projects/74</a>
<a href="https://www.datacamp.com/projects/738">projects/738</a>
<a href="https://www.datacamp.com/projects/712">projects/712</a>
<a href="https://www.datacamp.com/projects/697">projects/697</a>
<a href="https://www.datacamp.com/projects/691">projects/691</a>
<a href="https://www.datacamp.com/projects/68">projects/68</a>
<a href="https://www.datacamp.com/projects/677">projects/677</a>
<a href="https://www.datacamp.com/projects/673">projects/673</a>
<a href="https://www.datacamp.com/projects/668">projects/668</a>
<a href="https://www.datacamp.com/projects/664">projects/664</a>
<a href="https://www.datacamp.com/projects/643">projects/643</a>
<a href="https://www.datacamp.com/projects/638">projects/638</a>
<a href="https://www.datacamp.com/projects/62">projects/62</a>
<a href="https://www.datacamp.com/projects/614">projects/614</a>
<a href="https://www.datacamp.com/projects/584">projects/584</a>
<a href="https://www.datacamp.com/projects/567">projects/567</a>
<a href="https://www.datacamp.com/projects/561">projects/561</a>
<a href="https://www.datacamp.com/projects/552">projects/552</a>
<a href="https://www.datacamp.com/projects/547">projects/547</a>
<a href="https://www.datacamp.com/projects/515">projects/515</a>
<a href="https://www.datacamp.com/projects/511">projects/511</a>
<a href="https://www.datacamp.com/projects/496">projects/496</a>
<a href="https://www.datacamp.com/projects/49">projects/49</a>
<a href="https://www.datacamp.com/projects/489">projects/489</a>
<a href="https://www.datacamp.com/projects/478">projects/478</a>
<a href="https://www.datacamp.com/projects/464">projects/464</a>
<a href="https://www.datacamp.com/projects/458">projects/458</a>
<a href="https://www.datacamp.com/projects/445">projects/445</a>
<a href="https://www.datacamp.com/projects/438">projects/438</a>
<a href="https://www.datacamp.com/projects/435">projects/435</a>
<a href="https://www.datacamp.com/projects/41">projects/41</a>
<a href="https://www.datacamp.com/projects/309">projects/309</a>
<a href="https://www.datacamp.com/projects/208">projects/208</a>
<a href="https://www.datacamp.com/projects/182">projects/182</a>
<a href="https://www.datacamp.com/projects/177">projects/177</a>
<a href="https://www.datacamp.com/projects/166">projects/166</a>
<a href="https://www.datacamp.com/projects/139">projects/139</a>

<h2>Customizing Startup the R environment</h2>
R will always source the Rprofile.site file first. 
On Windows, the file is in the C:\Program Files\R\R-n.n.n\etc directory. 

You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory.

At startup, R will source the Rprofile.site file. 
It will then look for a .Rprofile file to source in the current working directory. 
If it doesn't find it, it will look for one in the user's home directory. 

There are two special functions you can place in these files. 
.First( ) will be run at the start of the R session and .Last( ) will be run at the end of the session.

# Sample Rprofile.site file

# Things you might want to change
# options(papersize="a4")
# options(editor="notepad")
# options(pager="internal")

# R interactive prompt
# options(prompt="> ")
# options(continue="+ ")

# to prefer Compiled HTML
help options(chmhelp=TRUE)
# to prefer HTML help
# options(htmlhelp=TRUE)

# General options
options(tab.width = 2)
options(width = 130)
options(graphics.record=TRUE)

.First <- function(){
 library(Hmisc)
 library(R2HTML)
 cat("\nWelcome at", date(), "\n")
}

.Last <- function(){
 cat("\nGoodbye at ", date(), "\n")
}

<h2>Managing R</h2>
with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf
Upon startup, R and RStudio look for a few different files you can use to control the behavior of your R session, for example by setting options or environment variables. 
In the context of RStudio Team, these settings are often used to set RStudio Server Pro to search for packages in an RStudio Package Manager repository.

This article is a practical guide to how to set particular options on R startup. 
General information on how to manage R package environments is available at <a href="https://environments.rstudio.com" target="_blank" rel="noopener">environments.rstudio.com</a> , and a deeper treatment of R process startup is available in <a href="https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/" target="_blank" rel="noopener">this article</a>.&nbsp;

Here is a summary table of how to control R options and environment variables on startup. 
More details are below.

<table border="black">
<tbody>
<tr><td>File</td><td>Who Controls</td><td>Level</td><td>Limitations</td></tr>
<tr><td><code>.Rprofile</code></td><td>User or Admin</td><td>User or Project</td><td>None, sourced as R code.</td></tr>
<tr><td><code>.Renviron</code></td><td>User or Admin</td><td>User or Project</td><td>Set environment variables only.</td></tr>
<tr><td><code>Rprofile.site</code></td><td>Admin</td><td>Version of R</td><td>None, sourced as R code.</td></tr>
<tr><td><code>Renviron.site</code></td><td>Admin</td><td>Version of R</td><td>Set environment variables only.</td></tr>
<tr><td><code>rsession.conf</code></td><td>Admin</td><td>Server</td><td>Only RStudio settings, only single repository.</td></tr>
<tr><td><code>repos.conf</code></td><td>Admin</td><td>Server</td><td>Only for setting repositories.</td></tr>
</tbody>
</table>
<h3><code>.Rprofile</code></h3>

<code>.Rprofile</code> files are user-controllable files to set options and environment variables. 
<code>.Rprofile</code> files can be either at the user or project level. 
User-level <code>.Rprofile</code> files live in the base of the user's home directory, and project-level <code>.Rprofile</code> files live in the base of the project directory.&nbsp;

R will source only one <code>.Rprofile</code> file. 
So if you have both a project-specific <code>.Rprofile</code> file and a user <code>.Rprofile</code> file that you want to use, you explicitly source the user-level <code>.Rprofile</code> at the top of your project-level <code>.Rprofile</code> with <code>source("~/.Rprofile")</code>.

<code>.Rprofile</code> files are sourced as regular R code, so setting environment variables must be done inside a <code>Sys.setenv(key = "value")</code> call.&nbsp;

One easy way to edit your <code>.RProfile</code> file is to use the <code>usethis::edit_r_profile()</code> function from within an R session. 
You can specify whether you want to edit the user or project level <code>.Rprofile.</code>

<h3><code>.Renviron</code></h3>

<code>.Renviron</code> is a user-controllable file that can be used to create environment variables. 
This is especially useful to avoid including credentials like API keys inside R scripts. 
This file is written in a key-value format, so environment variables are created in the format:

Key1=value1
Key2=value2
...

And then <code>Sys.getenv("Key1")</code> will return <code>"value1"</code> in an R session.

Like with the <code>.Rprofile</code> file, <code>.Renviron</code> files can be at either the user or project level. 
If there is a project-level <code>.Renviron</code>, the user-level file will not be sourced. 
The <code>usethis</code> package includes a helper function for editing <code>.Renviron</code> files from an R session with <code>usethis::edit_r_environ()</code>.

<h3><code>Rprofile.site</code> and <code>Renviron.site</code></h3>

Both <code>.Rprofile</code> and <code>.Renviron</code> files have equivalents that apply server wide. 
<code>Rprofile.site</code>&nbsp;and<code>Renviron.site</code> (no leading dot) files are managed by admins on RStudio Server and are specific to a particular version of R.&nbsp;The most common settings for these&nbsp;files involve access to package repositories. 
For example, using the <a href="https://environments.rstudio.com/shared.html" target="_blank" rel="noopener">shared-baseline</a> package management strategy is generally done from an <code>Rprofile.site</code>.

Users can override settings in these files&nbsp;with their individual <code>.Rprofile</code>&nbsp;files.

These files are set for each version of R and should be located in <code>R_HOME/etc/</code>. 
You can find<code>R_HOME</code> by running the command&nbsp;<code>R.home(component
  = "home")</code> in a session of that version of R. 
So, for example, if you find that <code>R_HOME</code> is <code>/opt/R/3.6.2/lib/R</code>, the<code>Rprofile.site</code> for R 3.6.2 would go in <code>/opt/R/3.6.2/lib/R/etc/Rprofile.site</code>.

<h3><code>rsession.conf</code> and <code>repos.conf</code></h3>

RStudio Server allows server admins to configure particular server-wide R package repositories via the <code>rsession.conf</code> and <code>repos.conf</code> files. 
Only one repository can be configured in <code>rsession.conf</code>. 
If multiple repositories are needed, <code>repos.conf</code> should be used. 
Details on configuring RStudio Server with these files are in this <a href="https://support.rstudio.com/hc/en-us/articles/360009863114-Configuring-RStudio-Server-to-use-RStudio-Package-Manager" target="_blank" rel="noopener">support article</a>.

<h2>R startup mechanism is as follows</h2>
Unless --no-environ was given on the command line, R searches for site and user files to process for setting environment variables. 
The name of the site file is the one pointed to by the environment variable R_ENVIRON; if this is unset, ‘R_HOME/etc/Renviron.site’ is used (if it exists, which it does not in a ‘factory-fresh’ installation). 
The name of the user file can be specified by the R_ENVIRON_USER environment variable; if this is unset, the files searched for are ‘.Renviron’ in the current or in the user's home directory (in that order). 
See ‘Details’ for how the files are read.

Then R searches for the site-wide startup profile file of R code unless the command line option --no-site-file was given. 
The path of this file is taken from the value of the R_PROFILE environment variable (after tilde expansion). 
If this variable is unset, the default is ‘R_HOME/etc/Rprofile.site’, which is used if it exists (it contains settings from the installer in a ‘factory-fresh’ installation). 
This code is sourced into the base package. 
Users need to be careful not to unintentionally overwrite objects in base, and it is normally advisable to use local if code needs to be executed: see the examples.

Then, unless --no-init-file was given, R searches for a user profile, a file of R code. 
The path of this file can be specified by the R_PROFILE_USER environment variable (and tilde expansion will be performed). 
If this is unset, a file called ‘.Rprofile’ is searched for in the current directory or in the user's home directory (in that order). 
The user profile file is sourced into the workspace.

Note that when the site and user profile files are sourced only the base package is loaded, so objects in other packages need to be referred to by e.g. 
utils::dump.frames or after explicitly loading the package concerned.

R then loads a saved image of the user workspace from ‘.RData’ in the current directory if there is one (unless --no-restore-data or --no-restore was specified on the command line).

Next, if a function .First is found on the search path, it is executed as .First(). 
Finally, function .First.sys() in the base package is run. 
This calls require to attach the default packages specified by options("defaultPackages"). 
If the methods package is included, this will have been attached earlier (by function .OptRequireMethods()) so that namespace initializations such as those from the user workspace will proceed correctly.

A function .First (and .Last) can be defined in appropriate ‘.Rprofile’ or ‘Rprofile.site’ files or have been saved in ‘.RData’. 
If you want a different set of packages than the default ones when you start, insert a call to options in the ‘.Rprofile’ or ‘Rprofile.site’ file. 
For example, options(defaultPackages = character()) will attach no extra packages on startup (only the base package) (or set R_DEFAULT_PACKAGES=NULL as an environment variable before running R). 
Using options(defaultPackages = "") or R_DEFAULT_PACKAGES="" enforces the R system default.

On front-ends which support it, the commands history is read from the file specified by the environment variable R_HISTFILE (default ‘.Rhistory’ in the current directory) unless --no-restore-history or --no-restore was specified.

The command-line option --vanilla implies --no-site-file, --no-init-file, --no-environ and (except for R CMD) --no-restore Under Windows, it also implies --no-Rconsole, which prevents loading the ‘Rconsole’ file.

Details
Note that there are two sorts of files used in startup: environment files which contain lists of environment variables to be set, and profile files which contain R code.

Lines in a site or user environment file should be either comment lines starting with #, or lines of the form name=value. 
The latter sets the environmental variable name to value, overriding an existing value. 
If value contains an expression of the form ${foo-bar}, the value is that of the environmental variable foo if that exists and is set to a non-empty value, otherwise bar. 
(If it is of the form ${foo}, the default is "".) This construction can be nested, so bar can be of the same form (as in ${foo-${bar-blah}}). 
Note that the braces are essential: for example $HOME will not be interpreted.

Leading and trailing white space in value are stripped. 
value is then processed in a similar way to a Unix shell: in particular the outermost level of (single or double) quotes is stripped, and backslashes are removed except inside quotes.

On systems with sub-architectures (mainly Windows), the files ‘Renviron.site’ and ‘Rprofile.site’ are looked for first in architecture-specific directories, e.g. 
‘R_HOME/etc/i386/Renviron.site’. 
And e.g. 
‘.Renviron.i386’ will be used in preference to ‘.Renviron’.

Note
It is not intended that there be interaction with the user during startup code. 
Attempting to do so can crash the R process.

The startup options are for Rgui, Rterm and R but not for Rcmd: attempting to use e.g. 
--vanilla with the latter will give a warning or error.

Unix versions of R have a file ‘R_HOME/etc/Renviron’ which is read very early in the start-up processing. 
It contains environment variables set by R in the configure process, and is not used on R for Windows.

R CMD check and R CMD build do not always read the standard startup files, but they do always read specific Renviron files. 
The location of these can be controlled by the environment variables R_CHECK_ENVIRON and R_BUILD_ENVIRON. 
If these are set their value is used as the path for the Renviron file; otherwise, files ‘~/.R/check.Renviron’ or ‘~/.R/build.Renviron’ or sub-architecture-specific versions are employed.

If you want ‘~/.Renviron’ or ‘~/.Rprofile’ to be ignored by child R processes (such as those run by R CMD check and R CMD build), set the appropriate environment variable R_ENVIRON_USER or R_PROFILE_USER to (if possible, which it is not on Windows) "" or to the name of a non-existent file.

See Also
For the definition of the ‘home’ directory on Windows see the ‘rw-FAQ’ Q2.14. 
It can be found from a running R by Sys.getenv("R_USER").

.Last for final actions at the close of an R session. 
commandArgs for accessing the command line arguments.

There are examples of using startup files to set defaults for graphics devices in the help for windows.options.

An Introduction to R for more command-line options: those affecting memory management are covered in the help file for Memory.

readRenviron to read ‘.Renviron’ files.

For profiling code, see Rprof.

Examples
## Not run: 
## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", "onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First <- function() cat("\n   Welcome to R!\n\n")
.Last <- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, set a CRAN mirror
  old <- getOption("defaultPackages"); r <- getOption("repos")
  r["CRAN"] <- "http://my.local.cran"
  options(defaultPackages = c(old, "MASS"), repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols <- Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), "\n")
# coo\bardoh\exabc"def'

## End(Not run)

<h2>R-Studio size/positioning</h2>
<a href="https://community.rstudio.com/t/r-studio-size-positioning-changing-from-screen-to-screen/28906/3" class="whitebut ">R-Studio size/positioning changing from screen to screen</a>

The following worked (windows):

Note that launching RStudio in this way will only disable GPU rendering for that particular RStudio session (that is, only for RStudio sessions that see that environment variable active). If you'd like to make this change more permanently, you can directly modify RStudio Desktop's options file. The option file is located at:

Windows
%APPDATA%\Roaming\RStudio\desktop.ini

In each case, you can modify the entry called desktop.renderingEngine and set it to software to force software rendering. For example:

[General]
desktop.renderingEngine=software

<h2>Introduction to V8 for R</h2>
V8 is Google’s open source, high performance JavaScript engine. It is written in C++ and implements ECMAScript as specified in ECMA-262, 5th edition. The V8 R package builds on the C++ library to provide a completely standalone JavaScript engine within R:

<code># Create a new context
ct &lt;- v8()

# Evaluate some code
ct$eval("var foo = 123")
ct$eval("var bar = 456")
ct$eval("foo + bar")</code>

<code>[1] "579"</code>

A major advantage over the other foreign language interfaces is that V8 requires no compilers, external executables or other run-time dependencies. The entire engine is contained within a 6MB package (2MB zipped) and works on all major platforms.

<code># Create some JSON
cat(ct$eval("JSON.stringify({x:Math.random()})"))</code>

<code>{"x":0.5580623043314792}</code>

<code># Simple closure
ct$eval("(function(x){return x+1;})(123)")</code>

<code>[1] "124"</code>

However note that V8 by itself is just the naked JavaScript engine. Currently, there is no DOM (i.e. no <em>window</em> object), no network or disk IO, not even an event loop. Which is fine because we already have all of those in R. In this sense V8 resembles other foreign language interfaces such as Rcpp or rJava, but then for JavaScript.

<h3>Loading JavaScript Libraries</h3>
The <code>ct$source</code> method is a convenience function for loading JavaScript libraries from a file or url.

<code>ct$source(system.file("js/underscore.js", package="V8"))
ct$source("https://cdnjs.cloudflare.com/ajax/libs/crossfilter/1.3.11/crossfilter.min.js")</code>

<h3>Data Interchange</h3>
By default all data interchange between R and JavaScript happens via JSON using the bidirectional mapping implemented in the <a href="http://arxiv.org/abs/1403.2805">jsonlite</a> package.

<code>ct$assign("mydata", mtcars)
ct$get("mydata")</code>

<code>                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
...
Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2</code>

Alternatively use <code>JS()</code> to assign the value of a JavaScript expression (without converting to JSON):

<code>ct$assign("foo", JS("function(x){return x*x}"))
ct$assign("bar", JS("foo(9)"))
ct$get("bar")</code>

<code>[1] 81</code>

<h3>Function Calls</h3>
The <code>ct$call</code> method calls a JavaScript function, automatically converting objects (arguments and return value) between R and JavaScript:

<code>ct$call("_.filter", mtcars, JS("function(x){return x.mpg &lt; 15}"))</code>

<code>                     mpg cyl disp  hp drat    wt  qsec vs am gear carb
Duster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4
Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4
Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4</code>

It looks a bit like <code>.Call</code> but then for JavaScript instead of C.

<h3>Interactive JavaScript Console</h3>
A fun way to learn JavaScript or debug a session is by entering the interactive console:

<code># Load some data
data(diamonds, package = "ggplot2")
ct$assign("diamonds", diamonds)
ct$console()</code>

From here you can interactively work in JavaScript without typing <code>ct$eval</code> every time:

<code>var cf = crossfilter(diamonds)
var price = cf.dimension(function(x){return x.price})
var depth = cf.dimension(function(x){return x.depth})
price.filter([2000, 3000])
output = depth.top(10)</code>

To exit the console, either press <code>ESC</code> or type <code>exit</code>. Afterwards you can retrieve the objects back into R:

<code>output &lt;- ct$get("output")
print(output)</code>

<h3>warnings, errors and console.log</h3>
Evaluating invalid JavaScript code results in a SyntaxError:

<code># A common typo
ct$eval('var foo &lt;- 123;')</code>

<code>Error in context_eval(join(src), private$context, serialize): SyntaxError: Unexpected token '&lt;'</code>

JavaScript runtime exceptions are automatically propagated into R errors:

<code># Runtime errors
ct$eval("123 + doesnotexit")</code>

<code>Error in context_eval(join(src), private$context, serialize): ReferenceError: doesnotexit is not defined</code>

Within JavaScript we can also call back to the R console manually using <code>console.log</code>, <code>console.warn</code> and <code>console.error</code>. This allows for explicitly generating output, warnings or errors from within a JavaScript application.

<code>ct$eval('console.log("this is a message")')</code>

<code>this is a message</code>

<code>ct$eval('console.warn("Heads up!")')</code>

<code>Warning: Heads up!</code>

<code>ct$eval('console.error("Oh no! An error!")')</code>

<code>Error in context_eval(join(src), private$context, serialize): Oh no! An error!</code>

A example of using <code>console.error</code> is to verify that external resources were loaded:

<code>ct &lt;- v8()
ct$source("https://cdnjs.cloudflare.com/ajax/libs/crossfilter/1.3.11/crossfilter.min.js")
ct$eval('var cf = crossfilter || console.error("failed to load crossfilter!")')</code>

<h3>The Global Namespace</h3>
Unlike what you might be used to from Node or your browser, the global namespace for a new context if very minimal. By default it contains only a few objects: <code>global</code> (a reference to itself), <code>console</code> (for <code>console.log</code> and friends) and <code>print</code> (an alias of console.log needed by some JavaScript libraries)

<code>ct &lt;- v8(typed_arrays = FALSE);
ct$get(JS("Object.keys(global)"))</code>

<code>[1] "print"   "console" "global" </code>

If typed arrays are enabled it contains some additional functions:

<code>ct &lt;- v8(typed_arrays = TRUE);
ct$get(JS("Object.keys(global)"))</code>

<code>[1] "print"   "console" "global" </code>

A context always has a global scope, even when no name is set. When a context is initiated with <code>global = NULL</code>, it can still be reached by evaluating the <code>this</code> keyword within the global scope:

<code>ct2 &lt;- v8(global = NULL, console = FALSE)
ct2$get(JS("Object.keys(this).length"))</code>

<code>[1] 1</code>

<code>ct2$assign("cars", cars)
ct2$eval("var foo = 123")
ct2$eval("function test(x){x+1}")
ct2$get(JS("Object.keys(this).length"))</code>

<code>[1] 4</code>

<code>ct2$get(JS("Object.keys(this)"))</code>

<code>[1] "print" "cars"  "foo"   "test" </code>

To create your own global you could use something like:

<code>ct2$eval("var __global__ = this")
ct2$eval("(function(){var bar = [1,2,3,4]; __global__.bar = bar; })()")
ct2$get("bar")</code>

<code>[1] 1 2 3 4</code>

<h3>Syntax Validation</h3>
V8 also allows for validating JavaScript syntax, without actually evaluating it.

<code>ct$validate("function foo(x){2*x}")</code>

<code>[1] TRUE</code>

<code>ct$validate("foo = function(x){2*x}")</code>

<code>[1] TRUE</code>

This might be useful for all those R libraries that generate browser graphics via templated JavaScript. Note that JavaScript does not allow for defining anonymous functions in the global scope:

<code>ct$validate("function(x){2*x}")</code>

<code>[1] FALSE</code>

To check if an anonymous function is syntactically valid, prefix it with <code>!</code> or wrap in <code>()</code>. These are OK:

<code>ct$validate("(function(x){2*x})")</code>

<code>[1] TRUE</code>

<code>ct$validate("!function(x){2*x}")</code>

<code>[1] TRUE</code>

<h3>Callback To R</h3>
A recently added feature is to interact with R from within JavaScript using the <code>console.r</code> API`. This is most easily demonstrated via the interactive console.

<code>ctx &lt;- v8()
ctx$console()</code>

From JavaScript we can read/write R objects via <code>console.r.get</code> and <code>console.r.assign</code>. The final argument is an optional list specifying arguments passed to <code>toJSON</code> or <code>fromJSON</code>.

<code>// read the iris object into JS
var iris = console.r.get("iris")
var iris_col = console.r.get("iris", {dataframe : "col"})

//write an object back to the R session
console.r.assign("iris2", iris)
console.r.assign("iris3", iris, {simplifyVector : false})</code>

To call R functions use <code>console.r.call</code>. The first argument should be a string which evaluates to a function. The second argument contains a list of arguments passed to the function, similar to <code>do.call</code> in R. Both named and unnamed lists are supported. The return object is returned to JavaScript via JSON.

<code>//calls rnorm(n=2, mean=10, sd=5)
var out = console.r.call('rnorm', {n: 2,mean:10, sd:5})
var out = console.r.call('rnorm', [2, 20, 5])

//anonymous function
var out = console.r.call('function(x){x^2}', {x:12})</code>

There is also an <code>console.r.eval</code> function, which evaluates some code. It takes only a single argument (the string to evaluate) and does not return anything. Output is printed to the console.

<code>console.r.eval('sessionInfo()')</code>

Besides automatically converting objects, V8 also propagates exceptions between R, C++ and JavaScript up and down the stack. Hence you can catch R errors as JavaScript exceptions when calling an R function from JavaScript or vice versa. If nothing gets caught, exceptions bubble all the way up as R errors in your top-level R session.

<code>//raise an error in R
console.r.call('stop("ouch!")')

//catch error from JavaScript
try {
  console.r.call('stop("ouch!")')
} catch (e) {
  console.log("Uhoh R had an error: " + e)
}
//# Uhoh R had an error: ouch!</code>

<h2>sprintf Function</h2>
<strong>Basic R Syntax of sprintf:</strong>
sprintf("%f", x)

<strong>Definition of sprintf:</strong>

The sprintf function returns character objects containing a formatted combination of input values.

<h3>Example 1: Format Decimal Places with sprintf Function in R</h3>
x &lt;- 123.456               # Create example data

The default number of decimal places is six digits after the decimal point

sprintf("%f", x)           # sprintf with default specification
#  "123.456000"

We can control the number of decimal places by adding a point and a number between the percentage sign and the f. 
For instance, we can print ten digits after the decimal point…

sprintf("%.10f", x)        # sprintf with ten decimal places
# "123.4560000000"

…or we can round our numeric input value to only two digits after the decimal point:

sprintf("%.2f", x)         # sprintf with two rounded decimal places
# "123.46"

<strong>Note:</strong> The output of sprintf is a <a href="http://www.r-tutor.com/r-introduction/basic-data-types/character" rel="noopener noreferrer" target="_blank">character string</a> and not a numeric value as the input was.

<h3>Example 2: Format Places Before Decimal Point</h3>
sprintf also enables the formatting of the number of digits before the decimal separator. 
We can tell sprintf to print all digits before the decimal point, but no digits after the decimal point…

sprintf("%1.0f", x)        # sprintf without decimal places
# "123"

…or we can print a certain amount of leading blanks before our number without decimal places (as illustrated by the quotes below)…

sprintf("%10.0f", x)       # sprintf with space before number
# "       123"

…or with decimal places…

sprintf("%10.1f", x)       # Space before number &amp; decimal places
# "     123.5"

…or we can print blanks at the right side of our output by writing a minus sign in front of the number within the sprintf function:

sprintf("%-15f", x)        # Space on right side
# "123.456000     "

<h3>Example 3: Print Non-Numeric Values with sprintf (e.g. 
+ or %)</h3>
It is also possible to combine numeric with non-numeric inputs. 
The following R code returns a plus sign in front of our example number…

sprintf("%+f", x)          # Print plus sign before number
# "+123.456000"

…and the following R code prints a percentage sign at the end of our number:

paste0(sprintf("%f", x),   # Print %-sign at the end of number
       "%")
# "123.456000%"

<h3>Example 4: Control Scientific Notation</h3>
The sprintf R function is also used to <a href="https://statisticsglobe.com/disable-exponential-scientific-notation-in-r">control exponential notation in R</a>. 
The following syntax returns our number as scientific notation with a lower case e…

sprintf("%e", x)           # Exponential notation
# "1.234560e+02"

…and the following code returns an upper case E to the RStudio console:

sprintf("%E", x)           # Exponential with upper case E
# "1.234560e+02"

<h3>Example 5: Control Amount of Decimal Zeros</h3>
We can also control the amount of decimal zeros that we want to print to the RStudio console. 
The following R code prints our example number without any decimal zeros…

sprintf("%g", x)           # sprintf without decimal zeros
# "123.456"

…the following R code returns our example number * 1e10 in scientific notation…

sprintf("%g", 1e10 * x)    # Scientific notation
# "1.23456e+12"

…and by adding a number before the g within the sprintf function we can control the amount of decimal zeros that we want to print:

sprintf("%.13g", 1e10 * x) # Fixed decimal zeros
# "1234560000000"

<h3>Example 6: Several Input Values for sprintf Function</h3>
So far, we have only used a single numeric value (i.e. our example data object x) as input for sprintf. 
However, the sprintf command allows as many input values as we want.

Furthermore, we can print these input values within more complex character strings. 
Have a look at the following sprintf example:

sprintf("Let's create %1.0f more complex example %1.0f you.", 1, 4)
# "Let's create 1 more complex example 4 you."

The first specification (i.e. %1.0f) within the previous R code corresponds to the input value 1 and the second specification corresponds to the input value 4.

Of cause we could use sprintf in even more complex settings. 
Have a look at the sprintf examples of the R help documentation, if you are interested in more complex examples:

<img src="https://statisticsglobe.com/wp-content/uploads/2019/04/sprintf-r-help-documentation-more-sophisticated-examples.png">

<em><strong>Figure 1: Complex sprintf Examples in R Help Documentation.</strong></em>

Examples
## be careful with the format: most things in R are floats
## only integer-valued reals get coerced to integer.

sprintf("%s is %f feet tall\n", "Sven", 7.1)      # OK
try(sprintf("%s is %i feet tall\n", "Sven", 7.1)) # not OK
try(sprintf("%s is %i feet tall\n", "Sven", 7))   # OK

## use a literal % :
sprintf("%.0f%% said yes (out of a sample of size %.0f)", 66.666, 3)

## no truncation:
sprintf("%1.f",101)
## re-use one argument three times, show difference between %x and %X
xx <- sprintf("%1$d %1$x %1$X", 0:15)
xx <- matrix(xx, dimnames=list(rep("", 16), "%d%x%X"))
noquote(format(xx, justify="right"))

## More sophisticated:

sprintf("min 10-char string '%10s'",
        c("a", "ABC", "and an even longer one"))

n <- 1:18
sprintf(paste("e with %2d digits = %.",n,"g",sep=""), n, exp(1))

## Using arguments out of order
sprintf("second %2$1.0f, first %1$5.2f, third %3$1.0f", pi, 2, 3)

## Using asterisk for width or precision
sprintf("precision %.*f, width '%*.3f'", 3, pi, 8, pi)

## Asterisk and argument re-use, 'e' example reiterated:
sprintf("e with %1$2d digits = %2$.*1$g", n, exp(1))

## re-cycle arguments 
sprintf("%s %d", "test", 1:3)

sprintf(fmt, …)
Create a character string that contains values from R objects. 

fmt – A character string with some occurrences of %s, which will be places that object values are inserted.
… – The R objects to be inserted. 
The number of items should correspond to the number of %s occurrences in fmt.

Example. 
x <- 2349
sprintf("Substitute in a string or number: %s", x)
"Substitute in a string or number: 2349"

sprintf("Can have multiple %s occurrences %s", x, "- got it?")
"Can have multiple 2349 occurrences - got it?"

Creating Custom Themes for RStudio
<a href="https://github.com/gadenbuie/rsthemes" class="whitebut ">rsthemes</a>

Creating an rstheme
Another straightforward method would be to copy an existing rstheme and then modify the values.

Because of the structure of the elements being styled, not all the CSS rule sets may end up being used. 
Below is a table that describes the most relevant selectors, which tmTheme scope they correspond to, if any, and how they impact the style of RStudio.

Selector	Scope	Description
.ace_bracket		Overrides default styling for matching bracket highlighting provided by Ace.
.ace_comment	comment	Changes the color and style of comments.
.ace_constant	constant	Changes the color and style of constants like TRUE, FALSE, and numeric literals.
.ace_constant.ace_language	constant.language	Changes the color and style of language constants like TRUE and FALSE. 
This rule set will override rules in .ace_constant for language constants. 
Also in RMarkdown files, everything surrounded in *.
.ace_constant.ace_numeric	constant.numeric	Changes the color and style of numeric literals. 
This value will override the settings in the “constant” scope, if set. 
Also in RMarkdown files, everything surrounded in **.
.ace_cusor		Changes the color and style of the text cursor in the editor window.
.ace_editor		Changes the default color and background of the RStudio editor windows. 
This selector will usually be the first in a list of other selectors for the same rule set, such as .rstudio-themes-flat.ace_editor_theme and so on.
.ace_gutter		Changes the color and style of the gutter: the panel on the left-hand side of the editor which holds line numbers, breakpoints, and fold widgets.
.ace_gutter-active-line		Changes the color and style of the gutter at the active line in the editor.
.ace_heading		Changes the color and style of headings in RMarkdown documents.
.ace_indent-guide		Changes the color and style of the indent guide, which can be enabled or disabled through Global Options > Code > Display > Show indent guides.
.ace_invisible		Changes the color and style of invisible characters, which can be enabled or disabled through Global Options > Code Display > Show whitespace characters.
.ace_keyword	keyword	Changes the color and style of keywords like function, if, else, stop, and operators.
.ace_keyword.ace_operator	keyword.operator	Changes the color and style of operators like (, ), =, +, and -. 
This value will override the settings in the .ace_keyword block for operators, if set.
.ace_meta.ace_tag	meta.tag	Changes the color and style of metadata tags in RMarkdown documents, like title and output.
.ace_marker-layer .ace_active-debug-line	marker-layer.active_debug_line	Changes the color and style of the highlighting on the line of code which is currently being debugged.
.ace_marker-layer .ace_bracket		Changes the color and style of the highlighting on matching brackets.
.ace_marker-layer .ace_selection		Changes the color and style of the highlighting for the currently selected line or block of lines.
.ace_markup.ace_heading	markup.heading	Changes the color and style of the characters that start a heading in RMarkdown documents.
.ace_print-margin		Changes the color and style, if applicable, of the line-width margin that can be enabled or disabled through Global Options > Code > Display > Show margin.
.ace_selection.ace_start		Changes the color and style of the highlighting for the start of the currently selected block of lines.
.ace_string	string	Changes the color and style of string literals.
.ace_support.ace_function	support.function	Changes the color and style of code blocks in RMarkdown documents.
In addition to these rule sets, you will also find a number of rule sets related to the Terminal pane, with selectors that include .terminal or selectors that begin with .xterm. 
It is possible to change these values as well, but it may be advisable to keep a back up copy of your original theme in case you don’t like any of the changes. 
There are also a number of classes that can be used to modify parts of RStudio unrelated to the editor. 
These classes are all prefixed with rstheme_, with the exception of dataGridHeader and themedPopupPanel. 
Any classes you find in the html of RStudio which are not prefixed with rstheme_, ace_, or explicitly listed in this article are subject to change at anytime, and so are unsafe to use in custom themes.

Since an rstheme is just CSS, anything that you can do with CSS you can do in an rstheme.

Testing Changes to a Theme
If you’re modifying a theme which has already been added to RStudio, you may need to restart RStudio desktop in order to make the changes take effect.

Sharing a Theme
Once you’re satisfied with your theme, you can easily share it with anyone by simply sharing the tmTheme or rstheme file. 
You can find rstheme files in 
C:\Users\<your user account>\Documents\.R\rstudio\themes on Windows

You can also use the theme related functions provided in the RStudio API to save a local copy of your converted theme.

If you upload your rstheme file to a URL-addressable location, you can also share a snippet of code that anyone can run to try your theme:

rstudioapi::addTheme("http://your/theme/path/theme.rstheme", apply = TRUE)
This will download, install, and apply the theme immediately on the user’s machine.

<h2>theme in RStudio</h2>
<a href="https://tmtheme-editor.herokuapp.com/#!/editor/theme/Monokai" class="whitebut ">Monokai theme</a>
Select a theme from this theme editor and save it to your machine as a .tmTheme file. 
Add the theme in RStudio. 
RStudio automatically creates an .rstheme file from that. 
the rstheme file in Users/Eric/Documents/.R/rstudio/themes folder (Windows machine). 
Open the file in text editor and find the .ace entry that matches the one you want to change. 
This will have an rgba value similar to: background-color: rgba(238, 252, 81, 0.8);
Note that the last value is the alpha (transparency) value and should be between 0 and 1. 

<a href="https://stackoverflow.com/questions/40369595/altering-rstudio-editor-theme" class="whitebut ">Altering RStudio Editor Theme</a>
<a href="https://stackoverflow.com/questions/25582588/any-way-to-change-colors-in-rstudio-to-something-other-than-default-options" class="whitebut ">change colors in Rstudio</a>
<a href="https://stackoverflow.com/questions/37635237/editing-r-studio-theme-in-cache-css-theme-file-ace-editor" class="whitebut ">Editing R studio theme in cache.css theme file</a>

There's a much faster way to deal with this and 100% doable.

Open RStudio with your favourite Editor theme and open an .R script

Inspect the Source layout (Right-click>Inspect) and Ctrl + f an unique class selector such as .ace_comment. 
In the matched CSS rules box in the side pane copy an attribute as unique as possible (i.e. 
color: #0088FF; I use Cobalt theme).

Go to RStudio's install path and dive into /www/rstudio/. 
As jorloff rightly said, you'll find a bunch of files like this: VERYUGLYNAME.cache.css. 
Open all of them with your favourite text editor as administrator.

Find in files: Ctrl+ Shift + f (in sublime text) and type the unique attribute value you previously chosed. 
BOOM, there you have it.

Now delight yourself editing your crazy style, but remember to back it up first!

As Jonathan said, RStudio's editor is based on ACE themes, so all clases have the ace_ prefix. 
Take your time inspecting and understanding the editor hierarchy. 
I recommend you to take some time inspecting the html code to understand its structure. 
The editor starts in id="rstudio_source_text_editor"


i am new to R Studio and i would like to share how i was able to customize the color scheme of R Studio:

How to change the color of comments in Rstudio

Rstudio Pane Appearance > Set editor theme to monokai
Right click on editor pane > Inspect > find the specific file name (i.e. 
838C7F60FB885BB7E5EED7F698E453B9.cache.css)
Open drive C > open Progam Files folder > open Rstudio folder
Open www folder > rstudio folder > find the 838C7F60FB885BB7E5EED7F698E453B9.cache.css (name of the theme you want to change)
Make a backup copy of the original
Change .ace_comment {color: #75715E} to .ace_comment {color: #F92672} > save to another location (don't change file name)
Copy the recently saved code and paste it in rstudio folder (step 4) > replace the original 838C7F60FB885BB7E5EED7F698E453B9.cache.css file with the modified 838C7F60FB885BB7E5EED7F698E453B9.cache.csss file
Click continue
Quit Rstudio
Open Rstudio
Check if the color of comment has changed from nightsand(#75715E) to orchid(#F92672)


I am using RStudio 1.0.136. 
According to all the posts, right click on the Editor -> Inspect. 
The Web Inspector comes up and shows the Elements tab. 
Then click the Sources tab, select "Only enable for this session", click "Enable Debugging" button. 
You will see the code for the theme xxxxxxx.cache.css file. 
If nothing in the editor, try the left top "Show Navigator" button right under the "Elements" menu. 
Select the .css file in the list and it should open.

My line number seems dim. 
So changed color: #222; to color: #818222; in this section: (forgive my bad color sense). 
And you can see the color change right away! How amazing!

.ace_gutter {
  background-color: #3d3d3d;
  background-image: -moz-linear-gradient(left, #3D3D3D, #333);
  background-image: -ms-linear-gradient(left, #3D3D3D, #333);
  background-image: -webkit-gradient(linear, 0 0, 0 100%, from(#3D3D3D), to(#333));
  background-image: -webkit-linear-gradient(left, #3D3D3D, #333);
  background-image: -o-linear-gradient(left, #3D3D3D, #333);
  background-image: linear-gradient(left, #3D3D3D, #333);
  background-repeat: repeat-x;
  border-right: 1px solid #4d4d4d;
  text-shadow: 0px 1px 1px #4d4d4d;
  color: #818222;
}
@skan mentioned selected words are too dim. 
I have the same problem. 
So here I found it:

.ace_marker-layer .ace_selected-word {
  border-radius: 4px;
  border: 8px solid #ff475d;
  box-shadow: 0 0 4px black;
}
I changed border: 8px solid #ff475d;. 
It is now very bright, or may be too bright. 
Anyway, it works. 
Thanks for every one. 
And hope this can help.

This is for current session only. 
Now you know which .css to modify and what you should do, it will be easy to modify the original .css file to keep it permanent.

<h2>file or folder is locked after script quited</h2>
switch to other folder before quit
setwd("C:/Users/User/Desktop")

<h2>Write file as UTF-8 encoding</h2>
<a href="https://tomizonor.wordpress.com/2013/04/17/file-utf8-windows/" class="whitebut ">Write file as UTF-8 encoding in R for Windows</a>

While the R uses UTF-8 encoding as default on Linux and Mac OS, the R for Windows does not use UTF-8 as default. So reading and writing UTF-8 files are something troublesome on Windows. In this article, I will show you a small script to help UTF-8 encoding.

options("encoding" = "UTF-8")
t2 <- "®"
getOption("encoding")
Encoding(t2) <- "UTF-8"
sink("test.txt")
cat("123")
cat(t2)
sink()

Sys.getlocale('LC_CTYPE')

writeLines(Sys.setlocale("LC_CTYPE", locale), con)
Sys.setlocale("LC_CTYPE")
<h2>S4 Classes</h2>
<h3>The Basic Idea</h3>
The S4 approach differs from the S3 approach to creating a class in that it is a more rigid definition. 
The idea is that an object is created using the <em>setClass</em> command. 
The command takes a number of options. 
Many of the options are not required, but we make use of several of the optional arguments because they represent good practices with respect to object oriented programming.
We first construct a trivial, contrived class simply to demonstrate the basic idea. 
Next we demonstrate how to create a method for an S4 class. 
This example is a little more involved than what we saw in the section on S3 classes.
In this example, the name of the class is <em>FirstQuadrant</em>, and the class is used to keep track of an <em>(x,y)</em> coordinate pair in the first quadrant. 
There is a restriction that both values must be greater than or equal to zero. 
There are two data elements, called <em>slots</em>, and they are called <em>x</em> and <em>y</em>. 
The default values for the coordinate is the origin, <em>x=0</em> and <em>y=0</em>.
######################################################################
# Create the first quadrant class
#
# This is used to represent a coordinate in the first quadrant.
FirstQuadrant = setClass(
    # Set the name for the class
    "FirstQuadrant",

    # Define the slots
    slots = c( x = "numeric", y = "numeric" ),

    # Set the default values for the slots. (optional)
    prototype=list( x = 0.0, y = 0.0 ),

    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object){
        if((object@x &lt; 0) || (object@y &lt; 0)) {
            return("A negative number for one of the coordinates was given.")
        }
        return(TRUE)
    }
)



Note that the way to access one of the data elements is to use the “@” symbol. 
An example if given below. 
In the example three elements of the class defined above are created. 
The first uses the default values for the slots, the second overrides the defaults, and finally an attempt is made to create a coordinate in the second quadrant.
> x = FirstQuadrant()
> x
An object of class "FirstQuadrant"
Slot "x":
[1] 0
Slot "y":
[1] 0
> y = FirstQuadrant(x=5,y=7)
> y
An object of class "FirstQuadrant"
Slot "x":
[1] 5
Slot "y":
[1] 7
> y@x
[1] 5
> y@y
[1] 7
> z = FirstQuadrant(x=3,y=-2)
Error in validObject(.Object) :
      invalid class “FirstQuadrant” object: A negative number for one of the coordinates was given.
> z
Error: object 'z' not found



In the next example we create a method that is associated with the class. 
The method is used to set the values of a coordinate. 
The first step is to reserve the name using the <em>setGeneric</em> command, and then the <em>setMethod</em> command is used to define the function to be called when the first argument is an object from the <em>FirstQuadrant</em> class.
# create a method to assign the value of a coordinate setGeneric(name="setCoordinate",
    def=function(theObject,xVal,yVal)
    {
        standardGeneric("setCoordinate")
    }
    )
setMethod(f="setCoordinate",
    signature="FirstQuadrant",
    definition=function(theObject,xVal,yVal)
    {
       theObject@x = xVal
       theObject@y = yVal
       return(theObject)
    }
    )



It is important to note that R generally passes objects as values. 
For this reason the methods defined above return the updated object. 
When the method is called, it is used to replace the former object with the updated object.
> z = FirstQuadrant(x=2.5,y=10)
> z
An object of class "FirstQuadrant"
Slot "x":
[1] 2.5
Slot "y":
[1] 10
> z = setCoordinate(z,-3.0,-5.0)
> z
An object of class "FirstQuadrant"
Slot "x":
[1] -3
Slot "y":
[1] -5



Note that the <em>validity</em> function given in the original class definition is not called. 
It is called when an object is first defined. 
It can be called later, but only when an explicit request is made using the <em>validObject</em> command.


<h3>Creating an S4 Class</h3>
An S4 class is created using the <em>setClass()</em> command. 
At a minimum the name of the class is specified and the names of the data elements
(slots) is specified. 
There are a number of other options, and just as a matter of good practice we also specify a function to verify that the data is consistent (validation), and we specify the default values (the prototype). 
In the last section of this page,
S4 inheritance, we include an additional parameter used to specify a class hierarchy.
In this section we look at another example, and we examine some of the functions associated with S4 classes. 
The example we define will be used to motivate the use of methods associated with a class, and it will be used to demonstrate inheritance later. 
The idea is that we want to create a program to simulate a cellular automata model of a predator-prey system.
We do not develop the whole code here but concentrate on the data structures. 
In particular we will create a base class for the agents. 
In the next section we will create the basic methods for the class. 
In the inheritance section we will discuss how to build on the class to create different predators and different prey species. 
The basic structure of the class is shown in Figure 1.

<img src="https://www.cyclismo.org/tutorial/R/_images/s4AgentClass.png" />
Figure 1.

Diagram of the base class, Agent, used for the agents in a simulation.

The methods for this class are defined in the following section. 
Here we define the class and its slots, and the code to define the class is given below:
######################################################################
# Create the base Agent class
#
# This is used to represent the most basic agent in a simulation.
Agent = setClass(
    # Set the name for the class
    "Agent",
    # Define the slots
    slots = c(
        location = "numeric",
        velocity   = "numeric",
        active   = "logical"
        ),
    # Set the default values for the slots. 
(optional)
    prototype=list(
        location = c(0.0,0.0),
        active   = TRUE,
        velocity = c(0.0,0.0)
        ),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>100.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    }
    )



Now that the code to define the class is given we can create an object whose class is Agent.
> a = Agent()
> a
An object of class "Agent"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE



Before we define the methods for the class a number of additional commands are explored. 
The first set of functions explored are the
<em>is.object</em> and the <em>isS4</em> commands. 
The <em>is.object</em> command determines whether or not a variable refers to an object. 
The <em>isS4</em>
command determines whether or not the variable is an S4 object. 
The reason both are required is that the <em>isS4</em> command alone cannot determine if a variable is an S3 object. 
You need to determine if the variable is an object and then decide if it is S4 or not.
> is.object(a)
[1] TRUE
> isS4(a)
[1] TRUE



The next set of commands are used to get information about the data elements, or slots, within an object. 
The first is the <em>slotNames</em>
command. 
This command can take either an object or the name of a class. 
It returns the names of the slots associated with the class as strings.
> slotNames(a)
[1] "location" "velocity" "active"
> slotNames("Agent")
[1] "location" "velocity" "active"



The <em>getSlots</em> command is similar to the <em>slotNames</em> command. 
It takes the name of a class as a string. 
It returns a vector whose entries are the types associated with the slots, and the names of the entries are the names of the slots.
> getSlots("Agent")
     location  velocity    active
"numeric" "numeric" "logical"
> s = getSlots("Agent")
> s[1]
     location
"numeric"
> s[[1]]
[1] "numeric"
> names(s)
[1] "location" "velocity" "active"



The next command examined is the <em>getClass</em> command. 
It has two forms. 
If you give it a variable that is an S4 class it returns a list of slots for the class associated with the variable. 
If you give it a character string with the name of a class it gives the slots and their data types.
> getClass(a)
An object of class "Agent"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> getClass("Agent")
Class "Agent" [in ".GlobalEnv"]
Slots:
Name:  location velocity   active
Class:  numeric  numeric  logical



The final command examined is the <em>slot</em> command. 
It can be used to get or set the value of a slot in an object. 
It can be used in place of the “@” operator.
> slot(a,"location")
[1] 0 0
> slot(a,"location") = c(1,5)
> a
An object of class "Agent"
Slot "location":
[1] 1 5
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE





<h3>Creating Methods</h3>
We now build on the Agent class defined above. 
Once the class and its data elements are defined we can define the methods associated with the class. 
The basic idea is that if the name of a function has not been defined, the name must first be reserved using the <em>setGeneric</em>
function. 
The <em>setMethod</em> can then be used to define which function is called based on the class names of the objects sent to it.
We define the methods associated with the Agent method given in the previous section. 
Note that the <em>validity</em> function for an object is only called when it is first created and when an explicit call to the
<em>validObject</em> function is made. 
We make use of the <em>validObject</em>
command in the methods below that are used to change the value of a data element within an object.
# create a method to assign the value of the location setGeneric(name="setLocation",
    def=function(theObject,position)
    {
        standardGeneric("setLocation")
    }
    )
setMethod(f="setLocation",
    signature="Agent",
    definition=function(theObject,position)
    {
       theObject@location = position
       validObject(theObject)
       return(theObject)
    }
    )
# create a method to get the value of the location setGeneric(name="getLocation",
    def=function(theObject)
    {
        standardGeneric("getLocation")
    }
    )
setMethod(f="getLocation",
    signature="Agent",
    definition=function(theObject)
    {
       return(theObject@location)
    }
    )

# create a method to assign the value of active setGeneric(name="setActive",
    def=function(theObject,active)
    {
        standardGeneric("setActive")
    }
    )
setMethod(f="setActive",
    signature="Agent",
    definition=function(theObject,active)
    {
       theObject@active = active
       validObject(theObject)
       return(theObject)
    }
    )
# create a method to get the value of active setGeneric(name="getActive",
    def=function(theObject)
    {
        standardGeneric("getActive")
    }
    )
setMethod(f="getActive",
    signature="Agent",
    definition=function(theObject)
    {
       return(theObject@active)
    }
    )

# create a method to assign the value of velocity setGeneric(name="setVelocity",
    def=function(theObject,velocity)
    {
        standardGeneric("setVelocity")
    }
    )
setMethod(f="setVelocity",
    signature="Agent",
    definition=function(theObject,velocity)
    {
       theObject@velocity = velocity
       validObject(theObject)
       return(theObject)
    }
    )
# create a method to get the value of the velocity setGeneric(name="getVelocity",
    def=function(theObject)
    {
        standardGeneric("getVelocity")
    }
    )
setMethod(f="getVelocity",
    signature="Agent",
    definition=function(theObject)
    {
       return(theObject@velocity)
    }
    )



With these definitions the data elements are encapsulated and can be accessed and set using the methods given above. 
It is generally good practice in object oriented programming to keep your data private and not show them to everybody willy nilly.
> a = Agent()
> getVelocity(a)
[1] 0 0
> a = setVelocity(a,c(1.0,2.0))
> getVelocity(a)
[1] 1 2



The last topic examined is the idea of overloading functions. 
In the examples above the signature is set to a single element. 
The signature is a vector of characters and specifies the data types of the argument list for the method to be defined. 
Here we create two new methods. 
The name of the method is <em>resetActivity</em>, and there are two versions.
The first version accepts two arguments whose types are <em>Agent</em> and
<em>logical</em>. 
This version of the method will set the activity slot to a given value. 
The second version accepts two arguments whose types are
<em>Agent</em> and <em>numeric</em>. 
This version will set the activity to TRUE and then set the energy level to the value passed to it. 
Note that the names of the variables in the argument list must be exactly the same.
# create a method to reset the velocity and the activity setGeneric(name="resetActivity",
    def=function(theObject,value)
    {
        standardGeneric("resetActivity")
    }
    )
setMethod(f="resetActivity",
    signature=c("Agent","logical"),
    definition=function(theObject,value)
    {
       theObject = setActive(theObject,value)
       theObject = setVelocity(theObject,c(0.0,0.0))
       return(theObject)
    }
    )
setMethod(f="resetActivity",
    signature=c("Agent","numeric"),
    definition=function(theObject,value)
    {
       theObject = setActive(theObject,TRUE)
       theObject = setVelocity(theObject,value)
       return(theObject)
    }
    )



This definition of the function yields two options for the
<em>resetActivity</em> function. 
The decision to determine which function to call depends on two arguments and their type. 
For example, if the first argument is from the Agent class and the second is a value of
TRUE or FALSE, then the first version of the function is called. 
Otherwise, if the second argument is a number the second version of the function is called.
> a = Agent()
> a
An object of class "Agent"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> a = resetActivity(a,FALSE)
> getActive(a)
[1] FALSE
>  a = resetActivity(a,c(1,3))
> getVelocity(a)
[1] 1 3





<h3>Inheritance</h3>
A class’ inheritance hiearchy can be specified when the class is defined using the <em>contains</em> option. 
The <em>contains</em> option is a vector that lists the classes the new class inherits from. 
In the following example we build on the Agent class defined in the previous section. 
The idea is that we need agents that represent a predator and two prey. 
We will focus on two predators for this example.
The hierarchy for the classes is shown in
Figure 2.. 
 In this example we have one Prey class that is derived from the Agent class. 
There are two predator classes, Bobcat and Lynx. 
The Bobcat class is derived from the Agent class, and the Lynx class is derived from the Bobcat class. 
We will keep this very simple, and the only methods associated with the new classes is a <em>move</em> method. 
For our purposes it will only print out a message and set the values of the position and velocity to demonstrate the order of execution of the methods associated with the classes.

<img src="https://www.cyclismo.org/tutorial/R/_images/s4AgentPredPrey.png" />
Figure 2.

Diagram of the predator and prey classes derived from the Agent class.

The first step is to create the three new classes.
######################################################################
# Create the Prey class
#
# This is used to represent a prey animal
Prey = setClass(
    # Set the name for the class
    "Prey",
    # Define the slots - in this case it is empty...
    slots = character(0),
    # Set the default values for the slots. 
(optional)
    prototype=list(),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>70.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    },
    # Set the inheritance for this class
    contains = "Agent"
    )

######################################################################
# Create the Bobcat class
#
# This is used to represent a smaller predator
Bobcat = setClass(
    # Set the name for the class
    "Bobcat",
    # Define the slots - in this case it is empty...
    slots = character(0),
    # Set the default values for the slots. 
(optional)
    prototype=list(),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>85.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    },
    # Set the inheritance for this class
    contains = "Agent"
    )
######################################################################
# Create the Lynx class
#
# This is used to represent a larger predator
Lynx = setClass(
    # Set the name for the class
    "Lynx",
    # Define the slots - in this case it is empty...
    slots = character(0),
    # Set the default values for the slots. 
(optional)
    prototype=list(),
    # Make a function that can test to see if the data is consistent.
    # This is not called if you have an initialize function defined!
    validity=function(object)
    {
        if(sum(object@velocity^2)>95.0) {
            return("The velocity level is out of bounds.")
        }
        return(TRUE)
    },
    # Set the inheritance for this class
    contains = "Bobcat"
    )



The inheritance is specified using the <em>contains</em> option in the
<em>setClass</em> command. 
Note that this can be a vector allowing for multiple inheritance. 
We choose not to use that to keep things simpler. 
If you are feeling like you need more self-loathing in your life you should try it out and experiment.
Next we define a method, <em>move</em>, for the new classes. 
We will include methods for the Agent, Prey, Bobcat, and Lynx classes. 
The methods do not really do anything but are used to demonstrate the idea of how methods are executed.
# create a method to move the agent.
setGeneric(name="move",
    def=function(theObject)
    {
        standardGeneric("move")
    }
    )
setMethod(f="move",
    signature="Agent",
    definition=function(theObject)
    {
       print("Move this Agent dude")
       theObject = setVelocity(theObject,c(1,2))
       validObject(theObject)
       return(theObject)
    }
    )
setMethod(f="move",
    signature="Prey",
    definition=function(theObject)
    {
       print("Check this Prey before moving this dude")
       theObject = callNextMethod(theObject)
       print("Move this Prey dude")
       validObject(theObject)
       return(theObject)
    }
    )
setMethod(f="move",
    signature="Bobcat",
    definition=function(theObject)
    {
       print("Check this Bobcat before moving this dude")
       theObject = setLocation(theObject,c(2,3))
       theObject = callNextMethod(theObject)
       print("Move this Bobcat dude")
       validObject(theObject)
       return(theObject)
    }
    )
setMethod(f="move",
    signature="Lynx",
    definition=function(theObject)
    {
       print("Check this Lynx before moving this dude")
       theObject = setActive(theObject,FALSE)
       theObject = callNextMethod(theObject)
       print("Move this Lynx dude")
       validObject(theObject)
       return(theObject)
    }
    )



There are a number of things to note. 
First each method calls the
<em>callNextMethod</em> command. 
This command will execute the next version of the same method for the previous class in the hierarchy. 
Note that
I have included the arguments (in the same order) as those called by the original function. 
Also note that the function returns a copy of the object and is used to update the object passed to the original function.
Another thing to note is that the methods associated with the Lync,
Bobcat, and Agent classes arbitrarily change the values of the position, velocity, and activity for the given object. 
This is done to demonstrate the changes that take place and reinforce the necessity for using the <em>callNextMethod</em> function the way it is used here.
Finally, it should be noted that the <em>validObject</em> command is called in every method. 
You should try adding a print statement in the validity function. 
You might find that the order is a bit odd. 
You should experiment with this and play with it. 
There are times you do not get the expected results so be careful!
We now give a brief example to demonstrate the order that the functions are called. 
In the example we create a Bobcat object and then call the <em>move</em> method. 
We next create a Lynx object and do the same. 
We print out the slots for both agents just to demonstrate the values that are changed.
> robert = Bobcat()
> robert
An object of class "Bobcat"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> robert = move(robert)
[1] "Check this Bobcat before moving this dude"
[1] "Move this Agent dude"
[1] "Move this Bobcat dude"
> robert
An object of class "Bobcat"
Slot "location":
[1] 2 3
Slot "velocity":
[1] 1 2
Slot "active":
[1] TRUE
>
>
>
> lionel = Lynx()
> lionel
An object of class "Lynx"
Slot "location":
[1] 0 0
Slot "velocity":
[1] 0 0
Slot "active":
[1] TRUE
> lionel = move(lionel)
[1] "Check this Lynx before moving this dude"
[1] "Check this Bobcat before moving this dude"
[1] "Move this Agent dude"
[1] "Move this Bobcat dude"
[1] "Move this Lynx dude"
> lionel
An object of class "Lynx"
Slot "location":
[1] 2 3
Slot "velocity":
[1] 1 2
Slot "active":
[1] FALSE

<h2>convert named character to vector</h2>
a = unname(resultTable[,1][which(klineWave[,7]==TRUE)])

<h2>Neural Network Models in R</h2>
Neural Network (or Artificial Neural Network) has the ability to learn by examples. 
ANN is an information processing model inspired by the biological neuron system. 
It is composed of a large number of highly interconnected processing elements known as the neuron to solve problems. 
It follows the non-linear path and process information in parallel throughout the nodes. 
A neural network is a complex adaptive system. 
Adaptive means it has the ability to change its internal structure by adjusting weights of inputs. 
(<a href="https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol1/cs11/article1.html]">Source</a>)

The neural network was designed to solve problems which are easy for humans and difficult for machines such as identifying pictures of cats and dogs, identifying numbered pictures. 
These problems are often referred to as pattern recognition. 
Its application ranges from optical character recognition to object detection. 

In this tutorial, you are going to cover the following topics:

Introduction to neural network
Forward Propagation and Back Propagation
Activation Function
Implementation of the neural network in R
Use-cases of NN
Pros and Cons
Conclusion

<h3>Introduction to Neural Network</h3>
In 1943, Warren McCulloch and Walter Pitts developed the first mathematical model of a neuron. 
In their research paper "A logical calculus of the ideas immanent in nervous activity”, they described the simple mathematical model for a neuron, which represents a single cell of the neural system that takes inputs, processes those inputs, and returns an output. 
This model is known as the McCulloch-Pitts neural model. 
(<a href="http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html">Source</a>)

NN is algorithms are inspired by the human brain to performs a particular task or functions. 
 NN perform computations through a process by learning. 
The neural network is a set of connected input/output units in which each connection has a weight associated with it. 
In the learning phase, the network learns by adjusting the weights to predict the correct class label of the given inputs.

The human brain consists of billions of neural cells that process information. 
Each neural cell considered a simple processing system. 
The Interconnected web of neurons known as biological neural network transmits information through electrical signals. 
This parallel interactive system makes the brain to think and process information. 
Dendrites of a neuron receive input signals from another neuron and respond output based on those inputs to an axon of some other neuron. 
Based on those inputs, fire an output signal via an axon. 

<img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/1_a74o1a.png">

Dendrites receive signals from other neurons. 
Cell body sums all the inputs signals to generate output. 
Axon through output When the sum reaches to a threshold. 
Synapses is a point of interaction neurons. 
It transmits electrical or chemical signals to another neuron. 
Synapse is derived from the Greek word which means conjunction.

<img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/2_i1cdwq.png">

<center><img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672506/7_f65unr.png" /></center>

Here, x1,x2....xn are input variables. 
w1,w2....wn are weights of respective inputs. 
b is the bias, which is summed with the weighted inputs to form the net inputs. 
Bias and weights are both adjustable parameters of the neuron. 
Parameters are adjusted using some learning rules. 
 The output of a neuron can range from -inf to +inf. 
The neuron doesn’t know the boundary. 
So we need a mapping mechanism between the input and output of the neuron. 
 This mechanism of mapping inputs to output is known as Activation Function.

<h3>Feedforward and Feedback Artificial Neural Networks</h3>
There are two main types of artificial neural networks: Feedforward and feedback artificial neural networks. 
Feedforward neural network is a network which is not recursive. 
Neurons in this layer were only connected to neurons in the next layer, and they are don't form a cycle. 
In Feedforward signals travel in only one direction towards the output layer. 
(<a href="http://faculty.simpson.edu/lydia.sinapova/www/cmsc310/LN310_Cawsey/L11-ML_NN.htm">Source</a>)

Feedback neural networks contain cycles. 
Signals travel in both directions by introducing loops in the network. 
The feedback cycles can cause the network's behavior change over time based on its input. 
Feedback neural network also known as recurrent neural networks. 
(<a href="http://faculty.simpson.edu/lydia.sinapova/www/cmsc310/LN310_Cawsey/L11-ML_NN.htm">Source</a>)

<center><img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/3_qwv5gr.png" /></center>

<h3>Activation Functions</h3>
Activation function defines the output of a neuron in terms of a local induced field. 
 Activation functions are a single line of code that gives the neural nets non-linearity and expressiveness. 
There are many activation functions. 
Some of them are as follows (<a href="https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/]">Source</a>):

<strong>Identity function</strong> is a function that maps input to the same output value. 
It is a linear operator in vector space. 
Also, known straight line function where activation is proportional to the input.
In <strong>Binary Step Function</strong>, if the value of Y is above a certain value known as the threshold, the output is True(or activated), and if it’s less than the threshold, then the output is false (or not activated). 
It is very useful in the classifier.
<strong>Sigmoid Function</strong> called S-shaped functions. 
Logistic and hyperbolic tangent functions are commonly used sigmoid functions. 
There are two types of sigmoid functions.
<strong>Binary Sigmoid Function</strong> is a logistic function where the output values are either binary or vary from 0 to 1.
<strong>Bipolar Sigmoid Function</strong> is a logistic function where the output value varies from -1 to 1. 
Also known as Hyperbolic Tangent Function or tanh.

<strong>Ramp Function:</strong> The name of the ramp function is derived from the appearance of its graph. 
It maps negative inputs to 0 and positive inputs to the same output.
<strong>ReLu</strong> stands for the rectified linear unit (ReLU). 
It is the most used activation function in the world. 
It output 0 for negative values of x.

<center><img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/4_jouacz.png" /></center>

<h3>Implementation of a Neural Network in R</h3>
<h3>Install required package</h3>
Let's first install the neuralnet library:

<code class="lang-python"># install package
install.packages("neuralnet")
</code>

<code>Updating HTML index of packages in '.Library'
Making 'packages.html' ... 
done
</code>
<h3>Create training dataset</h3>
<center><img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/5_lrfb0r.png" /></center>

Let's create your own dataset. 
Here you need two kinds of attributes or columns in your data: Feature and label. 
In the table shown above, you can see the technical knowledge, communication skills score and placement status of the student. 
So the first two columns(Technical Knowledge Score and Communication Skills Score) are features and third column(Student Placed) is the binary label.

<code class="lang-python"># creating training data set
TKS=c(20,10,30,20,80,30)
CSS=c(90,20,40,50,50,80)
Placed=c(1,0,0,0,1,1)
# Here, you will combine multiple columns or features into a single set of data
df=data.frame(TKS,CSS,Placed)
</code>

Let's build a NN classifier model using the neuralnet library.

First, import the neuralnet library and create NN classifier model by passing argument set of label and features, dataset, number of neurons in hidden layers, and error calculation.

<code class="lang-python"># load library
require(neuralnet)

# fit neural network
nn=neuralnet(Placed~TKS+CSS,data=df, hidden=3,act.fct = "logistic",
               linear.output = FALSE)
</code>

Here,

<code>  - Placed~TKS+CSS, Placed is label annd TKS and CSS are features.
 - df is dataframe,
 - hidden=3: represents single layer with 3 neurons respectively.
 - act.fct = "logistic" used for smoothing the result.
 - linear.ouput=FALSE: set FALSE for apply act.fct otherwise TRUE
</code>
<h3>Plotting Neural Network</h3>
Let's plot your neural net model.

<code class="lang-python"># plot neural network
plot(nn)
</code>

<center><img class="lazy" data-src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672260/6_y5jnhr.png" /></center>

<h3>Create test dataset</h3>
Create test dataset using two features Technical Knowledge Score and Communication Skills Score

<code class="lang-python"># creating test set
TKS=c(30,40,85)
CSS=c(85,50,40)
test=data.frame(TKS,CSS)
</code>

<h3>Predict the results for the test set</h3>
Predict the probability score for the test data using the compute function.

<code class="lang-python">## Prediction using neural network
Predict=compute(nn,test)
Predict$net.result
</code>

0.9928202080
0.3335543925
0.9775153014

Now, Convert probabilities into binary classes.

<code class="lang-python"># Converting probabilities into binary classes setting threshold level 0.5
prob &lt;- Predict$net.result
pred &lt;- ifelse(prob&gt;0.5, 1, 0)
pred
</code>

1
0
1

Predicted results are 1,0, and 1.

<h3>Pros and Cons</h3>
Neural networks are more flexible and can be used with both regression and classification problems. 
Neural networks are good for the nonlinear dataset with a large number of inputs such as images. 
Neural networks can work with any number of inputs and layers. 
Neural networks have the numerical strength that can perform jobs in parallel.

There are more alternative algorithms such as SVM, Decision Tree and Regression are available that are simple, fast, easy to train, and provide better performance. 
Neural networks are much more of the black box, require more time for development and more computation power. 
Neural Networks requires more data than other Machine Learning algorithms. 
NNs can be used only with numerical inputs and non-missing value datasets. 
A well-known neural network researcher said <i> "A neural network is the second best way to solve any problem. 
The best way is to actually understand the problem,"</i>

<h3>Use-cases of NN</h3>
NN's wonderful properties offer many applications such as:

<strong>Pattern Recognition:</strong>  neural networks are very suitable for pattern recognition problems such as facial recognition, object detection, fingerprint recognition, etc.
<strong>Anomaly Detection:</strong> neural networks are good at pattern detection, and they can easily detect the unusual patterns that don’t fit in the general patterns.
<strong>Time Series Prediction:</strong> Neural networks can be used to predict time series problems such as stock price, weather forecasting.
<strong>Natural Language Processing:</strong> Neural networks offer a wide range of applications in Natural Language Processing tasks such as text classification, Named Entity Recognition (NER), Part-of-Speech Tagging, Speech Recognition, and Spell Checking.

<h3>Conclusion</h3>
Congratulations, you have made it to the end of this tutorial!

In this tutorial, you have covered a lot of details about the Neural Network. 
You have learned what Neural Network, Forward Propagation, and Back Propagation are, along with Activation Functions, Implementation of the neural network in R, Use-cases of NN, and finally Pros, and Cons of NN.

<h2>Commonly used Machine Learning Algorithms (with Python and R Codes)</h2>
<h3>Overview</h3>
Major focus on commonly used <a href="https://www.analyticsvidhya.com/machine-learning/?utm_source=blog&amp;utm_medium=commonly-used-machine-learning-algorithms" target="_blank" rel="noopener noreferrer">machine learning</a> algorithms
Algorithms covered- Linear regression, logistic regression, Naive Bayes, kNN, Random forest, etc.
Learn both theory and implementation of these algorithms in R and python

<h3>Introduction</h3>
We are probably living in the most defining period of human history. 
The period when computing moved from large mainframes to PCs to cloud. 
But what makes it defining is not what has happened, but what is coming our way in years to come.

What makes this period exciting and enthralling for someone like me is the democratization of the various tools and techniques, which followed the boost in computing. 
Welcome to the world of <a href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog&amp;utm_medium=essentialMLalgorithmsarticle">data science</a>!

Today, as a data scientist, I can build data-crunching machines with complex algorithms for a few dollars per hour. 
But reaching here wasn't easy! I had my dark days and nights.

<em>Are you a beginner looking for a place to start your data science journey? Presenting two comprehensive courses, full of knowledge and data science learning, curated just for you to learn data science (using Python) from scratch:</em>

<em><a href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog&amp;utm_medium=essentialMLalgorithmsarticle" target="_blank" rel="noopener noreferrer">Introduction to Data Science</a></em>
<em><a href="https://courses.analyticsvidhya.com/bundles/data-science-beginners-with-interview" target="_blank" rel="noopener noreferrer">Certified Program: Data Science for Beginners (with Interviews)</a></em>

<h3>Who can benefit the most from this guide?</h3>
<h5>What I am giving out today is probably the most valuable guide, I have ever created.</h5>

The idea behind creating this guide is to simplify the journey of aspiring data scientists and <a href="https://www.analyticsvidhya.com/machine-learning/?utm_source=blog&amp;utm_medium=commonly-used-machine-learning-algorithms" target="_blank" rel="noopener noreferrer">machine learning</a> enthusiasts across the world. 
Through this guide, I will enable you to work on machine learning problems and gain from experience. 
<strong>I am providing a high-level understanding of various machine learning algorithms along with R &amp; Python codes to run them. 
These should be sufficient to get your hands dirty.</strong>

Essentials of machine learning algorithms with implementation in R and Python

I have deliberately skipped the statistics behind these techniques, as you don't need to understand them at the start. 
So, if you are looking for statistical understanding of these algorithms, you should look elsewhere. 
But, if you are looking to equip yourself to start building machine learning project, you are in for a treat.

<h3>Broadly, there are 3 <a href="https://www.analyticsvidhya.com/machine-learning/?utm_source=blog&amp;utm_medium=commonly-used-machine-learning-algorithms" target="_blank" rel="noopener noreferrer">types of Machine Learning</a> Algorithms</h3>
<h4>1. Supervised Learning</h4>

<strong>How it works:</strong> This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). 
Using these set of variables, we generate a function that map inputs to desired outputs. 
The training process continues until the model achieves a desired level of accuracy on the training data. 
Examples of Supervised Learning: Regression, <a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="noopener noreferrer">Decision Tree</a>, <a href="https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/" target="_blank" rel="noopener noreferrer">Random Forest</a>, KNN, Logistic Regression etc.

<h4>2. Unsupervised Learning</h4>

<strong>How it works: </strong>In this algorithm, we do not have any target or outcome variable to predict / estimate. 
It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. 
Examples of Unsupervised Learning: Apriori algorithm, K-means.

<h4>3. Reinforcement Learning:</h4>

<strong>How it works:</strong> Using this algorithm, the machine is trained to make specific decisions. 
It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. 
This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. 
Example of Reinforcement Learning: Markov Decision Process

<h3><strong>List of Common Machine Learning Algorithms</strong></h3>

Here is the list of commonly used machine learning algorithms. 
These algorithms can be applied to almost any data problem:

Linear Regression
Logistic Regression
Decision Tree
SVM
Naive Bayes
kNN
K-Means
Random Forest
Dimensionality Reduction Algorithms
Gradient Boosting algorithms
GBM
XGBoost
LightGBM
CatBoost

<h3>1. Linear Regression</h3>

It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). 
Here, we establish relationship between independent and dependent variables by fitting a best line. 
This best fit line is known as regression line and represented by a linear equation Y= a *X + b.

The best way to understand linear regression is to relive this experience of childhood. 
Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. 
This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.

In this equation:

Y – Dependent Variable
a – Slope
X – Independent variable
b – Intercept

These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.

Look at the below example. 
Here we have identified the best fit line having linear equation <strong>y=0.2811x+13.9</strong>. 
Now using this equation, we can find the weight, knowing the height of a person.

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Linear_Regression.png">

Linear Regression is mainly of two types: Simple Linear Regression and Multiple Linear Regression. 
Simple Linear Regression is characterized by one independent variable. 
And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. 
While finding the best fit line, you can fit a polynomial or curvilinear regression. 
And these are known as polynomial or curvilinear regression.

Here's a coding window to try out your hand and build your own linear regression model in Python:

<strong>R Code</strong>

#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays
x_train &lt;- input_variables_values_training_datasets
y_train &lt;- target_variables_values_training_datasets
x_test &lt;- input_variables_values_test_datasets
x &lt;- cbind(x_train,y_train)
# Train the model using the training sets and check score
linear &lt;- lm(y_train ~ ., data = x)
summary(linear)
#Predict Output
predicted= predict(linear,x_test)

<h3>2. Logistic Regression</h3>

Don't get confused by its name! It is a classification not a regression algorithm. 
It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). 
In simple words, it predicts the probability of occurrence of an event by fitting data to a <a href="https://en.wikipedia.org/wiki/Logistic_function" target="_blank" rel="nofollow noopener noreferrer">logit function</a>. 
Hence, it is also known as <strong>logit regression</strong>. 
Since, it predicts the probability, its output values lies between 0 and 1 (as expected).

Again, let us try and understand this through a simple example.

Let's say your friend gives you a puzzle to solve. 
There are only 2 outcome scenarios – either you solve it or you don't. 
Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. 
The outcome to this study would be something like this – if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. 
On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. 
This is what Logistic Regression provides you.

Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.

odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk

Above, p is the probability of presence of the characteristic of interest. 
It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).

Now, you may ask, why take a log? For the sake of simplicity, let's just say that this is one of the best mathematical way to replicate a step function. 
I can go in more details, but that will beat the purpose of this article.

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Logistic_Regression.png">
Build your own logistic regression model in Python here and check the accuracy:

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Logistic-Regression?lite=true">﻿</iframe>

<strong>R Code</strong>

x &lt;- cbind(x_train,y_train)
# Train the model using the training sets and check score
logistic &lt;- glm(y_train ~ ., data = x,family='binomial')
summary(logistic)
#Predict Output
predicted= predict(logistic,x_test)

<h4>Furthermore..</h4>

There are many different steps that could be tried in order to improve the model:

including interaction terms
removing features
<a href="https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/" target="_blank" rel="noopener noreferrer">regularization techniques</a>
using a non-linear model

<h3>3. Decision Tree</h3>

This is one of my favorite algorithm and I use it quite frequently. 
It is a type of supervised learning algorithm that is mostly used for classification problems. 
Surprisingly, it works for both categorical and continuous dependent variables. 
In this algorithm, we split the population into two or more homogeneous sets. 
This is done based on most significant attributes/ independent variables to make as distinct groups as possible. 
For more details, you can read: <a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="noopener noreferrer">Decision Tree Simplified</a>.

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/IkBzK.png>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/IkBzK.png">

source: <a href="http://stats.stackexchange.com" target="_blank" rel="nofollow noopener noreferrer">statsexchange</a>

In the image above, you can see that population is classified into four different groups based on multiple attributes to identify &#8216;if they will play or not'. 
To split the population into different heterogeneous groups, it uses various techniques like Gini, Information Gain, Chi-square, entropy.

The best way to understand how decision tree works, is to play Jezzball – a classic game from Microsoft (image below). 
Essentially, you have a room with moving walls and you need to create walls such that maximum area gets cleared off with out the balls.

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/download.jpg>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/download.jpg">

So, every time you split the room with a wall, you are trying to create 2 different populations with in the same room. 
Decision trees work in very similar fashion by dividing a population in as different groups as possible.

<em>More</em>: <a href="https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="noopener noreferrer">Simplified Version of Decision Tree Algorithms</a>

Let's get our hands dirty and code our own decision tree in Python!

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Decision-Tree?lite=true">﻿</iframe>

<strong>R Code</strong>

library(rpart)
x &lt;- cbind(x_train,y_train)
# grow tree 
fit &lt;- rpart(y_train ~ ., data = x,method="class")
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>4. SVM (Support Vector Machine)</h3>

It is a classification method. 
In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.

For example, if we only had two features like Height and Hair length of an individual, we'd first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as <strong>Support Vectors</strong>)

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM1.png>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM1.png">

Now, we will find some <em>line</em> that splits the data between the two differently classified groups of data. 
This will be the line such that the distances from the closest point in each of the two groups will be farthest away.

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM2.png>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM2-300x204.png">

In the example shown above, the line which splits the data into two differently classified groups is the <em>black</em> line, since the two closest points are the farthest apart from the line. 
This line is our classifier. 
Then, depending on where the testing data lands on either side of the line, that's what class we can classify the new data as.

More: <a href="https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/" target="_blank" rel="noopener noreferrer">Simplified Version of Support Vector Machine</a>

<strong>Think of this algorithm as playing JezzBall in n-dimensional space. 
The tweaks in the game are:</strong>

You can draw lines/planes at any angles (rather than just horizontal or vertical as in the classic game)
The objective of the game is to segregate balls of different colors in different rooms.
And the balls are not moving.

Try your hand and design an SVM model in Python through this coding window:

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Support-Vector-Machine?lite=true">﻿</iframe>
<h5></h5>

<strong>R Code</strong>

library(e1071)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-svm(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>5. Naive Bayes</h3>

It is a classification technique based on <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="nofollow noopener noreferrer">Bayes’ theorem</a> with an assumption of independence between predictors. 
In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. 
For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. 
Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.

<a href="https://courses.analyticsvidhya.com/courses/naive-bayes?utm_source=blog&amp;utm_medium=common-machine-learning-algorithms">Naive Bayesian</a> model is easy to build and particularly useful for very large data sets. 
Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.

Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). 
Look at the equation below:<br />
<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_rule.png>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_rule-300x172.png">

Here,

<i>P</i>(<i>c|x</i>) is the posterior probability of <i>class</i> (<i>target</i>) given <i>predictor</i> (<i>attribute</i>). 

<i>P</i>(<i>c</i>) is the prior probability of <i>class</i>. 

<i>P</i>(<i>x|c</i>) is the likelihood which is the probability of <i>predictor</i> given <i>class</i>. 

<i>P</i>(<i>x</i>) is the prior probability of <i>predictor</i>.

<strong>Example: </strong>Let's understand it using an example. 
Below I have a training data set of weather and corresponding target variable &#8216;Play'. 
Now, we need to classify whether players will play or not based on weather condition. 
Let's follow the below steps to perform it.

Step 1: Convert the data set to frequency table

Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41.png>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41.png">

Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. 
The class with the highest posterior probability is the outcome of prediction.

<strong>Problem: </strong>Players will pay if weather is sunny, is this statement is correct?

We can solve it using above discussed method, so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)

Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64

Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.

<a href="https://courses.analyticsvidhya.com/courses/naive-bayes?utm_source=blog&amp;utm_medium=common-machine-learning-algorithms">Naive Bayes</a> uses a similar method to predict the probability of different class based on various attributes. 
This algorithm is mostly used in text classification and with problems having multiple classes.

Code a Naive Bayes classification model in Python:

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Naive-Bayes?lite=true">﻿</iframe>

<strong>R Code</strong>

library(e1071)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-naiveBayes(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>6. kNN (k- Nearest Neighbors)</h3>

It can be used for both classification and regression problems. 
However, it is more widely used in classification problems in the industry. 
K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. 
The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.

These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. 
First three functions are used for continuous function and fourth one (Hamming) for categorical variables. 
If K = 1, then the case is simply assigned to the class of its nearest neighbor. 
At times, choosing K turns out to be a challenge while performing kNN modeling.

More: <a href="http://Introduction to k-nearest neighbors : Simplified">Introduction to k-nearest neighbors : Simplified</a>.

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/KNN.png>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/KNN.png">

KNN can easily be mapped to our real lives. 
If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!

<strong>Things to consider before selecting kNN:</strong>

KNN is computationally expensive
Variables should be normalized else higher range variables can bias it
Works on pre-processing stage more before going for kNN like an outlier, noise removal

<h5>Python Code</h5>
<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/K-Nearest-Neighbours?lite=true">﻿</iframe>

<strong>R Code</strong>

library(knn)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-knn(y_train ~ ., data = x,k=5)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>7. K-Means</h3>

It is a type of unsupervised algorithm which solves the clustering problem. 
Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). 
Data points inside a cluster are homogeneous and heterogeneous to peer groups.

Remember figuring out shapes from ink blots? k means is somewhat similar this activity. 
You look at the shape and spread to decipher how many different clusters / population are present!

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/splatter_ink_blot_texture_by_maki_tak-d5p6zph.jpg>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/splatter_ink_blot_texture_by_maki_tak-d5p6zph-284x300.jpg">

<strong>How K-means forms cluster:</strong>

K-means picks k number of points for each cluster known as centroids.
Each data point forms a cluster with the closest centroids i.e. 
k clusters.
Finds the centroid of each cluster based on existing cluster members. 
Here we have new centroids.
As we have new centroids, repeat step 2 and 3. 
Find the closest distance for each data point from new centroids and get associated with new k-clusters. 
Repeat this process until convergence occurs i.e. 
centroids does not change.

<strong>How to determine value of K:</strong>

In K-means, we have clusters and each cluster has its own centroid. 
Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster. 
Also, when the sum of square values for all the clusters are added, it becomes total within sum of square value for the cluster solution.

We know that as the number of cluster increases, this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k, and then much more slowly after that. 
Here, we can find the optimum number of cluster.

<a rel="lightbox" href=https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Kmenas.png>
<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Kmenas-1024x516.png">

<h5>Python Code</h5>
<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/K-Means?lite=true">﻿</iframe>

<strong>R Code</strong>

library(cluster)
fit &lt;- kmeans(X, 3) # 5 cluster solution

<h3>8. Random Forest</h3>

Random Forest is a trademark term for an ensemble of decision trees. 
In Random Forest, we've collection of decision trees (so known as &#8220;Forest&#8221;). 
To classify a new object based on attributes, each tree gives a classification and we say the tree &#8220;votes&#8221; for that class. 
The forest chooses the classification having the most votes (over all the trees in the forest).

Each tree is planted &amp; grown as follows:

If the number of cases in the training set is N, then sample of N cases is taken at random but <em>with replacement</em>. 
This sample will be the training set for growing the tree.
If there are M input variables, a number m&lt;&lt;M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. 
The value of m is held constant during the forest growing.
Each tree is grown to the largest extent possible. 
There is no pruning.

For more details on this algorithm, comparing with decision tree and tuning model parameters, I would suggest you to read these articles:

<a href="https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/">Introduction to Random forest – Simplified</a>

<a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/">Comparing a CART model to Random Forest (Part 1)</a>

<a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/">Comparing a Random Forest to a CART model (Part 2)</a>

<a href="https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/">Tuning the parameters of your Random Forest model</a>

<strong>Python Code:</strong>

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Random-Forest?lite=true">﻿</iframe>

<strong>R Code</strong>

library(randomForest)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;- randomForest(Species ~ ., x,ntree=500)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)

<h3>9. Dimensionality Reduction Algorithms</h3>

In the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. 
Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.

For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.

As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. 
How'd you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others.

To know more about this algorithms, you can read &#8220;<a href="https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/">Beginners Guide To Learn Dimension Reduction Techniques</a>&#8220;.

<h5>Python Code</h5>
<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Dimensionality-Reduction-PCA?lite=true">﻿</iframe>
<h5>R Code</h5>

library(stats)
pca &lt;- princomp(train, cor = TRUE)
train_reduced  &lt;- predict(pca,train)
test_reduced  &lt;- predict(pca,test)

<h3>10. Gradient Boosting Algorithms</h3>
<h4>10.1. 
GBM</h4>

GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. 
Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. 
It combines multiple weak or average predictors to a build strong predictor. 
These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.

More: <a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/" target="_blank" rel="noopener noreferrer">Know about Boosting algorithms in detail</a>

<h5>Python Code</h5>
<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/Gradient-Boosting?lite=true">﻿</iframe>
<h5>R Code</h5>

library(caret)
x &lt;- cbind(x_train,y_train)
# Fitting model
fitControl &lt;- trainControl( method = "repeatedcv", number = 4, repeats = 4)
fit &lt;- train(y ~ ., data = x, method = "gbm", trControl = fitControl,verbose = FALSE)
predicted= predict(fit,x_test,type= "prob")[,2] 

GradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the <a href="http://discuss.analyticsvidhya.com/t/what-is-the-fundamental-difference-between-randomforest-and-gradient-boosting-algorithms/2341" target="_blank" rel="noopener noreferrer">difference between these two algorithms</a>.

<h4>10.2. 
XGBoost</h4>

Another classic gradient boosting algorithm that's known to be the decisive choice between winning and losing in some Kaggle competitions.

The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques.

The support includes various objective functions, including regression, classification and ranking.

One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. 
This helps to reduce overfit modelling and has a massive support for a range of languages such as Scala, Java, R, Python, Julia and C++.

Supports distributed and widespread training on many machines that encompass GCE, AWS, Azure and Yarn clusters. 
XGBoost can also be integrated with Spark, Flink and other cloud dataflow systems with a built in cross validation at each iteration of the boosting process.

To learn more about XGBoost and parameter tuning, visit <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a>.

<strong>Python Code:</strong>

<iframe width="100%" height="1400px" frameborder="no" scrolling="no" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" allowfullscreen="allowfullscreen" data-src="https://repl.it/@LakshayArora1/XGBoost?lite=true">﻿</iframe>

R Code:

require(caret)

x &lt;- cbind(x_train,y_train)

# Fitting model

TrainControl &lt;- trainControl( method = "repeatedcv", number = 10, repeats = 4)

model&lt;- train(y ~ ., data = x, method = "xgbLinear", trControl = TrainControl,verbose = FALSE)

OR 

model&lt;- train(y ~ ., data = x, method = "xgbTree", trControl = TrainControl,verbose = FALSE)

predicted &lt;- predict(model, x_test)

<h4>10.3. 
LightGBM</h4>

LightGBM is a gradient boosting framework that uses tree based learning algorithms. 
It is designed to be distributed and efficient with the following advantages:

Faster training speed and higher efficiency
Lower memory usage
Better accuracy
Parallel and GPU learning supported
Capable of handling large-scale data

The framework is a fast and high-performance gradient boosting one based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. 
It was developed under the Distributed Machine Learning Toolkit Project of Microsoft.

Since the LightGBM is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. 
So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.

Also, it is surprisingly very fast, hence the word &#8216;Light'.

Refer to the article to know more about LightGBM: <a href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/">https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/</a>

Python Code:

data = np.random.rand(500, 10) # 500 entities, each contains 10 features
label = np.random.randint(2, size=500) # binary target

train_data = lgb.Dataset(data, label=label)
test_data = train_data.create_valid('test.svm')

param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}
param['metric'] = 'auc'

num_round = 10
bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])

bst.save_model('model.txt')

# 7 entities, each contains 10 features
data = np.random.rand(7, 10)
ypred = bst.predict(data)

R Code:

library(RLightGBM)
data(example.binary)
#Parameters

num_iterations &lt;- 100
config &lt;- list(objective = "binary",  metric="binary_logloss,auc", learning_rate = 0.1, num_leaves = 63, tree_learner = "serial", feature_fraction = 0.8, bagging_freq = 5, bagging_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_leaf = 5.0)

#Create data handle and booster
handle.data &lt;- lgbm.data.create(x)

lgbm.data.setField(handle.data, "label", y)

handle.booster &lt;- lgbm.booster.create(handle.data, lapply(config, as.character))

#Train for num_iterations iterations and eval every 5 steps

lgbm.booster.train(handle.booster, num_iterations, 5)

#Predict
pred &lt;- lgbm.booster.predict(handle.booster, x.test)

#Test accuracy
sum(y.test == (y.pred &gt; 0.5)) / length(y.test)

#Save model (can be loaded again via lgbm.booster.load(filename))
lgbm.booster.save(handle.booster, filename = "/tmp/model.txt")

If you're familiar with the Caret package in R, this is another way of implementing the LightGBM.

require(caret)
require(RLightGBM)
data(iris)

model &lt;-caretModel.LGBM()

fit &lt;- train(Species ~ ., data = iris, method=model, verbosity = 0)
print(fit)
y.pred &lt;- predict(fit, iris[,1:4])

library(Matrix)
model.sparse &lt;- caretModel.LGBM.sparse()

#Generate a sparse matrix
mat &lt;- Matrix(as.matrix(iris[,1:4]), sparse = T)
fit &lt;- train(data.frame(idx = 1:nrow(iris)), iris$Species, method = model.sparse, matrix = mat, verbosity = 0)
print(fit)

<h4>10.4. Catboost</h4>

CatBoost is a recently open-sourced machine learning algorithm from Yandex. 
It can easily integrate with deep learning frameworks like Google's TensorFlow and Apple's Core ML.

The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be.

Make sure you handle missing data well before you proceed with the implementation.

Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.

Learn more about Catboost from this article: <a href="https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/">https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/</a>

Python Code:

import pandas as pd
import numpy as np

from catboost import CatBoostRegressor

#Read training and testing files
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

#Imputing missing values for both train and test
train.fillna(-999, inplace=True)
test.fillna(-999,inplace=True)

#Creating a training set for modeling and validation set to check model performance
X = train.drop(['Item_Outlet_Sales'], axis=1)
y = train.Item_Outlet_Sales

from sklearn.model_selection import train_test_split

X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)
categorical_features_indices = np.where(X.dtypes != np.float)[0]

#importing library and building model
from catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')

model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)

submission = pd.DataFrame()

submission['Item_Identifier'] = test['Item_Identifier']
submission['Outlet_Identifier'] = test['Outlet_Identifier']
submission['Item_Outlet_Sales'] = model.predict(test)

R Code:

set.seed(1)

require(titanic)

require(caret)

require(catboost)

tt &lt;- titanic::titanic_train[complete.cases(titanic::titanic_train),]

data &lt;- as.data.frame(as.matrix(tt), stringsAsFactors = TRUE)

drop_columns = c("PassengerId", "Survived", "Name", "Ticket", "Cabin")

x &lt;- data[,!(names(data) %in% drop_columns)]y &lt;- data[,c("Survived")]

fit_control &lt;- trainControl(method = "cv", number = 4,classProbs = TRUE)

grid &lt;- expand.grid(depth = c(4, 6, 8),learning_rate = 0.1,iterations = 100, l2_leaf_reg = 1e-3,            rsm = 0.95, border_count = 64)

report &lt;- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = TRUE, preProc = NULL,tuneGrid = grid, trControl = fit_control)

print(report)

importance &lt;- varImp(report, scale = FALSE)

print(importance)

<h3>Projects</h3>

Now, its time to take the plunge and actually play with some other real datasets. 
So are you ready to take on the challenge? Accelerate your data science journey with the following Practice Problems:

<table border="1">
<tbody>
<tr><td><a href="https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">
<img class="lazy" data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2016/01/food_500x250-300x150.jpg"></td><td><a href="https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=common-machine-learning-algorithms&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem: Food Demand Forecasting Challenge</a></td><td>Predict the demand of meals for a meal delivery company</td></tr>
<tr><td><a href="https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">
<img class="lazy" data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2016/01/hr_500x250-300x150.jpg"></td><td><a href="https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=common-machine-learning-algorithms&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem: HR Analytics Challenge</a></td><td>Identify the employees most likely to get promoted</td></tr>
<tr><td><a href="https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">
<img class="lazy" data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2016/01/upvote_500x250-300x150.jpg"></td><td><a href="https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=common-machine-learning-algorithms&amp;utm_medium=blog" target="_blank" rel="noopener noreferrer">Practice Problem: Predict Number of Upvotes</a></td><td>Predict number of upvotes on a query asked at an online question &amp; answer platform</td></tr>
</tbody>
</table>




<br>
<br>
<br>
<br>

<script type='text/javascript' src='readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
</body>
</html>
