<html>

<head>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">


<link rel="stylesheet" type="text/css" href="mystyle.css">
<script src="jquery.min.js"></script>

</head>
<body>

<h1 itemprop="name" class="entry-title">21 Steps to Get Started with Apache Spark using Scala</h1>

<h2>Introduction</h2>
<p style="text-align: justify">If you ask any industry expert, what language should you learn for big data, they would definitely suggest you to start with Scala. Scala has gained a lot of recognition for itself and is used by the large number of companies. Scala and Spark are being used at Facebook, Pinterest, NetFlix, Conviva, TripAdvisor for Big Data and Machine Learning applications.</p>
<p style="text-align: justify">Still not convinced &#8211; look at this trend of number of job postings for Scala on Indeed:<img src="https://static.analyticsvidhya.com/wp-content/uploads/2017/01/24160919/Screen-Shot-2017-01-24-at-9.36.18-PM.png"></p>
<p style="text-align: justify">But, learning a new language can be intimidating. To help you learn Scala from scratch, I have created this comprehensive guide. The guide is aimed at beginners and enables you to write simple codes in Apache Spark using Scala. I have kept the content simple to get you started.</p>
<p>By the end of this guide, you will have a thorough understanding of working with Apache Spark in Scala. Read on to learn one more language and add more skills to your resume.</p>
<h2>Table of Contents</h2>
<p>This guide is broadly divided into 2 parts. The first part is from section 1 to 14 where we discuss language Scala. Section 15 onwards is how we used Scala in Apache Spark.</p>
<ol>
<li>What is Scala?</li>
<li>About Scala</li>
<li>Installing Scala</li>
<li>Prerequisites for Scala</li>
<li>Choosing a development environment</li>
<li>Scala Basics Terms</li>
<li>Things to note about Scala</li>
<li>Variable declaration in Scala</li>
<li>Operations on variables</li>
<li>The if-else expression in Scala</li>
<li>Iteration in Scala</li>
<li>Declare a simple function in Scala and call it by passing value</li>
<li>Some Data Structures in Scala</li>
<li>Write/Run codes in Scala using editor</li>
<li>Advantages of using Scala for Apache Spark</li>
<li>Comparing Scala, java, Python and R in Apache Spark</li>
<li>Installing Apache Spark</li>
<li>Working with RDD in Apache Spark using Scala</li>
<li>Working with DataFrame in Apache Spark using Scala</li>
<li>Building a Machine Learning Model</li>
<li>Additional Resources</li>
</ol>
<p>&nbsp;</p>
<h2>1. What is Scala</h2>
<p style="text-align: justify">Scala is an acronym for “Scalable Language”. It is a general-purpose programming language designed for the programmers who want to write programs in a concise, elegant, and type-safe way. Scala enables programmers to be more productive. Scala is developed as an object-oriented and functional programming language.</p>
<p style="text-align: justify">If you write a code in Scala, you will see that the style is similar to a scripting language. Even though Scala is a new language, it has gained enough users and has a wide community support. It is one of the most user-friendly languages.</p>
<p>&nbsp;</p>
<h2>2. About Scala</h2>
<p style="text-align: justify">The design of Scala started in 2001 in the programming methods laboratory at EPFL (École Polytechnique Fédérale de Lausanne). Scala made its first public appearance in January 2004 on the JVM platform and a few months later in June 2004, it was released on the .(dot)NET platform. The .(dot)NET support of Scala was officially dropped in 2012. A few more characteristics of Scala are:</p>
<p>&nbsp;</p>
<h3>2.1 Scala is pure Object-Oriented programming language</h3>
<p style="text-align: justify">Scala is an object-oriented programming language. Everything in Scala is an object and any operations you perform is a method call. Scala, allow you to add new operations to existing classes with the help of implicit classes.</p>
<p style="text-align: justify">One of the advantages of Scala is that it makes it very easy to interact with Java code. You can also write a Java code inside Scala class. The Scala supports advanced component architectures through classes and traits.</p>
<p>&nbsp;</p>
<h3>2.2 Scala is a functional language</h3>
<p style="text-align: justify">Scala is a programming language that has implemented major functional programming concepts. In Functional programming, every computation is treated as a mathematical function which avoids states and mutable data. The functional programming exhibits following characteristics:</p>
<ul>
<li>Power and flexibility</li>
<li>Simplicity</li>
<li>Suitable for parallel processing</li>
</ul>
<p style="text-align: justify">Scala is not a pure functional language. Haskell is an example of a pure functional language. If you want to read more about functional programming, please refer to this <a href="https://dzone.com/articles/concept-functional-programing" target="_blank" rel="nofollow">article</a>.</p>
<p>&nbsp;</p>
<h3>2.3 Scala is a compiler based language (and not interpreted)</h3>
<p style="text-align: justify">Scala is a compiler based language which makes Scala execution very fast if you compare it with Python (which is an interpreted language). The compiler in Scala works in similar fashion as Java compiler. It gets the source code and generates Java byte-code that can be executed independently on any standard JVM (Java Virtual Machine). If you want to know more about the difference between complied vs interpreted language please refer this <a href="http://www.programmerinterview.com/index.php/general-miscellaneous/whats-the-difference-between-a-compiled-and-an-interpreted-language/" target="_blank" rel="nofollow">article</a>.</p>
<p>There are more important points about Scala which I have not covered. Some of them are:</p>
<ul>
<li>Scala has thread based executors</li>
<li>Scala is statically typed language</li>
<li>Scala can execute Java code</li>
<li>You can do concurrent and Synchronized processing in Scala</li>
<li>Scala is JVM based languages</li>
</ul>
<p>&nbsp;</p>
<h3>2.4 Companies using Scala</h3>
<p style="text-align: justify">Scala is now big name. It is used by many companies to develop the commercial software. These are the following notable big companies which are using Scala as a programming alternative.</p>
<ul>
<li>LinkedIn</li>
<li>Twitter</li>
<li>Foursquare</li>
<li>Netflix</li>
<li>Tumblr</li>
<li>The Guardian</li>
<li>Precog</li>
<li>Sony</li>
<li>AirBnB</li>
<li>Klout</li>
<li>Apple</li>
</ul>
<p>If you want to read more about how and when these companies started using Scala please refer this <a href="http://www.scala-lang.org/old/node/1658" target="_blank" rel="nofollow">blog</a>.</p>
<p>&nbsp;</p>
<h2>3. Installing Scala</h2>
<p style="text-align: justify">Scala can be installed in any Unix or windows based system. Below are the steps to install for Ubuntu (14.04) for scala version 2.11.7. I am showing the steps for installing Scala (2.11.7) with Java version 7. It is necessary to install Java before installing Scala. You can also install latest version of Scala(2.12.1) as well.</p>
<p style="text-align: justify"><strong>Step 0:</strong> Open the terminal</p>
<p style="text-align: justify"><strong>Step 1:</strong> Install Java</p>
<pre> $ sudo apt-add-repository ppa:webupd8team/java
 $ sudo apt-get update
 $ sudo apt-get install oracle-java7-installer</pre>
<p style="text-align: justify">If you are asked to accept Java license terms, click on “Yes” and proceed. Once finished, let us check whether Java has installed successfully or not. To check the Java version and installation, you can type:</p>
<pre> $ java -version</pre>
<p><strong>Step 2:</strong> Once Java is installed, we need to install Scala</p>
<pre> $ cd ~/Downloads
 $ wget http://www.scala-lang.org/files/archive/scala-2.11.7.deb
 $ sudo dpkg -i scala-2.11.7.deb
 $ scala –version</pre>
<p>This will show you the version of Scala installed</p>
<p>&nbsp;</p>
<h2>4. Prerequisites for Learning Scala</h2>
<p style="text-align: justify">Scala being an easy to learn language has minimal prerequisites. If you are someone with basic knowledge of C/C++, then you will be easily able to get started with Scala. Since Scala is developed on top of Java. Basic programming function in Scala is similar to Java. So, if you have some basic knowledge of Java syntax and OOPs concept, it would be helpful for you to work in Scala.</p>
<p>&nbsp;</p>
<h2>5. Choosing a development environment</h2>
<p>Once you have installed Scala, there are various options for choosing an environment. Here are the 3 most common options:</p>
<ul>
<li>Terminal / Shell based</li>
<li>Notepad / Editor based</li>
<li>IDE (Integrated development environment)</li>
</ul>
<p style="text-align: justify">Choosing right environment depends on your preference and use case. I personally prefer writing a program on shell because it provides a lot of good features like suggestions for method call and you can also run your code while writing line by line.</p>
<p>&nbsp;</p>
<p><strong>Warming up: Running your first Scala program in Shell:</strong><br />
Let&#8217;s write a first program which adds two numbers.</p>
<p style="text-align: justify"><img class="aligncenter wp-image-32468 size-full" src="https://static.analyticsvidhya.com/wp-content/uploads/2017/01/21060534/ff.png" width="608" height="392" srcset="https://static.analyticsvidhya.com/wp-content/uploads/2017/01/21060534/ff.png 608w, https://static.analyticsvidhya.com/wp-content/uploads/2017/01/21060534/ff-300x193.png 300w" sizes="(max-width: 608px) 100vw, 608px" /></p>
<p>&nbsp;</p>
<h2>6. Scala Basics Terms</h2>
<p><strong>Object</strong>: An entity that has state and behavior is known as an object. For example: table, person, car etc.</p>
<p style="text-align: justify"><strong>Class:</strong> A class can be defined as a blueprint or a template for creating different objects which defines its properties and behavior.</p>
<p style="text-align: justify"><strong>Method:</strong> It is a behavior of a class. A class can contain one or more than one method. For example: deposit can be considered a method of bank class.</p>
<p style="text-align: justify"><strong>Closure:</strong> Closure is any function that closes over the environment in which it’s defined. A closure returns value depends on the value of one or more variables which is declared outside this closure.</p>
<p style="text-align: justify"><strong>Traits:</strong> Traits are used to define object types by specifying the signature of the supported methods. It is like interface in java.</p>
<p>&nbsp;</p>
<h2>7. Things to note about Scala</h2>
<ul>
<li>It is case sensitive</li>
<li>If you are writing a program in Scala, you should save this program using “.scala”</li>
<li>Scala execution starts from main() methods</li>
<li>Any identifier name cannot begin with numbers. For example, variable name &#8220;123salary&#8221; is invalid.</li>
<li>You can not use Scala reserved keywords for variable declarations or constant or any identifiers.</li>
</ul>
<p>&nbsp;</p>
<h2>8. Variable declaration in Scala</h2>
<p style="text-align: justify">In Scala, you can declare a variable using &#8216;var&#8217; or &#8216;val&#8217; keyword. The decision is based on whether it is a constant or a variable. If you use &#8216;var&#8217; keyword, you define a variable as mutable variable. On the other hand, if you use &#8216;val&#8217;, you define it as immutable. Let&#8217;s first declare a variable using &#8220;var&#8221; and then using &#8220;val&#8221;.</p>
<h3>8.1 Declare using var</h3>
<pre>var Var1 : String = "Ankit"</pre>
<p style="text-align: justify">In the above Scala statement, you declare a mutable variable called “Var1” which takes a string value. You can also write the above statement without specifying the type of variable. Scala will automatically identify it. For example:</p>
<pre>var Var1 = "Gupta"</pre>
<p>&nbsp;</p>
<h3>8.2 Declare using val</h3>
<pre>val Var2 : String = "Ankit"</pre>
<p style="text-align: justify">In the above Scala statement, we have declared an immutable variable “Var2” which takes a string “Ankit”. Try it for without specifying the type of variable. If you want to read about mutable and immutable please refer this <a href="http://stackoverflow.com/questions/8287425/mutable-vs-immutable-in-scala-collections" target="_blank" rel="nofollow">link</a>.</p>
<p>&nbsp;</p>
<h2>9. Operations on variables</h2>
<p style="text-align: justify">You can perform various operations on variables. There are various kinds of operators defined in Scala. For example: Arithmetic Operators, Relational Operators, Logical Operators, Bitwise Operators, Assignment Operators.</p>
<p>Lets see &#8220;+&#8221; , &#8220;==&#8221; operators on two variables &#8216;Var4&#8217;, “Var5”. But, before that, let us first assign values to &#8220;Var4&#8221; and &#8220;Var5&#8221;.</p>
<pre>scala&gt; var Var4 = 2
Output: Var4: Int = 2
scala&gt; var Var5 = 3
Output: Var5: Int = 3</pre>
<p>Now, let us apply some operations using operators in Scala.</p>
<h4>Apply &#8216;+&#8217; operator</h4>
<pre>Var4+Var5
Output:
res1: Int = 5</pre>
<p><strong>Apply “==” operator</strong></p>
<pre>Var4==Var5
Output:
res2: Boolean = false</pre>
<p>If you want to know complete list of operators in Scala refer this <a href="https://www.tutorialspoint.com/scala/scala_operators.htm" target="_blank" rel="nofollow">link</a>:</p>
<p>&nbsp;</p>
<h2>10. The if-else expression in Scala</h2>
<p>In Scala, if-else statement is used for conditional statements. You can write one or more conditions inside “if”.  Let&#8217;s declare a variable called “Var3” with a value 1 and then compare “Var3” using if-else statement.</p>
<pre>var Var3 =1 
if (Var3 ==1){
 println("True")}else{
 println("False")}
Output: True</pre>
<p style="text-align: justify">In the above snippet, the condition evaluates to True and hence True will be printed in the output.</p>
<p>&nbsp;</p>
<h2>11. Iteration in Scala</h2>
<p style="text-align: justify">Like most languages, Scala also has a FOR-loop which is the most widely used method for iteration. It has a simple syntax too.</p>
<pre>for( a &lt;- 1 to 10){
 println( "Value of a: " + a );
 }
Output:
Value of a: 1
Value of a: 2
Value of a: 3
Value of a: 4
Value of a: 5
Value of a: 6
Value of a: 7
Value of a: 8
Value of a: 9
Value of a: 10</pre>
<p>Scala also supports “while” and “do while” loops. If you want to know how both work, please refer this <a href="https://www.tutorialspoint.com/scala/scala_loop_types.htm" target="_blank" rel="nofollow">link</a>.</p>
<p>&nbsp;</p>
<h2>12. Declare a simple function in Scala and call it by passing value</h2>
<p style="text-align: justify">You can define a function in Scala using “def” keyword. Let&#8217;s define a function called “mul2” which will take a number and multiply it by 10. You need to define the return type of function, if a function not returning any value you should use the “Unit” keyword.</p>
<p style="text-align: justify">In the below example, the function returns an integer value. Let&#8217;s define the function “mul2”:</p>
<pre>def mul2(m: Int): Int = m * 10
Output: mul2: (m: Int)Int</pre>
<p>Now let&#8217;s pass a value 2 into mul2</p>
<pre>mul2(2)
Output:
res9: Int = 20</pre>
<p>If you want to read more about the function, please refer this <a href="https://www.tutorialspoint.com/scala/scala_functions.htm" target="_blank" rel="nofollow">tutorial</a>.</p>
<p>&nbsp;</p>
<h2>13. Few Data Structures in Scala</h2>
<ul>
<li>Arrays</li>
<li>Lists</li>
<li>Sets</li>
<li>Tuple</li>
<li>Maps</li>
<li>Option</li>
</ul>
<h3>13.1 Arrays in Scala</h3>
<p style="text-align: justify">In Scala, an array is a collection of similar elements. It can contain duplicates. Arrays are also immutable in nature. Further, you can access elements of an array using an index:</p>
<h4 style="text-align: justify">Declaring Array in Scala</h4>
<p style="text-align: justify">To declare any array in Scala, you can define it either using a new keyword or you can directly assign some values to an array.</p>
<h5 style="text-align: justify">Declare an array by assigning it some values</h5>
<pre>var name = Array("Faizan","Swati","Kavya", "Deepak", "Deepak")
Output:
name: Array[String] = Array(Faizan, Swati, Kavya, Deepak, Deepak)</pre>
<p>In the above program, we have defined an array called name with 5 string values.</p>
<h5>Declaring an array using “new” keywords</h5>
<p>The following is the syntax for declaring an array variable using a new keyword.</p>
<pre>var name:Array[String] = new Array[String](3)
or
var name = new Array[String](3)
Output:
name: Array[String] = Array(null, null, null)</pre>
<p style="text-align: justify">Here you have declared an array of Strings called “name” that can hold up to three elements. You can also assign values to “name” by using an index.</p>
<pre>scala&gt; name(0) = "jal"
scala&gt; name(1) = "Faizy"
scala&gt; name(2) = "Expert in deep learning"
</pre>
<p>Let&#8217;s print contents of &#8220;name&#8221; array.</p>
<pre>scala&gt; name
res3: Array[String] = Array(jal, Faizy, Expert in deep learning)</pre>
<p>&nbsp;</p>
<h4>Accessing an array</h4>
<p style="text-align: justify">You can access the element of an array by index. Lets access the first element of array “name”. By giving index 0. Index in Scala starts from 0.</p>
<pre>name(0)
Output:
res11: String = jal

</pre>
<h3>13.2 List in Scala</h3>
<p style="text-align: justify">Lists are one of the most versatile data structure in Scala. Lists contain items of different types in Python, but in Scala the items all have the same type. Scala lists are immutable.</p>
<p style="text-align: justify">Here is a quick example to define a list and then access it.</p>
<h4 style="text-align: justify">Declaring List in Scala</h4>
<p style="text-align: justify">You can define list simply by comma separated values inside the “List” method.</p>
<pre>scala&gt; val numbers = List(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)
numbers: List[Int] = List(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)</pre>
<p>You can also define multi dimensional list in Scala. Lets define a two dimensional list:</p>
<pre>val number1 = List( List(1, 0, 0), List(0, 1, 0), List(0, 0, 1) )
number1: List[List[Int]] = List(List(1, 0, 0), List(0, 1, 0), List(0, 0, 1))</pre>
<p>&nbsp;</p>
<h4>Accessing a list</h4>
<p>Let&#8217;s get the third element of the list “numbers” . The index should 2 because index in Scala start from 0.</p>
<pre>scala&gt; numbers(2)
res6: Int = 3</pre>
<p>We have discussed two of the most used data Structures. You can learn more from this <a href="https://twitter.github.io/scala_school/collections.html" target="_blank" rel="nofollow">link</a>.</p>
<p>&nbsp;</p>
<h2>14. Writing &amp; Running a program in Scala using an editor</h2>
<p style="text-align: justify">Let us start with a “Hello World!” program. It is a good simple way to understand how to write, compile and run codes in Scala. No prizes for telling the outcome of this code!</p>
<pre>object HelloWorld {
 def main(args: Array[String]) {
 println("Hello, world!")
 }
 }</pre>
<p style="text-align: justify">As mentioned before, if you are familiar with Java, it will be easier for you to understand Scala. If you know Java, you can easily see that the structure of above “HelloWorld” program is very similar to Java program.</p>
<p style="text-align: justify">This program contains a method “main” (not returning any value) which takes an argument &#8211; a string array through command line. Next, it calls a predefined method called “Println” and passes the argument “Hello, world!”.</p>
<p style="text-align: justify">You can define the main method as static in Java but in Scala, the static method is no longer available. Scala programmer can&#8217;t use static methods because they use singleton objects. To read more about singleton object you can refer this <a href="http://www.javaworld.com/article/2073352/core-java/simply-singleton.html" target="_blank" rel="nofollow">article</a>.</p>
<p>&nbsp;</p>
<h3>14.1 Compile a Scala Program</h3>
<p style="text-align: justify">To run any Scala program, you first need to compile it. “Scalac” is the compiler which takes source program as an argument and generates object files as output.</p>
<p style="text-align: justify">Let&#8217;s start compiling your “HelloWorld” program using the following steps:</p>
<p style="padding-left: 30px;text-align: justify">1. For compiling it, you first need to paste this program into a text file then you need to save this program as HelloWorld.scala<br />
2. Now you need change your working directory to the directory where your program is saved<br />
3. After changing the directory you can compile the program by issuing the command.</p>
<pre style="padding-left: 30px">scalac HelloWorld.scala</pre>
<p style="padding-left: 30px;text-align: justify">4. After compiling,  you will get Helloworld.class as an output in the same directory. If you can see the file, you have successfully compiled the above program.</p>
<p>&nbsp;</p>
<h3>14.2 Running Scala Program</h3>
<p>After compiling, you can now run the program using following command:</p>
<pre>scala HelloWorld</pre>
<p>You will get an output if the above command runs successfully. The program will print “Hello, world!”</p>
<p>&nbsp;</p>
<h2>15. Advantages of using Scala for Apache Spark</h2>
<p style="text-align: justify">If you are working with Apache Spark then you would know that it has 4 different APIs support for different languages: <strong>Scala, Java, Python and R</strong>.</p>
<p style="text-align: justify">Each of these languages have their own unique advantages. But using Scala is more advantageous than other languages. These are the following reasons why Scala is taking over big data world.</p>
<ul>
<li style="text-align: justify">Working with Scala is more productive than working with Java</li>
<li>Scala is faster than Python and R because it is compiled language</li>
<li>Scala is a functional language</li>
</ul>
<p>&nbsp;</p>
<h2>16. Comparing Scala, Java, Python and R APIs in Apache Spark</h2>
<p>Let&#8217;s compare 4 major languages which are supported by Apache Spark API.</p>
<table style="height: 483px" border="1" width="756" cellspacing="0" cellpadding="4">
<colgroup>
<col width="51*" />
<col width="51*" />
<col width="51*" />
<col width="51*" />
<col width="51*" /> </colgroup>
<tbody>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Metrics</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Scala</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Java</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Python</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">R</span></p>
</td>
</tr>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Type</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Compiled</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Compiled</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Interpreted</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Interpreted</span></p>
</td>
</tr>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">JVM based</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Yes</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Yes</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">No</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">No</span></p>
</td>
</tr>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Verbosity</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less </span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">More </span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less</span></p>
</td>
</tr>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'"> Code Length</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less </span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">More </span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less </span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less </span></p>
</td>
</tr>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Productivity</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">High</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">High</span></p>
</td>
<td width="20%">
<p align="center"><span style="color: #222222"><span style="font-family: arial, sans-serif"><span style="font-size: medium">High</span></span></span></p>
</td>
</tr>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Scalability</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">High</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">High</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Less</span></p>
</td>
</tr>
<tr valign="top">
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">OOPS Support</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Yes</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Yes</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Yes</span></p>
</td>
<td width="20%">
<p align="center"><span style="font-family: 'Abyssinica SIL'">Yes</span></p>
</td>
</tr>
</tbody>
</table>
<h2></h2>
<h2>17. Install Apache Spark &amp; some basic concepts about Apache Spark</h2>
<p style="text-align: justify">To know the basics of Apache Spark and installation, please refer to my first <a href="https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/" target="_blank" rel="nofollow">article</a> on Pyspark. I have introduced basic terminologies used in Apache Spark like big data, cluster computing, driver, worker, spark context, In-memory computation, lazy evaluation, DAG, memory hierarchy and Apache Spark architecture in the previous article.</p>
<p style="text-align: justify">As a quick refresher, I will be explaining some of the topics which are very useful to proceed further. If you are a beginner, then I strongly recommend you to go through my first article before proceeding further.</p>
<ul>
<li><strong>Lazy operation:</strong> Operations which do not execute until we require results.</li>
<li><strong>Spark Context:</strong>  holds a connection with Spark cluster manager.</li>
<li style="text-align: justify"><strong>Driver and Worker:</strong> A driver is in charge of the process of running the main() function of an application and creating the SparkContext.</li>
<li style="text-align: justify"><strong>In-memory computation:</strong> Keeping the data in RAM instead of Hard Disk for fast processing.</li>
</ul>
<p>&nbsp;</p>
<p style="text-align: justify">Spark has three data representations viz RDD, Dataframe, Dataset. To use Apache Spark functionality, we must use one of them for data manipulation. Let&#8217;s discuss each of them briefly:</p>
<ul>
<li style="text-align: justify"><strong>RDD: </strong>RDD (Resilient Distributed Database) is a collection of elements, that can be divided across multiple nodes in a cluster for parallel processing. It is also fault tolerant collection of elements, which means it can automatically recover from failures. RDD is immutable, we can create RDD once but can’t change it.</li>
<li style="text-align: justify"><strong>Dataset: </strong>It is also a distributed collection of data. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). As I have already discussed in my previous articles, dataset API is only available in Scala and Java. It is not available in Python and R.</li>
<li style="text-align: justify"><strong>DataFrame:</strong> In Spark, a DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame. It is mostly used for structured data processing. In Scala, a DataFrame is represented by a Dataset of Rows. A DataFrame can be constructed by wide range of arrays for example, existing RDDs, Hive tables, database tables.</li>
<li><strong>Transformation:</strong> Transformation refers to the operation applied on a RDD to create new RDD.</li>
<li style="text-align: justify"><strong>Action:</strong> Actions refer to an operation which also apply on RDD that perform computation and send the result back to driver.</li>
<li><strong>Broadcast:</strong> We can use the Broadcast variable to save the copy of data across all node.</li>
<li><strong>Accumulator:</strong> In Accumulator, variables are used for aggregating the information.</li>
</ul>
<p>&nbsp;</p>
<h2>18. Working with RDD in Apache Spark using Scala</h2>
<p style="text-align: justify">First step to use RDD functionality is to create a RDD. In Apache Spark, RDD can be created by two different ways. One is from existing Source and second is from an external source.</p>
<p style="text-align: justify">So before moving further let&#8217;s open the Apache Spark Shell with Scala. Type the following command after switching into the home directory of Spark. It will also load the spark context as sc.</p>
<pre>$ ./bin/spark-shell</pre>
<p>After typing above command you can start programming of Apache Spark in Scala.</p>
<h3>18.1 Creating a RDD from existing source</h3>
<p style="text-align: justify">When you want to create a RDD from existing storage in driver program (which we would like to be parallelized). For example, converting an array to RDD, which is already created in a driver program.</p>
<pre>val data = Array(1, 2, 3, 4, 5,6,7,8,9,10)
val distData = sc.parallelize(data)</pre>
<p style="text-align: justify">In the above program, I first created an array for 10 elements and then I created a distributed data called RDD from that array using “parallelize” method. SparkContext has a parallelize method, which is used for creating the Spark RDD from an iterable already present in driver program.</p>
<p>To see the content of any RDD we can use “collect” method. Let&#8217;s see the content of distData.</p>
<pre>scala&gt; distData.collect()
Output: res1: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</pre>
<p>&nbsp;</p>
<h3>18.2 Creating a RDD from External sources</h3>
<p style="text-align: justify">You can create a RDD through external sources such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format. So let&#8217;s create a RDD from the text file:</p>
<p>The name of the text file is text.txt. and it has only 4 lines given below.<br />
I love solving data mining problems.<br />
I don&#8217;t like solving data mining problems.<br />
I love solving data science problems.<br />
I don&#8217;t like solving data science problems.</p>
<p>Let&#8217;s create the RDD by loading it.</p>
<pre>val lines = sc.textFile("text.txt");</pre>
<p>Now let&#8217;s see first two lines in it.</p>
<pre>lines.take(2)</pre>
<p>The output is received is as below:</p>
<pre>Output: Array(I love solving data mining problems., I don't like solving data mining problems)</pre>
<p>&nbsp;</p>
<h3>18.3 Transformations and Actions on RDD</h3>
<h4>18.3.1. Map Transformations</h4>
<p style="text-align: justify">A map transformation is useful when we need to transform a RDD by applying a function to each element. So how can we use map transformation on ‘rdd’ in our case?<br />
Let&#8217;s calculate the length (number of characters) of each line in “text.txt”</p>
<pre>val Lenght = lines.map(s =&gt; s.length)
Length.collect()</pre>
<p>After applying above map operation, we get the following output:</p>
<pre>Output: res6: Array[Int] = Array(36, 42, 37, 43)</pre>
<p>&nbsp;</p>
<h4>18.3.2 Count Action</h4>
<p>Let&#8217;s count the number of lines in RDD “lines”.</p>
<pre>lines.count()
res1: Long = 4</pre>
<p>The above action on “lines1” will give 4 as the output.</p>
<p>&nbsp;</p>
<h4>18.3.3 Reduce Action</h4>
<p>Let&#8217;s take the sum of total number of characters in text.txt.</p>
<pre>val totalLength = Length.reduce((a, b) =&gt; a + b)
totalLength: Int = 158</pre>
<p>&nbsp;</p>
<h4>18.3.4 flatMap transformation and reduceByKey Action</h4>
<p>Let&#8217;s calculate frequency of each word in “text.txt”</p>
<pre>val counts = lines.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_ + _)
counts.collect()
Output:
res6: Array[(String, Int)] = Array((solving,4), (mining,2), (don't,2), (love,2), (problems.,4), (data,4), (science,2), (I,4), (like,2))</pre>
<p>&nbsp;</p>
<h4>18.3.5 filter Transformation</h4>
<p>Let&#8217;s filter out the words in “text.txt” whose length is more than 5.</p>
<pre>val lg5 = lines.flatMap(line =&gt; line.split(" ")).filter(_.length &gt; 5)
Output:
res7: Array[String] = Array(solving, mining, problems., solving, mining, problems., solving, science, problems., solving, science, problems.)</pre>
<p>&nbsp;</p>
<h2>19. Working with DataFrame in Apache Spark using Scala</h2>
<p>A DataFrame in Apache Spark can be created in multiple ways:</p>
<ul>
<li>It can be created using different data formats. For example, by loading the data from JSON, CSV</li>
<li>Loading data from Existing RDD</li>
<li>Programmatically specifying schema</li>
</ul>
<p>Let&#8217;s create a DataFrame using a csv file and perform some analysis on that.</p>
<p style="text-align: justify">For reading a csv file in Apache Spark, we need to specify a new library in our Scala shell. To perform this action, first, we need to download Spark-csv package (Latest version) and extract this package into the home directory of Spark. Then, we need to open a PySpark shell and include the package ( I am using “spark-csv_2.10:1.3.0”).</p>
<pre>$ ./bin/spark-shell --packages com.databricks:spark-csv_2.10:1.3.0</pre>
<p>Now let&#8217;s load the csv file into a DataFrame df. You can download the file(train) from this <a href="https://datahack.analyticsvidhya.com/contest/black-friday/" target="_blank" rel="nofollow">link</a>.</p>
<pre>val df = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("train.csv")</pre>
<p>&nbsp;</p>
<h3>19.1 Name of columns</h3>
<p>Let&#8217;s see the name of columns in df by using “columns” method.</p>
<pre>df.columns
Output:
res0: Array[String] = Array(User_ID, Product_ID, Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years, Marital_Status, Product_Category_1, Product_Category_2, Product_Category_3, Purchase)</pre>
<p>&nbsp;</p>
<h3>19.2 Number of observations</h3>
<p>To see the number of observation in df you can apply “count” method.</p>
<pre>df.count()
Output:
res1: Long = 550068</pre>
<p>&nbsp;</p>
<h3>19.3 Print the columns datatype</h3>
<p>You can use “printSchema” method on df. Let&#8217;s print the schema of df.</p>
<pre>df.printSchema()
Output:
root
 |-- User_ID: integer (nullable = true)
 |-- Product_ID: string (nullable = true)
 |-- Gender: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Occupation: integer (nullable = true)
 |-- City_Category: string (nullable = true)
 |-- Stay_In_Current_City_Years: string (nullable = true)
 |-- Marital_Status: integer (nullable = true)
 |-- Product_Category_1: integer (nullable = true)
 |-- Product_Category_2: integer (nullable = true)
 |-- Product_Category_3: integer (nullable = true)
 |-- Purchase: integer (nullable = true)</pre>
<p>&nbsp;</p>
<h3>19.4 Show first n rows</h3>
<p>You can use “show” method on DataFrame. Let&#8217;s print the first 2 rows of df.</p>
<pre>df.show(2)
Output:
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
|1000001| P00069042| F|0-17| 10| A| 2| 0| 3| null| null| 8370|
|1000001| P00248942| F|0-17| 10| A| 2| 0| 1| 6| 14| 15200|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
only showing top 2 rows</pre>
<p>&nbsp;</p>
<h3>19.5 Subsetting or select columns</h3>
<p>To select columns you can use “select” method. Let&#8217;s apply select on df for “Age” columns.</p>
<pre>df.select("Age").show(10)
Output:
+-----+
| Age|
+-----+
| 0-17|
| 0-17|
| 0-17|
| 0-17|
| 55+|
|26-35|
|46-50|
|46-50|
|46-50|
|26-35|
+-----+
only showing top 10 rows</pre>
<p>&nbsp;</p>
<h3>19.6 Filter rows</h3>
<p style="text-align: justify">To filter the rows you can use “filter” method. Let&#8217;s apply filter on “Purchase” column of df and get the purchase which is greater than 10000.</p>
<pre>df.filter(df("Purchase") &gt;= 10000).select("Purchase").show(10)
+--------+
|Purchase|
+--------+
| 15200|
| 15227|
| 19215|
| 15854|
| 15686|
| 15665|
| 13055|
| 11788|
| 19614|
| 11927|
+--------+
only showing top 10 rows</pre>
<p>&nbsp;</p>
<h3>19.7 Group DataFrame</h3>
<p style="text-align: justify">To groupby columns, you can use groupBy method on DataFrame. Let&#8217;s see the distribution on “Age” columns in df.</p>
<pre style="text-align: justify">df.groupBy("Age").count().show()</pre>
<pre>Output:
+-----+------+ 
| Age| count|
+-----+------+
|51-55| 38501|
|46-50| 45701|
| 0-17| 15102|
|36-45|110013|
|26-35|219587|
| 55+| 21504|
|18-25| 99660|
+-----+------+</pre>
<p>&nbsp;</p>
<h3>19.8 Apply SQL queries on DataFrame</h3>
<p>To apply queries on DataFrame You need to register DataFrame(df) as table. Let&#8217;s first register df as temporary table called (B_friday).</p>
<pre>df.registerTempTable("B_friday")</pre>
<p style="text-align: justify">Now you can apply SQL queries on “B_friday” table using sqlContext.sql. Lets select columns “Age” from the “B_friday” using SQL statement.</p>
<pre>sqlContext.sql("select Age from B_friday").show(5)
+----+
| Age|
+----+
|0-17|
|0-17|
|0-17|
|0-17|
| 55+|
+----+</pre>
<p>&nbsp;</p>
<h2>20. Building a machine learning model</h2>
<p style="text-align: justify">If you have come this far, you are in for a treat! I&#8217;ll complete this tutorial by building a machine learning model.</p>
<p style="text-align: justify">I will use only three dependent features and the independent variable in df1. Let&#8217;s create a DataFrame df1 which has only 4 columns (3 dependent and 1 target).</p>
<pre>val df1 = df.select("User_ID","Occupation","Marital_Status","Purchase")
</pre>
<p>In above DataFrame df1 &#8220;User_ID&#8221;,&#8221;Occupation&#8221; and &#8220;Marital_Status&#8221; are features and “Purchase” is target column.</p>
<p style="text-align: justify">Let’s try to create a formula for Machine learning model like we do in R. First, we need to import RFormula. Then we need to specify the dependent and independent column inside this formula. We also have to specify the names for features column and label column.</p>
<pre>import org.apache.spark.ml.feature.RFormula
val formula = new RFormula().setFormula("Purchase ~ User_ID+Occupation+Marital_Status").setFeaturesCol("features").setLabelCol("label")</pre>
<p style="text-align: justify">After creating the formula, we need to fit this formula on df1 and transform df1 through this formula. Let&#8217;s fit this formula.</p>
<pre>val train = formula.fit(df1).transform(df1)</pre>
<p style="text-align: justify">After applying the formula we can see that train dataset has 2 extra columns called features and label. These are the ones we have specified in the formula (featuresCol=”features” and labelCol=”label”)</p>
<p>&nbsp;</p>
<h3>20.1 Applying Linear Regression on train</h3>
<p style="text-align: justify">After applying the RFormula and transforming the DataFrame, we now need to develop the machine learning model on this data. I want to apply a Linear Regression for this task. Let us import a Linear regression and apply on train. Before fitting the model, I am setting the hyperparameters.</p>
<pre>import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)
val lrModel = lr.fit(train)</pre>
<p>You can also make predictions on unseen data. But I am not showing this here. Let&#8217;s print the coefficient and intercept for linear regression.</p>
<pre>println(s"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}")
Output:
Coefficients: [0.015092115630330033,16.12117786898672,-10.520580986444338] Intercept: -5999.754797883323</pre>
<p>Let&#8217;s summarize the model over the training set and print out some metrics.</p>
<pre>val trainingSummary = lrModel.summary
Now, See the residuals for train's first 10 rows.
trainingSummary.residuals.show(10)
+-------------------+
| residuals|
+-------------------+
| -883.5877032522076|
| 5946.412296747792|
| -7831.587703252208|
| -8196.587703252208|
|-1381.3298625817588|
| 5892.776223171599|
| 10020.251134994305|
| 6659.251134994305|
| 6491.251134994305|
|-1533.3392694181512|
+-------------------+
only showing top 10 rows</pre>
<p>Now, let&#8217;s see RMSE on train.</p>
<pre>println(s"RMSE: ${trainingSummary.rootMeanSquaredError}") 
Output:
RMSE: 5021.899441991144</pre>
<p>Let&#8217;s repeat above procedure for taking the prediction on cross-validation set. Let&#8217;s read the train dataset again.</p>
<pre>val train = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("train.csv")</pre>
<p>Now, randomly divide the train in two part train_cv and test_cv</p>
<pre>val splits = train.randomSplit(Array(0.7, 0.3))
val (train_cv,test_cv) = (splits(0), splits(1))

</pre>
<p>Now, Transform train_cv and test_cv using RFormula.</p>
<pre>import org.apache.spark.ml.feature.RFormula
val formula = new RFormula().setFormula("Purchase ~ User_ID+Occupation+Marital_Status").setFeaturesCol("features").setLabelCol("label")</pre>
<pre>val train_cv1 = formula.fit(train_cv).transform(train_cv)
val test_cv1 = formula.fit(train_cv).transform(test_cv)</pre>
<p style="text-align: justify">After transforming using RFormula, we can build a machine learning model and take the predictions. Let&#8217;s apply Linear Regression on training and testing data.</p>
<pre>import org.apache.spark.ml.regression.LinearRegression
val lr = new LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)
val lrModel = lr.fit(train_cv1)
val train_cv_pred = lrModel.transform(train_cv1)
val test_cv_pred = lrModel.transform(test_cv1)</pre>
<p>In train_cv_pred and test_cv_pred, you will find a new column for prediction.</p>
<p>&nbsp;</p>
<h2>21. Additional Resources</h2>
<ul>
<li>In Scala there are some libraries which are specially written for Data Analysis purpose, refer this <a href="https://github.com/lauris/awesome-scala#science-and-data-analysis" target="_blank" rel="nofollow">link</a>.</li>
<li>If you want to learn Scala programming refer this <a href="http://docs.scala-lang.org/index.html" target="_blank" rel="nofollow">link</a>.</li>
<li>For quick introduction to the Spark API refer this <a href="https://spark.apache.org/docs/2.0.2/quick-start.html" target="_blank" rel="nofollow">link</a>.</li>
<li>For, Spark Programming Guide: refer this <a href="https://spark.apache.org/docs/2.0.2/programming-guide.html" target="_blank" rel="nofollow">link</a>.</li>
<li>To learn about Datasets, and DataFrames, Spark SQL. Refer this <a href="https://spark.apache.org/docs/2.0.2/sql-programming-guide.html" target="_blank" rel="nofollow">link</a>.</li>
</ul>

</body>
</html>
