<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" target=_blank>
<style type="text/css" target=_blank>a {text-decoration: none}</style>
</head>
<body bgcolor="#000000" text="#109030" leftmargin="10" topmargin="10" marginwidth="100" link="#00CCCC" vlink="#CC66CC" alink="#FFFF00" target=_blank>
<FONT size=3>

<h1>Integrating R with Apache Hadoop</h1>
<p>Integrating R to work on Hadoop is to address the requirement to scale R program to work with petabyte scale data. The primary goal of this post is to elaborate different techniques for integrating R with Hadoop.</p>
<h2>Approach 1: Using R and Streaming APIs in Hadoop</h2>
<p>In order to integrate an R function with Hadoop and see it running in a MapReduce mode, Hadoop supports Streaming APIs for R. These Streaming APIs primary help running any script that can access and operate with standard I/O in a map- reduce mode. So, in case of R, there wouldn&#8217;t be any explicit client side integration done with R. Following is an example for R and streaming:</p>
<pre>$ ${HADOOP_HOME}/bin/Hadoop jar
${HADOOP_HOME}/contrib/streaming/*.jar 
-inputformat
org.apache.hadoop.mapred.TextInputFormat 
-input input_data.txt 
-output output 
-mapper /home/tst/src/map.R 
-reducer /home/tst/src/reduce.R 
-file /home/tst/src/map.R 
-file /home/tst/src/reduce.R</pre>
<h2>Approach 2: Using Rhipe package of R</h2>
<p>There is a package in R called &#8220;Rhipe&#8221; that allows running a MapReduce job within R. To use this way of implementing R on Hadoop there are some pre-requisites. R needs to be installed on each Data node in the Hadoop Cluster, Protocol Buffers will be installed and available on each Data node (for more on Protocol Buffer refer  http://wiki.apache.org/hadoop/ProtocolBuffers) and Rhipe should be available on each data node.</p>
<p>Following is a sample format for using Rhipe library in R to implement MapReduce:</p>
<pre>library(Rhipe)
rhinit(TRUE, TRUE);
map&lt;-expression ( {lapply (map.values, function(mapper)…)})
reduce&lt;-expression(
pre = {…},
reduce = {…},
post = {…},
)
x &lt;- rhmr(map=map, reduce=reduce,
 ifolder=inputPath,
 ofolder=outputPath,
 inout=c(&#039;text&#039;, &#039;text&#039;),
 jobname=&#039;test name&#039;))
rhex(x)</pre>
<h2>Approach 3: Using RHADOOP</h2>
<p>RHadoop, very similar to RHipe, facilitates running R functions in a MapReduce mode. It is an open source library built by Revolution Analytics. Following are some packages the are a part of the RHadoop library. <code>plyrmr</code> apackage that provides functions for common data manipulation requirements for large datasets running on Hadoop. <code>rmr</code> a package that has collection of functions that integrate R and Hadoop. <code>rdfs</code> a package with functions that help interface R and HDFS. <code>rhbase</code> a package with functions that help interface R and HBase</p>
<p>Following is an example that uses <code>rmr</code> package and demonstrates the steps to integrate R and Hadoop using the functions from that package.</p>
<pre>library(rmr)
maplogic&lt;-function(k,v) { …}
reducelogic&lt;-function(k,vv) { …}
mapreduce( input =&quot;data.txt&quot;,
output=&quot;output&quot;,
textinputformat =rawtextinputformat,
map = maplogic,
reduce=reducelogic
)</pre>
<h2>Summary of R / Hadoop integration approaches</h2>
<p>In summary, all the above three approaches yield results and facilitate integrating R and Hadoop and help scaling R to operate on large scale data that will be help on HDFS and each of these approaches has pros and cons. </p>
<h3>Below is a summary of conclusions:</h3>
<p>Hadoop Streaming API is the simplest of all the approaches as there are not any complications in terms of installation and set-up requirements. Both Rhipe and RHadoop requires some effort to set up R and related packages on the Hadoop cluster. In terms of implementation approach Streaming API is more of a command line map and reduce functions are inputs to the function while both Rhipe and RHadoop allows developers to define and call custom MapReduce function within R. In case of Hadoop Streaming API, there is no client side integration required while both Rhipe and RHadoop require client side integration. The other alternatives to scaling machine learning are Apache Mahout, Apache Hive and some commercials versions of R from Revolution Analytics, Segue framework and others.</p>
<p>This post is an extract from my latest publication on <a href="https://www.packtpub.com/big-data-and-business-intelligence/practical-machine-learning" onclick="_gaq.push(['_trackEvent', 'outbound-article', 'https://www.packtpub.com/big-data-and-business-intelligence/practical-machine-learning', 'Machine learning']);" rel="nofollow" ref="nofollow" target="_blank">Machine learning</a>.</p>


<a href="http://www.r-bloggers.com/using-hadoop-streaming-api-to-perform-a-word-count-job-in-r-and-c/">
Using Hadoop Streaming API to perform a word count job in R and C++
</a>
<br><br>
<a href="http://www.r-bloggers.com/how-can-r-and-hadoop-be-used-together/">
How can R and Hadoop be used together?
</a>
