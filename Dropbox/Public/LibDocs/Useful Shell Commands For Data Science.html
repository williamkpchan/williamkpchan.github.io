<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<style>

html {
  overflow:   scroll;
}
::-webkit-scrollbar {
    width: 0px;
    background: transparent; /* make scrollbar transparent */
}

body {
 margin: auto;
 width: 70%;
 font-size: 24px;
 background-color: #000000;
 color: #20C030;
}
a { text-decoration: none; 
	color: #28B8B8; }
a:visited {	color: #389898; }
A:hover {	color: yellow; }
A:focus {	color: red; }
code { color: gray;  background-color: #001010}
pre { color: gray;  background-color: #001010;  font-size: 16px; }
img { display: inline-block; width: 700px; margin-top: 2%;margin-bottom: 5%;}
</style>

</head>
<body>
<br>
<h1>Useful Shell Commands For Data Science</h1><br>
Which shell commands do data scientists use nearly every day? Discover and learn how to use them in this tutorial!<br>
There are many scenarios where you need to quickly analyze, modify and process large files, both in number and size. However, files are often text-based Comma Separated Values (CSV) files. Opening them with standard spreadsheet applications, such as Excel, LibreOffice or OpenOffice, overloads a machine's memory. Also, processing a large number of files in one batch often fails after a few hours because of some unexpected file anomaly. <br>
You inevitably spend a lot of time in frozen screens, restarts and long waits. <br>
However, most of these tasks could be carried out with a few lines of code. What's more, familiarity with a few simple shell command lines can go a long way in saving time and reducing frustration.<br>
This post will give an overview of some shell commands that I use nearly every day. I hope you find them useful.<br>
<h2>Quick Notes</h2><br>
<strong><br>
Note<br>
</strong><br>
 that there are different <br>
<a href="https://www.quora.com/What-is-difference-between-bash-zsh-tcsh-sh-etc-Which-one-should-I-use"><br>
types of shell</a> (bash, zsh, ...), with bash shell the most common as it is the default shell on OS X and major linux distributions. <a href="http://www.zsh.org/">Zsh</a> (and <a href="http://ohmyz.sh/">Oh My Zsh</a><br>
) is a popular and powerful alternative. The shell commands that are included in this blog post have been tested on bash on OS X (macOS) and should work with other shells and environments.<br>
All the following examples are based on the <br>
<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/adult"><br>
adult dataset<br>
</a><br>
 from the UCI Machine Learning repository also known as &quot;Census Income&quot; dataset and available <br>
<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"><br>
here<br>
</a><br>
. This data set is commonly used to predict whether income exceeds \$50K/yr based on census data. With 48842 rows and 14 attributes, it is not a large dataset by far but will be sufficient to illustrate the examples. Although the data is stored with the <br>
<code>.data</code> extension, it is a well-formatted CSV file. Feel free to download the dataset to follow along!<br>
<br>
<h2 id="count-with-wc-">Count with <code>wc</code></h2><br>
Given a new text based file, you want to know how many lines it contains. This can be done with the word count <br>
<code>wc -l</code> command:<br>
<code>$ wc -l adult.data  32562 adult.data</code><br>
</pre><br>
<br>
Which tells you that <br>
<code>adult.data</code><br>
 file contains 32562 rows. The <br>
<code>-l</code><br>
 flag tells <br>
<code>wc</code><br>
 to count the lines. But you can also use <br>
<code>wc</code><br>
 to count words instead using the <br>
<code>-w</code><br>
 flag.<br>
<br>
<code>$ wc -w adult.data<br>
  488415 adult.data<br>
</code><br>
<br>
</pre><br>
<br>
The <br>
<code>adult.data</code><br>
 file contains nearly 500k words.<br>
The <br>
<code>wc</code><br>
 command can also count the number of files in a directory by using the output of a simple <br>
<code>ls  -l</code><br>
 command as input to <br>
<code>wc</code><br>
. Using the output of a command as input to another command using the pipe symbol <br>
<code>|</code><br>
  is a useful shell pattern called <br>
<a href="https://en.wikipedia.org/wiki/Pipeline_%28Unix%29"><br>
pipelining<br>
</a><br>
. You'll see it in several examples throughout this post.<br>
Assume now you have a folder with many files. The following command will tell you exactly how many file it contains:<br>
<br>
<code>$ ls -l &lt;folder&gt; | wc -l<br>
 508<br>
</code><br>
<br>
</pre><br>
<br>
There are many other applications for <br>
<code>wc</code><br>
 if you start adding wildcards and subfolders.<br>
Let's go back to your <br>
<code>adult.data</code><br>
 file and look at the first lines with the <br>
<code>head</code><br>
 command. By default, <br>
<code>head</code><br>
 outputs 10 lines, you limit the output to the first 2 lines using the <br>
<code>-n</code><br>
 flag.<br>
<br>
<code>$ head -n 2 adult.data<br>
39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K<br>
50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &lt;=50K<br>
</code><br>
<br>
</pre><br>
<br>
And you notice that the file does not have a header line. The names of columns are not within the file. You can add that header line by concatenating two files using the <br>
<code>cat</code><br>
 command.<br>
<h2 id="concatenate-files-with-cat-"><br>
Concatenate files with <br>
<code>cat</code><br>
<br>
</h2><br>
The <br>
<code>cat</code><br>
 command prints a file's content to the standard output (aka your terminal). It can also be used to concatenate a series of files. The command <br>
<code>cat file_1.csv file_2.csv &gt; target_file.csv</code><br>
 will merge the content of both <br>
<code>file_1.csv</code><br>
 and <br>
<code>file_2.csv</code><br>
  into <br>
<code>target_file.csv</code><br>
, adding <br>
<code>file_2.csv</code><br>
 at the end of <br>
<code>file_1.csv</code><br>
.<br>
The header file is not in the original dataset and you need to create it. To do so, you <br>
<code>echo</code><br>
 the comma separated list of column names into a <br>
<code>header.csv</code><br>
 file.<br>
<br>
<code>$ echo &quot;age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class&quot; &gt; header.csv<br>
</code><br>
<br>
</pre><br>
<br>
In this example, you used the shell redirection <br>
<code>&gt;</code><br>
 character to dump the output of <br>
<code>cat</code><br>
 to the <br>
<code>adult.csv</code><br>
 file. The <br>
<code>&gt;</code><br>
 will either create a file or replace its content entirely if the file already exists. Doubling the symbol <br>
<code>&gt;&gt;</code><br>
 will append the new content to an already existing file without erasing its content.<br>
Let's now add the <br>
<code>header.csv</code><br>
 file at the beginning of the <br>
<code>adult.data</code><br>
 file. At the same time, you rename <br>
<code>adult.data</code><br>
 to <br>
<code>adult.csv</code><br>
 since it is, after all, a CSV formatted file.<br>
<br>
<code>$ cat header.csv adult.data &gt; adult.csv<br>
</code><br>
<br>
</pre><br>
<br>
Check that the first row of <br>
<code>adult.csv</code><br>
 contains the column names with:<br>
<br>
<code>$ head -n 1 adult.csv<br>
age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,native-country,class<br>
39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K<br>
</code><br>
<br>
</pre><br>
<br>
<h2 id="modify-a-file-with-sed-"><br>
Modify a file with <br>
<code>sed</code><br>
<br>
</h2><br>
Another frequent data mining scenario happens when a file is corrupted or badly formatted, such as with non UTF-8 characters or a misplaced comma. You can correct that file without actually opening it using the <br>
<code>sed</code><br>
 command.<br>
The generic <br>
<code>sed</code><br>
 pattern is<br>
<br>
<code>$ sed &quot;s/&lt;string to replace&gt;/&lt;string to replace it with&gt;/g&quot; &lt;source_file&gt; &gt; &lt;target_file&gt;.<br>
</code><br>
<br>
</pre><br>
<br>
The <br>
<code>adult.csv</code><br>
 dataset uses the <br>
<code>?</code><br>
 character to denote a missing value. You can replace that character with a better suited default value using <br>
<code>sed</code><br>
. The empty string is preferable as an indication of a missing value as it will be interpreted as a <br>
<code>NaN</code><br>
 value when <br>
<a href="pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"><br>
loading the data<br>
</a><br>
 in a Pandas DataFrame.<br>
<br>
<code>$ grep &quot;, ?,&quot; adult.csv | wc -l<br>
2399<br>
</code><br>
<br>
</pre><br>
<br>
This gives you a count of 2399 lines with at least one column with a missing value denoted by <br>
<code>?</code><br>
. The following command will replace all the columns with <br>
<code>?</code><br>
 by an empty string. All the cell that only contain the <br>
<code>?</code><br>
, followed by a space will now be truly empty.<br>
<br>
<code>$ sed &quot;s/, ?,/,,/g&quot; adult.csv &gt;  adult.csv<br>
</code><br>
<br>
</pre><br>
<br>
Note that you use the column delimiter <br>
<code>,</code><br>
 in the source and target strings to avoid replacing legit question marks that could be present elsewhere in the dataset.<br>
<h2 id="subset-a-large-file"><br>
Subset a large file<br>
</h2><br>
Imagine now that you're dealing with a file with over 30 Million rows. As you craft your Python or R script, you will want to test your workflow on just a sample of that original large file and not have to load the whole dataset into memory. You can use a combination of <br>
<code>head</code><br>
 and <br>
<code>tail</code><br>
 to create a subsample of the original dataset.<br>
<ul><br>
<li><br>
<br>
<code>head</code><br>
 outputs the first part of files;<br>
</li><br>
<li><br>
<br>
<code>tail</code><br>
 outputs the last part of files<br>
</li><br>
</ul><br>
As you've seen before, these commands take a <br>
<code>-n</code><br>
 flag, which stands for <br>
<code>--lines</code><br>
, that restrict the number of lines in the output. <br>
<code>tail</code><br>
 also takes a <br>
<code>-f</code><br>
 flag where <br>
<code>f</code><br>
 stands for <br>
<code>--follow</code><br>
 which outputs appended data as lines are added to the end of the file. This is particularly convenient to monitor log files as your scripts are running using <br>
<code>tail -f &lt;logfile&gt;</code><br>
.<br>
Mixing in the pipe symbol <br>
<code>|</code><br>
 with <br>
<code>head</code><br>
 and <br>
<code>tail</code><br>
, you can extract a certain number of lines from within a source file and export the content to a subset file. For example, to extract 20 lines starting at line 100:<br>
<br>
<code>$ head -n 120 adult.csv | tail -n 20 &gt; adult_sample.csv<br>
</code><br>
<br>
</pre><br>
<br>
Note that you first take the <br>
<strong><br>
first_line + number_of_lines<br>
</strong><br>
 with <br>
<code>head</code><br>
 and then <br>
<code>tail</code><br>
 the <br>
<strong><br>
number_of_lines<br>
</strong><br>
. The generic subsetting command is:<br>
<br>
<code>$ head -n &lt;total_lines&gt; &lt;source_file&gt; | tail -n &lt;number_of_lines&gt; &gt; &lt;target_file&gt;<br>
</code><br>
<br>
</pre><br>
<br>
where <br>
<code>total_lines = first_line + number_of_lines</code><br>
. However, your new sampled file no longer contains the header row. We already know how to add the header back in the subset file using <br>
<code>cat</code><br>
 to concatenate the subset file and the <br>
<code>header.csv</code><br>
 file created before:<br>
<br>
<code>$ cat adult_sample.csv header.csv &gt; adult_sample_with_header.csv<br>
</code><br>
<br>
</pre><br>
<br>
Note that you did not use the source filename <br>
<code>adult\_sample.csv</code><br>
 as the target filename, but instead created a new file titled <br>
<code>adult\_sample\_with\_header.csv</code><br>
. Having the same name as one of the sources and as the target while using <br>
<code>cat</code><br>
 will result in unexpected file content. It is better to create a new file as the output of the <br>
<code>cat</code><br>
 command.<br>
<h2 id="finding-duplicates-with-uniq-"><br>
Finding duplicates with <br>
<code>uniq</code><br>
<br>
</h2><br>
With the <br>
<code>uniq</code><br>
 command you can find adjacent repeated lines in a file.  <br>
<code>uniq</code><br>
 takes several flags, the more useful ones being:<br>
<ul><br>
<li><br>
<br>
<code>uniq -c</code><br>
: which adds the repetition count to each line;<br>
</li><br>
<li><br>
<br>
<code>uniq -d</code><br>
: which only outputs duplicate lines; And<br>
</li><br>
<li><br>
<br>
<code>uniq -u</code><br>
: which only outputs unique lines.<br>
</li><br>
</ul><br>
However, <br>
<code>uniq</code><br>
 is not a smart command. Repeated lines will not be detected if they are not adjacent. Which means that you first need the <br>
<code>sort</code><br>
 the file. This command counts the number of duplicated lines in <br>
<code>adult.csv</code><br>
.<br>
<br>
<code>$ sort adult.csv | uniq -d | wc -l<br>
23<br>
</code><br>
<br>
</pre><br>
<br>
and shows that there are 23 duplicates. The next command takes the output of all lines with added repetition counts, sorts in reverse and outputs the first 3 duplicates:<br>
<br>
<code>$ sort adult.csv | uniq -c | sort -r | head -n 3<br>
3 25, Private, 195994, 1st-4th, 2, Never-married, ...<br>
2 90, Private, 52386, Some-college, 10, Never-married, ...<br>
2 49, Self-emp-not-inc, 43479, Some-college, 10, Married-civ-spouse, ...<br>
</code><br>
<br>
</pre><br>
<br>
There are many powerful options that can be obtained by combining <br>
<code>sort</code><br>
 and <br>
<code>uniq</code><br>
 with different flags. Use the <br>
<code>man sort</code><br>
 and <br>
<code>man uniq</code><br>
 to further explore these commands.<br>
<h2 id="selecting-columns-with-cut-"><br>
Selecting columns with <code>cut</code><br>
<br>
</h2><br>
The great thing about CSV files and shell commands is that you can also work at the column level by using <br>
<code>cut</code> to select a particular column. <br>
<code>cut</code> takes two main flags: <br>
<code>-d</code> to specify the column delimiter and <br>
<code>-f</code> to specify the columns you want to work on. In the following example, you use <br>
<code>cut</code> to find the number of unique values taken by the categorical variable <br>
<code>workclass</code> (column 2).<br>
First select the column <code>workclass</code> and pipe to head to verify that you have the right column:<br>
<br>
<code>$ cut -d &quot;,&quot; -f 2 adult.csv | head -3<br>
workclass<br>
State-gov<br>
Self-emp-not-inc<br>
</code><br>
<br>
</pre><br>
<br>
Now, to count uniques, you sort the output of cut and pipe the result to <br>
<code>uniq -c</code><br>
, as such:<br>
<br>
<code>$  cut -d &quot;,&quot; -f 2 adult.csv | sort | uniq -c<br>
1837<br>
 960  Federal-gov<br>
2093  Local-gov<br>
   7  Never-worked<br>
22696  Private<br>
1116  Self-emp-inc<br>
2541  Self-emp-not-inc<br>
1298  State-gov<br>
  14  Without-pay<br>
   1 workclass<br>
</code><br>
<br>
</pre><br>
<br>
Which tells you, for instance, that you have 1837 null values, and that the main class is by far the <br>
<code>Private</code><br>
 class with 22969 occurrences.<br>
The command line above is similar to the <br>
<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"><br>
<br>
<code>value_counts()</code><br>
<br>
</a><br>
 method applied to a DataFrame containing the <br>
<code>adult.csv</code><br>
 data.<br>
<h2 id="looping">Looping</h2><br>
So far, you've mostly worked on one file. To rename, process or transfer large number of files, you should add loops to our shell toolkit. The generic format of a loop in bash shell is:<br>
<code>while true; do<br>
    _do something_ ;<br>
done</code><br>
</pre><br>
<br>
Let's put that to work. Imagine you have 1000 files with spaces in their filename and you want to replace each space by an undescore &quot;_&quot;.<br>
<br>
<code>replace_source=' '<br>
replace_target='_'<br>
for filename in ./*.csv; do<br>
    new_filename=${filename//$replace_source/$replace_target}<br>
    mv &quot;$filename&quot; &quot;$new_filename&quot;<br>
done<br>
</code><br>
</pre><br>
In this example,<br>
<ul><br>
<li><br>
You first declare two variables: <br>
<code>replace_source</code> and <code>replace_target</code> as placeholders for the space and the underscore characters<br>
</li><br>
<li><br>
You loop over all the <br>
<code>*.csv</code><br>
 files in the current folder<br>
</li><br>
<li><br>
for each <code>filename</code>, you create a <code>new_filename</code> by replacing each space by an underscore<br>
</li><br>
<li><br>
and you rename the file from its current filename to the new filename with the <br>
<code>mv</code><br>
 command<br>
</li><br>
</ul><br>
In fact, not only have you used shell to loop through files in a directory, but you also have created variables.<br>
<h2 id="variables"><br>
Variables<br>
</h2><br>
You will end this intro to shell command with variables. Variable creation in shell is simply done by<br>
<code>$ varname='&lt;a string&gt;'<br>
$ varname=a number</code><br>
<br>
</pre><br>
For instance:<br>
<code>$ varname='Hello world'<br>
$ varname=123.4</code><br>
</pre><br>
<br>
Note that there is no space around the '=' sign. <br>
<code>var = '&lt;a string&gt;'</code><br>
 would not work. To return the variable value, simply <br>
<code>echo</code> it:<br>
<code>$ echo $varname<br>
123.4</code><br>
</pre><br>
<br>
And to use it in a script, encapsulates it within quotes as you've seen before:<br>
<br>
<code>$ mv &quot;$filename&quot; &quot;$new_filename&quot;<br>
</code><br>
<br>
</pre><br>
<br>
Writing loops and using variables opens the door for more complex file manipulations and is a gateway to shell scripting which is beyond this introductory article. However, shell scripting is simple enough if you start small and improve as your confidence grows.<br>
<h2 id="conclusion"><br>
Conclusion<br>
</h2><br>
Shell command lines are extremely useful in your daily work as data scientists. There are many more examples and use cases that could be explored. And as you will see there are often always different <br>
<a href="https://stackoverflow.com/questions/1806868/linux-replacing-spaces-in-the-file-names"><br>
ways<br>
</a><br>
 to achieve a particular result using a wide range of available shell commands and related flags. In that case, keeping it simple is always the winning strategy.<br>
