<base target="_blank">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<style type="text/css">

body {
 margin-top: 2%;
 margin-bottom: 2%;
 margin-right: 10%;
 margin-left: 10%;
 background-color: #000000;
 color: #109080;
 font-size: 24px;
}
a { text-decoration: none;
	color: #28B8B8;}
a:visited {	color: #389898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: pink; background-color: #302030}
pre { color: gray; background-color: #001010}
</style>
</head>
<body>
<h1>Why are GPUs necessary for training Deep Learning models?</h1>

<h2>Introduction</h2>
<p>Most of you would have heard exciting stuff happening using deep learning. You would have also heard that Deep Learning requires a lot of hardware. I have seen people training a simple deep learning model for days on their laptops (typically without GPUs) which leads to an impression that Deep LearningÂ requires big systems to run execute.</p>
<p>However, this is only partly true and this creates a myth around deep learning which creates a roadblock for beginners. Numerous people have asked me as to what kind of hardware would be better for doing deep learning. With this article, I hope to answer them.</p>
<p><em>Note: I assume that you have fundamental knowledge of deep learning concepts. If not, you should go through <a href="https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/" target="_blank" rel="noopener noreferrer">this article</a>.</em></p>
<p>&nbsp;</p>
<h2>Table of Contents</h2>
<ul>
<li>Fact #101: DL requires <del>a lot of</del> hardware</li>
<li>Training a deep learning model</li>
<li>How to train your model faster?</li>
<li>CPU vs GPU</li>
<li>Brief History of GPUs &#8211; how did we reach here</li>
<li>Which GPU to use today?</li>
<li>The future looks exciting</li>
</ul>
<p>&nbsp;</p>
<h2>Fact #101: Deep Learning requires <del>a lot of</del> hardware</h2>
<p>When I first got introduced with deep learning, I thought that deep learning necessarily needs large Datacenter to run on, and &#8220;deep learning experts&#8221; would sit in their control rooms to operate these systems.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/6/67/Inside_Suite.jpg" width="1000" height="655" /></p>
<p>This is because every book that I referred or every talk that I heard, the author or speaker always say that deep learning requires a lot of computational power to run on. But when I built my first deep learning model on my meager machine, I felt relieved! I don&#8217;t have to take over Google to be a deep learning expert ðŸ˜€</p>
<p>This is a common misconception that every beginner faces when diving into deep learning. Although, it is true that deep learning needs considerable hardwareÂ to run efficiently, you don&#8217;t need it to be infinite to do your task. You can even run deep learning models on your laptop!</p>
<p>Just a smallÂ disclaimer; the smaller your system, more is the time you will need to get a trained model which performs good enough. You may basically look like this:</p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2016/10/dl_meme2.jpg" alt="" width="259" height="286" /></p>
<p>&nbsp;</p>
<p>Let&#8217;s just ask ourselves a simple question; why do we need more hardware for deep learning?</p>
<p>The answer is simple, deep learning is an<strong> algorithm</strong> &#8211; a software construct. We define an artificial neural network in our favorite programming language which would then be converted into a set of commands that run on the computer.</p>
<p>If you would have to guess which components of neural network do you think would require intense hardware resource, what would be your answer?</p>
<p>A few candidates from top of my mind are:</p>
<ul>
<li>Preprocessing input data</li>
<li>Training the deep learning model</li>
<li>Storing the trained deep learning model</li>
<li>Deployment of the model</li>
</ul>
<p>Among all these, training the deep learning model is the most intensive task. Lets see in detail why is this so.</p>
<p>&nbsp;</p>
<h2>Training a deep learning model</h2>
<p>When you train a deep learning model, two main operations are performed:</p>
<ul>
<li>Forward Pass</li>
<li>Backward Pass</li>
</ul>
<p>In forward pass, input is passed through the neural network and after processing the input, an output is generated. Whereas in backward pass, we update the weights of neural network on the basis of error we get in forward pass.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/b/b6/Artificial_neural_network.png">
</p>
<p>Both of these operations are essentially matrix multiplications. A simple matrix multiplication can be represented by the image below</p>
<p><img src="https://s-media-cache-ak0.pinimg.com/736x/b4/9e/54/b49e544ec297e6e27906ebe1c1fde28a.jpg" alt="" width="381" height="293" /></p>
<p>Here, we can see that each element in one row of first array is multiplied with one column of second array. So in a neural network, we can consider first array as input to the neural network, and the second array can be considered as weights of the network.</p>
<p>This seems to be a simple task. Now just to give you a sense of what kind of scale deep learning &#8211; VGG16 (a convolutional neural network of 16 hidden layers which is frequently used in deep learning applications) has ~140 million parameters; aka weights and biases. Now think of all the matrix multiplications you would have to do to pass just one input to this network! It would take years to train this kind of systems if we take traditional approaches.</p>
<p>&nbsp;</p>
<h2>How to train your neural net faster?</h2>
<p>We saw that the computationally intensive part of neural network is made up of multiple matrix multiplications. So how can we make it faster?</p>
<p>We can simply do this by doing all the operations at the same time instead of doing it one after the other. This is in a nutshell why we use GPU (graphics processing units) instead of a CPU (central processing unit) for training a neural network.</p>
<p>To give you a bit of an intuition, we go back to history when we proved GPUs were better than CPUs for the task.</p>
<p>Before the boom of Deep learning, Google had a extremely powerful system to do their processing, which they had specially built for training huge nets. This system was monstrous and was of $5 billion total cost, with multiple clusters of CPUs.</p>
<p>Now researchers at Stanford built the same system in terms of computation to train their deep nets using GPU. And guess what; they reduced the costs to just $33K ! This system was built using GPUs, and it gave the same processing power as Google&#8217;s system. Pretty impressive right?</p>
<p>&nbsp;</p>
<table>
<tbody>
<tr>
<td style="text-align: center;" width="208"></td>
<td style="text-align: center;" width="208"><strong>Google</strong></td>
<td style="text-align: center;" width="208"><strong>Stanford</strong></td>
</tr>
<tr>
<td style="text-align: center;" width="208"><strong>Number of cores</strong></td>
<td style="text-align: center;" width="208">1K CPUs = 16K cores</td>
<td style="text-align: center;" width="208">3GPUs = 18K cores</td>
</tr>
<tr>
<td style="text-align: center;" width="208"><strong>Cost</strong></td>
<td style="text-align: center;" width="208">$5B</td>
<td style="text-align: center;" width="208">$33K</td>
</tr>
<tr>
<td style="text-align: center;" width="208"><strong>Training time</strong></td>
<td style="text-align: center;" width="208">week</td>
<td style="text-align: center;" width="208">week</td>
</tr>
</tbody>
</table>
<p>We can see that GPUs rule. But what exactly is the difference between a CPU and a GPU?</p>
<p>&nbsp;</p>
<h2>Difference between CPU and GPU</h2>
<p>To understand the difference, we take a classic analogy which explains the difference intuitively.</p>
<p>Suppose you have to transfer goods from one place to the other. You have an option to choose between a Ferrari and a freight truck.</p>
<p>Ferrari would be extremely fast and would help you transfer a batch of goods in no time. But the amount of goods you can carry is small, and usage of fuel would be very high.</p>
<p>A freight truck would be slow and would take a lot of time to transfer goods. But the amount of goods it can carry is larger in comparison to Ferrari. Also, it is more fuel efficient so usage is lower.</p>
<p>So which would you chose for your work?</p>
<p>Obviously, you would first see what the task is; if you have to pick up your girlfriend urgently, you would definitely choose a Ferrari over a freight truck. But if you are moving your home, you would use a freight truck to transfer the furniture.</p>
<p>Here&#8217;s how you would technically differentiate the two:</p>
<p><img src="https://image.slidesharecdn.com/w69ejoigsewjwetvbmwx-signature-600322f58dd0972f78d9395dd13259e83b846b0f09d6bd4f43956745ecd4e6f8-poli-150502101952-conversion-gate02/95/gpu-power-consumption-and-performance-trends-14-638.jpg?cb=1430578632" width="638" height="359" /></p>
<p style="text-align: center;"><a href="https://www.slideshare.net/AlessioVillardita/ca-1st-presentation-final-published" target="_blank" rel="nofollow noopener noreferrer">Source</a></p>
<p>Here&#8217;s another video which would make your concept even clearer.</p>
<p><iframe width="500" height="281" src="https://www.youtube.com/embed/-P28LKWTzrI?feature=oembed&amp;width=500&amp;height=750" frameborder="0" allowfullscreen></iframe></p>
<p><em>Note: GPU is mostly used for gaming and doing complex simulations. These tasks and mainly graphics computations, and so GPU is graphics processing unit. If GPU is used for non-graphical processing, they are termed as GPGPUs &#8211; general purpose graphics processing unit</em></p>
<p>&nbsp;</p>
<h2>Brief History of GPUs &#8211; how did we reach here</h2>
<p>Now, you might be asking this question that why are GPUs so much rage right now. Let us travel through a brief history of development of GPUs</p>
<p>Basically a GPGPU is a parallel programming setup involving GPUs &amp; CPUs which can process &amp; analyze data in a similar way to image or other graphic form. GPGPUs were created for better and more general graphic processing, but were later found to fit scientific computing well. This is because most of the graphic processing involves applying operations on large matrices.</p>
<p>The use of GPGPUs for scientific computing started some time back in 2001 with implementation of Matrix multiplication. One of the first common algorithm to be implemented on GPU in faster manner was LU factorization in 2005. But, at this time researchers had to code every algorithm on a GPU and had to understand low level graphic processing.</p>
<p>In 2006, Nvidia came out with a high level language CUDA, which helps you write programs from graphic processors in a high level language. This was probably one of the most significant change in they way researchers interacted with GPUs</p>
<p>&nbsp;</p>
<h2>Which GPU to use today?</h2>
<p>Here I will quickly give a few know-hows before you go on to buy a GPU for deep learning.</p>
<p><strong>Scenario 1:</strong></p>
<p>The first thing you should determine is what kind of resource does your tasks require. If your tasks are going to be small or can fit in complex sequential processing, you don&#8217;t need a big system to work on. You could even skip the use of GPUs altogether. So, if you are planning to work mainly on &#8220;other&#8221; ML areas / algorithms, you don&#8217;t necessarily need a GPU.</p>
<p>&nbsp;</p>
<p><strong>Scenario 2:</strong></p>
<p>If your task is a bit intensive, and has a handle-able data, a reasonable GPU would be a better choice for you. I generally use my laptop to work on toy problems, which has a slightly out of date GPU (a 2GB Nvidia GT 740M). Having a laptop with GPU helps me run things wherever I go. There are a few high end (and expectedly heavy) laptops with Nvidia GTX 1080 (a 8 GB VRAM) which you can check out at the extreme.</p>
<p>&nbsp;</p>
<p><strong>Scenario 3:</strong></p>
<p>If you are regularly working on complex problems or are a company which leverages deep learning, you would probably be better off building a deep learning system or use a cloud service like AWS or FloydHub. We at Analytics Vidhya built a deep learning system for ourselves, for which we shared our specifications. Here&#8217;s <a href="https://www.analyticsvidhya.com/blog/2016/11/building-a-machine-learning-deep-learning-workstation-for-under-5000/" target="_blank" rel="noopener noreferrer">the article.</a></p>
<p>&nbsp;</p>
<p><strong>Scenario 4:</strong></p>
<p>If you are Google, you probably need another datacenter to sustain! Jokes aside, if your task is of a bigger scale than usual, and you have enough pocket money to cover up the costs; you can opt for a GPU cluster and do multi-GPU computing. There are also some options which may be available in the near future &#8211; like TPUs and faster FPGAs, which would make your life easier.</p>
<p>&nbsp;</p>
<h2>The future looks exciting</h2>
<p>As mentioned above, there is a lot of research and active work happening to think of ways to accelerate computing. Google is expected to come out with Tensorflow Processing Units (TPUs) later this year, which promises an acceleration over and above current GPUs.</p>
<p>Similarly Intel is working on creating faster FPGAs, which may provide higher flexibility in coming days. In addition, the offerings from Cloud service providers (e.g. AWS) is also increasing. We will see each of them emerge in coming months.</p>
<p>&nbsp;</p>
<h2>End Notes</h2>
<p>In this article, we covered the motivations of using a GPU for deep learning applications and saw how to choose them for your task. I hope this article was helpful to you. If you have any specific questions regarding the topic, feel free to comment below or ask them on <a href="http://discuss.analyticsvidhya.com/" target="_blank" rel="nofollow noopener noreferrer">discussion portal</a>.</p>


</body>
</html>
