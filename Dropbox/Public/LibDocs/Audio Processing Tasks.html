<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: gray; background-color: #001010; font-size: 18px;}
pre { color: gray; background-color: #001010; font-size: 18px;}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
img{
	margin-top:1%;
	margin-bottom:2%;
}
.topic{
    color: lime;
}
.goldsha {
    color: white;
    border: 1px solid gold;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px gold inset;
}
.redsha {
    color: gold;
    border: 1px solid red;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px red inset;
}
.whitesha {
    color: red;
    border: 1px solid white;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -3px -2px 3px white inset;
}
.orangesha {
    color: yellow;
    border: 1px solid orange;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px orange inset;
}
.yellowsha {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
	display: inline-block;
}
.greensha {
    color: lightblue;
    border: 1px solid green;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px green inset;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.yellowbord {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
}
.bluebord {
    color: white;
    border: 1px solid lightblue;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px silver inset;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 15%;
	margin-right: 15%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.mywords{
    color: Crimson;
}
.orangeword{
    color: orange;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>


</head>
<body>

<center><h1>Audio Processing Tasks</h1></center>
<div id="toc"><ul></ul></div>
<br>
<br>
<br>

<h2>1. Audio Classification</h2>
<p>Audio classification is a fundamental problem in the field of audio processing. The task is essentially to extract features from the audio, and then identify which set of class the audio belongs to. Many useful applications pertaining to audio classification can be found in the wild &#8211; such as genre classification, instrument recognition and artist identification.</p>
<p>The task is also the most explored topic in audio processing with many papers published on the topic in the last year. In fact, we have also hosted a <a href="https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification/" target="_blank" rel="nofollow noopener">practice hackathon</a> for community collaboration in solving this particular task.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="http://ieeexplore.ieee.org/document/5664796/?reload=true" target="_blank" rel="nofollow noopener">http://ieeexplore.ieee.org/document/5664796/?reload=true</a></p>
<p>A common approach to solve an audio classification task is to pre-process the audio inputs to extract useful features, and then apply a classification algorithm on it. For example, in the case study below &#8211; given a 5 second excerpt of a sound the task is to identify which class does belong to &#8211; whether it is a dog barking or a drilling sound. An approach to the problem as mentioned in the article is to extract an audio feature called MFCC and then pass it though a neural network to get the appropriate class.</p>
<p><strong>Case Study</strong> &#8211; <a href="https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/</a></p>
<p>&nbsp;</p>
<h2>2. Audio Fingerprinting</h2>
<p>The aim of audio fingerprinting is to determine a digital “summary” of the audio. This is done to identify the audio from an audio sample. Shazam is an excellent example of an application of audio fingerprinting in the industry. It recognises the music on the basis of the first two to five seconds of a song. This task is useful as far as industry standards go, but still there are situations where the system fails especially where there is a high amount of background noise.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="http://www.cs.toronto.edu/~dross/ChandrasekharSharifiRoss_ISMIR2011.pdf" target="_blank" rel="nofollow noopener">http://www.cs.toronto.edu/~dross/ChandrasekharSharifiRoss_ISMIR2011.pdf</a></p>
<p>To solve this problem, an approach could be to represent the audio in a different manner, so that it is easily deciphered and then find out the patterns that differentiates the audio from others. In the case study below, the author converts raw audio to spectrograms and then uses peak finding and fingerprint hashing algorithms to define the fingerprints of that audio file.</p>
<p><strong>Case Study</strong> &#8211; <a href="http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/" target="_blank" rel="nofollow noopener">http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/</a></p>
<p>&nbsp;</p>
<h2>3. Automatic Music Tagging</h2>
<p><em>Music Tagging</em> is a more complex version of audio classification. Here, we can have multiple classes that each audio may belong to, aka, a multi-label classification problem. A potential application of this task is to create metadata for the audio so that it can be searched later on. Deep learning has helped solve this task to a certain extent which can be seen in the case study below.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="https://link.springer.com/article/10.1007/s10462-012-9362-y" target="_blank" rel="nofollow noopener">https://link.springer.com/article/10.1007/s10462-012-9362-y</a></p>
<p>As seen with most of the tasks, the first step always is to extract features from the audio sample. Then according to the nuances of the audio (for example, if the audio contains very less singing voice as compared to the instruments used, a tag could be &#8220;instrumental&#8221;). This can be done either by machine learning or deep learning methods. The case study mentioned below uses deep learning to solve the problem, specifically convolutional recurrent neural network along with Mel Frequeny Extraction.</p>
<p><strong>Case Study</strong> &#8211; <a href="https://github.com/keunwoochoi/music-auto_tagging-keras" target="_blank" rel="nofollow noopener">https://github.com/keunwoochoi/music-auto_tagging-keras</a></p>
<p>&nbsp;</p>
<h2>4. Audio Segmentation</h2>
<p>Segmentation literally means dividing a particular object into parts (or segments) based on a defined set of characteristics. Segmentation, especially for audio data analysis, is an important pre-processing step. This is because we can segment a noisy and lengthy audio signal into short homogeneous segments (handy short sequences of audio) which are used for further processing. An application of the task is heart sound segmentation, i.e. to identify sounds specific to the heart.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="http://www.mecs-press.org/ijitcs/ijitcs-v6-n11/IJITCS-V6-N11-1.pdf" target="_blank" rel="nofollow noopener">http://www.mecs-press.org/ijitcs/ijitcs-v6-n11/IJITCS-V6-N11-1.pdf</a></p>
<p>An approach to solve this problem is to convert it into a supervised learning problem, where each time stamp is classified on the basis of the segments required. Then apply an audio classification approach to solve the problem. In the case study below, the task is to segment the heart sound into two segments (lub and dub), so that you can identify an anomaly in each of the segment. It is solved by using audio feature extraction and then apply deep learning for classification.</p>
<p><strong>Case Study</strong> &#8211; <a href="https://www.analyticsvidhya.com/blog/2017/11/heart-sound-segmentation-deep-learning/" target="_blank" rel="nofollow noopener">https://www.analyticsvidhya.com/blog/2017/11/heart-sound-segmentation-deep-learning/</a></p>
<p>&nbsp;</p>
<h2>5. Audio Source Separation</h2>
<p><i>Audio Source Separation</i> consists of isolating one or more source signals from a mixture of signals. One of the most common applications of this is <b>identifying the lyrics from the audio</b> for simultaneous translation (karaoke, for instance). This is a classic example shown in Andrew Ng’s machine learning course where he separates the sound of the speaker from the background music.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="http://ijcert.org/ems/ijcert_papers/V3I1103.pdf" target="_blank" rel="nofollow noopener">http://ijcert.org/ems/ijcert_papers/V3I1103.pdf</a></p>
<p>A typical usage scenario involves loading an audio file, computing a time-frequency transform to obtain a spectrogram, and using some of the source separation algorithm such as non-negative matrix factorization to obtain a time-frequency mask. The mask is then multiplied with the spectrogram and the result is converted back to the time domain.</p>
<p><strong>Case Study</strong> &#8211; <a href="https://github.com/IoSR-Surrey/untwist" target="_blank" rel="nofollow noopener">https://github.com/IoSR-Surrey/untwist</a></p>
<p>&nbsp;</p>
<h2>6. Beat Tracking</h2>
<p>As the name suggests, the goal here is to track the location of each beat in a collection of audio files. <em>Beat tracking</em> can be utilized to automate time-consuming tasks that must be completed in order to synchronize events with music. It is useful in various applications, such as video editing, audio editing, and human-computer improvisation.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/01-students/2012_GroschePeter_MusicSignalProcessing_PhD-Thesis.pdf" target="_blank" rel="nofollow noopener">https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/01-students/2012_GroschePeter_MusicSignalProcessing_PhD-Thesis.pdf</a></p>
<p>An approach to solve beat tracking would be parse the audio file and use an onset detection algorithm to track the beats. Although the techniques used to for onset detection rely heavily on audio feature engineering and machine learning, deep learning can easily be used here to optimize the results.</p>
<p><strong>Case Study</strong> &#8211; <a href="https://github.com/adamstark/BTrack" target="_blank" rel="nofollow noopener">https://github.com/adamstark/BTrack</a></p>
<p>&nbsp;</p>
<h2>7. Music Recommendation</h2>
<p>Thanks to the internet, we now have millions of songs we can listen to anytime. Ironically however, this has made it even harder to discover new music because of the plethora of options out there. <em>Music recommendation</em> systems help deal with this information overload by automatically recommending new music to listeners. Content providers like Spotify and Saavn have developed highly sophisticated music recommendation engines. These models leverage the user&#8217;s past listening history among many other features to build customized recommendation lists.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="https://pdfs.semanticscholar.org/7442/c1ebd6c9ceafa8979f683c5b1584d659b728.pdf" target="_blank" rel="nofollow noopener">https://pdfs.semanticscholar.org/7442/c1ebd6c9ceafa8979f683c5b1584d659b728.pdf</a></p>
<p>We can tackle the problem of predicting listening preferences from audio signals by training a regression/deep learning model to predict the latent representations of songs that were obtained from a collaborative filtering model. This way, we could predict the representation of a song in the collaborative filtering space, even if no usage data was available.</p>
<p><strong>Case Study</strong> &#8211; <a href="http://benanne.github.io/2014/08/05/spotify-cnns.html" target="_blank" rel="nofollow noopener">http://benanne.github.io/2014/08/05/spotify-cnns.html</a></p>
<p>&nbsp;</p>
<h2>8. Music Retrieval</h2>
<p>One of the most difficult tasks in audio processing, M<em>usic Retrieval</em> essentially aims to build a search engine based on audio. Although we can do this by solving sub-tasks like audio fingerprinting, this task encompasses much more that that. For example, you also have to solve different smaller tasks for different types of  music retrieval (timbre detection would be great for gender identification). Currently, there is no other system that has been developed to match the industry expected standards.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="http://www.nowpublishers.com/article/Details/INR-042" target="_blank" rel="nofollow noopener">http://www.nowpublishers.com/article/Details/INR-042</a></p>
<p>To solve music retrieval, the task is divided into simpler ones, which include tonal analysis (e.g. melody and harmony) and rhythm or tempo (e.g. beat tracking). Then on the basis these individual analysis, information is extracted which is used for retrieval of similar audio samples.</p>
<p><strong>Case Study</strong> &#8211; <a href="https://youtu.be/oGGVvTgHMHw" target="_blank" rel="nofollow noopener">https://youtu.be/oGGVvTgHMHw</a></p>
<p>&nbsp;</p>
<h2>9. Music Transcription</h2>
<p><em>Music Transcription</em> is another challenging audio processing task. It comprises of annotating audio and creating a kind of “sheet” for generating music from it at a later point of time. The manual effort involved in <i>transcribing music</i> from recordings can be vast. It varies enormously depending on the complexity of the <i>music</i>, how good our listening skills are and how detailed we want our <i>transcription</i> to be.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="http://ieeexplore.ieee.org/abstract/document/7955698/" target="_blank" rel="nofollow noopener">http://ieeexplore.ieee.org/abstract/document/7955698</a></p>
<p>The approach for music transcription is similar to that of speech recognition, where musical notes are transcribed into lyrical excerpts of instruments.</p>
<p><strong>Case Study</strong> &#8211; <a href="https://youtu.be/9boJ-Ai6QFM" target="_blank" rel="nofollow noopener">https://youtu.be/9boJ-Ai6QFM</a></p>
<p>&nbsp;</p>
<h2>10. Onset Detection</h2>
<p>Onset detection is the first step in analysing an audio/music sequence. For most of the tasks mentioned above, it is somewhat necessary to perform onset detection, i.e. detecting the start of an audio event. Onset detection was essentially the first task that researchers intended to solve in audio processing.</p>
<p><strong>Whitepaper</strong> &#8211; <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.989&amp;rep=rep1&amp;type=pdf" target="_blank" rel="nofollow noopener">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.989&amp;rep=rep1&amp;type=pdf</a></p>
<p>Onset detection is typically done by</p>
<ol>
<li>Computing a spectral novelty function.</li>
<li>Finding peaks in the spectral novelty function.</li>
<li>Backtracking from each peak to a preceding local minimum. Backtracking can be useful for finding segmentation points such that the onset occurs shortly after the beginning of the segment.</li>
</ol>
<p><strong>Case Study</strong> &#8211; <a href="https://musicinformationretrieval.com/onset_detection.html" target="_blank" rel="nofollow noopener">https://musicinformationretrieval.com/onset_detection.html</a></p>
<p>&nbsp;</p>
<h2>End Notes</h2>
<p>In this article, I have mentioned a couple of tasks that can be looked at when solving audio processing problems. I hope you find the article insightful in dealing with audio/speech related projects.</p>
<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2, h3').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
