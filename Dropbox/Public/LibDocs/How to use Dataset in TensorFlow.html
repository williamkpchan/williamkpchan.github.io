<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width"/>
<link rel="stylesheet" href="..\maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .apply, div.title').click(function(){
    parent.history.back();
    return false;
    });
});
</script>
<style>
body{font-size: x-large;width:80%;margin-left: 10%}
h1, h2 {color: gold;}
</style>
</head><body>
<center><h1>How to use Dataset in TensorFlow</h1>
<div id="toc"></div></center>
<br>
<br>
<br>

<p name="3b13" id="3b13" class="graf graf--p graf-after--h3">The built-in Input Pipeline. Never use ‘feed-dict’ anymore
</p>
<p name="c805" id="c805" class="graf graf--p graf-after--p">
<em class="markup--em markup--p-em">Update 2/06/2018: Added second full example to read csv directly into the dataset
</em>
</p>
<p name="bac6" id="bac6" class="graf graf--p graf-after--p">
<em class="markup--em markup--p-em">Update 25/05/2018: Added second full example with a 
</em>
<strong class="markup--strong markup--p-strong">
<em class="markup--em markup--p-em">Reinitializable iterator
</em>
</strong>
</p>
<p name="b2c5" id="b2c5" class="graf graf--p graf-after--p">
<em class="markup--em markup--p-em">Updated to TensorFlow 1.8
</em>
</p>
<p name="c637" id="c637" class="graf graf--p graf-after--p">As you should know, 
<code class="markup--code markup--p-code">feed-dict
</code> is the slowest possible way to pass information to TensorFlow and it must be avoided. The correct way to feed data into your models is to use an input pipeline to ensure that the GPU has never to wait for new stuff to come in.
</p>
<p name="eb59" id="eb59" class="graf graf--p graf-after--p">Fortunately, TensorFlow has a built-in API, called 
<a href="https://www.tensorflow.org/programmers_guide/datasets" data-href="https://www.tensorflow.org/programmers_guide/datasets" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dataset
</a> to make it easier to accomplish this task. In this tutorial, we are going to see how we can create an input pipeline and how to feed the data into the model efficiently.
</p>
<p name="ab9c" id="ab9c" class="graf graf--p graf-after--p">This article will explain the basic mechanics of the Dataset, covering the most common use cases.
</p>
<p name="a258" id="a258" class="graf graf--p graf-after--p">You can found all the code as a jupyter notebook here :
</p>
<p name="1e68" id="1e68" class="graf graf--p graf-after--p">
<a href="https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb" data-href="https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb
</a>
</p>
<h3 name="2ba7" id="2ba7" class="graf graf--h3 graf-after--p">Generic Overview
</h3>
<p name="e98a" id="e98a" class="graf graf--p graf-after--h3">In order to use a Dataset we need three steps:
</p>
<ul class="postList">
<li name="6411" id="6411" class="graf graf--li graf-after--p">
<strong class="markup--strong markup--li-strong">Importing Data
</strong>. Create a Dataset instance from some data
</li>
<li name="7614" id="7614" class="graf graf--li graf-after--li">
<strong class="markup--strong markup--li-strong">Create an Iterator.
</strong> By using the created dataset to make an Iterator instance to iterate thought the dataset
</li>
<li name="a8e1" id="a8e1" class="graf graf--li graf-after--li">
<strong class="markup--strong markup--li-strong">Consuming Data
</strong>. By using the created iterator we can get the elements from the dataset to feed the model
</li>
</ul>
<h3 name="72a5" id="72a5" class="graf graf--h3 graf-after--li">Importing Data
</h3>
<p name="3d41" id="3d41" class="graf graf--p graf-after--h3">We first need some data to put inside our dataset
</p>
<h4 name="c87f" id="c87f" class="graf graf--h4 graf-after--p">From numpy
</h4>
<p name="2001" id="2001" class="graf graf--p graf-after--h4">This is the common case, we have a numpy array and we want to pass it to tensorflow.
</p>
<pre name="94c1" id="94c1" class="graf graf--pre graf-after--p"># create a random vector of shape (100,2)
<br>x = np.random.sample((100,2))
<br># make a dataset from a numpy array
<br>dataset = tf.data.Dataset.from_tensor_slices(x)
</pre>
<p name="ea94" id="ea94" class="graf graf--p graf-after--pre">We can also pass more than one numpy array, one classic example is when we have a couple of data divided into features and labels
</p>
<pre name="92da" id="92da" class="graf graf--pre graf-after--p">features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>dataset = tf.data.Dataset.from_tensor_slices((features,labels))
</pre>
<h4 name="7209" id="7209" class="graf graf--h4 graf-after--pre">From tensors
</h4>
<p name="64a2" id="64a2" class="graf graf--p graf-after--h4">We can, of course, initialise our dataset with some tensor
</p>
<pre name="778f" id="778f" class="graf graf--pre graf-after--p"># using a tensor
<br>dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))
</pre>
<h4 name="c832" id="c832" class="graf graf--h4 graf-after--pre">From a placeholder
</h4>
<p name="f0e4" id="f0e4" class="graf graf--p graf-after--h4">This is useful when we want to dynamic change the data inside the Dataset, we will se later how.
</p>
<pre name="862e" id="862e" class="graf graf--pre graf-after--p">x = tf.placeholder(tf.float32, shape=[None,2])
<br>dataset = tf.data.Dataset.from_tensor_slices(x)
</pre>
<h4 name="1d27" id="1d27" class="graf graf--h4 graf-after--pre">From generator
</h4>
<p name="fefe" id="fefe" class="graf graf--p graf-after--h4">We can also initialise a Dataset from a generator, this is useful when we have an array of different elements length (e.g a sequence):
</p>
<pre name="dadf" id="dadf" class="graf graf--pre graf-after--p"># from generator
<br>sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])
</pre>
<pre name="c310" id="c310" class="graf graf--pre graf-after--pre">def generator():
<br>    for el in sequence:
<br>        yield el
</pre>
<pre name="6fe7" id="6fe7" class="graf graf--pre graf-after--pre">dataset = tf.data.Dataset().batch(1).from_generator(generator,
<br>                                           output_types= tf.int64, 
<br>                                           output_shapes=(tf.TensorShape([None, 1])))
</pre>
<pre name="70f9" id="70f9" class="graf graf--pre graf-after--pre">iter = dataset.make_initializable_iterator()
<br>el = iter.get_next()
</pre>
<pre name="ad45" id="ad45" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    sess.run(iter.initializer)
<br>    print(sess.run(el))
<br>    print(sess.run(el))
<br>    print(sess.run(el))
</pre>
<p name="758d" id="758d" class="graf graf--p graf-after--pre">Ouputs:
</p>
<pre name="fa24" id="fa24" class="graf graf--pre graf-after--p">[[1]]
<br>[[2]
<br> [3]]
<br>[[3]
<br> [4]
<br> [5]]
</pre>
<p name="4f2d" id="4f2d" class="graf graf--p graf-after--pre">In this case you also need specify the types and the shapes of your data that will be used to create the correct tensors.
</p>
<h4 name="5e3f" id="5e3f" class="graf graf--h4 graf-after--p">From csv file
</h4>
<p name="c1d9" id="c1d9" class="graf graf--p graf-after--h4">You can directly read a csv file into a dataset. For example, I have a csv file with tweets and their sentiment.
</p>
<figure name="343f" id="343f" class="graf graf--figure graf-after--p">
<div class="aspectRatioPlaceholder is-locked" style="max-width: 424px; max-height: 167px;">
<div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.4%;">
</div>
<div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*LE5N8IQTcgahmZt_odeo8Q.png" data-width="424" data-height="167">
<img src="https://cdn-images-1.medium.com/freeze/max/30/1*LE5N8IQTcgahmZt_odeo8Q.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail">
<canvas class="progressiveMedia-canvas js-progressiveMedia-canvas">
</canvas>
<img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*LE5N8IQTcgahmZt_odeo8Q.png">
<noscript class="js-progressiveMedia-inner">
<img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*LE5N8IQTcgahmZt_odeo8Q.png">
</noscript>
</div>
</div>
<figcaption class="imageCaption">tweets.csv
</figcaption>
</figure>
<p name="b807" id="b807" class="graf graf--p graf-after--figure">I can now easily create a 
<code class="markup--code markup--p-code">Dataset
</code> from it by calling 
<code class="markup--code markup--p-code">tf.contrib.data.make_csv_dataset
</code> . Be aware that the iterator will create a dictionary with key as the column names and values as Tensor with the correct row value.
</p>
<pre name="ab21" id="ab21" class="graf graf--pre graf-after--p"># load a csv
<br>CSV_PATH = &#39;./tweets.csv&#39;
<br>dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)
<br>iter = dataset.make_one_shot_iterator()
<br>next = iter.get_next()
<br>print(next) # next is a dict with key=columns names and value=column data
<br>inputs, labels = next[&#39;text&#39;], next[&#39;sentiment&#39;]
</pre>
<pre name="34dd" id="34dd" class="graf graf--pre graf-after--pre">with  tf.Session() as sess:
<br>    sess.run([inputs, labels])
</pre>
<p name="ff66" id="ff66" class="graf graf--p graf-after--pre">Where 
<code class="markup--code markup--p-code">next
</code> is
</p>
<pre name="8295" id="8295" class="graf graf--pre graf-after--p">{&#39;sentiment&#39;: &lt;tf.Tensor &#39;IteratorGetNext_15:0&#39; shape=(?,) dtype=int32&gt;, &#39;text&#39;: &lt;tf.Tensor &#39;IteratorGetNext_15:1&#39; shape=(?,) dtype=string&gt;}
</pre>
<h3 name="e56f" id="e56f" class="graf graf--h3 graf-after--pre">Create an Iterator
</h3>
<p name="8aa5" id="8aa5" class="graf graf--p graf-after--h3">We have seen how to create a dataset, but how to get our data back? We have to use an 
<code class="markup--code markup--p-code">Iterator
</code>, that will give us the ability to iterate through the dataset and retrieve the real values of the data. There exist four types of iterators.
</p>
<ul class="postList">
<li name="5f45" id="5f45" class="graf graf--li graf-after--p">
<strong class="markup--strong markup--li-strong">One shot. 
</strong>It can iterate once through a dataset, you 
<strong class="markup--strong markup--li-strong">cannot feed
</strong> any value to it.
</li>
<li name="cdcd" id="cdcd" class="graf graf--li graf-after--li">
<strong class="markup--strong markup--li-strong">Initializable
</strong>: You can dynamically change calling its 
<code class="markup--code markup--li-code">initializer
</code> operation and passing the new data with 
<code class="markup--code markup--li-code">feed_dict
</code> . It’s basically a bucket that you can fill with stuff.
</li>
<li name="1fb7" id="1fb7" class="graf graf--li graf-after--li">
<strong class="markup--strong markup--li-strong">Reinitializable
</strong>: It can be initialised from different 
<code class="markup--code markup--li-code">Dataset.
</code> Very useful when you have a training dataset that needs some additional transformation, eg. shuffle, and a testing dataset. It’s like using a tower crane to select different container.
</li>
<li name="d202" id="d202" class="graf graf--li graf-after--li">
<strong class="markup--strong markup--li-strong">Feedable
</strong>:
<strong class="markup--strong markup--li-strong"> 
</strong>It can be used to select with iterator to use. Following the previous example, it’s like a tower crane that selects which tower crane to use to select which container to take. In my opinion is useless.
</li>
</ul>
<h4 name="419b" id="419b" class="graf graf--h4 graf-after--li">One shot Iterator
</h4>
<p name="1bf5" id="1bf5" class="graf graf--p graf-after--h4">This is the easiest iterator. Using the first example
</p>
<pre name="1d3a" id="1d3a" class="graf graf--pre graf-after--p">x = np.random.sample((100,2))
<br># make a dataset from a numpy array
<br>dataset = tf.data.Dataset.from_tensor_slices(x)
</pre>
<pre name="da8c" id="da8c" class="graf graf--pre graf-after--pre"># create the iterator
<br>iter = dataset.make_one_shot_iterator()
</pre>
<p name="8e08" id="8e08" class="graf graf--p graf-after--pre">Then you need to call 
<code class="markup--code markup--p-code">get_next()
</code> to get the tensor that will contain your data
</p>
<pre name="5f65" id="5f65" class="graf graf--pre graf-after--p">...
<br># create the iterator
<br>iter = dataset.make_one_shot_iterator()
<br>el = iter.get_next()
</pre>
<p name="f142" id="f142" class="graf graf--p graf-after--pre">We can run 
<code class="markup--code markup--p-code">el
</code> in order to see its value
</p>
<pre name="496f" id="496f" class="graf graf--pre graf-after--p">with tf.Session() as sess:
<br>    print(sess.run(el)) # output: [ 0.42116176  0.40666069]
</pre>
<h4 name="4fcf" id="4fcf" class="graf graf--h4 graf-after--pre">Initializable Iterator
</h4>
<p name="ebe2" id="ebe2" class="graf graf--p graf-after--h4">In case we want to build a dynamic dataset in which we can change the data sourceat runtime, we can create a dataset with a placeholder. Then we can initialize the placeholder using the common 
<code class="markup--code markup--p-code">feed-dict
</code> mechanism. This is done with an 
<em class="markup--em markup--p-em">initializable iterator
</em>. Using example three from last section
</p>
<pre name="d1b6" id="d1b6" class="graf graf--pre graf-after--p"># using a placeholder
<br>x = tf.placeholder(tf.float32, shape=[None,2])
<br>dataset = tf.data.Dataset.from_tensor_slices(x)
</pre>
<pre name="13bd" id="13bd" class="graf graf--pre graf-after--pre">data = np.random.sample((100,2))
</pre>
<pre name="0187" id="0187" class="graf graf--pre graf-after--pre">iter = dataset.make_initializable_iterator() # create the iterator
<br>el = iter.get_next()
</pre>
<pre name="aa1d" id="aa1d" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    # feed the placeholder with data
<br>    sess.run(iter.initializer, feed_dict={ x: data }) 
<br>    print(sess.run(el)) # output [ 0.52374458  0.71968478]
</pre>
<p name="3fa6" id="3fa6" class="graf graf--p graf-after--pre">This time we call 
<code class="markup--code markup--p-code">make_initializable_iterator
</code> . Then, inside the
<code class="markup--code markup--p-code">sess
</code> scope, we run the 
<code class="markup--code markup--p-code">initializer
</code> operation in order to pass our data, in this case a random numpy array. .
</p>
<p name="3640" id="3640" class="graf graf--p graf-after--p">Imagine that now we have a train set and a test set, a real common scenario:
</p>
<pre name="581f" id="581f" class="graf graf--pre graf-after--p">train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>test_data = (np.array([[1,2]]), np.array([[0]]))
</pre>
<p name="c347" id="c347" class="graf graf--p graf-after--pre">Then we would like to train the model and then evaluate it on the test dataset, this can be done by initialising the iterator again after training
</p>
<pre name="ddc5" id="ddc5" class="graf graf--pre graf-after--p"># initializable iterator to switch between dataset
<br>EPOCHS = 10
</pre>
<pre name="e6c1" id="e6c1" class="graf graf--pre graf-after--pre">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
<br>dataset = tf.data.Dataset.from_tensor_slices((x, y))
</pre>
<pre name="a8f4" id="a8f4" class="graf graf--pre graf-after--pre">train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>test_data = (np.array([[1,2]]), np.array([[0]]))
</pre>
<pre name="b529" id="b529" class="graf graf--pre graf-after--pre">iter = dataset.make_initializable_iterator()
<br>features, labels = iter.get_next()
</pre>
<pre name="1963" id="1963" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>#     initialise iterator with train data
<br>    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
<br>    for _ in range(EPOCHS):
<br>        sess.run([features, labels])
<br>#     switch to test data
<br>    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})
<br>    print(sess.run([features, labels]))
</pre>
<h4 name="f9f6" id="f9f6" class="graf graf--h4 graf-after--pre">
<strong class="markup--strong markup--h4-strong">Reinitializable Iterator
</strong>
</h4>
<p name="dcc3" id="dcc3" class="graf graf--p graf-after--h4">The concept is similar to before, we want to dynamic switch between data. But instead of feed new data to the same dataset, we switch dataset. As before, we want to have a train dataset and a test dataset
</p>
<pre name="ad4e" id="ad4e" class="graf graf--pre graf-after--p"># making fake data using numpy
<br>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
</pre>
<p name="117a" id="117a" class="graf graf--p graf-after--pre">We can create two Datasets
</p>
<pre name="5807" id="5807" class="graf graf--pre graf-after--p"># create two datasets, one for training and one for test
<br>train_dataset = tf.data.Dataset.from_tensor_slices(train_data)
<br>test_dataset = tf.data.Dataset.from_tensor_slices(test_data)
</pre>
<p name="9c7a" id="9c7a" class="graf graf--p graf-after--pre">Now this is the trick, we create a generic Iterator
</p>
<pre name="5034" id="5034" class="graf graf--pre graf-after--p"># create a iterator of the correct shape and type
<br>iter = tf.data.Iterator.from_structure(train_dataset.output_types,
<br>                                           train_dataset.output_shapes)
</pre>
<p name="0001" id="0001" class="graf graf--p graf-after--pre">and then two initialisation operations:
</p>
<pre name="01dc" id="01dc" class="graf graf--pre graf-after--p"># create the initialisation operations
<br>train_init_op = iter.make_initializer(train_dataset)
<br>test_init_op = iter.make_initializer(test_dataset)
</pre>
<p name="dbd0" id="dbd0" class="graf graf--p graf-after--pre">We get the next element as before
</p>
<pre name="73b2" id="73b2" class="graf graf--pre graf-after--p">features, labels = iter.get_next()
</pre>
<p name="c288" id="c288" class="graf graf--p graf-after--pre">Now, we can directly run the two initialisation operation using our session. Putting all together we get:
</p>
<pre name="a1e9" id="a1e9" class="graf graf--pre graf-after--p"># Reinitializable iterator to switch between Datasets
<br>EPOCHS = 10
<br># making fake data using numpy
<br>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
<br># create two datasets, one for training and one for test
<br>train_dataset = tf.data.Dataset.from_tensor_slices(train_data)
<br>test_dataset = tf.data.Dataset.from_tensor_slices(test_data)
<br># create a iterator of the correct shape and type
<br>iter = tf.data.Iterator.from_structure(train_dataset.output_types,
<br>                                           train_dataset.output_shapes)
<br>features, labels = iter.get_next()
<br># create the initialisation operations
<br>train_init_op = iter.make_initializer(train_dataset)
<br>test_init_op = iter.make_initializer(test_dataset)
</pre>
<pre name="62df" id="62df" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    sess.run(train_init_op) # switch to train dataset
<br>    for _ in range(EPOCHS):
<br>        sess.run([features, labels])
<br>    sess.run(test_init_op) # switch to val dataset
<br>    print(sess.run([features, labels]))
</pre>
<h4 name="6090" id="6090" class="graf graf--h4 graf-after--pre">Feedable Iterator
</h4>
<p name="32ad" id="32ad" class="graf graf--p graf-after--h4">This is very similar to the 
<code class="markup--code markup--p-code">reinitializable
</code> iterator, but instead of switch between datasets, it switch between iterators. After we created two datasets
</p>
<pre name="fea0" id="fea0" class="graf graf--pre graf-after--p">train_dataset = tf.data.Dataset.from_tensor_slices((x,y))
<br>test_dataset = tf.data.Dataset.from_tensor_slices((x,y))
</pre>
<p name="0a6d" id="0a6d" class="graf graf--p graf-after--pre">One for training and one for testing. Then, we can create our iterator, in this case we use the 
<code class="markup--code markup--p-code">initializable
</code> iterator, but you can also use a 
<code class="markup--code markup--p-code">one shot
</code> iterator
</p>
<pre name="82a0" id="82a0" class="graf graf--pre graf-after--p">train_iterator = train_dataset.make_initializable_iterator()
<br>test_iterator = test_dataset.make_initializable_iterator()
</pre>
<p name="add3" id="add3" class="graf graf--p graf-after--pre">Now, we need to defined and 
<code class="markup--code markup--p-code">handle
</code> , that will be out placeholder that can be dynamically changed.
</p>
<pre name="3e79" id="3e79" class="graf graf--pre graf-after--p">handle = tf.placeholder(tf.string, shape=[])
</pre>
<p name="59dc" id="59dc" class="graf graf--p graf-after--pre">Then, similar to before, we define a generic iterator using the shape of the dataset
</p>
<pre name="10a3" id="10a3" class="graf graf--pre graf-after--p">iter = tf.data.Iterator.from_string_handle(
<br>    handle, train_dataset.output_types, train_dataset.output_shapes)
</pre>
<p name="b239" id="b239" class="graf graf--p graf-after--pre">Then, we get the next elements
</p>
<pre name="b224" id="b224" class="graf graf--pre graf-after--p">next_elements = iter.get_next()
</pre>
<p name="cc53" id="cc53" class="graf graf--p graf-after--pre">In order to switch between the iterators we just have to call the 
<code class="markup--code markup--p-code">next_elemenents
</code> operation passing the correct 
<code class="markup--code markup--p-code">handle
</code> in the feed_dict. For example, to get one element from the train set:
</p>
<pre name="7032" id="7032" class="graf graf--pre graf-after--p">sess.run(next_elements, feed_dict = {handle: train_handle})
</pre>
<p name="df57" id="df57" class="graf graf--p graf-after--pre">If you are using 
<code class="markup--code markup--p-code">initializable
</code> iterators, as we are doing, just remember to initialize them before starting
</p>
<pre name="05c0" id="05c0" class="graf graf--pre graf-after--p">sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
<br>    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})
</pre>
<p name="125f" id="125f" class="graf graf--p graf-after--pre">Putting all together we get:
</p>
<pre name="978d" id="978d" class="graf graf--pre graf-after--p"># feedable iterator to switch between iterators
<br>EPOCHS = 10
<br># making fake data using numpy
<br>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
<br># create placeholder
<br>x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
<br># create two datasets, one for training and one for test
<br>train_dataset = tf.data.Dataset.from_tensor_slices((x,y))
<br>test_dataset = tf.data.Dataset.from_tensor_slices((x,y))
<br># create the iterators from the dataset
<br>train_iterator = train_dataset.make_initializable_iterator()
<br>test_iterator = test_dataset.make_initializable_iterator()
<br># same as in the doc 
<a href="https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator" data-href="https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator
</a>
<br>handle = tf.placeholder(tf.string, shape=[])
<br>iter = tf.data.Iterator.from_string_handle(
<br>    handle, train_dataset.output_types, train_dataset.output_shapes)
<br>next_elements = iter.get_next()
</pre>
<pre name="5d0f" id="5d0f" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    train_handle = sess.run(train_iterator.string_handle())
<br>    test_handle = sess.run(test_iterator.string_handle())
<br>    
<br>    # initialise iterators. 
<br>    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
<br>    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})
<br>    
<br>    for _ in range(EPOCHS):
<br>        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})
<br>        print(x, y)
<br>        
<br>    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})
<br>    print(x,y)
</pre>
<h3 name="85f9" id="85f9" class="graf graf--h3 graf-after--pre">Consuming data
</h3>
<p name="8426" id="8426" class="graf graf--p graf-after--h3">In the previous example we have used the session to print the value of the 
<code class="markup--code markup--p-code">next
</code> element in the Dataset.
</p>
<pre name="28ae" id="28ae" class="graf graf--pre graf-after--p">...
<br>next_el = iter.get_next()
<br>...
<br>print(sess.run(next_el)) # will output the current element
</pre>
<p name="55e4" id="55e4" class="graf graf--p graf-after--pre">In order to pass the data to a model we have to just pass the tensors generated from 
<code class="markup--code markup--p-code">get_next()
</code>
</p>
<p name="340f" id="340f" class="graf graf--p graf-after--p">In the following snippet we have a Dataset that contains two numpy arrays, using the same example from the first section. Notice that we need to wrap the 
<code class="markup--code markup--p-code">.random.sample
</code> in another numpy array to add a dimension that we is needed to batch the data
</p>
<pre name="1fdf" id="1fdf" class="graf graf--pre graf-after--p"># using two numpy arrays
<br>features, labels = (np.array([np.random.sample((100,2))]), 
<br>                    np.array([np.random.sample((100,1))]))
</pre>
<pre name="f3cb" id="f3cb" class="graf graf--pre graf-after--pre">dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)
</pre>
<p name="f5f5" id="f5f5" class="graf graf--p graf-after--pre">Then as always we create an iterator
</p>
<pre name="0643" id="0643" class="graf graf--pre graf-after--p">iter = dataset.make_one_shot_iterator()
<br>x, y = iter.get_next()
</pre>
<p name="2b4f" id="2b4f" class="graf graf--p graf-after--pre">We make a model, a simple neural network
</p>
<pre name="d506" id="d506" class="graf graf--pre graf-after--p"># make a simple model
<br>net = tf.layers.dense(x, 8) # pass the first value from iter.get_next() as input
<br>net = tf.layers.dense(net, 8)
<br>prediction = tf.layers.dense(net, 1)
</pre>
<pre name="eb7b" id="eb7b" class="graf graf--pre graf-after--pre">loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label
<br>train_op = tf.train.AdamOptimizer().minimize(loss)
</pre>
<p name="14aa" id="14aa" class="graf graf--p graf-after--pre">We 
<strong class="markup--strong markup--p-strong">directly
</strong> use the Tensors from 
<code class="markup--code markup--p-code">iter.get_next()
</code> as input to the first layer and as labels for the loss function. Wrapping all together:
</p>
<pre name="5e8a" id="5e8a" class="graf graf--pre graf-after--p">EPOCHS = 10
<br>BATCH_SIZE = 16
<br># using two numpy arrays
<br>features, labels = (np.array([np.random.sample((100,2))]), 
<br>                    np.array([np.random.sample((100,1))]))
</pre>
<pre name="7bca" id="7bca" class="graf graf--pre graf-after--pre">dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)
</pre>
<pre name="7fb5" id="7fb5" class="graf graf--pre graf-after--pre">iter = dataset.make_one_shot_iterator()
<br>x, y = iter.get_next()
</pre>
<pre name="489a" id="489a" class="graf graf--pre graf-after--pre"># make a simple model
<br>net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
<br>net = tf.layers.dense(net, 8, activation=tf.tanh)
<br>prediction = tf.layers.dense(net, 1, activation=tf.tanh)
</pre>
<pre name="4d52" id="4d52" class="graf graf--pre graf-after--pre">loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label
<br>train_op = tf.train.AdamOptimizer().minimize(loss)
</pre>
<pre name="b8cd" id="b8cd" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    sess.run(tf.global_variables_initializer())
<br>    for i in range(EPOCHS):
<br>        _, loss_value = sess.run([train_op, loss])
<br>        print(&quot;Iter: {}, Loss: {:.4f}&quot;.format(i, loss_value))
</pre>
<p name="2d5f" id="2d5f" class="graf graf--p graf-after--pre">Output:
</p>
<pre name="6cb0" id="6cb0" class="graf graf--pre graf-after--p">Iter: 0, Loss: 0.1328 
<br>Iter: 1, Loss: 0.1312 
<br>Iter: 2, Loss: 0.1296 
<br>Iter: 3, Loss: 0.1281 
<br>Iter: 4, Loss: 0.1267 
<br>Iter: 5, Loss: 0.1254 
<br>Iter: 6, Loss: 0.1242 
<br>Iter: 7, Loss: 0.1231 
<br>Iter: 8, Loss: 0.1220 
<br>Iter: 9, Loss: 0.1210
</pre>
<h3 name="ce25" id="ce25" class="graf graf--h3 graf-after--pre">Useful Stuff
</h3>
<h4 name="2f21" id="2f21" class="graf graf--h4 graf-after--h3">Batch
</h4>
<p name="8843" id="8843" class="graf graf--p graf-after--h4">Usually batching data is a pain in the ass, with the 
<code class="markup--code markup--p-code">Dataset 
</code>API we can use the method 
<code class="markup--code markup--p-code">batch(BATCH_SIZE)
</code> that automatically batches the dataset with the provided size. The default value is one. In the following example, we use a batch size of 4
</p>
<pre name="770a" id="770a" class="graf graf--pre graf-after--p"># BATCHING
<br>BATCH_SIZE = 4
<br>x = np.random.sample((100,2))
<br># make a dataset from a numpy array
<br>dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)
</pre>
<pre name="fc7e" id="fc7e" class="graf graf--pre graf-after--pre">iter = dataset.make_one_shot_iterator()
<br>el = iter.get_next()
</pre>
<pre name="a9c4" id="a9c4" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    print(sess.run(el)) 
</pre>
<p name="2023" id="2023" class="graf graf--p graf-after--pre">Output:
</p>
<pre name="9f17" id="9f17" class="graf graf--pre graf-after--p">[[ 0.65686128  0.99373963]
<br> [ 0.69690451  0.32446826]
<br> [ 0.57148422  0.68688242]
<br> [ 0.20335116  0.82473219]]
</pre>
<h4 name="cfbd" id="cfbd" class="graf graf--h4 graf-after--pre">Repeat
</h4>
<p name="5fdb" id="5fdb" class="graf graf--p graf-after--h4">Using 
<code class="markup--code markup--p-code">.repeat()
</code> we can specify the number of times we want the dataset to be iterated. If no parameter is passed it will loop forever, usually is good to just loop forever and directly control the number of epochs with a standard loop.
</p>
<h4 name="9598" id="9598" class="graf graf--h4 graf-after--p">Shuffle
</h4>
<p name="7450" id="7450" class="graf graf--p graf-after--h4">We can shuffle the Dataset by using the method 
<code class="markup--code markup--p-code">shuffle()
</code> that shuffles the dataset by default every epoch.
</p>
<p name="7de7" id="7de7" class="graf graf--p graf-after--p">
<em class="markup--em markup--p-em">Remember: shuffle the dataset is very important to avoid overfitting.
</em>
</p>
<p name="8af5" id="8af5" class="graf graf--p graf-after--p">We can also set the parameter 
<code class="markup--code markup--p-code">buffer_size
</code> , a fixed size buffer from which the next element will be uniformly chosen from. Example:
</p>
<pre name="0dc4" id="0dc4" class="graf graf--pre graf-after--p"># BATCHING
<br>BATCH_SIZE = 4
<br>x = np.array([[1],[2],[3],[4]])
<br># make a dataset from a numpy array
<br>dataset = tf.data.Dataset.from_tensor_slices(x)
<br>dataset = dataset.shuffle(buffer_size=100)
<br>dataset = dataset.batch(BATCH_SIZE)
</pre>
<pre name="f7d7" id="f7d7" class="graf graf--pre graf-after--pre">iter = dataset.make_one_shot_iterator()
<br>el = iter.get_next()
</pre>
<pre name="5882" id="5882" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    print(sess.run(el))
</pre>
<p name="8621" id="8621" class="graf graf--p graf-after--pre">First run output:
</p>
<pre name="7448" id="7448" class="graf graf--pre graf-after--p">[[4]
<br> [2]
<br> [3]
<br> [1]]
</pre>
<p name="fd3a" id="fd3a" class="graf graf--p graf-after--pre">Second run output:
</p>
<pre name="096e" id="096e" class="graf graf--pre graf-after--p">[[3]
<br> [1]
<br> [2]
<br> [4]]
</pre>
<p name="f06d" id="f06d" class="graf graf--p graf-after--pre">Yep. It was shuffled. If you want, you can also set the 
<code class="markup--code markup--p-code">seed
</code> parameter.
</p>
<h3 name="b01e" id="b01e" class="graf graf--h3 graf-after--p">Map
</h3>
<p name="c92c" id="c92c" class="graf graf--p graf-after--h3">You can apply a custom function to each member of a dataset using the 
<code class="markup--code markup--p-code">map
</code> method. In the following example we multiply each element by two:
</p>
<pre name="655f" id="655f" class="graf graf--pre graf-after--p"># MAP
<br>x = np.array([[1],[2],[3],[4]])
<br># make a dataset from a numpy array
<br>dataset = tf.data.Dataset.from_tensor_slices(x)
<br>dataset = dataset.map(lambda x: x*2)
</pre>
<pre name="7c19" id="7c19" class="graf graf--pre graf-after--pre">iter = dataset.make_one_shot_iterator()
<br>el = iter.get_next()
</pre>
<pre name="36e4" id="36e4" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>#     this will run forever
<br>        for _ in range(len(x)):
<br>            print(sess.run(el))
</pre>
<p name="55c3" id="55c3" class="graf graf--p graf-after--pre">Output:
</p>
<pre name="bf55" id="bf55" class="graf graf--pre graf-after--p">[2]
<br>[4]
<br>[6]
<br>[8]
</pre>
<h3 name="02e2" id="02e2" class="graf graf--h3 graf-after--pre">Full example
</h3>
<h4 name="ac00" id="ac00" class="graf graf--h4 graf-after--h3">
<strong class="markup--strong markup--h4-strong">Initializable
</strong> iterator
</h4>
<p name="1a9b" id="1a9b" class="graf graf--p graf-after--h4">In the example below we train a simple model using batching and we switch between train and test dataset using a 
<strong class="markup--strong markup--p-strong">Initializable iterator
</strong>
</p>
<pre name="0c0f" id="0c0f" class="graf graf--pre graf-after--p"># Wrapping all together -&gt; Switch between train and test set using Initializable iterator
<br>EPOCHS = 10
<br># create a placeholder to dynamically switch between batch sizes
<br>batch_size = tf.placeholder(tf.int64)
</pre>
<pre name="aba9" id="aba9" class="graf graf--pre graf-after--pre">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
<br>dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()
</pre>
<pre name="c369" id="c369" class="graf graf--pre graf-after--pre"># using two numpy arrays
<br>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>test_data = (np.random.sample((20,2)), np.random.sample((20,1)))
</pre>
<pre name="c6bd" id="c6bd" class="graf graf--pre graf-after--pre">iter = dataset.make_initializable_iterator()
<br>features, labels = iter.get_next()
<br># make a simple model
<br>net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
<br>net = tf.layers.dense(net, 8, activation=tf.tanh)
<br>prediction = tf.layers.dense(net, 1, activation=tf.tanh)
</pre>
<pre name="99eb" id="99eb" class="graf graf--pre graf-after--pre">loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label
<br>train_op = tf.train.AdamOptimizer().minimize(loss)
</pre>
<pre name="6a05" id="6a05" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    sess.run(tf.global_variables_initializer())
<br>    # initialise iterator with train data
<br>    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})
<br>    print(&#39;Training...&#39;)
<br>    for i in range(EPOCHS):
<br>        tot_loss = 0
<br>        for _ in range(n_batches):
<br>            _, loss_value = sess.run([train_op, loss])
<br>            tot_loss += loss_value
<br>        print(&quot;Iter: {}, Loss: {:.4f}&quot;.format(i, tot_loss / n_batches))
<br>    # initialise iterator with test data
<br>    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})
<br>    print(&#39;Test Loss: {:4f}&#39;.format(sess.run(loss)))
</pre>
<p name="a4b0" id="a4b0" class="graf graf--p graf-after--pre">
<strong class="markup--strong markup--p-strong">Notice that we use a placeholder for the batch size in order to dynamically switch it after training
</strong>
</p>
<p name="6e20" id="6e20" class="graf graf--p graf-after--p">Output
</p>
<pre name="abe0" id="abe0" class="graf graf--pre graf-after--p">Training...
<br>Iter: 0, Loss: 0.2977
<br>Iter: 1, Loss: 0.2152
<br>Iter: 2, Loss: 0.1787
<br>Iter: 3, Loss: 0.1597
<br>Iter: 4, Loss: 0.1277
<br>Iter: 5, Loss: 0.1334
<br>Iter: 6, Loss: 0.1000
<br>Iter: 7, Loss: 0.1154
<br>Iter: 8, Loss: 0.0989
<br>Iter: 9, Loss: 0.0948
<br>Test Loss: 0.082150
</pre>
<h4 name="d1a5" id="d1a5" class="graf graf--h4 graf-after--pre">Reinitializable Iterator
</h4>
<p name="da06" id="da06" class="graf graf--p graf-after--h4">In the example below we train a simple model using batching and we switch between train and test dataset using a 
<strong class="markup--strong markup--p-strong">Reinitializable Iterator
</strong>
</p>
<pre name="ef22" id="ef22" class="graf graf--pre graf-after--p"># Wrapping all together -&gt; Switch between train and test set using Reinitializable iterator
<br>EPOCHS = 10
<br># create a placeholder to dynamically switch between batch sizes
<br>batch_size = tf.placeholder(tf.int64)
</pre>
<pre name="9b9e" id="9b9e" class="graf graf--pre graf-after--pre">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
<br>train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()
<br>test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it
<br># using two numpy arrays
<br>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
<br>test_data = (np.random.sample((20,2)), np.random.sample((20,1)))
</pre>
<pre name="96b5" id="96b5" class="graf graf--pre graf-after--pre"># create a iterator of the correct shape and type
<br>iter = tf.data.Iterator.from_structure(train_dataset.output_types,
<br>                                           train_dataset.output_shapes)
<br>features, labels = iter.get_next()
<br># create the initialisation operations
<br>train_init_op = iter.make_initializer(train_dataset)
<br>test_init_op = iter.make_initializer(test_dataset)
</pre>
<pre name="3a30" id="3a30" class="graf graf--pre graf-after--pre"># make a simple model
<br>net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
<br>net = tf.layers.dense(net, 8, activation=tf.tanh)
<br>prediction = tf.layers.dense(net, 1, activation=tf.tanh)
</pre>
<pre name="e0a3" id="e0a3" class="graf graf--pre graf-after--pre">loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label
<br>train_op = tf.train.AdamOptimizer().minimize(loss)
</pre>
<pre name="7b18" id="7b18" class="graf graf--pre graf-after--pre">with tf.Session() as sess:
<br>    sess.run(tf.global_variables_initializer())
<br>    # initialise iterator with train data
<br>    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})
<br>    print(&#39;Training...&#39;)
<br>    for i in range(EPOCHS):
<br>        tot_loss = 0
<br>        for _ in range(n_batches):
<br>            _, loss_value = sess.run([train_op, loss])
<br>            tot_loss += loss_value
<br>        print(&quot;Iter: {}, Loss: {:.4f}&quot;.format(i, tot_loss / n_batches))
<br>    # initialise iterator with test data
<br>    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})
<br>    print(&#39;Test Loss: {:4f}&#39;.format(sess.run(loss)))
</pre>
<h3 name="b6d7" id="b6d7" class="graf graf--h3 graf-after--pre">Other resources
</h3>
<p name="e071" id="e071" class="graf graf--p graf-after--h3">TensorFlow dataset tutorial: 
<a href="https://www.tensorflow.org/programmers_guide/datasets" data-href="https://www.tensorflow.org/programmers_guide/datasets" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://www.tensorflow.org/programmers_guide/datasets
</a>
</p>
<p name="78e0" id="78e0" class="graf graf--p graf-after--p">Dataset docs:
</p>
<p name="21c9" id="21c9" class="graf graf--p graf-after--p">
<a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" data-href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://www.tensorflow.org/api_docs/python/tf/data/Dataset
</a>
</p>
<h3 name="efb9" id="efb9" class="graf graf--h3 graf-after--p">Conclusion
</h3>
<p name="fbfa" id="fbfa" class="graf graf--p graf-after--h3">The 
<code class="markup--code markup--p-code">Dataset
</code> API gives us a fast and robust way to create optimized input pipeline to train, evaluate and test our models. In this article, we have seen most of the common operation we can do with them.
</p>
<p name="9559" id="9559" class="graf graf--p graf-after--p">You can use the 
<a href="https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb" data-href="https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">jupyter-notebook
</a> that I’ve made for this article as a reference.
</p>
<br>
<br>
<br>
<br>

<script>
  $(function() {
    var toc = $('#toc');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
