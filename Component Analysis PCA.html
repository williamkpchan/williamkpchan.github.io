<base target="_blank"><html><head><title>pca analysis</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var bookid = "pca analysis"
  var markerName = "h2, h3"
</script>
<style>
body{width:90%;margin-left: 5%; font-size:24px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>Principal Component Analysis (PCA) in R & Python</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
</div>
<pre>
<br>
<br>


<h2>What is Principal Component Analysis ?</h2>
Principal component analysis is a method of extracting important variables (in form of components) from a large set of variables available in a data set. 
It extracts low dimensional set of features from a high dimensional data set with a motive to capture as much information as possible. 
With fewer variables, visualization also becomes much more meaningful. 
PCA is more useful when dealing with 3 or higher dimensional data.

It is always performed on a symmetric correlation or covariance matrix. 
This means the matrix should be numeric and have standardized data.

Statistical techniques such as <b>factor analysis, principal component analysis</b>
Let's understand it using an example:

Let's say we have a data set of dimension 300 (<em>n</em>) × 50 (<em>p</em>). <em>n</em> represents the number of observations and <em>p</em> represents number of predictors. 
Since we have a large p = 50, there can be <code>p(p-1)/2</code> scatter plots i.e more than 1000 plots possible to analyze the variable relationship. Wouldn't is be a tedious job to perform exploratory analysis on this data ?

In this case, it would be a lucid approach to select a subset of <em>p</em> <em>(p &lt;&lt; 50)</em> predictor which captures as much information. 
Followed by plotting the observation in the resultant low dimensional space.

The image below shows the transformation of a high dimensional data (3 dimension) to low dimensional data (2 dimension) using PCA. 
Not to forget, each resultant dimension is a linear combination of <em>p</em> features

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/03/1-1.png">

<h2>What are principal components ?</h2>
A principal component is a normalized linear combination of the original predictors in a data set. 
In image above, <em>PC1</em> and <em>PC2</em> are the principal components. 
Let's say we have a set of predictors as <code>X¹, X²...,X<sup>p</sup></code>

The principal component can be written as:

<code>Z¹ = Φ¹¹X¹ + Φ²¹X² + Φ³¹X³ + .... 
+</code>Φ<sup>p</sup><code>¹X<sup>p</sup></code>

where, Z¹ is first principal component
<code>Φ<sup>p</sup>¹</code> is the loading vector comprising of loadings (<code>Φ¹, Φ²..</code>) of first principal component. 
The loadings are constrained to a sum of square equals to 1. 
This is because large magnitude of loadings may lead to large variance. 
It also defines the direction of the principal component (Z¹) along which data varies the most. 
It results in a line in <em>p</em> dimensional space which is closest to the <em>n</em> observations. 
Closeness is measured using average squared euclidean distance.
<code>X¹..X<sup>p</sup></code> are normalized predictors. 
Normalized predictors have mean equals to zero and standard deviation equals to one.

Therefore,

<strong>First principal component</strong> is a linear combination of original predictor variables which captures the maximum variance in the data set. It determines the direction of highest variability in the data. Larger the variability captured in first component, larger the information captured by component. No other component can have variability higher than first principal component.

The first principal component results in a line which is closest to the data i.e. 
it minimizes the sum of squared distance between a data point and the line.

Similarly, we can compute the second principal component also.

<strong>Second principal component</strong> (<code>Z²</code>) is also a linear combination of original predictors which captures the remaining variance in the data set and is uncorrelated with <code>Z¹</code>. In other words, the correlation between first and second component should is zero. 
It can be represented as:

<code>Z² = Φ¹²X¹ + Φ²²X² + Φ³²X³ + .... 
+ Φ<sup>p2</sup><code>X<sup>p</sup></code></code>

If the two components are uncorrelated, their directions should be orthogonal (image below). 
This image is based on a simulated data with 2 predictors. 
Notice the direction of the components, as expected they are orthogonal. 
This suggests the correlation b/w these components in zero.

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/03/2-1-e1458494877196.png">
All succeeding principal component follows a similar concept i.e. 
they capture the remaining variation without being correlated with the previous component. In general, for <em>n × p</em> dimensional data, min(<em>n-1, p)</em> principal component can be constructed.

The directions of these components are identified in an unsupervised way i.e. 
the response variable(Y) is not used to determine the component direction. 
Therefore, it is an unsupervised approach.

<em>Note: Partial least square (PLS) is a supervised alternative to PCA. 
PLS assigns higher weight to variables which are strongly related to response variable to determine principal components.</em>

<h2>Why is normalization of variables necessary ?</h2>
The principal components are supplied with normalized version of original predictors. 
This is because, the original predictors may have different scales. 
For example: Imagine a data set with variables' measuring units as gallons, kilometers, light years etc. 
It is definite that the scale of variances in these variables will be large.

Performing PCA on un-normalized variables will lead to insanely large loadings for variables with high variance. 
In turn, this will lead to dependence of a principal component on the variable with high variance. 
This is undesirable.

As shown in image below, PCA was run on a data set twice (with unscaled and scaled predictors). 
This data set has ~40 variables. 
You can see, first principal component is dominated by a variable Item_MRP. 
And, second principal component is dominated by a variable Item_Weight. 
This domination prevails due to high value of variance associated with a variable. 
When the variables are scaled, we get a much better representation of variables in 2D space.

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/03/6-1.png">

<h2>Implement PCA in R &amp; Python (with interpretation)</h2>
How many principal components to choose ? I could dive deep in theory, but it would be better to answer these question practically.

For this demonstration, I'll be using the data set from <a href="http://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii" target="_blank" rel="nofollow">Big Mart Prediction Challenge III</a>.

Remember, PCA can be applied only on numerical data. 
Therefore, if the data has categorical variables they must be converted to numerical. 
Also, make sure you have done the basic data cleaning prior to implementing this technique. 
Let's quickly finish with initial data loading and cleaning steps:

<code>#directory path</code>
<code> &gt; path &lt;- ".../Data/Big_Mart_Sales"</code>

<code>#set working directory</code>
<code> &gt; setwd(path)</code>

<code>#load train and test file</code>
<code> &gt; train &lt;- read.csv("train_Big.csv")</code>
<code> &gt; test &lt;- read.csv("test_Big.csv")</code>

<code>#add a column</code>
<code> &gt; test$Item_Outlet_Sales &lt;- 1</code>

<code>#combine the data set</code>
<code> &gt; combi &lt;- rbind(train, test)</code>

<code>#impute missing values with median</code>
<code> &gt; combi$Item_Weight[is.na(combi$Item_Weight)] &lt;- median(combi$Item_Weight, na.rm = TRUE)</code>

<code>#impute 0 with median</code>
<code> &gt; combi$Item_Visibility &lt;- ifelse(combi$Item_Visibility == 0, median(combi$Item_Visibility),                                   combi$Item_Visibility)</code>

<code>#find mode and impute</code>
<code> &gt; table(combi$Outlet_Size, combi$Outlet_Type)</code>
<code> &gt; levels(combi$Outlet_Size)[1] &lt;- "Other"</code>

Till here, we've imputed missing values. 
Now we are left with removing the dependent (response) variable and other identifier variables( if any). 
As we said above, we are practicing an unsupervised learning technique, hence response variable must be removed.

<code>#remove the dependent and identifier variables</code>
<code> &gt; my_data &lt;- subset(combi, select = -c(Item_Outlet_Sales, Item_Identifier,                                       Outlet_Identifier))</code>

Let's check the available variables ( a.k.a predictors) in the data set.

<code>#check available variables</code>
<code> &gt; colnames(my_data)</code>

Since PCA works on numeric variables, let's see if we have any variable other than numeric.

<code>#check variable class</code>
<code> &gt; str(my_data)</code>

<code>'data.frame': 14204 obs. 
of 9 variables:</code>
<code> $ Item_Weight : num 9.3 5.92 17.5 19.2 8.93 ...</code>
<code> $ Item_Fat_Content : Factor w/ 5 levels "LF","low fat",..: 3 5 3 5 3 5 5 3 5 5 ...</code>
<code> $ Item_Visibility : num 0.016 0.0193 0.0168 0.054 0.054 ...</code>
<code> $ Item_Type : Factor w/ 16 levels "Baking Goods",..: 5 15 11 7 10 1 14 14 6 6 ...</code>
<code> $ Item_MRP : num 249.8 48.3 141.6 182.1 53.9 ...</code>
<code> $ Outlet_Establishment_Year: int 1999 2009 1999 1998 1987 2009 1987 1985 2002 2007 ...</code>
<code> $ Outlet_Size : Factor w/ 4 levels "Other","High",..: 3 3 3 1 2 3 2 3 1 1 ...</code>
<code> $ Outlet_Location_Type : Factor w/ 3 levels "Tier 1","Tier 2",..: 1 3 1 3 3 3 3 3 2 2 ...</code>
<code> $ Outlet_Type : Factor w/ 4 levels "Grocery Store",..: 2 3 2 1 2 3 2 4 2 2 ...</code>

Sadly, 6 out of 9 variables are categorical in nature. We have some additional work to do now. 
We'll convert these categorical variables into numeric using one hot encoding.

<code>#load library</code>
<code>&gt; library(dummies)</code>

<code>#create a dummy data frame</code>
<code>&gt; new_my_data &lt;- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",</code>
<code>                                "Outlet_Establishment_Year","Outlet_Size",</code>
<code>                                "Outlet_Location_Type","Outlet_Type"))</code>

To check, if we now have a data set of integer values, simple write:

<code>#check the data set</code>
<code>&gt; str(new_my_data)</code>

And, we now have all the numerical values. 
Let's divide the data into test and train.

<code>#divide the new data</code>
<code>&gt; pca.train &lt;- new_my_data[1:nrow(train),]</code>
<code>&gt; pca.test &lt;- new_my_data[-(1:nrow(train)),]</code>

We can now go ahead with PCA.

<h3><span class="red">The base R function prcomp() is used to perform PCA.</span></h3>
By default, it centers the variable to have mean equals to zero. 
With parameter <code>scale. = T</code>, we normalize the variables to have standard deviation equals to 1.

<code>#principal component analysis</code>
<code> &gt; prin_comp &lt;- prcomp(pca.train, scale. = T)</code>
<code> &gt; names(prin_comp)</code>
<code> [1] "sdev"     "rotation" "center"   "scale"    "x"</code>

The prcomp() function results in 5 useful measures:

1. 
<em>center</em> and <em>scale</em> refers to respective mean and standard deviation of the variables that are used for normalization prior to implementing PCA

<code>#outputs the mean of variables</code>
<code> prin_comp$center</code>

<code>#outputs the standard deviation of variables</code>
<code> prin_comp$scale</code>

2. 
The rotation measure provides the principal component loading. 
Each column of rotation matrix contains the principal component loading vector. 
This is the most important measure we should be interested in.

<code>&gt; prin_comp$rotation</code>

This returns 44 principal components loadings. 
Is that correct ? Absolutely. 
In a data set, the maximum number of principal component loadings is a minimum of (n-1, p). 
Let's look at first 4 principal components and first 5 rows.

<code>&gt; prin_comp$rotation[1:5,1:4]
                                PC1            PC2            PC3             PC4
Item_Weight                0.0054429225   -0.001285666   0.011246194   0.011887106
Item_Fat_ContentLF        -0.0021983314    0.003768557  -0.009790094  -0.016789483
Item_Fat_Contentlow fat   -0.0019042710    0.001866905  -0.003066415  -0.018396143
Item_Fat_ContentLow Fat    0.0027936467   -0.002234328   0.028309811   0.056822747
Item_Fat_Contentreg        0.0002936319    0.001120931   0.009033254  -0.001026615
</code>

3. 
In order to compute the principal component score vector, we don't need to multiply the loading with data. 
Rather, the matrix x has the principal component score vectors in a 8523 × 44 dimension.

&gt; dim(prin_comp$x)
[1] 8523    44

Let's plot the resultant principal components.

<code>&gt; biplot(prin_comp, scale = 0)</code>

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/03/4-1.png">

The parameter <code>scale = 0</code> ensures that arrows are scaled to represent the loadings. 
To make inference from image above, focus on the extreme ends (top, bottom, left, right) of this graph.

We infer than first principal component corresponds to a measure of Outlet_TypeSupermarket, Outlet_Establishment_Year 2007. 
Similarly, it can be said that the second component corresponds to a measure of Outlet_Location_TypeTier1, Outlet_Sizeother. 
For exact measure of a variable in a component, you should look at rotation matrix(above) again.

4. 
The prcomp() function also provides the facility to compute standard deviation of each principal component. 
<em>sdev</em> refers to the standard deviation of principal components.

<code>#compute standard deviation of each principal component</code>
<code> &gt; std_dev &lt;- prin_comp$sdev</code>

<code>#compute variance</code>
<code> &gt; pr_var &lt;- std_dev^2</code>

<code>#check variance of first 10 components</code>
<code> &gt; pr_var[1:10]</code>
<code> [1] 4.563615 3.217702 2.744726 2.541091 2.198152 2.015320 1.932076 1.256831</code>
<code> [9] 1.203791 1.168101</code>

We aim to find the components which explain the maximum variance. 
This is because, we want to retain as much information as possible using these components. 
So, higher is the explained variance, higher will be the information contained in those components.

To compute the proportion of variance explained by each component, we simply divide the variance by sum of total variance. 
This results in:

<code>#proportion of variance explained</code>
<code> &gt; prop_varex &lt;- pr_var/sum(pr_var)</code>
<code> &gt; prop_varex[1:20]</code>
<code> [1] 0.10371853 0.07312958 0.06238014 0.05775207 0.04995800 0.04580274</code>
<code> [7] 0.04391081 0.02856433 0.02735888 0.02654774 0.02559876 0.02556797</code>
<code> [13] 0.02549516 0.02508831 0.02493932 0.02490938 0.02468313 0.02446016</code>
<code> [19] 0.02390367 0.02371118</code>

This shows that first principal component explains 10.3% variance. 
Second component explains 7.3% variance. 
Third component explains 6.2% variance and so on. 
So, how do we decide how many components should we select for modeling stage ?

The answer to this question is provided by a scree plot. 
A scree plot is used to access components or factors which explains the most of variability in the data. 
It represents values in descending order.

<code>#scree plot</code>
<code>&gt; plot(prop_varex, xlab = "Principal Component",</code>
<code>             ylab = "Proportion of Variance Explained",</code>
<code>             type = "b")</code>

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/03/7-1-e1458531438291.png">

The plot above shows that ~ 30 components explains around 98.4% variance in the data set. 
In order words, using PCA we have reduced 44 predictors to 30 without compromising on explained variance. 
This is the power of PCA&gt; Let's do a confirmation check, by plotting a cumulative variance plot. 
This will give us a clear picture of number of components.

<code>#cumulative scree plot</code>
<code> &gt; plot(cumsum(prop_varex), xlab = "Principal Component",</code>
<code>              ylab = "Cumulative Proportion of Variance Explained",</code>
<code>              type = "b")</code>

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/03/8-1-e1458532011651.png">

This plot shows that 30 components results in variance close to ~ 98%. 
Therefore, in this case, we'll select number of components as 30 [PC1 to PC30] and proceed to the modeling stage. 
This completes the steps to implement PCA on train data. 
For modeling, we'll use these 30 components as predictor variables and follow the normal procedures.

<h3>Predictive Modeling with PCA Components</h3>
After we've calculated the principal components on training set, let's now understand the process of predicting on test data using these components. 
The process is simple. 
Just like we've obtained PCA components on training set, we'll get another bunch of components on testing set. 
Finally, we train the model.

But, few important points to understand:

We should not combine the train and test set to obtain PCA components of whole data at once. 
Because, this would violate the entire assumption of generalization since test data would get &#8216;leaked' into the training set. 
In other words, the test data set would no longer remain &#8216;unseen'. 
Eventually, this will hammer down the generalization capability of the model.
We should not perform PCA on test and train data sets separately. 
Because, the resultant vectors from train and test PCAs will have different directions ( due to unequal variance). 
Due to this, we'll end up comparing data registered on different axes. 
Therefore, the resulting vectors from train and test data should have same axes.

So, what should we do?

We should do exactly the same transformation to the test set as we did to training set, including the center and scaling feature. 
Let's do it in R:

<code>#add a training set with principal components</code>
<code>&gt; train.data &lt;- data.frame(Item_Outlet_Sales = train$Item_Outlet_Sales, prin_comp$x)</code>

<code>#we are interested in first 30 PCAs</code>
<code>&gt; train.data &lt;- train.data[,1:31]</code>

<code>#run a decision tree</code>
<code>&gt; install.packages("rpart")</code>
<code>&gt; library(rpart)</code>
<code>&gt; rpart.model &lt;- rpart(Item_Outlet_Sales ~ .,data = train.data, method = "anova")</code>
<code>&gt; rpart.model</code>

<code>#transform test into PCA</code>
<code>&gt; test.data &lt;- predict(prin_comp, newdata = pca.test)</code>
<code>&gt; test.data &lt;- as.data.frame(test.data)</code>

<code>#select the first 30 components</code>
<code>&gt; test.data &lt;- test.data[,1:30]</code>

<code>#make prediction on test data</code>
<code>&gt; rpart.prediction &lt;- predict(rpart.model, test.data)</code>

<code>#For fun, finally check your score of leaderboard</code>
<code>&gt; sample &lt;- read.csv("SampleSubmission_TmnO39y.csv")</code>
<code>&gt; final.sub &lt;- data.frame(Item_Identifier = sample$Item_Identifier, Outlet_Identifier = sample$Outlet_Identifier, Item_Outlet_Sales = rpart.prediction)</code>
<code>&gt; write.csv(final.sub, "pca.csv",row.names = F)</code>

That's the complete modeling process after PCA extraction. 
I'm sure you wouldn't be happy with your leaderboard rank after you upload the solution. 
Try using random forest!

<strong>For Python Users:</strong> To implement PCA in python, simply import PCA from sklearn library. 
The interpretation remains same as explained for R users above. 
Ofcourse, the result is some as derived after using R. 
The data set used for Python is a cleaned version where missing values have been imputed, and categorical variables are converted into numeric. 
The modeling process remains same, as explained for R users above.

<code>import numpy as np</code>
<code> from sklearn.decomposition import PCA</code>
<code> import pandas as pd</code>
<code> import matplotlib.pyplot as plt</code>
<code> from sklearn.preprocessing import scale</code>
<code> %matplotlib inline</code>

<code>#Load data set</code>
<code> data = pd.read_csv('Big_Mart_PCA.csv')</code>

<code>#convert it to numpy arrays</code>
<code> X=data.values</code>

<code>#Scaling the values</code>
<code> X = scale(X)</code>

<code>pca = PCA(n_components=44)</code>

<code>pca.fit(X)</code>

<code>#The amount of variance that each PC explains</code>
<code> var= pca.explained_variance_ratio_</code>

<code>#Cumulative Variance explains</code>
<code> var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)</code>

<code>print var1</code>
<code> [  10.37   17.68   23.92   29.7    34.7    39.28   43.67   46.53   49.27</code>
<code> 51.92   54.48   57.04   59.59   62.1    64.59   67.08   69.55   72.</code>
<code> 74.39   76.76   79.1    81.44   83.77   86.06   88.33   90.59   92.7</code>
<code> 94.76   96.78   98.44  100.01  100.01  100.01  100.01  100.01  100.01</code>
<code> 100.01  100.01  100.01  100.01  100.01  100.01  100.01  100.01]</code>

<code>plt.plot(var1</code>)

<img class="lazy" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/03/9-1.png">
<code>#Looking at above plot I'm taking 30 variables</code>
<code> pca = PCA(n_components=30)</code>
<code> pca.fit(X)</code>
<code> X1=pca.fit_transform(X)</code>

<code>print X1</code>

For more information on PCA in python, visit <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank" rel="nofollow">scikit learn documentation</a>.

<h2>Points to Remember</h2>

PCA is used to overcome features redundancy in a data set.
These features are low dimensional in nature.
These features a.k.a components are a resultant of normalized linear combination of original predictor variables.
These components aim to capture as much information as possible with high explained variance.
The first component has the highest variance followed by second, third and so on.
The components must be uncorrelated (remember orthogonal direction ? ). 
See above.
Normalizing data becomes extremely important when the predictors are measured in different units.
PCA works best on data set having 3 or higher dimensions. 
Because, with higher dimensions, it becomes increasingly difficult to make interpretations from the resultant cloud of data.
PCA is applied on a data set with numeric variables.
PCA is a tool which helps to produce better visualizations of high dimensional data.

<h2><span class="gold">Computing and visualizing PCA in R</span></h2>
In this post I will use the function <code>prcomp</code> from the <code>stats</code> package. 
I will also show how to visualize PCA in R using Base R graphics. 
However, my favorite visualization function for PCA is <code>ggbiplot</code>, which is implemented by <a href="http://www.vince.vu/software/#ggbiplot" target="_blank">Vince Q. 
Vu</a> and available on <a href="https://github.com/vqv/ggbiplot" target="_blank">github</a>. 
Please, let me know if you have better ways to visualize PCA in R.

<b> Computing the Principal Components (PC) </b>

I will use the classical <code>iris</code> dataset for the demonstration. 
The data contain four continuous variables which corresponds to physical measures of flowers and a categorical variable describing the flowers’ species.

# Load data
data(iris)
head(iris, 3)

  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa

We will apply PCA to the four continuous variables and use the categorical variable to visualize the PCs later. 
Notice that in the following code we apply a log transformation to the continuous variables as suggested by <a href="https://tgmstat.wordpress.com/2013/11/28/computing-and-visualizing-pca-in-r/#ref1" target="_blank">[1]</a> and set <code>center</code> and <code>scale.</code> equal to <code>TRUE</code> in the call to <code>prcomp</code> to standardize the variables prior to the application of PCA:

# log transform 
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]

# apply PCA - scale. = TRUE is highly 
# advisable, but default is FALSE. 

<code>ir.pca <- prcomp(log.ir, center = TRUE, scale. = TRUE) </code>

Since skewness and the magnitude of the variables influence the resulting PCs, it is good practice to apply skewness transformation, center and scale the variables prior to the application of PCA. 
In the example above, we applied a log transformation to the variables but we could have been more general and applied a Box and Cox transformation <a href="https://tgmstat.wordpress.com/2013/11/28/computing-and-visualizing-pca-in-r/#ref2" target="_blank">[2]</a>. 
See at the end of this post how to perform all those transformations and then apply PCA with only one call to the <code>preProcess</code> function of the <code>caret</code> package.

<b> Analyzing the results </b>

The <code>prcomp</code> function returns an object of class <code>prcomp</code>, which have some methods available. 
The <code>print</code> method returns the standard deviation of each of the four PCs, and their rotation (or loadings), which are the coefficients of the linear combinations of the continuous variables.

# print method
print(ir.pca)

Standard deviations:
[1] 1.7124583 0.9523797 0.3647029 0.1656840

Rotation:
                    PC1         PC2        PC3         PC4
Sepal.Length  0.5038236 -0.45499872  0.7088547  0.19147575
Sepal.Width  -0.3023682 -0.88914419 -0.3311628 -0.09125405
Petal.Length  0.5767881 -0.03378802 -0.2192793 -0.78618732
Petal.Width   0.5674952 -0.03545628 -0.5829003  0.58044745

The <code>plot</code> method returns a plot of the variances (y-axis) associated with the PCs (x-axis). 
The Figure below is useful to decide how many PCs to retain for further analysis. 
In this simple case with only 4 PCs this is not a hard task and we can see that the first two PCs explain most of the variability in the data.

# plot method
plot(ir.pca, type = "l")

<img class="lazy" data-src="https://tgmstat.files.wordpress.com/2013/11/scree.png">

The <code>summary</code> method describe the importance of the PCs. 
The first row describe again the standard deviation associated with each PC. 
The second row shows the proportion of the variance in the data explained by each component while the third row describe the cumulative proportion of explained variance. 
We can see there that the first two PCs accounts for more than 95% of the variance of the data.

# summary method
summary(ir.pca)

Importance of components:
                          PC1    PC2     PC3     PC4
Standard deviation     1.7125 0.9524 0.36470 0.16568
Proportion of Variance 0.7331 0.2268 0.03325 0.00686
Cumulative Proportion  0.7331 0.9599 0.99314 1.00000

We can use the <code>predict</code> function if we observe new data and want to predict their PCs values. 
Just for illustration pretend the last two rows of the <code>iris</code> data has just arrived and we want to see what is their PCs values:

# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))

          PC1         PC2        PC3         PC4
149 1.0809930 -1.01155751 -0.7082289 -0.06811063
150 0.9712116 -0.06158655 -0.5008674 -0.12411524

The Figure below is a biplot generated by the function <code>ggbiplot</code> of the <code>ggbiplot</code> package available on <a href="https://github.com/vqv/ggbiplot" target="_blank">github</a>.

<img class="lazy" data-src="https://tgmstat.files.wordpress.com/2013/11/ggbiplot.png">

The code to generate this Figure is given by

library(devtools)
install_github("ggbiplot", "vqv")

library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, 
              groups = ir.species, ellipse = TRUE, 
              circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(g)

It projects the data on the first two PCs. 
Other PCs can be chosen through the argument <code>choices</code> of the function. 
It colors each point according to the flowers’ species and draws a Normal contour line with <code>ellipse.prob</code> probability (default to 68%) for each group. 
More info about <code>ggbiplot</code> can be obtained by the usual <code>?ggbiplot</code>. 
I think you will agree that the plot produced by <code>ggbiplot</code> is much better than the one produced by <code>biplot(ir.pca)</code> (Figure below).

<img class="lazy" data-src="https://tgmstat.files.wordpress.com/2013/11/biplot_base.png">

I also like to plot each variables coefficients inside a unit circle to get insight on a possible interpretation for PCs. 
Figure 4 was generated by <a href="https://gist.github.com/thigm85/7689508" target="_blank">this code available on gist</a>.

<img class="lazy" data-src="https://tgmstat.files.wordpress.com/2013/11/loadings1.png">

<b> PCA on caret package </b>

As I mentioned before, it is possible to first apply a Box-Cox transformation to correct for skewness, center and scale each variable and then apply PCA in one call to the <code>preProcess</code> function of the <code>caret</code> package.
<span class=".smallfont brown">(Box-Cox Transformations
George Box and Sir David Cox collaborated on one paper.
transformation of a non-normal dependent variables into a normal shape.)
</span>
require(caret)
trans = preProcess(iris[,1:4], method=c("BoxCox", "center", "scale", "pca"))
PC = predict(trans, iris[,1:4])

By default, the function keeps only the PCs that are necessary to explain at least 95% of the variability in the data, but this can be changed through the argument <code>thresh</code>.

# Retained PCs
head(PC, 3)

        PC1        PC2
1 -2.303540 -0.4748260
2 -2.151310  0.6482903
3 -2.461341  0.3463921

# Loadings
trans$rotation

                    PC1         PC2
Sepal.Length  0.5202351 -0.38632246
Sepal.Width  -0.2720448 -0.92031253
Petal.Length  0.5775402 -0.04885509
Petal.Width   0.5672693 -0.03732262

See <a href="https://tgmstat.wordpress.com/2013/11/07/unsupervised-data-pre-processing-for-predictive-modeling/" target="_blank">Unsupervised data pre-processing for predictive modeling</a> for an introduction of the <code>preProcess</code> function.

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
</pre></body></html>
